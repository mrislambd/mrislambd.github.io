[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n\n\n\n\n\nLinear Algebra\n\n\nMathematics\n\n\n\n\n\n\n\n\n\nJan 25, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)\n\n\n\n\n\n\nData Science\n\n\nMachine Learning\n\n\nStochastic Gradient Descent\n\n\nOptimization\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n\n\n\n\n\nData Science\n\n\nMachine Learning\n\n\nComputational Mathematics\n\n\nAlgorithmic Complexity\n\n\nProgramming\n\n\nComputer Science\n\n\n\n\n\n\n\n\n\nNov 9, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Representation: Change of Basis\n\n\n\n\n\n\nLinear Algebra\n\n\nMathematics\n\n\n\n\n\n\n\n\n\nJan 21, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix multiplication: Let’s make it less expensive!\n\n\n\n\n\n\nData Science\n\n\nMachine Learning\n\n\nComputational Mathematics\n\n\nAlgorithmic Complexity\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nModeling viral disease\n\n\n\n\n\n\nApplied Mathematics\n\n\nMath Biology\n\n\nMathematical Modeling\n\n\n\n\n\n\n\n\n\nFeb 23, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nSome Linear Algebra Proofs\n\n\n\n\n\n\nLinear Algebra\n\n\nMathematics\n\n\n\n\n\n\n\n\n\nJan 24, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nবাংলা ভাষায় আমার লেখা || My Blog in Benglali Language\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\nমোহাম্মদ রকিবুল ইসলাম\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "dsandml/multiclass/index.html",
    "href": "dsandml/multiclass/index.html",
    "title": "Classification: Techniques to handle multi-class classification problems",
    "section": "",
    "text": "In machine learning, classification is one of the most common tasks, where the goal is to assign a label to an input from a set of possible categories. While binary classification, where there are only two labels (e.g., spam vs. not spam), is well understood, real-world problems often involve more than two classes—this is where multi-class classification comes into play. In this post, we’ll explore various techniques and algorithms used to solve multi-class classification problems effectively."
  },
  {
    "objectID": "dsandml/multiclass/index.html#intro",
    "href": "dsandml/multiclass/index.html#intro",
    "title": "Classification: Techniques to handle multi-class classification problems",
    "section": "",
    "text": "In machine learning, classification is one of the most common tasks, where the goal is to assign a label to an input from a set of possible categories. While binary classification, where there are only two labels (e.g., spam vs. not spam), is well understood, real-world problems often involve more than two classes—this is where multi-class classification comes into play. In this post, we’ll explore various techniques and algorithms used to solve multi-class classification problems effectively."
  },
  {
    "objectID": "dsandml/multiclass/index.html#what",
    "href": "dsandml/multiclass/index.html#what",
    "title": "Classification: Techniques to handle multi-class classification problems",
    "section": "What is Multi-class Classification?",
    "text": "What is Multi-class Classification?\n\nMulti-class classification involves assigning an input to one of several distinct classes. For instance, given an image of an animal, the task may be to classify it as either a dog, cat, horse, or bird. The key challenge here is to handle more than two classes, which introduces additional complexity compared to binary classification."
  },
  {
    "objectID": "dsandml/multiclass/index.html#key",
    "href": "dsandml/multiclass/index.html#key",
    "title": "Classification: Techniques to handle multi-class classification problems",
    "section": "Key Approaches to Multi-class Classification",
    "text": "Key Approaches to Multi-class Classification\nThere are two main ways of handling multi-class classification:\n\nNative Multi-class Algorithms: Some algorithms are inherently designed to work with multiple classes without any modifications.\nBinary to Multi-class Strategies: These approaches decompose the multi-class problem into multiple binary classification problems.\n\n\nLet’s consider the classic Iris dataset that contains three classes of iris species: setosa, versicolor, virginica. We will use this dataset to demonstrate different multi-class classification techniques in python.\n\n\nimport pandas as pd \nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n# set the background color\nsns.set(rc={'axes.facecolor': '#f4f4f4', 'figure.facecolor':'#f4f4f4'})\n  \n\niris = load_iris()\ndf = pd.DataFrame(data = iris.data, columns = iris.feature_names)\ndf['species'] = iris.target\ndf['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\nprint(df.head())\nsns.pairplot(df, hue='species', height=1.8, aspect=0.99)\nplt.show()\n\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                5.1               3.5                1.4               0.2   \n1                4.9               3.0                1.4               0.2   \n2                4.7               3.2                1.3               0.2   \n3                4.6               3.1                1.5               0.2   \n4                5.0               3.6                1.4               0.2   \n\n  species  \n0  setosa  \n1  setosa  \n2  setosa  \n3  setosa  \n4  setosa  \n\n\n\n\n\n\n\n\n\n\nNative Multi-class Algorithms\nThese are algorithms that can directly handle multiple classes in their formulation:\na. Decision Trees (See more here)\n\nDecision Trees can naturally handle multi-class classification tasks. At each split, the tree decides on a rule that best separates the data into groups. The terminal nodes (leaves) represent the class predictions.\n\n\nAdvantages: Easy to interpret, no need for extensive pre-processing, and handles both categorical and numerical features.\nDisadvantages: Prone to overfitting and can produce unstable models if not carefully tuned.\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report \n\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, \n    iris.target, \n    test_size=0.3,\n    random_state=123\n    )\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        18\n  versicolor       0.83      1.00      0.91        10\n   virginica       1.00      0.88      0.94        17\n\n    accuracy                           0.96        45\n   macro avg       0.94      0.96      0.95        45\nweighted avg       0.96      0.96      0.96        45\n\n\n\nb. Random Forests (See more here)\n\nRandom Forests are ensembles of decision trees and can also naturally handle multi-class classification. They aggregate the predictions from multiple trees to make a final classification decision.\n\n\nAdvantages: Higher accuracy and reduced overfitting compared to single decision trees.\nDisadvantages: Less interpretable than individual trees, and training can be computationally intensive.\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        18\n  versicolor       0.77      1.00      0.87        10\n   virginica       1.00      0.82      0.90        17\n\n    accuracy                           0.93        45\n   macro avg       0.92      0.94      0.92        45\nweighted avg       0.95      0.93      0.93        45\n\n\n\nc. Naive Bayes (See more here)\n\nNaive Bayes is a probabilistic classifier based on Bayes’ theorem, assuming that the features are independent. The algorithm calculates the probability of each class and predicts the one with the highest probability.\n\n\nAdvantages: Fast, simple, and works well for text classification.\nDisadvantages: Assumes feature independence, which might not hold in many real-world datasets.\n\n\nfrom sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        18\n  versicolor       0.37      1.00      0.54        10\n   virginica       0.00      0.00      0.00        17\n\n    accuracy                           0.62        45\n   macro avg       0.46      0.67      0.51        45\nweighted avg       0.48      0.62      0.52        45\n\n\n\n/Users/macpc/Library/CloudStorage/OneDrive-FloridaStateUniversity/OnlineLearning/python_environments/pytorch-env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/Users/macpc/Library/CloudStorage/OneDrive-FloridaStateUniversity/OnlineLearning/python_environments/pytorch-env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/Users/macpc/Library/CloudStorage/OneDrive-FloridaStateUniversity/OnlineLearning/python_environments/pytorch-env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n\n\nd. K-Nearest Neighbors (KNN) (See more here)\n\nKNN is a non-parametric algorithm that classifies a data point based on the majority class of its k-nearest neighbors. It can handle multi-class problems by considering the most frequent class among the neighbors.\n\n\nAdvantages: Simple to implement, no training phase.\nDisadvantages: Slow at prediction time, sensitive to the choice of k and the distance metric.\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclf = KNeighborsClassifier(n_neighbors=5)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        18\n  versicolor       1.00      0.90      0.95        10\n   virginica       0.94      1.00      0.97        17\n\n    accuracy                           0.98        45\n   macro avg       0.98      0.97      0.97        45\nweighted avg       0.98      0.98      0.98        45\n\n\n\n\n\nBinary to Multi-class Strategies\nSome algorithms are inherently binary, but they can be adapted to handle multiple classes using strategies like:\na. One-vs-Rest (OvR)\n\nThis technique involves training one classifier per class. Each classifier is trained to distinguish one class from the rest (i.e., treat it as a binary classification problem). During prediction, the classifier that outputs the highest confidence score assigns the label.\n\n\nAdvantages: Simple and works well with many binary classifiers like logistic regression and support vector machines.\nDisadvantages: Can become inefficient when there are many classes, since it requires training one model per class.\n\nExample with Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\n\nclf = OneVsRestClassifier(LogisticRegression())\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        18\n  versicolor       0.83      1.00      0.91        10\n   virginica       1.00      0.88      0.94        17\n\n    accuracy                           0.96        45\n   macro avg       0.94      0.96      0.95        45\nweighted avg       0.96      0.96      0.96        45\n\n\n\nb. One-vs-One (OvO)\n\nThis strategy involves training a binary classifier for every possible pair of classes. For a dataset with \\(n\\) classes, \\(\\frac{n(n-1)}{2}\\) classifiers are trained. The class with the most “votes” from the classifiers is the predicted label.\n\n\nAdvantages: Works well when there are fewer classes.\nDisadvantages: Computationally expensive for large class numbers due to the many classifiers needed.\n\nExample with support vector classifier\n\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.svm import SVC\n\nclf = OneVsOneClassifier(SVC())\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        18\n  versicolor       0.71      1.00      0.83        10\n   virginica       1.00      0.76      0.87        17\n\n    accuracy                           0.91        45\n   macro avg       0.90      0.92      0.90        45\nweighted avg       0.94      0.91      0.91        45\n\n\n\n\n3. Neural Networks for Multi-class Classification\na. Softmax Regression\n\nIn neural networks, multi-class classification is typically handled using the softmax function in the output layer. Softmax converts raw output scores (logits) into probabilities for each class, ensuring they sum to 1. The class with the highest probability is chosen as the predicted class.\n\n\nAdvantages: Can model complex non-linear relationships and works well with large datasets.\nDisadvantages: Requires more data and computational resources compared to simpler models.\n\n\nimport tensorflow as tf \nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import SGD\n\ninput_layer = Input(shape = (X_train.shape[1],))\n\nmodel = Sequential([\n    input_layer,\n    Dense(64, activation = 'relu'),\n    \n    Dense(64, activation = 'relu'),\n    \n    Dense(3, activation = 'softmax')\n])\n\noptimizer = SGD(learning_rate=0.001)\nmodel.compile(\n    optimizer = optimizer, \n    loss = 'sparse_categorical_crossentropy',\n    metrics = ['accuracy']\n    )\nmodel.fit(X_train, y_train, epochs = 50, batch_size = 10, verbose = 0)\ntest_loss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy}')  \n\n1/2 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - accuracy: 0.9688 - loss: 0.6508\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.9748 - loss: 0.6497 \nTest Accuracy: 0.9777777791023254\n\n\nTraining and Validation loss\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=0.10, random_state=123,\n    stratify=y_train\n)\n\nhistory = model.fit(\n    X_train, y_train, epochs = 150, \n    batch_size = 10, verbose = 0,\n    validation_data = (X_val, y_val)\n    )\n\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(train_loss)+1)\nplt.plot(epochs, train_loss, 'b-', label = \"Training Loss\")\nplt.plot(epochs, val_loss, 'r-', label = \"Validation loss\")\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nb. Convolutional Neural Networks (CNNs)\nFor image classification tasks, CNNs are widely used. CNNs automatically learn spatial hierarchies of features, making them highly effective for tasks like object recognition in images.\n\nAdvantages: Superior performance on image data, able to capture spatial dependencies.\nDisadvantages: Require large amounts of labeled data and significant computational power for training."
  },
  {
    "objectID": "dsandml/multiclass/index.html#perf",
    "href": "dsandml/multiclass/index.html#perf",
    "title": "Classification: Techniques to handle multi-class classification problems",
    "section": "Performance Evaluation in Multi-class Classification",
    "text": "Performance Evaluation in Multi-class Classification\nEvaluating multi-class classification models requires more nuanced metrics than binary classification. Some common evaluation metrics include:\n\nAccuracy: The percentage of correctly classified instances.\nConfusion Matrix: A table showing the actual versus predicted classes for each class.\nPrecision, Recall, and F1-score: These can be extended to multiple classes by calculating them per class (micro, macro, or weighted averages).\nReceiver Operating Characteristic (ROC) and Area Under the Curve (AUC): These are less commonly used for multi-class problems, but can still be adapted using OvR schemes.\n\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(\n    cm, annot=True, fmt='d', cmap='Blues',\n    xticklabels=iris.target_names,\n    yticklabels=iris.target_names\n)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()"
  },
  {
    "objectID": "dsandml/multiclass/index.html#conclusion",
    "href": "dsandml/multiclass/index.html#conclusion",
    "title": "Classification: Techniques to handle multi-class classification problems",
    "section": "Conclusion",
    "text": "Conclusion\n\nMulti-class classification is a critical aspect of many real-world applications, from medical diagnosis to image recognition and beyond. By understanding the strengths and limitations of different algorithms and strategies, we can choose the best approach for the task at hand. Whether using native multi-class models like decision trees or adapting binary models with OvR or OvO strategies, it’s essential to carefully consider the nature of the data, the number of classes, and computational constraints when building the models."
  },
  {
    "objectID": "dsandml/multiclass/index.html#references",
    "href": "dsandml/multiclass/index.html#references",
    "title": "Classification: Techniques to handle multi-class classification problems",
    "section": "References",
    "text": "References\n\nScikit-learn Documentation:\nThe Python code snippets for decision trees, random forests, KNN, logistic regression, and support vector machines (SVM) are based on the Scikit-learn library.\n\nScikit-learn: https://scikit-learn.org/stable/supervised_learning.html\n\nIris Dataset:\nThe Iris dataset is a well-known dataset for classification tasks and is included in the UCI Machine Learning Repository:\n\nUCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/iris\n\nConfusion Matrix & Evaluation Metrics:\nFor metrics such as accuracy, precision, recall, F1-score, and confusion matrices, the Scikit-learn library offers comprehensive functions to evaluate multi-class classification models:\n\nScikit-learn metrics documentation: https://scikit-learn.org/stable/modules/model_evaluation.html\n\nSoftmax and Neural Networks:\nThe Python code for neural networks using TensorFlow/Keras employs the softmax function for multi-class classification.\n\nTensorFlow/Keras: https://www.tensorflow.org/\n\nIntroduction to Multi-class Classification:\nGeneral information about multi-class classification can be found in machine learning books and resources, such as “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow” by Aurélien Géron:\n\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow on O’Reilly\n\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/dataengineering/index.html#introduction",
    "href": "dsandml/dataengineering/index.html#introduction",
    "title": "How do we treat categorical features for our data science project?",
    "section": "Introduction",
    "text": "Introduction\n\n Suppose we are working on a data science project and the data contains both contineous and categorical variables. For example, we want to build a predictive model for a life insurance company. The model will predict the annual company spending on individuals depending on their age, bmi, sex, smoking habit, number of children, and region in the US where the belong. So here, our target variable is a contineous variable and the feature variables contain both contineous and categorical variables.  To understand how important each feature is, there are many possible ways. For example, when we do the exploratory data analysis (EDA) we can do some plotting to see how each feature inteacts with the target variable, or maybe calculating correlations of the features and target variables. However, when the feature is contineous it is not a big issue to calculate the correlation matrix. But when the feature is categorical or ordinal, for example, in this predictive modeling case, how do we know if the number of children or smoking habit have impact on insurance charges? Plotting boxplot or countplot from the seaborn or any other library may help, give some primary idea. But how do we quantify the correlations?  Here comes the statistical method one-way Analysis of Variances (ANOVA) among many other alternatives. Machine libraries like scipy has built-in functions that can compute the ANOVA’s for each categorical feature. We will see the implementation of this at the end of this post. This blog post is about the simple explanation of the mathematics behind the ANOVA method."
  },
  {
    "objectID": "dsandml/dataengineering/index.html#anova",
    "href": "dsandml/dataengineering/index.html#anova",
    "title": "How do we treat categorical features for our data science project?",
    "section": "ANOVA",
    "text": "ANOVA\nThis is the 5 random sample data that we are talking about. We will use this data to explain the mathematical formulation of the model.\n\nimport pandas as pd\ndata = pd.read_csv('insurance.csv')\nprint(data.sample(5, random_state=111))\n\n      age   sex    bmi  children smoker     region     charges\n1000   30  male  22.99         2    yes  northwest  17361.7661\n53     36  male  34.43         0    yes  southeast  37742.5757\n432    42  male  26.90         0     no  southwest   5969.7230\n162    54  male  39.60         1     no  southwest  10450.5520\n1020   51  male  37.00         0     no  southwest   8798.5930\n\n\nWe will explain the method using the feature children.\n\nchild = data.children.value_counts().sort_index()\nc0=data[data['children']==0].charges.values.tolist()\nc1=data[data['children']==1].charges.values.tolist()\nc2=data[data['children']==2].charges.values.tolist()\nc3=data[data['children']==3].charges.values.tolist()\nc4=data[data['children']==4].charges.values.tolist()\nc5=data[data['children']==5].charges.values.tolist()\n\n\n\n\n\n\n\n\n\n\n\n\nChildren 0\nChildren 1\nChildren 2\nChildren 3\nChildren 4\nChildren 5\n\n\n\n\n[16884.924, 21984.47061, 3866.8552, 3756.6216, 28923.13692, 2721.3208]\n[1725.5523, 8240.5896, 1837.237, 10797.3362]\n[6406.4107, 6203.90175, 12268.63225]\n[4449.462, 7281.5056]\n[4504.6624, 11033.6617, 10407.08585]\n[4687.797, 6799.458]\n\n\nTotal 574\nTotal 324\nTotal 240\nTotal 157\nTotal 25\nTotal 18\n\n\n\nA one-way analysis of variance is a method to compare \\(k\\) homogenous groups when the experiment has \\(n_i\\) response values for each each group \\(i\\). Therefore, total data \\(n=\\sum_{i} n_i\\) and \\(y_{ij}\\) represent the \\(j\\)th observation of the \\(i\\)th group. For our example above, we have \\(\\sum_{i=1}^{6}n_i=\\)(574+324+240+157+25+18)= 1338 and \\(y_{12}=\\) 21984.47061 meaning, group 1 and second element.\nNow let’s define \\[\n\\mu_i = \\frac{1}{n_i}\\sum_{j=1}^{n_i}\\frac{y_{ij}}{n_i};\\hspace{4mm}\\text{for } i=1,2,\\cdots, 6\n\\]\nSince all the groups are coming from the same sample/population, we must assume that they all have common variance. This \\(\\textcolor{red}{\\text{homogeneity assumption is crucial}}\\) for ANOVA analysis. So, irrespective of their group assignment, each \\(y_{ij}\\sim (\\mu_i, \\sigma^2)\\)\n\nWhat does one-sided ANOVA do?\n\nThe main purpose of one-sided ANOVA is to act as a judge like in a court house. It assumes that there is no variation in any group. All group has the same mean. So it sets a null hypthesis and declares that there is no difference in the groups whereas the alternative is set to the opposite. Let’s see what happens to our data\n\n\nimport numpy as np \n\n\n\n\n\\(\\mu\\)\nValues\n\n\n\n\n\\(\\mu_1\\)\n12365.98\n\n\n\\(\\mu_2\\)\n12731.17\n\n\n\\(\\mu_3\\)\n15073.56\n\n\n\\(\\mu_4\\)\n15355.32\n\n\n\\(\\mu_5\\)\n13850.66\n\n\n\\(\\mu_6\\)\n8786.04\n\n\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/multiplelinreg/index.html",
    "href": "dsandml/multiplelinreg/index.html",
    "title": "Multiple Liear Regression",
    "section": "",
    "text": "The multiple linear regression takes the form\n\\[\ny=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\cdots +\\beta_d x_d+\\xi=\\vec{x}\\cdot \\vec{\\beta}+\\xi\n\\]\nwith \\(\\{\\beta_i\\}_{i=0}^{d}\\in \\mathbb{R}\\) constants or parameters of the model. In vector notation, \\(\\vec{\\beta}\\in \\mathbb{R}^{d+1}\\),\n\\[\n\\vec{\\beta}=\\begin{pmatrix}\\beta_0\\\\ \\beta_1\\\\ \\vdots \\\\ \\beta_d \\end{pmatrix};\\hspace{4mm}\\vec{x}=\\begin{pmatrix}1\\\\ x_1\\\\ x_2\\\\ \\vdots\\\\ x_d\\end{pmatrix}\n\\]\nFor \\(n\\) data points, in matrix algebra notation, we can write \\(y=X\\vec{\\beta}+\\xi\\) where \\(X\\in \\mathcal{M}_{n\\times (d+1)}\\) and \\(y\\in \\mathbb{R}^{d+1}\\) with\n\\[\nX=\\begin{pmatrix}1&x_{11}&x_{12}&\\cdots&x_{1d}\\\\1&x_{21}&x_{22}&\\cdots&x_{2d}\\\\ \\vdots& \\vdots &\\vdots&\\ddots &\\vdots\\\\1&x_{n1}&x_{n2}&\\cdots&x_{nd} \\end{pmatrix};\\hspace{4mm} y=\\begin{pmatrix}y_1\\\\y_2\\\\ \\vdots\\\\ y_n\\end{pmatrix};\\hspace{4mm} \\xi=\\begin{pmatrix}\\xi_1\\\\ \\xi_2\\\\ \\vdots\\\\ \\xi_n\\end{pmatrix}\n\\]\nWe fit the \\(n\\) data points with the objective to minimize the loss function, mean squared error\n\\[\nMSE(\\vec{\\beta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-f_{\\vec{\\beta}}(\\vec{x}_i)\\right)^2=\\frac{1}{n}\\left|\\vec{y}-X\\vec{\\beta}\\right|^2\n\\]"
  },
  {
    "objectID": "dsandml/multiplelinreg/index.html#multiple-linear-regression",
    "href": "dsandml/multiplelinreg/index.html#multiple-linear-regression",
    "title": "Multiple Liear Regression",
    "section": "",
    "text": "The multiple linear regression takes the form\n\\[\ny=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\cdots +\\beta_d x_d+\\xi=\\vec{x}\\cdot \\vec{\\beta}+\\xi\n\\]\nwith \\(\\{\\beta_i\\}_{i=0}^{d}\\in \\mathbb{R}\\) constants or parameters of the model. In vector notation, \\(\\vec{\\beta}\\in \\mathbb{R}^{d+1}\\),\n\\[\n\\vec{\\beta}=\\begin{pmatrix}\\beta_0\\\\ \\beta_1\\\\ \\vdots \\\\ \\beta_d \\end{pmatrix};\\hspace{4mm}\\vec{x}=\\begin{pmatrix}1\\\\ x_1\\\\ x_2\\\\ \\vdots\\\\ x_d\\end{pmatrix}\n\\]\nFor \\(n\\) data points, in matrix algebra notation, we can write \\(y=X\\vec{\\beta}+\\xi\\) where \\(X\\in \\mathcal{M}_{n\\times (d+1)}\\) and \\(y\\in \\mathbb{R}^{d+1}\\) with\n\\[\nX=\\begin{pmatrix}1&x_{11}&x_{12}&\\cdots&x_{1d}\\\\1&x_{21}&x_{22}&\\cdots&x_{2d}\\\\ \\vdots& \\vdots &\\vdots&\\ddots &\\vdots\\\\1&x_{n1}&x_{n2}&\\cdots&x_{nd} \\end{pmatrix};\\hspace{4mm} y=\\begin{pmatrix}y_1\\\\y_2\\\\ \\vdots\\\\ y_n\\end{pmatrix};\\hspace{4mm} \\xi=\\begin{pmatrix}\\xi_1\\\\ \\xi_2\\\\ \\vdots\\\\ \\xi_n\\end{pmatrix}\n\\]\nWe fit the \\(n\\) data points with the objective to minimize the loss function, mean squared error\n\\[\nMSE(\\vec{\\beta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-f_{\\vec{\\beta}}(\\vec{x}_i)\\right)^2=\\frac{1}{n}\\left|\\vec{y}-X\\vec{\\beta}\\right|^2\n\\]"
  },
  {
    "objectID": "dsandml/multiplelinreg/index.html#ordinary-least-square-method",
    "href": "dsandml/multiplelinreg/index.html#ordinary-least-square-method",
    "title": "Multiple Liear Regression",
    "section": "Ordinary Least Square Method",
    "text": "Ordinary Least Square Method\n\nThe scikit-learn library uses Ordinary Least Squares (OLS) method to find the parameters. This method is good for a simple and relatively smaller dataset. Here is a short note on this method. However, when the dimension is very high and the dataset is bigger, scikit-learn uses another method called Stochastic Gradient Descent for optimization which is discussed in the next section.\n\nThe goal of OLS is to find the parameter vector \\(\\hat{\\beta}\\) that minimizes the sum of squared errors (SSE) between the observed target values \\(y\\) and the predicted values \\(\\hat{y}\\):\n\\[\n\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - X_i\\beta)^2\n\\]\nThis can be expressed in matrix form as:\n\\[\n\\text{SSE} = (y - X\\beta)^T(y - X\\beta)\n\\]\nTo minimize the SSE, let’s first expand the expression:\n\\[\\begin{align}\n\\text{SSE} &= (y - X\\beta)^T(y - X\\beta)\\\\\n&=(y^T-\\beta^TX^T)(y-X\\beta)\\\\\n& = y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta\n\\end{align}\\]\nSince \\(\\beta^T X^T y\\) is a scalar (a 1x1 matrix), it is equal to its transpose. That is\n\\[\\begin{align*}\n\\beta^TX^Ty&=\\left(\\beta^TX^Ty\\right)^T\\\\\n&= \\left((\\beta^TX^T)y\\right)^T\\\\\n&=y^T(\\beta^TX^T)^T\\\\\n&=y^T(\\beta^TX^T)^T\\\\\n&=y^T\\left(X^T\\right)^T\\left(\\beta^T\\right)^T\\\\\n&=y^TX\\beta\n\\end{align*}\\]\nand therefore,\n\\[\n\\text{SSE} = y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta\n\\]\nTo find the minimum of the SSE, we take the derivative with respect to \\(\\beta\\) and set it to zero:\n\\[\n\\frac{\\partial \\text{SSE}}{\\partial \\beta} = -2X^T y + 2X^T X \\beta = 0\n\\]\nNow, solve for \\(\\beta\\):\n\\[\nX^T X \\beta = X^T y\n\\]\nTo isolate \\(\\beta\\), we multiply both sides by \\((X^T X)^{-1}\\) (assuming \\(X^T X\\) is invertible):\n\\[\n\\beta = (X^T X)^{-1} X^T y\n\\]\n\nThe vector \\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\) gives the estimated coefficients that minimize the sum of squared errors between the observed target values \\(y\\) and the predicted values \\(\\hat{y} = X\\hat{\\beta}\\). This method is exact and works well when \\(X^T X\\) is invertible and the dataset size is manageable.   This method is very efficient for small to medium-sized datasets but can become computationally expensive for very large datasets due to the inversion of the matrix \\(X^TX\\)."
  },
  {
    "objectID": "dsandml/multiplelinreg/index.html#iterative-method",
    "href": "dsandml/multiplelinreg/index.html#iterative-method",
    "title": "Multiple Liear Regression",
    "section": "Iterative Method",
    "text": "Iterative Method\n\nGradient Descent\n\n  GIF Credit: gbhat.com\n Gradient Descent is an optimization algorithm used to minimize the cost function. The cost function \\(f(\\beta)\\) measures how well a model with parameters \\(\\beta\\) fits the data. The goal is to find the values of \\(\\beta\\) that minimize this cost function. In terms of the iterative method, we want to find \\(\\beta_{k+1}\\) and \\(\\beta_k\\) such that \\(f(\\beta_{k+1})&lt;f(\\beta_k)\\).   For a small change in \\(\\beta\\), we can approximate \\(f(\\beta)\\) using Taylor series expansion\n\\[f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}\\]\n\nThe update rule for vanilla gradient descent is given by:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nWhere:\n\n\\(\\beta_k\\) is the current estimate of the parameters at iteration \\(k\\).\n\\(\\eta\\) is the learning rate, a small positive scalar that controls the step size.\n\\(\\nabla f(\\beta_k)\\) is the gradient of the cost function \\(f\\) with respect to \\(\\beta\\) at the current point \\(\\beta_k\\).\n\n\nThe update rule comes from the idea of moving the parameter vector \\(\\beta\\) in the direction that decreases the cost function the most.\n\nGradient: The gradient \\(\\nabla f(\\beta_k)\\) represents the direction and magnitude of the steepest ascent of the function \\(f\\) at the point \\(\\beta_k\\). Since we want to minimize the function, we move in the opposite direction of the gradient.\nStep Size: The term \\(\\eta \\nabla f(\\beta_k)\\) scales the gradient by the learning rate \\(\\eta\\), determining how far we move in that direction. If \\(\\eta\\) is too large, the algorithm may overshoot the minimum; if it’s too small, the convergence will be slow.\nIterative Update: Starting from an initial guess \\(\\beta_0\\), we repeatedly apply the update rule until the algorithm converges, meaning that the changes in \\(\\beta_k\\) become negligible, and \\(\\beta_k\\) is close to the optimal value \\(\\beta^*\\).\n\n\n\nStochastic Gradient Descent (SGD)\n\nStochastic Gradient Descent is a variation of the vanilla gradient descent. Instead of computing the gradient using the entire dataset, SGD updates the parameters using only a single data point or a small batch of data points at each iteration. The later one we call it mini batch SGD.\n\nSuppose our cost function is defined as the average over a dataset of size \\(n\\):\n\\[\nf(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\beta)\n\\]\nWhere \\(f_i(\\beta)\\) represents the contribution of the \\(i\\)-th data point to the total cost function. The gradient of the cost function with respect to \\(\\beta\\) is:\n\\[\n\\nabla f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\beta)\n\\]\nVanilla gradient descent would update the parameters as:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nInstead of using the entire dataset to compute the gradient, SGD approximates the gradient by using only a single data point (or a small batch). The update rule for SGD is:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f_{i_k}(\\beta_k)\n\\]\nWhere:\n\n\\(i_k\\) is the index of a randomly selected data point at iteration \\(k\\).\n\\(\\nabla f_{i_k}(\\beta_k)\\) is the gradient of the cost function with respect to the parameter \\(\\beta_k\\), evaluated only at the data point indexed by \\(i_k\\)."
  },
  {
    "objectID": "dsandml/multiplelinreg/index.html#python-execution",
    "href": "dsandml/multiplelinreg/index.html#python-execution",
    "title": "Multiple Liear Regression",
    "section": "Python Execution",
    "text": "Python Execution\n\nSynthetic Data\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nX=np.random.randn(1000,2)\ny=3*X[:,0]+2*X[:,1]+1+np.random.randn(1000)\n\nSo for this project, our known relationship is \\(y=1+3x_1+2x_2+\\xi\\).\n\n\nFit the data: Using scikit-learn Library\n\nmlr=LinearRegression()\nmlr.fit(X,y)\ncoefficients=mlr.coef_.tolist()\nslope=mlr.intercept_.tolist()\n\nSo the model parameters: slope \\(\\beta_0=\\) 1.0523 and coefficients \\(\\beta_1=\\) 2.9708, and \\(\\beta_2=\\) 1.9942\n\n\nFit the data: Using Custom Library OLS\nFirst we create our custom NewLinearRegression using the OLS formula above and save this python class as mlreg.py\nimport numpy as np\n\n\nclass NewLinearRegression:\n    def __init__(self) -&gt; None:\n        self.beta = None\n\n    def fit(self, X, y):\n        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n        X_transpose_X = np.dot(X.transpose(), X)\n        X_transpose_X_inverse = np.linalg.inv(X_transpose_X)\n        X_transpose_y = np.dot(X.transpose(), y)\n        self.beta = np.dot(X_transpose_X_inverse, X_transpose_y)\n\n    def predict(self, X):\n        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n        return np.dot(X, self.beta)\n\n    def coeff_(self):\n        return self.beta[1:].tolist()\n\n    def interceptt_(self):\n        return self.beta[0].tolist()\nNow it’s time to use the new class\n\nfrom mlreg import NewLinearRegression\nmlr1 = NewLinearRegression()\nmlr1.fit(X,y)\ncoefficients1=mlr1.coeff_()\nslope1=mlr1.interceptt_()\n\nSo the model parameters: slope \\(\\beta_0=\\) 1.0523 and coefficients \\(\\beta_1=\\) 2.9708, and \\(\\beta_2=\\) 1.9942\n\n\nFit the data: Using Gradient Descent\nWe create the class\nclass GDLinearRegression:\n    def __init__(self, learning_rate=0.01, number_of_iteration=1000) -&gt; None:\n        self.learning_rate = learning_rate\n        self.number_of_iteration = number_of_iteration\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        num_of_samples, num_of_features = X.shape\n        self.weights = np.zeros(num_of_features)\n        self.bias = 0\n\n        for _ in range(self.number_of_iteration):\n            y_predicted = np.dot(X, self.weights) + self.bias\n\n            d_weights = (1 / num_of_samples) * np.dot(X.T, (y_predicted - y))\n            d_bias = (1 / num_of_samples) * np.sum(y_predicted - y)\n\n            self.weights -= self.learning_rate * d_weights\n            self.bias -= self.learning_rate * d_bias\n\n    def predict(self, X):\n        y_predicted = np.dot(X, self.weights) + self.bias\n        return y_predicted\n\n    def coefff_(self):\n        return self.weights.tolist()\n\n    def intercepttt_(self):\n        return self.bias\nNow we use this similarly as before,\n\nfrom mlreg import GDLinearRegression\nmlr2= GDLinearRegression(learning_rate=0.008)\nmlr2.fit(X,y)\ncoefficients2=mlr2.coefff_()\nslope2=mlr2.intercepttt_()\n\nSo the model parameters: slope \\(\\beta_0=\\) 1.0518 and coefficients \\(\\beta_1=\\) 2.9703, and \\(\\beta_2=\\) 1.9935\n\n\nFit the data: Using Stochastic Gradient Descent\nFirst we define the class\nclass SGDLinearRegression:\n    def __init__(self, learning_rate=0.01, num_iterations=1000, batch_size=1) -&gt; None:\n        self.learning_rate = learning_rate\n        self.num_iterations = num_iterations\n        self.batch_size = batch_size\n        self.theta = None\n        self.mse_list = None  # Initialize mse_list as an instance attribute\n\n    def _loss_function(self, X, y, beta):\n        num_samples = len(y)\n        y_predicted = X.dot(beta)\n        mse = (1/num_samples) * np.sum(np.square(y_predicted - y))\n        return mse\n\n    def _gradient_function(self, X, y, beta):\n        num_samples = len(y)\n        y_predicted = X.dot(beta)\n        grad = (1/num_samples) * X.T.dot(y_predicted - y)\n        return grad\n\n    def fit(self, X, y):\n        # Adding the intercept term (bias) as a column of ones\n        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n        num_features = X.shape[1]\n        self.theta = np.zeros((num_features, 1))\n\n        self.mse_list = np.zeros(self.num_iterations)  # Initialize mse_list\n\n        for i in range(self.num_iterations):\n            # Randomly select a batch of data points\n            indices = np.random.choice(\n                len(y), size=self.batch_size, replace=False)\n            X_i = X[indices]\n            y_i = y[indices].reshape(-1, 1)\n\n            # Compute the gradient and update the weights\n            gradient = self._gradient_function(X_i, y_i, self.theta)\n            self.theta = self.theta - self.learning_rate * gradient\n\n            # Calculate loss for the entire dataset (optional)\n            self.mse_list[i] = self._loss_function(X, y, self.theta)\n\n        return self.theta, self.mse_list\n\n    def predict(self, X):\n        # Adding the intercept term (bias) as a column of ones\n        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n        return X.dot(self.theta)\n\n    def coef_(self):\n        # Return the coefficients (excluding the intercept term)\n        return self.theta[1:].flatten().tolist()\n\n    def intercept_(self):\n        # Return the intercept term\n        return self.theta[0].item()\n\n    def mse_losses(self):\n        # Return the mse_list\n        return self.mse_list.tolist()\nNow\n\nimport matplotlib.pyplot as plt\nfrom mlreg import SGDLinearRegression\nmlr3=SGDLinearRegression(learning_rate=0.01, num_iterations=1000, batch_size=10)\ntheta, _ = mlr3.fit(X, y)\n\nSo the model parameters: slope \\(\\beta_0=\\) array([1.07636653]) and coefficients \\(\\beta_1=\\) array([2.97410781]), and \\(\\beta_2=\\) array([1.93948401])\nUp next knn regression\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/gradientboosting/index.html",
    "href": "dsandml/gradientboosting/index.html",
    "title": "Ensemble Methods: Gradient Boosting - A detailed overview",
    "section": "",
    "text": "Gradient Boosting is one of the most powerful techniques for building predictive models. It has gained popularity in the realms of both classification and regression due to its flexibility and effectiveness, particularly with decision trees as weak learners."
  },
  {
    "objectID": "dsandml/gradientboosting/index.html#introduction",
    "href": "dsandml/gradientboosting/index.html#introduction",
    "title": "Ensemble Methods: Gradient Boosting - A detailed overview",
    "section": "",
    "text": "Gradient Boosting is one of the most powerful techniques for building predictive models. It has gained popularity in the realms of both classification and regression due to its flexibility and effectiveness, particularly with decision trees as weak learners."
  },
  {
    "objectID": "dsandml/gradientboosting/index.html#what-is-gradient-boosting",
    "href": "dsandml/gradientboosting/index.html#what-is-gradient-boosting",
    "title": "Ensemble Methods: Gradient Boosting - A detailed overview",
    "section": "What is Gradient Boosting?",
    "text": "What is Gradient Boosting?\n\nGradient Boosting is an ensemble learning technique where several weak learners (typically decision trees) are combined to form a strong learner. The key idea behind boosting is to train models sequentially, where each new model tries to correct the errors of the previous ones. Gradient Boosting achieves this by minimizing a loss function using gradient descent.\n\nKey Concepts:\n\nWeak Learners: These are models that are only slightly better than random guessing. Decision trees with few splits (depth-1 trees) are commonly used as weak learners.\n\nSequential Learning: Models are trained one after another. Each model focuses on the errors (residuals) made by the previous models.\n\nGradient Descent: Gradient Boosting relies on gradient descent to minimize the loss function."
  },
  {
    "objectID": "dsandml/gradientboosting/index.html#mathematical-derivation-of-gradient-boosting",
    "href": "dsandml/gradientboosting/index.html#mathematical-derivation-of-gradient-boosting",
    "title": "Ensemble Methods: Gradient Boosting - A detailed overview",
    "section": "Mathematical Derivation of Gradient Boosting",
    "text": "Mathematical Derivation of Gradient Boosting\nLet’s consider a regression problem where we aim to predict the target values \\(y \\in \\mathbb{R}\\) using the features \\(X \\in \\mathbb{R}^d\\). We aim to find a function \\(F(x)\\) that minimizes the expected value of a loss function \\(L(y, F(x))\\), where \\(L\\) could be mean squared error or any other appropriate loss function.\nThe idea behind Gradient Boosting is to improve the current model by adding a new model that reduces the loss:\n\\[\nF_{m+1}(x) = F_m(x) + \\eta h_m(x)\n\\]\nwhere:\n\n\\(F_m(x)\\) is the current model after \\(m\\) iterations,\n\n\\(h_m(x)\\) is the new weak learner added at iteration \\(m\\),\n\n\\(\\eta\\) is the learning rate, which controls how much the new learner impacts the final model.\n\n\nWe aim to minimize the loss function \\(L(y, F(x))\\). At each iteration, Gradient Boosting fits a new model \\(h_m(x)\\) to the negative gradient of the loss function. The negative gradient represents the direction of steepest descent, essentially capturing the errors or residuals of the model.  Given a loss function \\(L(y, F(x))\\), we compute the residuals (or pseudo-residuals) as:\n\n\\[\nr_{i,m} = - \\frac{\\partial L(y_i, F_m(x_i))}{\\partial F_m(x_i)}\n\\]\nThese residuals are then used to fit the new weak learner \\(h_m(x)\\). In the case of squared error (for regression), the residuals simplify to the difference between the observed and predicted values:\n\\[\nr_{i,m} = y_i - F_m(x_i)\n\\]\nThus, the new learner is fit to minimize these residuals.\nSteps\nInitialize the model with a constant prediction: \\[\nF_0(x) = \\arg \\min_{c} \\sum_{i=1}^{n} L(y_i, c)\n\\] For squared error loss, \\(F_0(x)\\) would be the mean of the target values \\(y\\).\nFor each iteration \\(m = 1, 2, \\dots, M\\):\n\nCompute the residuals: \\[\nr_{i,m} = - \\frac{\\partial L(y_i, F_m(x_i))}{\\partial F_m(x_i)}\n\\]\n\nFit a weak learner \\(h_m(x)\\) to the residuals \\(r_{i,m}\\).\n\nUpdate the model: \\[\nF_{m+1}(x) = F_m(x) + \\eta h_m(x)\n\\]\n\nContinue until a stopping criterion is met (e.g., a fixed number of iterations or convergence)."
  },
  {
    "objectID": "dsandml/gradientboosting/index.html#assumptions-of-gradient-boosting",
    "href": "dsandml/gradientboosting/index.html#assumptions-of-gradient-boosting",
    "title": "Ensemble Methods: Gradient Boosting - A detailed overview",
    "section": "Assumptions of Gradient Boosting",
    "text": "Assumptions of Gradient Boosting\nGradient Boosting, like any algorithm, comes with its own set of assumptions and limitations. Key assumptions include:\n\nIndependence of Features: Gradient Boosting assumes that the features are independent. Correlated features can lead to overfitting.\nWeak Learners: It assumes that weak learners, typically shallow decision trees, are adequate for capturing the patterns in the data, though overly complex learners may lead to overfitting.\nAdditive Model: The model is additive, meaning it combines weak learners to improve performance. This makes it sensitive to noisy data, as adding too many learners might lead to overfitting."
  },
  {
    "objectID": "dsandml/gradientboosting/index.html#when-to-use-gradient-boosting",
    "href": "dsandml/gradientboosting/index.html#when-to-use-gradient-boosting",
    "title": "Ensemble Methods: Gradient Boosting - A detailed overview",
    "section": "When to Use Gradient Boosting?",
    "text": "When to Use Gradient Boosting?\nGradient Boosting is ideal in the following scenarios:\n\nHigh Predictive Power: When accuracy is a top priority, Gradient Boosting often outperforms simpler algorithms like linear regression or basic decision trees.\n\nComplex Datasets: It works well with datasets that have complex patterns, non-linear relationships, or multiple feature interactions.\n\nFeature Engineering: It is less reliant on extensive feature engineering because decision trees are capable of handling mixed types of features (numerical and categorical) and automatically learning interactions.\n\nImbalanced Data: Gradient Boosting can handle class imbalances by tuning the loss function, making it suitable for classification tasks like fraud detection.\n\nHowever, due to its complexity, Gradient Boosting can be computationally expensive, so it’s less ideal for very large datasets or real-time predictions."
  },
  {
    "objectID": "dsandml/gradientboosting/index.html#python-implementation-of-gradient-boosting",
    "href": "dsandml/gradientboosting/index.html#python-implementation-of-gradient-boosting",
    "title": "Ensemble Methods: Gradient Boosting - A detailed overview",
    "section": "Python Implementation of Gradient Boosting",
    "text": "Python Implementation of Gradient Boosting\nBelow is a Python implementation using the scikit-learn library for a regression problem. We will use the Boston Housing dataset as an example.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Load the Boston Housing dataset\ndata_url = \"http://lib.stat.cmu.edu/datasets/boston\"\nraw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\ndata = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\ntarget = raw_df.values[1::2, 2]\n\nX = data\ny = target\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and fit the Gradient Boosting Regressor\ngb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\ngb_regressor.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = gb_regressor.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\nMean Squared Error: 6.240854334653895\n\n\nIn this example:\n\nWe use the GradientBoostingRegressor from scikit-learn for a regression task.\n\nWe fit the model on the Boston Housing dataset, and predict values on the test set.\n\nThe mean squared error (MSE) is used to evaluate the model’s performance.\n\nHyperparameters:\n\nn_estimators: Number of boosting stages to run.\n\nlearning_rate: Controls the contribution of each tree to the final model.\n\nmax_depth: Limits the depth of the individual decision trees (weak learners).\n\n\nGradient Boosting is a powerful ensemble technique, particularly effective for both classification and regression tasks. It builds models sequentially, focusing on correcting the mistakes of prior models. While it is computationally expensive and prone to overfitting if not properly regularized, it often achieves state-of-the-art results in predictive tasks."
  },
  {
    "objectID": "dsandml/gradientboosting/index.html#references",
    "href": "dsandml/gradientboosting/index.html#references",
    "title": "Ensemble Methods: Gradient Boosting - A detailed overview",
    "section": "References",
    "text": "References\n\n“The Elements of Statistical Learning” by Trevor Hastie, Robert Tibshirani, and Jerome Friedman (freely available online).\n“Pattern Recognition and Machine Learning” by Christopher M. Bishop:\n“Greedy Function Approximation: A Gradient Boosting Machine” by Jerome Friedman\n\n“A Short Introduction to Boosting” by Yoav Freund and Robert E. Schapire\n\n“Understanding Gradient Boosting Machines” by Terence Parr and Jeremy Howard\n\n“A Gentle Introduction to Gradient Boosting” by Jason Brownlee"
  },
  {
    "objectID": "dsandml/decisiontree/index.html",
    "href": "dsandml/decisiontree/index.html",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "",
    "text": "The Decision Tree Classifier is a powerful, interpretable, and widely-used algorithm in machine learning for binary or multi-class classification problems. Its simplicity and visual appeal make it a go-to choice for classification tasks. However, behind this simplicity lies a series of mathematical decisions that guide how the tree is constructed."
  },
  {
    "objectID": "dsandml/decisiontree/index.html#decision-tree",
    "href": "dsandml/decisiontree/index.html#decision-tree",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "",
    "text": "The Decision Tree Classifier is a powerful, interpretable, and widely-used algorithm in machine learning for binary or multi-class classification problems. Its simplicity and visual appeal make it a go-to choice for classification tasks. However, behind this simplicity lies a series of mathematical decisions that guide how the tree is constructed."
  },
  {
    "objectID": "dsandml/decisiontree/index.html#the-core-idea-behind-decision-trees",
    "href": "dsandml/decisiontree/index.html#the-core-idea-behind-decision-trees",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "The Core Idea Behind Decision Trees",
    "text": "The Core Idea Behind Decision Trees\n\nDecision Tree contains two main type of nodes, decision nodes and leaf nodes. A decision node is a node where a condition is applied to split the data and a leaf node contains the class of a data point. At its heart, a decision tree works by recursively splitting the dataset based on feature values. The goal of each split is to increase the homogeneity of the resulting subgroups, ideally separating the different classes as much as possible. The splitting process relies on a measure of impurity or disorder. The two most common metrics used for this purpose are Gini Impurity and Entropy (used in Information Gain).\n\nGini Impurity\nThe Gini Impurity measures the likelihood of misclassifying a randomly chosen element from the dataset if it were labeled according to the distribution of classes in that subset. Mathematically, the Gini Impurity for a node \\(t\\) is calculated as:\n\\[\\begin{align*}\nG(t) &= 1 - \\sum_{i=1}^{n} p_i^2\n\\end{align*}\\]\nwhere \\(p_i\\) is the proportion of samples belonging to class \\(i\\) at node \\(t\\).\nEntropy and Information Gain\nEntropy, borrowed from information theory, measures the disorder or uncertainty in the dataset. It is defined as:\n\\[H(t) = -\\sum_{i=1}^{n} p_i \\log_2(p_i)\\]\n\n\nCode\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt \n\nx=np.arange(0.01,0.99,0.0001)\ny=[-p*math.log(p,2)-(1-p)*math.log(1-p,2) for p in x]\nplt.plot(x,y)\nplt.xlabel('$p_{\\oplus}$')\nplt.ylabel('$H(t)$')\nplt.title('Entropy')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nInformation Gain is the reduction in entropy after a dataset is split on a feature. It is calculated as:\n\\[IG(D, A) = H(D) - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} H(D_v)\\]\nwhere:\n\n\\(D\\) is the dataset,\n\\(A\\) is the feature on which the split is made,\n\\(D_v\\) is the subset of \\(D\\) for which feature \\(A\\) has value \\(v\\).\n\n\nLet’s explain the math with following example.\nSay, I have the data set like this\n\n\n\n\\(x_0\\)\n\\(x_1\\)\nClass\n\n\n\n\n2\n3\n0\n\n\n3\n4\n0\n\n\n4\n6\n0\n\n\n6\n8\n1\n\n\n7\n10\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\nTotal 20 data points and the scatter plot looks like this\n\n\nCode\ndata = [\n    [2, 3, 0], [3, 4, 0], [4, 6, 0], [6, 8, 1], [7, 10, 1],\n    [8, 12, 1], [5, 7, 1], [2, 5, 0], [9, 15, 1], [1, 2, 0],\n    [11, 3, 0], [4, 13, 1], [8, 14, 1], [1, 5, 0], [6, 2, 1],\n    [9, 3, 1], [15, 13, 0], [7, 5, 0], [5, 9, 0], [8, 3, 1]\n]\n\nx0 = [row[0] for row in data]\nx1 = [row[1] for row in data]\nclasses = [row[2] for row in data]\n\ncolors = ['red' if c == 0 else 'blue' for c in classes]\n\nplt.figure(figsize=(7, 5))\nplt.grid(True)\n\nplt.scatter(x0, x1, color=colors, s=100, edgecolor='black')\n\n# Label points with class values\nfor i in range(len(x0)):\n    plt.text(x0[i] + 0.2, x1[i] + 0.2, str(classes[i]), fontsize=9)\n\n# Set limits for the axes\nplt.xlim(0, 16)\nplt.ylim(0, 16)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\n# Label axes and show plot\nplt.xlabel('$x_0$')\nplt.ylabel('$x_1$')\nplt.title('Figure 1: Scatter Plot of $x_0$ vs $x_1$ ')\nplt.show()\n\n\n\n\n\n\n\n\n\nAt this point, we see that the classes are not linearly separable, meaning, we can not draw any line that separate the two classes. Notice that the minimum and maximum of feature \\(x_0\\) is 1 and 15, respectively. So, let’s pick a few numbers in between these two numbers. Say, our first number is \\(3.5\\). In the first node, that is the root node, we divide the data based on the feature \\(x_0\\le 3.5\\)\n\n\n\nFigure 2: First Split\n\n\nAt the root node, we have equal number of blue and red points so the proportion of the data class is \\(p_1=p_2=0.5\\), so the entropy\n\\[\\begin{align*}\n    H(\\text{root node})&=-(0.5)\\log_2(0.5)-(0.5)\\log_2(0.5)=1\\\\\n\\end{align*}\\]\nBased on the condition \\(x_0\\le 3.5\\), the left and right child recieves 5 and 15 feature points \\(X=(x_0,x_1)\\), respectively. We see that the left node is a pure node, because it contains only the red points. Therefore, the entropies at these child nodes\n\\[\\begin{align*}\n    H(\\text{left child})&=-1\\log_2(1)-0\\log_2(0)=0\\\\\n    H(\\text{right child})&=-\\frac{5}{15}\\log_2\\left(\\frac{5}{15}\\right)-\\frac{10}{15}\\log_2\\left(\\frac{10}{15}\\right)=0.92\\\\\n\\end{align*}\\]\nand the information gain at this split\n\\[IG(split_1)=1-\\left(\\frac{5}{20}\\cdot 0+\\frac{15}{20}\\cdot 0.92\\right)=0.31\\]\nNow the burning question is how did we select the condition \\(x_0\\le 3.5\\)? It could have been any other number, say we set \\(x_0\\le 6.5\\). Then\n\n\n\nFigure 3: Alternative Split\n\n\nBased on the condition \\(x_0\\le 6.5\\), the left and right child recieves 11 and 9 feature points \\(X=(x_0,x_1)\\), respectively. But in this case we don’t see any pure nodes and the entropies at these child nodes\n\\[\\begin{align*}\n    H(\\text{left child})&=-\\frac{7}{11}\\log_2\\left(\\frac{7}{11}\\right)-\\frac{4}{11}\\log_2\\left(\\frac{4}{11}\\right)=0.95\\\\\n    H(\\text{right child})&=-\\frac{3}{9}\\log_2\\left(\\frac{3}{9}\\right)-\\frac{6}{9}\\log_2\\left(\\frac{6}{9}\\right)=0.92\\\\\n\\end{align*}\\]\nand the information gain at this split\n\\[IG(split_1)=1-\\left(\\frac{11}{20}\\cdot 0.95+\\frac{9}{20}\\cdot 0.92\\right)=0.06\\]\nNote that the information gain is much lower than the first option. Therefore, the first split is better than this alternative split. Because the goal is to have minimum entropy value and/or the maximum information gain. This is where the machine learning gets in the game. The algorithm finds the optimal split based on each feature values.\n\n\n\nFigure 4: Second Split\n\n\nNow say we have a new set of feature values \\((x_0,x_1,Class)=(10,7,1)\\). Based on our tree above, since \\(x_0\\) is NOT less than or equal to \\(3.5\\) so it goes to the right first child. Then it satisfies \\(x_0\\le 10\\). So it moves to the left grand child gradually traverse through the tree and ended up to the very bottom layer left leaf node."
  },
  {
    "objectID": "dsandml/decisiontree/index.html#building-a-decision-tree",
    "href": "dsandml/decisiontree/index.html#building-a-decision-tree",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Building a Decision Tree",
    "text": "Building a Decision Tree\n\nChoose the best feature to split on: Calculate Gini impurity or Information Gain for each feature and select the feature that results in the highest Information Gain or lowest Gini impurity.\nSplit the dataset: Partition the data based on the chosen feature and repeat the process for each partition.\nStop conditions: The tree stops growing when all samples in a node belong to the same class, the maximum depth is reached, or further splitting doesn’t add value."
  },
  {
    "objectID": "dsandml/decisiontree/index.html#implementation-of-decision-tree-scikit-learn",
    "href": "dsandml/decisiontree/index.html#implementation-of-decision-tree-scikit-learn",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Implementation of Decision Tree: Scikit-learn",
    "text": "Implementation of Decision Tree: Scikit-learn\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier,plot_tree\nfrom sklearn.metrics import accuracy_score\n\nX=pd.DataFrame({'Feature 1':x0, 'Feature 2':x1})\ny=classes\n\n\nclf= DecisionTreeClassifier(criterion=\"entropy\")\nclf.fit(X,y)\n\n\nX_test=pd.DataFrame({'Feature 1':[10,9,11],'Feature 2':[7,9,5]})\ny_test=pd.DataFrame({'Class':[1,0,1]})\n\ntest_data=pd.concat([X_test,y_test], axis=1)\nprint('Test Data \\n')\nprint(test_data)\n\n\ny_prediction=clf.predict(X_test)\nprediction=pd.DataFrame({'Predicted_Class':y_prediction})\nprediction=pd.concat([test_data,prediction],axis=1)\nprint('\\n')\nprint('Result \\n')\nprint(prediction)\nprint('\\n')\nprint('Accuracy score:',round(accuracy_score(y_prediction,y_test),2))\n\nplt.figure(figsize=(11,7))\nplot_tree(clf, filled=True, \n          feature_names=['$x_0$','$x_1$'], \n          class_names=['R', 'B'], impurity=True,\n          )\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\nTest Data \n\n   Feature 1  Feature 2  Class\n0         10          7      1\n1          9          9      0\n2         11          5      1\n\n\nResult \n\n   Feature 1  Feature 2  Class  Predicted_Class\n0         10          7      1                1\n1          9          9      0                1\n2         11          5      1                0\n\n\nAccuracy score: 0.33"
  },
  {
    "objectID": "dsandml/decisiontree/index.html#discussion-on-decision-tree",
    "href": "dsandml/decisiontree/index.html#discussion-on-decision-tree",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Discussion on Decision Tree",
    "text": "Discussion on Decision Tree\n\nBeing a simple algorithm, it has both pros and cons. It is robust to training data and the training data can contain missing values. However, it is a greedy algorithm, a problem-solving technique that chooses the best option in the current situation, without considering the overall outcome. It also face the overfitting issue."
  },
  {
    "objectID": "dsandml/decisiontree/index.html#reference",
    "href": "dsandml/decisiontree/index.html#reference",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Reference",
    "text": "Reference\nDecision Tree Classification Clearly Explained by Normalized Nerd\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/logreg/index.html",
    "href": "dsandml/logreg/index.html",
    "title": "Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code",
    "section": "",
    "text": "Logistic Regression is a popular classification algorithm used for binary and multi-class classification problems. Unlike Linear Regression, which is used for regression problems, Logistic Regression is used to predict categorical outcomes. In binary classification, the output is either 0 or 1, and the relationship between the input features and the outcome is modeled using a logistic function (also called the sigmoid function)."
  },
  {
    "objectID": "dsandml/logreg/index.html#introduction",
    "href": "dsandml/logreg/index.html#introduction",
    "title": "Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code",
    "section": "",
    "text": "Logistic Regression is a popular classification algorithm used for binary and multi-class classification problems. Unlike Linear Regression, which is used for regression problems, Logistic Regression is used to predict categorical outcomes. In binary classification, the output is either 0 or 1, and the relationship between the input features and the outcome is modeled using a logistic function (also called the sigmoid function)."
  },
  {
    "objectID": "dsandml/logreg/index.html#what-is-logistic-regression",
    "href": "dsandml/logreg/index.html#what-is-logistic-regression",
    "title": "Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code",
    "section": "What is Logistic Regression?",
    "text": "What is Logistic Regression?\n\nLogistic Regression is a type of regression analysis used when the dependent variable is categorical. In binary logistic regression, the output can have only two possible outcomes (e.g., 0 or 1, pass or fail, spam or not spam).  Logistic Regression works by modeling the probability of an event occurring based on one or more input features. It estimates the probability that a given input belongs to a particular category (0 or 1) using the logistic function (sigmoid function)."
  },
  {
    "objectID": "dsandml/logreg/index.html#the-sigmoid-function",
    "href": "dsandml/logreg/index.html#the-sigmoid-function",
    "title": "Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code",
    "section": "The Sigmoid Function",
    "text": "The Sigmoid Function\nThe sigmoid function maps any real-valued number to a value between 0 and 1, making it ideal for modeling probabilities.\nThe sigmoid function is given by the formula:\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\]\nWhere:\n\n\\(z\\) is the input to the sigmoid function (in logistic regression, \\(z = \\mathbf{x} \\cdot \\theta\\))\n\\(e\\) is the base of the natural logarithm\n\nThe output of the sigmoid function is interpreted as the probability \\(P(y=1|X)\\)."
  },
  {
    "objectID": "dsandml/logreg/index.html#logistic-regression-model",
    "href": "dsandml/logreg/index.html#logistic-regression-model",
    "title": "Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\nIn Logistic Regression, the hypothesis is modeled as:\n\\[\nh_\\theta(X) = \\frac{1}{1 + e^{-\\theta^T X}}\n\\]\nWhere:\n\n\\(X\\) is the input feature vector\n\\(\\theta\\) is the parameter vector (weights)"
  },
  {
    "objectID": "dsandml/logreg/index.html#cost-function-for-logistic-regression",
    "href": "dsandml/logreg/index.html#cost-function-for-logistic-regression",
    "title": "Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code",
    "section": "Cost Function for Logistic Regression",
    "text": "Cost Function for Logistic Regression\n\nUnlike Linear Regression, which uses the Mean Squared Error (MSE) as the cost function, Logistic Regression uses log loss or binary cross-entropy as the cost function, as the output is binary (0 or 1).\n\nSo, basically we model probability from the given data. In other words, we can write\n\\[\\begin{align*}\n\\mathbb{P}(y= 1 \\text{ or }0 |\\text{ given }X)&=p(\\mathbf{x})=\\sigma(\\mathbf{x}\\cdot\\theta)=\\frac{1}{1+e^{-\\mathbf{x}\\cdot \\theta}}\\\\\n\\implies p_{\\theta}(\\mathbf{x})& = \\frac{1}{1+e^{-(\\theta_0+\\theta_1x_1+\\cdots+\\theta_dx_d)}}\\\\\n\\implies p_{\\theta}(\\mathbf{x})& = \\begin{cases}\n                                p_{\\theta}(\\mathbf{x}) & \\text{ if } y=1\\\\\n                                1-p_{\\theta}(\\mathbf{x}) & \\text{ if } y=0\n                          \\end{cases}\n\\end{align*}\\]\nWhere, \\(\\mathbf{\\theta},\\mathbf{x}\\in \\mathbb{R}^{d+1}\\) and \\(d\\) is the dimension of the data. For single data vector \\(\\mathbf{x}\\) the binary cross-entropy function can be written as\n\\[\nl(\\theta) = yp_{\\theta}(\\mathbf{x})+ (1-y)(1-p_{\\theta}(\\mathbf{x}))\n\\]\nSince we have \\(n\\) of those i.i.d data vectors therefore, we can write\n\\[\nL(\\theta) = \\prod_{i=1}^{n} \\left(y_ip_{\\theta}(\\mathbf{x_i})+ (1-y_i)(1-p_{\\theta}(\\mathbf{x_i}))\\right)\n\\]\nSince our goal is to minimize the loss, we need to perform derivatives of the loss function. Therefore, to change from the product form to addition form we take negative log of the above expression\n\\[\\begin{align*}\n\\ell (\\theta) = -\\log{L(\\theta)} = -\\sum_{i=1}^{n}y_i\\log{p_{\\theta}(\\mathbf{x})}+(1-y_i)\\log{(1-p_{\\theta}(\\mathbf{x}))}\n\\end{align*}\\]\nFor the ease of calculation, let’s rewrite the above equation in terms of \\(m\\) and \\(b\\) where \\(m\\in \\mathbb{R}^d = (\\theta_1,\\theta_2,\\cdots,\\theta_d)^T\\) and \\(b\\in \\mathbb{R}\\).\n\\[\n\\ell (\\theta) = -\\sum_{i=1}^{n}y_i\\log{p_{m,b}(\\mathbf{x})}+(1-y_i)\\log{(1-p_{m,b}(\\mathbf{x}))}\n\\]\nWhere:\n\n\\(n\\) is the number of training examples\n\n\\(m\\) is the number of features\n\\(y^{(i)}\\) is the true label of the \\(i^{th}\\) example\n\\(b\\) is the bias for the \\(i^{th}\\) example"
  },
  {
    "objectID": "dsandml/logreg/index.html#gradient-descent",
    "href": "dsandml/logreg/index.html#gradient-descent",
    "title": "Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nTo minimize the cost function and find the optimal values for \\(\\theta\\), we use gradient descent. We start from the last form of the loss function and convert this to a form that is easy to take the partial dervivatives.\n\\[\\begin{align*}\n\\ell (\\theta) &= -\\sum_{i=1}^{n}y_i\\log{p_{m,b}(\\mathbf{x})}+(1-y_i)\\log{(1-p_{m,b}(\\mathbf{x}))}\\\\\n              &= -\\sum_{i=1}^{n}y_i\\log{(\\sigma(mx_i+b))}+(1-y_i)\\log{(1-\\sigma(mx_i+b))}\\\\\n              &= -\\sum_{i=1}^{n}y_i\\log{(\\sigma(mx_i+b))}+(1-y_i)\\log{(\\sigma(-(mx_i+b)))};\\hspace{3mm}\\text{ Since } 1-\\sigma(x)=\\sigma(-x)\\\\\n              &= -\\sum_{i=1}^{n}y_i\\left[\\log{(\\sigma(mx_i+b))}-\\log{(\\sigma(-(mx_i+b)))}\\right]+\\log{(-\\sigma(mx_i+b))}\\\\\n              &= -\\sum_{i=1}^{n}y_i\\log{\\left(\\frac{\\sigma(mx_i+b)}{\\sigma(-(mx_i+b))}\\right)}+\\log{(-\\sigma(mx_i+b))}\\\\\n              &= -\\sum_{i=1}^{n}y_i(mx_i+b)+\\log{(\\sigma(-(mx_i+b)))};\\hspace{3mm}\\text{ Since }\\frac{\\sigma(x)}{-\\sigma(x)}=e^x\\\\\n\\end{align*}\\]\nNow we again use the beautiful features of the sigmoid function\n\\[\\begin{align*}\n\\frac{d\\sigma(x)}{dx}&=\\frac{d}{dx}\\left(\\frac{1}{1+e^{-x}}\\right)=\\frac{e^{-x}}{\\left(1+e^{-x}\\right)^2}=\\frac{1}{1+e^{-x}}\\cdot \\frac{e^{-x}}{1+e^{-x}}\\\\\n&=\\sigma(x)\\left(1-\\frac{1}{1+e^{-x}}\\right)=\\sigma(x)(1-\\sigma(x))\\\\\n&=\\sigma(x)\\sigma(-x)\n\\end{align*}\\]\nFinally, we are ready to take the partial derivatives of the loss function with respect to \\(m\\) and \\(b\\),\n\\[\\begin{align*}\n\\frac{\\partial \\ell}{\\partial m} &= -\\sum_{i=1}^{n}y_ix_i+\\frac{1}{\\sigma(-(mx_i+b))}\\frac{d}{dx}(\\sigma(-(mx_i+b)))\\\\\n& =-\\sum_{i=1}^{n}y_ix_i+\\frac{1}{\\sigma(-(mx_i+b))}\\sigma(-(mx_i+b))\\sigma(mx_i+b)(-x_i)\\\\\n& = -\\sum_{i=1}^{n} x_i(y_i-\\sigma(mx_i+b))\\\\\n& = \\sum_{i=1}^{n}x_i(p_{m,b}(x_i)-y_i)=\\sum_{i=1}^{n} x_i(\\hat{y_i}-y_i)\\\\\n& = \\mathbf{x_i}\\cdot(\\mathbf{\\hat{y_i}}-\\mathbf{y_i})\\\\\n\\text{ and } & \\\\\n& \\\\\n\\frac{\\partial \\ell}{\\partial b} & = -\\sum_{i=1}^{n} y_i +\\frac{1}{\\sigma(-(mx_i+b))}\\frac{d}{dx}(\\sigma(-(mx_i+b)))\\\\\n& =  -\\sum_{i=1}^{n} y_i - \\frac{1}{\\sigma(-(mx_i+b))}\\sigma(-(mx_i+b))\\sigma(mx_i+b)\\\\\n& = \\sum_{i=1}^{n} p_{m,b}(x_i)-y_i= \\sum_{i=1}^{n} \\hat{y}_i-y_i\\\\\n& = \\hat{\\mathbf{y}}_i-\\mathbf{y}_i\n\\end{align*}\\]\nUsing this gradient, we update the parameter vector \\(\\theta\\) iteratively:\n\\[\n\\theta_{j+1} := \\theta_j - \\alpha \\nabla \\ell (\\theta_j)\n\\]\nWhere:\n\n\\(\\alpha\\) is the learning rate\n\\(\\nabla \\ell (\\theta_j)\\) is the partial derivative of the cost function with respect to \\(\\theta_j\\) and \\[\n\\nabla \\ell (\\theta) = \\begin{bmatrix}\\sum_{i=1}^{n} \\hat{y}_i-y_i \\\\\n\\sum_{i=1}^{n} x_i(\\hat{y_i}-y_i) \\end{bmatrix}  =\\begin{bmatrix}\\hat{\\mathbf{y}}_i-\\mathbf{y}_i \\\\\n\\mathbf{x_i}\\cdot(\\mathbf{\\hat{y_i}}-\\mathbf{y_i}) \\end{bmatrix}= X^T(\\hat{\\mathbf{y}}_i-\\mathbf{y}_i)=X^T(\\sigma(X\\vec{\\theta})-\\vec{y})\n\\]"
  },
  {
    "objectID": "dsandml/logreg/index.html#python-code-implementation-from-scratch",
    "href": "dsandml/logreg/index.html#python-code-implementation-from-scratch",
    "title": "Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code",
    "section": "Python Code Implementation from Scratch",
    "text": "Python Code Implementation from Scratch\nHere’s how to implement Logistic Regression from scratch in Python. We will use two different forms for our class\n\nimport numpy as np\n\nclass LogisticRegression1:\n    def __init__(self, learning_rate = 0.1, n_iterations = 1000):\n        \"\"\"\n        Hyper Parameters\n        - learning_rate: learning rate; float; default 0.01\n        - n_itearations: number of iterations; int; default 1000\n        Model Parameters\n        - weights: weights of the features; float or int\n        - bias: bias of the model; float or int\n        \"\"\"\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations \n        self.weights = None\n        self.bias = None \n    \n    def _sigmoid(self, x):\n        return 1/(1+np.exp(-x))\n\n    def fit(self, X,y):\n        \"\"\"\n        n_sample = number of samples in the data set: the value n\n        n_features = number of features or the dimension of the data set: the value d\n        \"\"\"\n        n_sample,n_features = X.shape\n        self.weights = np.zeros(n_features) \n        self.bias = 0\n\n        for _ in range(self.n_iterations):\n            linear = np.dot(X, self.weights) + self.bias\n            pred = self._sigmoid(linear)\n\n            dw = (1/n_sample)* np.dot(X.T,(pred-y))\n            db = (1/n_sample) * np.sum(pred-y)\n\n            self.weights = self.weights - self.learning_rate * dw \n            self.bias = self.bias - self.learning_rate * db\n    \n    def predict(self, X):\n        linear = np.dot(X, self.weights) + self.bias\n        predicted_y = self._sigmoid(linear)\n        class_of_y = [0 if y&lt;=0.5 else 1 for y in predicted_y]\n        return class_of_y\n\nNow let’s use this using the scikit-learn breast cancer data set.\n\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nb_cancer = load_breast_cancer()\nX, y = b_cancer.data, b_cancer.target\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=123, stratify=y, test_size=0.30)\n\nclf1 = LogisticRegression1(learning_rate=0.01)\nclf1.fit(X_train, y_train)\npredicted_y = clf1.predict(X_test)\nprint(np.round(accuracy_score(predicted_y, y_test),2))\n\n0.91\n\n\nNow lets compare this with the standard scikit-learn library\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf2 = LogisticRegression()\nclf2.fit(X_train, y_train)\npredicted_y = clf2.predict(X_test)\nprint(np.round(accuracy_score(predicted_y, y_test),2))\n\n0.96"
  },
  {
    "objectID": "dsandml/logreg/index.html#references",
    "href": "dsandml/logreg/index.html#references",
    "title": "Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code",
    "section": "References",
    "text": "References\n\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\nGradient descent is a widely used optimization technique in machine learning.\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nNocedal, J., & Wright, S. (2006). Numerical Optimization (2nd ed.). Springer.\nRegularization techniques like L2 (Ridge) and L1 (Lasso) are commonly used in logistic regression to prevent overfitting.\nNg, A. (2004). Feature Selection, L1 vs. L2 Regularization, and Rotational Invariance. ICML Proceedings.\nFriedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22.\nThe extension of logistic regression to multiclass classification via the softmax function is part of the core material for understanding classification tasks.\nMurphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\nVanderPlas, J. (2016). Python Data Science Handbook: Essential Tools for Working with Data. O’Reilly Media.\nRaschka, S., & Mirjalili, V. (2017). Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2. Packt Publishing.\n\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/regularization/index.html",
    "href": "dsandml/regularization/index.html",
    "title": "Model Fine Tuning: Regularization",
    "section": "",
    "text": "Regularization is a key concept in machine learning that helps prevent overfitting, improve model generalization, and make models more robust to new data. It adds a penalty to the loss function to discourage the model from fitting the noise in the training data, which leads to overfitting.\n\n\nOverfitting occurs when a model performs well on the training data but fails to generalize to new, unseen data. This happens when the model is too complex and captures both the signal and the noise in the data.\nUnderfitting, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, resulting in poor performance even on the training set.\n\n\nRegularization helps strike a balance between overfitting and underfitting by controlling model complexity and encouraging simpler models that generalize better."
  },
  {
    "objectID": "dsandml/regularization/index.html#introduction",
    "href": "dsandml/regularization/index.html#introduction",
    "title": "Model Fine Tuning: Regularization",
    "section": "",
    "text": "Regularization is a key concept in machine learning that helps prevent overfitting, improve model generalization, and make models more robust to new data. It adds a penalty to the loss function to discourage the model from fitting the noise in the training data, which leads to overfitting.\n\n\nOverfitting occurs when a model performs well on the training data but fails to generalize to new, unseen data. This happens when the model is too complex and captures both the signal and the noise in the data.\nUnderfitting, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, resulting in poor performance even on the training set.\n\n\nRegularization helps strike a balance between overfitting and underfitting by controlling model complexity and encouraging simpler models that generalize better."
  },
  {
    "objectID": "dsandml/regularization/index.html#types-of-regularization",
    "href": "dsandml/regularization/index.html#types-of-regularization",
    "title": "Model Fine Tuning: Regularization",
    "section": "Types of Regularization",
    "text": "Types of Regularization\nThere are several types of regularization techniques used in machine learning, with the most common being:\n\n\\(L_2\\) Regularization (Ridge Regression)\n\\(L_1\\) Regularization (Lasso Regression)\nElastic Net Regularization\nDropout (for neural networks)\n\nHere we will discus the first two kind only."
  },
  {
    "objectID": "dsandml/regularization/index.html#l_2-regularization-ridge-regression",
    "href": "dsandml/regularization/index.html#l_2-regularization-ridge-regression",
    "title": "Model Fine Tuning: Regularization",
    "section": "\\(L_2\\) Regularization (Ridge Regression)",
    "text": "\\(L_2\\) Regularization (Ridge Regression)\n\n\\(L_2\\) regularization (also known as Ridge regression in linear models) adds a penalty term to the loss function proportional to the sum of the squared coefficients (weights) of the model. The goal is to minimize both the original loss function and the magnitude of the coefficients.\n\nFor a linear regression model, the objective is to minimize the following regularized loss function:\n\\[\nJ(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} \\theta_j^2\n\\]\nWhere:\n\n\\(\\hat{y_i}\\) is the model’s predicted output for input \\(x_i\\).\n\\(y_i\\) is the true target value.\n\\(\\theta_j\\) are the model parameters (coefficients).\n\\(\\lambda\\) is the regularization strength, controlling the magnitude of the penalty (higher \\(\\lambda\\) increases regularization).\n\nMore about \\(\\lambda\\)\n\n\\(\\lambda\\) is a continuous non-negative scaler value, typically a floating-point number.\n\nMinimum \\(\\lambda=0\\), model becomes the standard linear regression model. For smaller \\(\\lambda\\) the regularization effect is minimal, allowing the model to fit the training data more closely.\nIn theory, there is no upper bound for \\(\\lambda\\). However, as \\(\\lambda\\) increases, the model becomes more regularized, and the coefficients tend to shrink toward zero.\n\n\nSelecting the optimal value of \\(\\lambda\\) is crucial. Typically, it’s done via cross-validation, where different values of \\(\\lambda\\) are tried, and the model is evaluated based on its performance on the validation set. The value that results in the best generalization is selected.\n\n\\(L_2\\) regularization shrinks the coefficients towards zero but doesn’t force them to be exactly zero, thus retaining all features in the model.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge,LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# Generate synthetic data\nnp.random.seed(0)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# LinearRegression model \nlinear_model = LinearRegression()\nlinear_model.fit(X_train,y_train)\ny_pred_linear = linear_model.predict(X_test)\nmse_linear = mean_squared_error(y_test, y_pred_linear)\nprint(f\"Mean Squared Error (Linear Regression): {mse_linear:.2f}\")\n\n# Train Ridge regression model (L2 Regularization)\nsc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_test_sc = sc.transform(X_test)\nridge_model = Ridge(alpha=10)  # alpha is the regularization strength (lambda)\nridge_model.fit(X_train_sc, y_train)\n\n# Predictions and evaluation\ny_pred_ridge = ridge_model.predict(X_test_sc)\nmse_ridge = mean_squared_error(y_test, y_pred_ridge)\nprint(f\"Mean Squared Error (Ridge Regression): {mse_ridge:.2f}\")\n\n# Plot the results\nplt.scatter(X_test, y_test, color='blue', label='True Data')\nplt.plot(X_test, y_pred_linear, color='green', label='Linear Prediction')\nplt.plot(X_test, y_pred_ridge, color='red', label='Ridge Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Ridge Regularization')\nplt.legend()\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.savefig('rg.png')\nplt.show()\n\nMean Squared Error (Linear Regression): 0.92\nMean Squared Error (Ridge Regression): 0.92\n\n\n\n\n\n\n\n\n\nIn this example, alpha corresponds to \\(\\lambda\\), the regularization strength. A higher value of alpha will result in stronger regularization, shrinking the model coefficients more."
  },
  {
    "objectID": "dsandml/regularization/index.html#l_1-regularization-lasso-regression",
    "href": "dsandml/regularization/index.html#l_1-regularization-lasso-regression",
    "title": "Model Fine Tuning: Regularization",
    "section": "\\(L_1\\) Regularization (Lasso Regression)",
    "text": "\\(L_1\\) Regularization (Lasso Regression)\n\\(L_1\\) regularization (also known as Lasso regression) adds a penalty term proportional to the sum of the absolute values of the coefficients. This type of regularization can force some coefficients to be exactly zero, effectively performing feature selection.\nThe objective function for L1 regularization is:\n\\[\nJ(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{p} |\\theta_j|\n\\]\nWhere:\n\nThe terms are the same as those for \\(L_2\\) regularization.\nThe penalty is the absolute value of the coefficients instead of the squared value.\n\n\\(L_1\\) regularization has the effect of making some coefficients exactly zero, which means it can be used to reduce the number of features in the model.\n\nfrom sklearn.linear_model import Lasso\n\nprint(f\"Mean Squared Error (Linear Regression): {mse_linear:.2f}\")\n\n# Train Lasso regression model (L1 Regularization)\nlasso_model = Lasso(alpha=.5)  # alpha is the regularization strength (lambda)\nlasso_model.fit(X_train_sc, y_train)\n\n# Predictions and evaluation\ny_pred_lasso = lasso_model.predict(X_test_sc)\nmse_lasso = mean_squared_error(y_test, y_pred_lasso)\nprint(f\"Mean Squared Error (Lasso Regression): {mse_lasso:.2f}\")\n\n# Plot the results\nplt.scatter(X_test, y_test, color='blue', label='Data')\nplt.plot(X_test, y_pred_linear, color='red', label='Linear Prediction')\nplt.plot(X_test, y_pred_lasso, color='green', label='Lasso Prediction')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Lasso Regularization')\nplt.legend()\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\nMean Squared Error (Linear Regression): 0.92\nMean Squared Error (Lasso Regression): 1.02"
  },
  {
    "objectID": "dsandml/regularization/index.html#discussion",
    "href": "dsandml/regularization/index.html#discussion",
    "title": "Model Fine Tuning: Regularization",
    "section": "Discussion",
    "text": "Discussion\n\nChoosing the Right \\(\\lambda\\)\nSelecting the optimal value of \\(\\lambda\\) is crucial. Typically, it’s done via cross-validation, where different values of \\(\\lambda\\) are tried, and the model is evaluated based on its performance on the validation set. The value that results in the best generalization is selected.\n\n\nImpact of \\(\\lambda\\) on Bias-Variance Trade-off\n\nLow \\(\\lambda\\): Leads to a low bias and high variance model because the model closely fits the training data.\nHigh \\(\\lambda\\): Leads to a high bias and low variance model, as the regularization prevents the model from fitting the training data too closely, reducing the variance but increasing the bias.\n\n\n\nFacts\nScaling is required for both Ridge and Lasso regression as they are not scale invariant due to the different norms in the definition.\n\n\n\n\n\n\n\n\nCriteria\nL1 Regularization (Lasso)\nL2 Regularization (Ridge)\n\n\n\n\nFeature Selection\nCan set some coefficients exactly to zero, effectively performing feature selection.\nDoes not set coefficients to zero; shrinks them but retains all features.\n\n\nHandling Multicollinearity\nNot ideal for handling highly correlated features, as it may arbitrarily select one feature and discard the others.\nWorks better in the presence of multicollinearity, as it tends to spread the penalty across correlated features.\n\n\nEffect on Coefficients\nSparse solutions; coefficients are either zero or relatively large, favoring simpler models with fewer features.\nCoefficients are small and distributed more evenly across all features, leading to less sparse solutions.\n\n\nInterpretability\nEasier to interpret, as some features are removed, simplifying the model.\nAll features remain in the model, making it harder to interpret when there are many features.\n\n\nComputational Complexity\nCan be computationally intensive with a large number of features due to the non-smooth nature of the L1 penalty.\nLess computationally expensive due to its smooth penalty term (squared coefficients).\n\n\nBest Suited For\nWhen you want a sparse model with feature selection, and when the number of irrelevant features is large.\nWhen you want to retain all features, especially in cases of multicollinearity, and avoid overfitting by shrinking coefficients.\n\n\nWhen to Use\n\nWhen you expect only a few features to be important.\nWhen you want automatic feature selection.\nWhen you need a simple, interpretable model.\n\n\nWhen you believe all features contribute to the target.\n\n\nWhen dealing with multicollinear data.\n\n\nWhen you want to prevent overfitting but don’t want feature elimination."
  },
  {
    "objectID": "dsandml/regularization/index.html#references",
    "href": "dsandml/regularization/index.html#references",
    "title": "Model Fine Tuning: Regularization",
    "section": "References",
    "text": "References\n\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267-288.\nHoerl, A. E., & Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1), 55-67.\nZou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301-320.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1), 1929-1958.\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.\nMurphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n\n\nShare on\n\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/classificationmetrics/index.html",
    "href": "dsandml/classificationmetrics/index.html",
    "title": "Model Evaluation and Fine Tuning: Classification Metrices",
    "section": "",
    "text": "In any classification problem, the goal is to build a model that accurately predicts labels or classes from input data. Once the model is built, it is important to evaluate its performance using a variety of metrics. Some of the most commonly used metrics are the confusion matrix, accuracy, precision, recall, F1 score, and ROC-AUC curve. This post will explain each metric and show how to compute them using real data in Python.\n\n\n\nA confusion matrix is a tabular summary of the performance of a classification algorithm. It shows the number of correct and incorrect predictions broken down by each class.\nFor a binary classification, the confusion matrix looks like this:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef plot_confusion_matrix():\n    \n    matrix_data = np.array([[100, 5], [10, 50]])\n\n    \n    extended_matrix = np.zeros((3, 3))  \n    extended_matrix[:2, :2] = matrix_data  \n\n    mask = np.zeros_like(extended_matrix, dtype=bool)\n    mask[2,:] = True\n    mask[:,2] = True\n\n    # Create a plot\n    fig, ax = plt.subplots(figsize=(8, 5.2))\n\n    fig.patch.set_facecolor('#f4f4f4') \n    ax.set_facecolor('#f4f4f4')\n\n    sns.heatmap(extended_matrix, mask=mask,annot=False, cmap=\"RdYlGn\", cbar=False, ax=ax, linewidths=2, linecolor='black')\n\n    # Add the original confusion matrix values (True Positive, False Negative, etc.)\n    ax.text(0.4, 0.3, 'True Positive (TP)', ha='center', va='center', fontsize=12, color=\"white\")\n    ax.text(1.45, 0.3, 'False Negative (FN)', ha='center', va='center', fontsize=12, color=\"white\")\n    ax.text(1.45, 0.60, '(Type II Error)', ha='center', va='center', fontsize=12, color=\"white\")\n    ax.text(0.45, 1.25, 'False Positive (FP)', ha='center', va='center', fontsize=12, color=\"white\")\n    ax.text(0.45, 1.40, '(Type I Error)', ha='center', va='center', fontsize=12, color=\"white\")\n    ax.text(1.45, 1.4, 'True Negative (TN)', ha='center', va='center', fontsize=12, color=\"red\")\n    ax.text(0.4, -0.1, 'Positive', ha='center', va='center', fontsize=12)\n    ax.text(1.45, -0.1, 'Negative', ha='center', va='center', fontsize=12)\n    ax.text(1, -0.3, 'Predicted Class', ha='center', va='center', fontsize=14)\n\n    # Add Precision and NPV in the bottom row of the confusion matrix\n    ax.text(0.17, 0.2, r'Precision= $\\frac{TP}{TP + FP}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.5, 0.2, r'NPV= $\\frac{TN}{TN + FN}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n\n    # Add Sensitivity and Specificity in the right column of the confusion matrix\n    ax.text(0.83, .95, r'TPR=Sensitivity= $\\frac{TP}{TP + FN}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .89, 'or Recall', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .8, 'False Neg. Rate (FNR)', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .75, r'Type II Error rate= $\\frac{FN}{TP + FN}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .6, r'TNR=Specificity= $\\frac{TN}{TN + FP}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .48, 'False Positive Rate', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .43, r'FPR= $\\frac{FP}{TN + FP}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .37, 'Type I Error Rate', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n\n    # Add Accuracy in the bottom-right corner of the extended grid\n    ax.text(0.83, 0.2, r'Accuracy= $\\frac{TP + TN}{TP+TN+FP+FN}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n\n    # Titles and labels\n    ax.set_ylabel('Actual Class', fontsize=14)\n    \n\n    # Set tick labels for actual and predicted\n    ax.xaxis.set_ticklabels([' ', ' ', ''], fontsize=12)\n    ax.yaxis.set_ticklabels(['Positive', 'Negative', ''], fontsize=12, rotation=0)\n\n    plt.tight_layout()\n    plt.savefig('conf.png')\n    plt.show()\n\n# Generate the confusion matrix plot\nplot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\nTrue Positive (TP): The model correctly predicted the positive class.\nFalse Positive (FP): The model incorrectly predicted the positive class (also known as a Type I error).\nTrue Negative (TN): The model correctly predicted the negative class.\nFalse Negative (FN): The model incorrectly predicted the negative class (also known as a Type II error).\n\n\n\n\nAccuracy is the ratio of correctly predicted observations to the total observations.\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\nIt is one of the most intuitive metrics, but it can be misleading if the classes are imbalanced.\n\n\n\nPrecision measures the proportion of positive predictions that are actually correct.\n\\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\]\nIt is useful when the cost of a false positive is high, such as in fraud detection.\n\n\n\nRecall measures the proportion of actual positives that are correctly predicted.\n\\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\]\nIt is important in cases where missing a positive is more costly, like in medical diagnoses.\n\n\n\nThe F1 score is the harmonic mean of precision and recall, giving a balanced measure when both metrics are important.\n\\[\n\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\n\n\n\nThe ROC-AUC curve helps visualize the performance of a classification model by plotting the true positive rate (recall) against the false positive rate (1 - specificity) at various threshold settings. The AUC (Area Under the Curve) gives a single number that summarizes the performance. A model with an AUC of 1 is perfect, while a model with an AUC of 0.5 is as good as random guessing.\n\n\n\n\n\n\n\n\n\n\n\nMetric\nFormula\n\n\n\n\nPrecision:\n\\(\\frac{TP}{TP+FP}\\)\n\n\nSensitivity or Recall or True Positive Rate (TPR):\n\\(\\frac{TP}{TP+FN}\\)\n\n\nType II Error Rate or False Negative Rate (FNR):\n\\(\\frac{FN}{FN+TP}\\)\n\n\nSepecificity or Selectivity or True Negative Rate (TNR):\n\\(\\frac{TN}{TN+FP}\\)\n\n\nType I Error Rate or False Positive Rate (FPR):\n\\(\\frac{FP}{FP+TN}\\)\n\n\nTotal Error Rate:\n\\(\\frac{FP+FN}{TN+TP+FN+FP}\\)\n\n\nAccuracy:\n\\(\\frac{TP+TN}{TN+TP+FN+FP}\\)"
  },
  {
    "objectID": "dsandml/classificationmetrics/index.html#introduction",
    "href": "dsandml/classificationmetrics/index.html#introduction",
    "title": "Model Evaluation and Fine Tuning: Classification Metrices",
    "section": "",
    "text": "In any classification problem, the goal is to build a model that accurately predicts labels or classes from input data. Once the model is built, it is important to evaluate its performance using a variety of metrics. Some of the most commonly used metrics are the confusion matrix, accuracy, precision, recall, F1 score, and ROC-AUC curve. This post will explain each metric and show how to compute them using real data in Python.\n\n\n\nA confusion matrix is a tabular summary of the performance of a classification algorithm. It shows the number of correct and incorrect predictions broken down by each class.\nFor a binary classification, the confusion matrix looks like this:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef plot_confusion_matrix():\n    \n    matrix_data = np.array([[100, 5], [10, 50]])\n\n    \n    extended_matrix = np.zeros((3, 3))  \n    extended_matrix[:2, :2] = matrix_data  \n\n    mask = np.zeros_like(extended_matrix, dtype=bool)\n    mask[2,:] = True\n    mask[:,2] = True\n\n    # Create a plot\n    fig, ax = plt.subplots(figsize=(8, 5.2))\n\n    fig.patch.set_facecolor('#f4f4f4') \n    ax.set_facecolor('#f4f4f4')\n\n    sns.heatmap(extended_matrix, mask=mask,annot=False, cmap=\"RdYlGn\", cbar=False, ax=ax, linewidths=2, linecolor='black')\n\n    # Add the original confusion matrix values (True Positive, False Negative, etc.)\n    ax.text(0.4, 0.3, 'True Positive (TP)', ha='center', va='center', fontsize=12, color=\"white\")\n    ax.text(1.45, 0.3, 'False Negative (FN)', ha='center', va='center', fontsize=12, color=\"white\")\n    ax.text(1.45, 0.60, '(Type II Error)', ha='center', va='center', fontsize=12, color=\"white\")\n    ax.text(0.45, 1.25, 'False Positive (FP)', ha='center', va='center', fontsize=12, color=\"white\")\n    ax.text(0.45, 1.40, '(Type I Error)', ha='center', va='center', fontsize=12, color=\"white\")\n    ax.text(1.45, 1.4, 'True Negative (TN)', ha='center', va='center', fontsize=12, color=\"red\")\n    ax.text(0.4, -0.1, 'Positive', ha='center', va='center', fontsize=12)\n    ax.text(1.45, -0.1, 'Negative', ha='center', va='center', fontsize=12)\n    ax.text(1, -0.3, 'Predicted Class', ha='center', va='center', fontsize=14)\n\n    # Add Precision and NPV in the bottom row of the confusion matrix\n    ax.text(0.17, 0.2, r'Precision= $\\frac{TP}{TP + FP}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.5, 0.2, r'NPV= $\\frac{TN}{TN + FN}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n\n    # Add Sensitivity and Specificity in the right column of the confusion matrix\n    ax.text(0.83, .95, r'TPR=Sensitivity= $\\frac{TP}{TP + FN}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .89, 'or Recall', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .8, 'False Neg. Rate (FNR)', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .75, r'Type II Error rate= $\\frac{FN}{TP + FN}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .6, r'TNR=Specificity= $\\frac{TN}{TN + FP}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .48, 'False Positive Rate', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .43, r'FPR= $\\frac{FP}{TN + FP}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n    ax.text(0.83, .37, 'Type I Error Rate', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n\n    # Add Accuracy in the bottom-right corner of the extended grid\n    ax.text(0.83, 0.2, r'Accuracy= $\\frac{TP + TN}{TP+TN+FP+FN}$', ha='center', va='center', transform=ax.transAxes, fontsize=12)\n\n    # Titles and labels\n    ax.set_ylabel('Actual Class', fontsize=14)\n    \n\n    # Set tick labels for actual and predicted\n    ax.xaxis.set_ticklabels([' ', ' ', ''], fontsize=12)\n    ax.yaxis.set_ticklabels(['Positive', 'Negative', ''], fontsize=12, rotation=0)\n\n    plt.tight_layout()\n    plt.savefig('conf.png')\n    plt.show()\n\n# Generate the confusion matrix plot\nplot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\nTrue Positive (TP): The model correctly predicted the positive class.\nFalse Positive (FP): The model incorrectly predicted the positive class (also known as a Type I error).\nTrue Negative (TN): The model correctly predicted the negative class.\nFalse Negative (FN): The model incorrectly predicted the negative class (also known as a Type II error).\n\n\n\n\nAccuracy is the ratio of correctly predicted observations to the total observations.\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\nIt is one of the most intuitive metrics, but it can be misleading if the classes are imbalanced.\n\n\n\nPrecision measures the proportion of positive predictions that are actually correct.\n\\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\]\nIt is useful when the cost of a false positive is high, such as in fraud detection.\n\n\n\nRecall measures the proportion of actual positives that are correctly predicted.\n\\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\]\nIt is important in cases where missing a positive is more costly, like in medical diagnoses.\n\n\n\nThe F1 score is the harmonic mean of precision and recall, giving a balanced measure when both metrics are important.\n\\[\n\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\n\n\n\n\nThe ROC-AUC curve helps visualize the performance of a classification model by plotting the true positive rate (recall) against the false positive rate (1 - specificity) at various threshold settings. The AUC (Area Under the Curve) gives a single number that summarizes the performance. A model with an AUC of 1 is perfect, while a model with an AUC of 0.5 is as good as random guessing.\n\n\n\n\n\n\n\n\n\n\n\nMetric\nFormula\n\n\n\n\nPrecision:\n\\(\\frac{TP}{TP+FP}\\)\n\n\nSensitivity or Recall or True Positive Rate (TPR):\n\\(\\frac{TP}{TP+FN}\\)\n\n\nType II Error Rate or False Negative Rate (FNR):\n\\(\\frac{FN}{FN+TP}\\)\n\n\nSepecificity or Selectivity or True Negative Rate (TNR):\n\\(\\frac{TN}{TN+FP}\\)\n\n\nType I Error Rate or False Positive Rate (FPR):\n\\(\\frac{FP}{FP+TN}\\)\n\n\nTotal Error Rate:\n\\(\\frac{FP+FN}{TN+TP+FN+FP}\\)\n\n\nAccuracy:\n\\(\\frac{TP+TN}{TN+TP+FN+FP}\\)"
  },
  {
    "objectID": "dsandml/classificationmetrics/index.html#example-in-python",
    "href": "dsandml/classificationmetrics/index.html#example-in-python",
    "title": "Model Evaluation and Fine Tuning: Classification Metrices",
    "section": "Example in Python",
    "text": "Example in Python\nLet’s use a real dataset and compute these metrics using Python. In python the actual confusion matrix looks like this\n\nWe’ll use the breast cancer dataset from sklearn, which is a binary classification problem where the task is to predict whether a tumor is malignant or benign.\n\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n\n# Load dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train a RandomForest Classifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\ny_pred_proba = clf.predict_proba(X_test)[:, 1]\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n# Plot confusion matrix\nplt.figure(figsize=(8,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Benign', 'Predicted Malignant'], yticklabels=['Actual Benign', 'Actual Malignant'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.gca().set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nNext, Compute Accuracy, Precision, Recall, F1 Score, ROC-AUC\n\ntn = cm[0,0]\nfp = cm[0,1]\nfn = cm[1,0]\ntp = cm[1,1]\naccuracy1 = np.round(accuracy_score(y_test, y_pred),4)\naccuracy2 = np.round(((tp+tn)/(tp+tn+fp+fn)),4)\n\nprecision1 = np.round(precision_score(y_test, y_pred),4)\nprecision2 = np.round(((tp)/(tp+fp)),4)\n\nrecall1 = np.round(recall_score(y_test, y_pred),4)\nrecall2 = np.round(((tp)/(tp+fn)),4)\n\nf1_1 = np.round(f1_score(y_test, y_pred),4) \nf1_2 = np.round((2*precision2*recall2)/(precision2+recall2),4)\n\nroc_auc = roc_auc_score(y_test, y_pred_proba)\n\nprint('Accuracy Using Library = {}, and Accuracy Using Formula = {}'.format(accuracy1,accuracy2))\nprint('Precision Using Library = {}, and Precision Using Formula = {}'.format(precision1,precision2))\nprint('Recall Using Library = {}, and Recall Using Formula = {}'.format(recall1,recall2))\nprint('F1 Score Using Library = {}, and F1 Score Using Formula = {}'.format(f1_1,f1_2))\nprint(f'ROC-AUC score={roc_auc:.4f}')\n\nAccuracy Using Library = 0.9708, and Accuracy Using Formula = 0.9708\nPrecision Using Library = 0.964, and Precision Using Formula = 0.964\nRecall Using Library = 0.9907, and Recall Using Formula = 0.9907\nF1 Score Using Library = 0.9772, and F1 Score Using Formula = 0.9772\nROC-AUC score=0.9968\n\n\nPlot ROC curve. ROC curve is found from plotting True Positive Rate (TPRs) against False Positive Rate (FPRs) for different cutoffs of probability values. To plot the ROC curve using the built-in function from sklearn we do the following:\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nplt.figure(figsize=(8,5))\nplt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], color='red', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.gca().set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nTo build our own\n\ncutoff_values = np.arange(0,0.99,0.001)\ntrue_pos_rates = []\nfalse_pos_rates = []\n\nfor cutoff in cutoff_values:\n    prediction = 1*(clf.predict_proba(X_test)[:,1] &gt;= cutoff)\n    conf_matrix = confusion_matrix(y_test, prediction)\n    tn = conf_matrix[0,0]\n    fp = conf_matrix[0,1]\n    fn = conf_matrix[1,0]\n    tp = conf_matrix[1,1]\n\n    true_pos_rates.append(tp/(tp+fn))\n    false_pos_rates.append(fp/(fp+tn))\n\nplt.figure(figsize=(8,5))\nplt.plot(false_pos_rates, true_pos_rates, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], color='red', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.gca().set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nNext, precision-recall score\n\ncutoff_values = np.arange(0,0.99,0.001)\nprecisions = []\nrecalls = []\n\nfor cutoff in cutoff_values:\n    prediction = 1*(clf.predict_proba(X_test)[:,1] &gt;= cutoff)\n\n    precisions.append(precision_score(y_test, prediction))\n    recalls.append(recall_score(y_test, prediction))\n\nplt.figure(figsize=(8,5))\nplt.plot(recalls, precisions, color='blue')\nplt.plot([0, 1], [0, 1], color='red', linestyle='--')\nplt.xlabel('Recalls')\nplt.ylabel('Precisions')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.gca().set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nExplanation of Results\n\nConfusion Matrix: The heatmap shows the number of true positives, false positives, true negatives, and false negatives, which gives a detailed insight into the model’s performance.\nAccuracy: This value tells us the overall correctness of the model. It may not always be reliable if the data is imbalanced.\nPrecision: A higher precision indicates fewer false positives. In this dataset, it tells us how well the model identifies malignant tumors correctly.\nRecall: A higher recall indicates fewer false negatives. This is particularly important in medical settings where missing a positive case (malignant tumor) can be dangerous.\nF1 Score: The F1 score balances precision and recall, especially when the class distribution is uneven.\nROC-AUC Curve: The ROC curve gives a visualization of the trade-off between sensitivity and specificity. The AUC gives a single number summarizing the overall ability of the model to distinguish between classes."
  },
  {
    "objectID": "dsandml/classificationmetrics/index.html#when-to-use-each-metric",
    "href": "dsandml/classificationmetrics/index.html#when-to-use-each-metric",
    "title": "Model Evaluation and Fine Tuning: Classification Metrices",
    "section": "When to Use Each Metric?",
    "text": "When to Use Each Metric?\nIt’s important to explain when to prioritize specific metrics based on the problem context:\n\nAccuracy: Use when classes are balanced and misclassification costs are similar across classes. Avoid if the dataset is imbalanced.\nPrecision: Useful when false positives are costly. For example, in spam detection, it’s better to have a few missed spams than to mark important emails as spam.\nRecall: Use when false negatives are costly. In medical diagnoses (e.g., cancer detection), it’s crucial to minimize missed positive cases (false negatives).\nF1 Score: Best when you need a balance between precision and recall, especially with imbalanced classes.\nROC-AUC: Useful for evaluating how well your model separates the two classes across various thresholds. Works well when you want an overall measure of performance."
  },
  {
    "objectID": "dsandml/classificationmetrics/index.html#threshold-tuning-and-decision-making",
    "href": "dsandml/classificationmetrics/index.html#threshold-tuning-and-decision-making",
    "title": "Model Evaluation and Fine Tuning: Classification Metrices",
    "section": "Threshold Tuning and Decision Making",
    "text": "Threshold Tuning and Decision Making\n\nFor classification problems, the decision threshold is crucial, especially for metrics like ROC-AUC. Often, models use a default threshold of 0.5 to classify whether an instance belongs to the positive class or not, but you can adjust this threshold to prioritize recall over precision or vice versa. You could add a section showing how adjusting the threshold can change model performance.\n\nHere’s an additional Python example showing how to adjust thresholds:\n\n# Adjust threshold\nthreshold = 0.4\ny_pred_thresholded = (y_pred_proba &gt;= threshold).astype(int)\n\n# Recompute metrics\nnew_precision = precision_score(y_test, y_pred_thresholded)\nnew_recall = recall_score(y_test, y_pred_thresholded)\nnew_f1 = f1_score(y_test, y_pred_thresholded)\n\nprint(f'New Precision: {new_precision:.4f}')\nprint(f'New Recall: {new_recall:.4f}')\nprint(f'New F1 Score: {new_f1:.4f}')\n\nNew Precision: 0.9554\nNew Recall: 0.9907\nNew F1 Score: 0.9727\n\n\nThis shows that the default threshold isn’t set in stone, and adjusting it can significantly affect precision, recall, and other metrics."
  },
  {
    "objectID": "dsandml/classificationmetrics/index.html#class-imbalance-and-its-effect-on-metrics",
    "href": "dsandml/classificationmetrics/index.html#class-imbalance-and-its-effect-on-metrics",
    "title": "Model Evaluation and Fine Tuning: Classification Metrices",
    "section": "Class Imbalance and Its Effect on Metrics",
    "text": "Class Imbalance and Its Effect on Metrics\n\nClass imbalance can skew metrics like accuracy. A discussion on how to handle imbalance through methods such as resampling (oversampling/undersampling) or using techniques like SMOTE (Synthetic Minority Over-sampling Technique) could provide further depth.\n\nFor example:\n\nfrom imblearn.over_sampling import SMOTE\n\n# Handling class imbalance using SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n# Retrain the model on resampled data\nclf_resampled = RandomForestClassifier(random_state=42)\nclf_resampled.fit(X_resampled, y_resampled)\n\n# Predictions and metrics\ny_pred_resampled = clf_resampled.predict(X_test)\naccuracy_resampled = accuracy_score(y_test, y_pred_resampled)\nprecision_resampled = precision_score(y_test, y_pred_resampled)\nrecall_resampled = recall_score(y_test, y_pred_resampled)\n\nprint(f'Resampled Accuracy: {accuracy_resampled:.4f}')\nprint(f'Resampled Precision: {precision_resampled:.4f}')\nprint(f'Resampled Recall: {recall_resampled:.4f}')\n\nResampled Accuracy: 0.9708\nResampled Precision: 0.9813\nResampled Recall: 0.9722\n\n\nThis demonstrates the effect of handling class imbalance on model performance."
  },
  {
    "objectID": "dsandml/classificationmetrics/index.html#precision-recall-curve",
    "href": "dsandml/classificationmetrics/index.html#precision-recall-curve",
    "title": "Model Evaluation and Fine Tuning: Classification Metrices",
    "section": "Precision-Recall Curve",
    "text": "Precision-Recall Curve\n\nWhile the ROC curve is useful, the Precision-Recall (PR) curve is often more informative when dealing with imbalanced datasets because it focuses on the performance of the positive class. Including a section on this can enhance the evaluation process.\n\n\nfrom sklearn.metrics import precision_recall_curve\n\n# Compute Precision-Recall curve\nprecision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n\n# Plot Precision-Recall curve\nplt.figure(figsize=(8, 5))\nplt.plot(recall_vals, precision_vals, marker='.')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.gca().set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nThe PR curve shows how precision and recall change with different classification thresholds."
  },
  {
    "objectID": "dsandml/classificationmetrics/index.html#kappa-score-and-matthews-correlation-coefficient-mcc",
    "href": "dsandml/classificationmetrics/index.html#kappa-score-and-matthews-correlation-coefficient-mcc",
    "title": "Model Evaluation and Fine Tuning: Classification Metrices",
    "section": "Kappa Score and Matthews Correlation Coefficient (MCC)",
    "text": "Kappa Score and Matthews Correlation Coefficient (MCC)\n\nCohen’s Kappa measures agreement between observed accuracy and expected accuracy.\nMatthews Correlation Coefficient (MCC) provides a balanced metric even when classes are imbalanced. It considers true and false positives and negatives, giving a correlation-like score between predictions and actuals.\n\n\nfrom sklearn.metrics import cohen_kappa_score, matthews_corrcoef\n\nkappa = cohen_kappa_score(y_test, y_pred)\nmcc = matthews_corrcoef(y_test, y_pred)\n\nprint(f'Cohen\\'s Kappa: {kappa:.4f}')\nprint(f'MCC: {mcc:.4f}')\n\nCohen's Kappa: 0.9365\nMCC: 0.9372"
  },
  {
    "objectID": "dsandml/classificationmetrics/index.html#references",
    "href": "dsandml/classificationmetrics/index.html#references",
    "title": "Model Evaluation and Fine Tuning: Classification Metrices",
    "section": "References",
    "text": "References\n\nScikit-learn Documentation\nPrecision-Recall vs ROC Curves article by Sebastian Raschka\n\nF1 Score Explained towardsdatascience.com blog post\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/datacollection/index.html",
    "href": "dsandml/datacollection/index.html",
    "title": "Data collection through Webscraping",
    "section": "",
    "text": "Collecting data and preparing it for a project is one of the most important tasks in any data science or machine learning project. There are many sources from where we can collect data for a project, such as\n\nConnecting to a SQL database server\n\nData Source Websites such as Kaggle, Google Dataset Search, UCI Machine Learning Repo etc\n\nWeb Scraping with Beautiful Soup\nUsing Python API"
  },
  {
    "objectID": "dsandml/datacollection/index.html#introduction",
    "href": "dsandml/datacollection/index.html#introduction",
    "title": "Data collection through Webscraping",
    "section": "",
    "text": "Collecting data and preparing it for a project is one of the most important tasks in any data science or machine learning project. There are many sources from where we can collect data for a project, such as\n\nConnecting to a SQL database server\n\nData Source Websites such as Kaggle, Google Dataset Search, UCI Machine Learning Repo etc\n\nWeb Scraping with Beautiful Soup\nUsing Python API"
  },
  {
    "objectID": "dsandml/datacollection/index.html#data-source-websites",
    "href": "dsandml/datacollection/index.html#data-source-websites",
    "title": "Data collection through Webscraping",
    "section": "Data Source Websites",
    "text": "Data Source Websites\nData source websites mainly falls into two categories such as data repositories and data science competitions. There are many such websites.\n\nThe UCI Machine Learning Repository\n\nThe Harvard Dataverse\nThe Mendeley Data Repository\nThe 538\nThe New Yourk Times\n\nThe International Data Analysis Olympiad\nKaggle Competition\n\nExample of collecting data from UCI Machine Learning Repository\n\nfrom ucimlrepo import fetch_ucirepo \n  \n# fetch dataset \niris = fetch_ucirepo(id=53) \n  \n# data (as pandas dataframes) \nX = iris.data.features \ny = iris.data.targets \n  \n# metadata \nprint(iris.metadata) \n  \n# variable information \nprint(iris.variables) \n\n{'uci_id': 53, 'name': 'Iris', 'repository_url': 'https://archive.ics.uci.edu/dataset/53/iris', 'data_url': 'https://archive.ics.uci.edu/static/public/53/data.csv', 'abstract': 'A small classic dataset from Fisher, 1936. One of the earliest known datasets used for evaluating classification methods.\\n', 'area': 'Biology', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 150, 'num_features': 4, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1936, 'last_updated': 'Tue Sep 12 2023', 'dataset_doi': '10.24432/C56C76', 'creators': ['R. A. Fisher'], 'intro_paper': {'ID': 191, 'type': 'NATIVE', 'title': 'The Iris data set: In search of the source of virginica', 'authors': 'A. Unwin, K. Kleinman', 'venue': 'Significance, 2021', 'year': 2021, 'journal': 'Significance, 2021', 'DOI': '1740-9713.01589', 'URL': 'https://www.semanticscholar.org/paper/4599862ea877863669a6a8e63a3c707a787d5d7e', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'This is one of the earliest datasets used in the literature on classification methods and widely used in statistics and machine learning.  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are not linearly separable from each other.\\n\\nPredicted attribute: class of iris plant.\\n\\nThis is an exceedingly simple domain.\\n\\nThis data differs from the data presented in Fishers article (identified by Steve Chadwick,  spchadwick@espeedaz.net ).  The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features.  ', 'purpose': 'N/A', 'funded_by': None, 'instances_represent': 'Each instance is a plant', 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': None, 'citation': None}}\n           name     role         type demographic  \\\n0  sepal length  Feature   Continuous        None   \n1   sepal width  Feature   Continuous        None   \n2  petal length  Feature   Continuous        None   \n3   petal width  Feature   Continuous        None   \n4         class   Target  Categorical        None   \n\n                                         description units missing_values  \n0                                               None    cm             no  \n1                                               None    cm             no  \n2                                               None    cm             no  \n3                                               None    cm             no  \n4  class of iris plant: Iris Setosa, Iris Versico...  None             no  \n\n\nyou may need to install the UCI Machine Learning Repository as a package using pip.\npip install ucimlrepo\n\nX.head()\n\n\n\n\n\n\n\n\nsepal length\nsepal width\npetal length\npetal width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2"
  },
  {
    "objectID": "dsandml/datacollection/index.html#web-scraping",
    "href": "dsandml/datacollection/index.html#web-scraping",
    "title": "Data collection through Webscraping",
    "section": "Web Scraping",
    "text": "Web Scraping\nWe scrapping is another way of collecting the data for the research if the data is not available in any repositiory. We can collect the data from a website using a library called BeautifulSoup if the website has permision for other people to collect data from the website.\n\nimport bs4                      # library for BeautifulSoup\nfrom bs4 import BeautifulSoup   # import the BeautifulSoup object\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom seaborn import set_style\nset_style(\"whitegrid\")\n\nNow let’s make a html object using BeautifulSoup. Let’s say we have a html website that looks like below\n\nhtml_doc=\"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;title&gt;My Dummy HTML Document&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Welcome to My Dummy HTML Document&lt;/h1&gt;\n    &lt;p&gt;This is a paragraph in my dummy HTML document.&lt;/p&gt;\n    &lt;a href=\"https://mrislambd.github.io/blog\" class=\"blog\" id=\"blog\"&gt; Blog &lt;/a&gt;\n    &lt;a href=\"htpps://mrislambd.github.io/research\" class=\"research\" id=\"research\"&gt; Research &lt;/a&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\nNow we want to grab information from the dummy html documnet above.\n\nsoup=BeautifulSoup(html_doc, features='html.parser')\n\nNow that we have the object soup we can walk through each elements in this object. For example, if we want to grab the title element,\n\nsoup.html.head.title\n\n&lt;title&gt;My Dummy HTML Document&lt;/title&gt;\n\n\nSince the html document has only one title, therefore, we can simply use the following command\n\nsoup.title \n\n&lt;title&gt;My Dummy HTML Document&lt;/title&gt;\n\n\nor this command to get the text only\n\nsoup.title.text\n\n'My Dummy HTML Document'\n\n\nThis soup object is like a family tree. It has parents, children, greatgrand parents etc.\n\nsoup.title.parent\n\n&lt;head&gt;\n&lt;title&gt;My Dummy HTML Document&lt;/title&gt;\n&lt;/head&gt;\n\n\nNow to grab an attribute from the soup object we can use\n\nsoup.a\n\n&lt;a class=\"blog\" href=\"https://mrislambd.github.io/blog\" id=\"blog\"&gt; Blog &lt;/a&gt;\n\n\nor any particular thing from the attribute\n\nsoup.a['class']\n\n['blog']\n\n\nWe can also find multiple attribute of the same kind\n\nsoup.findAll('a')\n\n[&lt;a class=\"blog\" href=\"https://mrislambd.github.io/blog\" id=\"blog\"&gt; Blog &lt;/a&gt;,\n &lt;a class=\"research\" href=\"htpps://mrislambd.github.io/research\" id=\"research\"&gt; Research &lt;/a&gt;]\n\n\nThen if we want any particular object from all a attribute\n\nsoup.findAll('a')[0]['id']\n\n'blog'\n\n\nFor any p tag\n\nsoup.p.text \n\n'This is a paragraph in my dummy HTML document.'\n\n\nSimilarly, if we want to grab all the hrefs from the a tags\n\n[h['href'] for h in soup.findAll('a')]\n\n['https://mrislambd.github.io/blog', 'htpps://mrislambd.github.io/research']"
  },
  {
    "objectID": "dsandml/datacollection/index.html#example-of-webscraping-from-a-real-website",
    "href": "dsandml/datacollection/index.html#example-of-webscraping-from-a-real-website",
    "title": "Data collection through Webscraping",
    "section": "Example of Webscraping from a real website",
    "text": "Example of Webscraping from a real website\nIn this example we want to obtain some information from NVIDIA Graduate Fellowship Program. Before accessing this website we need to know if we have permision to access their data through webscraping.\n\nimport requests\nresponse = requests.get(url=\"https://research.nvidia.com/graduate-fellowships/archive\")\nresponse.status_code\n\n200\n\n\nThe status_code \\(200\\) ensures that we have enough permision to acccess their website data. However, if we obtain status_code of \\(403, 400,\\) or \\(500\\) then we do not permision or a bad request. For more about the status codes click here.\n\nsoup = BeautifulSoup(response.text, 'html.parser')\n\nWe want to make an analysis based on the institution of the past graduate fellows. Insepecting the elements in this website we see that the div those have class=\"archive-group\" contains the information of the past graduate fellows.\n\npf = soup.find_all(\"div\", class_=\"archive-group\")\n\nand the first element of this pf contains the information of the graduate fellows in the year of 2021.\n\npf[0]\n\n&lt;div class=\"archive-group\"&gt;\n&lt;h4 class=\"archive-group__title\"&gt;2021 Grad Fellows&lt;/h4&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Alexander Sax&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;University of California, Berkeley&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Hanrui Wang&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Massachusetts Institute of Technology&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Ji Lin&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Massachusetts Institute of Technology&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Krishna Murthy Jatavallabhula&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;University of Montreal&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Rohan Sawhney&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Carnegie Mellon University&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Sana Damani&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Georgia Institute of Technology&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Thierry Tambe&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Harvard University&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Ye Yuan&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Carnegie Mellon University&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Yunzhu Li&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Massachusetts Institute of Technology&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Zhiqin Chen&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Simon Fraser University&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;/div&gt;\n\n\nNow let’s make a pandas dataframe using the information in this page. We can make an use of the output from the above chunk. To grab the year, we see that archive-group__title class with a h4 tag contains the year for all years. With strip=True, the text is cleaned by removing extra whitespace from the beginning and end. We need the first element so a split()[0] will do the job. Then we make another group called fellows that contains the fellows in a certian year by using the div and class\"views-row\". Once the new group created, we then iterate through this group to extract their names and corresponding institutions.\n\ndata=[]\n\nfor group in pf:\n    year = group.find(\n        \"h4\",class_=\"archive-group__title\"\n        ).get_text(strip=True).split()[0]\n\n    fellows = group.find_all(\"div\", class_=\"views-row\")\n    for fellow in fellows:\n        name = fellow.find(\n            \"div\", class_=\"views-field-title\"\n            ).get_text(strip=True) \n        institute = fellow.find(\n            \"div\", class_=\"views-field-field-grad-fellow-institution\"\n            ).get_text(strip=True)\n\n        data.append({\"Name\": name, \"Year\": year, \"Institute\": institute})\n\ndata=pd.DataFrame(data)\ndata.head()\n\n\n\n\n\n\n\n\nName\nYear\nInstitute\n\n\n\n\n0\nAlexander Sax\n2021\nUniversity of California, Berkeley\n\n\n1\nHanrui Wang\n2021\nMassachusetts Institute of Technology\n\n\n2\nJi Lin\n2021\nMassachusetts Institute of Technology\n\n\n3\nKrishna Murthy Jatavallabhula\n2021\nUniversity of Montreal\n\n\n4\nRohan Sawhney\n2021\nCarnegie Mellon University\n\n\n\n\n\n\n\nNow let’s perform some Exploratory Data Analysis (EDA). First, we analyze the unique values and distributions.\n\n# Count the number of fellows each year\nyear_counts = data['Year'].value_counts().sort_values(ascending=False)\n# Create a DataFrame where years are columns and counts are values in the next row\nyear_data = {\n    'Year': year_counts.index,\n    'Count': year_counts.values\n}\n# Create the DataFrame\nyear_data_counts = pd.DataFrame(year_data)\n\n# Transpose the DataFrame and reset index to get years as columns\nyear_data_counts = year_data_counts.set_index('Year').T\n\n# Display the DataFrame\nprint(year_data_counts)\n\nYear   2006  2018  2017  2007  2013  2012  2011  2008  2019  2021  2003  2009  \\\nCount    12    11    11    11    11    11    11    10    10    10    10    10   \n\nYear   2010  2005  2015  2004  2016  2002  2020  2014  \nCount     9     8     7     7     6     6     5     5  \n\n\nNext we see that most represented universities\n\nuniversity_counts = data['Institute'].value_counts()\nprint(university_counts.head(10))  # Display the top 10 universities\n\nInstitute\nStanford University                          24\nMassachusetts Institute of Technology        15\nUniversity of California, Berkeley           14\nCarnegie Mellon University                   13\nUniversity of Utah                           10\nUniversity of Washington                      9\nUniversity of Illinois, Urbana-Champaign      9\nUniversity of California, Davis               8\nGeorgia Institute of Technology               8\nUniversity of North Carolina, Chapel Hill     6\nName: count, dtype: int64\n\n\nTo visualize the award distributions per year,\n\nplt.figure(figsize=(9,5))\nsns.countplot(x='Year', data=data, order=sorted(data['Year'].unique()))\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.title('Number of Fellows Per Year')\nplt.show()\n\n\n\n\n\n\n\n\nTop 10 universities visualization\n\nplt.figure(figsize=(6,4))\ntop_universities = data['Institute'].value_counts().head(10)\nsns.barplot(y=top_universities.index, x=top_universities.values)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.title('Top 10 Universities by Number of Fellows')\nplt.xlabel('Number of Fellows')\nplt.ylabel('University')\nplt.show()\n\n\n\n\n\n\n\n\nTrend over time\n\nplt.figure(figsize=(9,5))\ndata['Year'] = data['Year'].astype(int)  \nyearly_trend = data.groupby('Year').size()\nyearly_trend.plot(kind='line', marker='o')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.title('Trend of Fellows Over Time')\nplt.xlabel('Year')\nplt.ylabel('Number of Fellows')\nplt.show()\n\n\n\n\n\n\n\n\nThis is just a simple example of collecting data through webscraping. This BeautifulSoup has endless potentials to use in many projects to collect the data that are not publicly available in cleaned or organized form. Thank you for reading."
  },
  {
    "objectID": "dsandml/datacollection/index.html#references",
    "href": "dsandml/datacollection/index.html#references",
    "title": "Data collection through Webscraping",
    "section": "References",
    "text": "References\n\nFisher,R. A.. (1988). Iris. UCI Machine Learning Repository.\n\nShare on\n\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/simplelinreg/index.html",
    "href": "dsandml/simplelinreg/index.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "A simple linear regression in multiple predictors/input variables/features/independent variables/ explanatory variables/regressors/ covariates (many names) often takes the form\n\n\\[\ny=f(\\mathbf{x})+\\epsilon =\\mathbf{\\beta}\\mathbf{x}+\\epsilon\n\\]\n\nwhere \\(\\mathbf{\\beta} \\in \\mathbb{R}^d\\) are regression parameters or constant values that we aim to estimate and \\(\\epsilon \\sim \\mathcal{N}(0,1)\\) is a normally distributed error term independent of \\(x\\) or also called the white noise.\n\nIn this case, the model:\n\\[\ny=f(x)+\\epsilon=\\beta_0+\\beta_1 x+\\epsilon\n\\]\n\nTherefore, in our model we need to estimate the parameters \\(\\beta_0,\\beta_1\\). The true relationship between the explanatory variables and the dependent variable is \\(y=f(x)\\). But our model is \\(y=f(x)+\\epsilon\\). Here, this \\(f(x)\\) is the working model with the data. In other words, \\(\\hat{y}=f(x)=\\hat{\\beta}_0+\\hat{\\beta}_1 x\\). Therefore, there should be some error in the model prediction which we are calling \\(\\epsilon=\\|y-\\hat{y}\\|\\) where \\(y\\) is the true value and \\(\\hat{y}\\) is the predicted value. This error term is normally distributed with mean 0 and variance 1. To get the best estimate of the parameters \\(\\beta_0,\\beta_1\\) we can minimize the error term as much as possible. So, we define the residual sum of squares (RSS) as:\n\n\\[\\begin{align}\nRSS &=\\epsilon_1^2+\\epsilon_2^2+\\cdots+\\epsilon_{10}^2\\\\\n&= \\sum_{i=1}^{10}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)^2\\\\\n\\hat{\\mathcal{l}}(\\bar{\\beta})&=\\sum_{i=1}^{10}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)^2\\\\\n\\end{align}\\]\nUsing multivariate calculus we see\n\\[\\begin{align}\n    \\frac{\\partial l}{\\partial \\beta_0}&=\\sum_{i=1}^{10} 2(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(-1)\\\\\n    \\frac{\\partial l}{\\partial \\beta_1}&= \\sum_{i=1}^{10} 2(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(-x_i)\n\\end{align}\\]\nSetting the partial derivatives to zero we solve for \\(\\hat{\\beta_0},\\hat{\\beta_1}\\) as follows\n\\[\\begin{align*}\n    \\frac{\\partial l}{\\partial \\beta_0}&=0\\\\\n    \\implies \\sum_{i=1}^{10} y_i-10 \\hat{\\beta_0}-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i\\right)&=0\\\\\n    \\implies \\hat{\\beta_0}&=\\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\end{align*}\\]\nand,\n\\[\\begin{align*}\n    \\frac{\\partial l}{\\partial \\beta_1}&=0\\\\\n    \\implies \\sum_{i=1}^{10} 2(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(-x_i)&=0\\\\\n    \\implies \\sum_{i=1}^{10} (y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(x_i)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\hat{\\beta_0}\\left(\\sum_{i=1}^{10} x_i\\right)-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\left(\\bar{y}-\\hat{\\beta_1}\\bar{x}\\right)\\left(\\sum_{i=1}^{10} x_i\\right)-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right)+\\hat{\\beta_1}\\bar{x}\\left(\\sum_{i=1}^{10} x_i\\right)-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right) -\\hat{\\beta_1}\\left(\\sum_{i=1}^{10}x_i^2-\\bar{x}\\sum_{i=1}^{10}x_i\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right) -\\hat{\\beta_1}\\left(\\sum_{i=1}^{10}x_i^2-10\\bar{x}^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right) -\\hat{\\beta_1}\\left(\\sum_{i=1}^{10}x_i^2-2\\times 10\\times \\bar{x}^2+10\\bar{x}^2\\right)&=0\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10} x_iy_i-10\\bar{x}\\bar{y}}{\\sum_{i=1}^{10}x_i^2-2\\times 10\\times \\bar{x}^2+10\\bar{x}^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10} x_iy_i -10\\bar{x}\\bar{y}-10\\bar{x}\\bar{y}+10\\bar{x}\\bar{y}}{\\sum_{i=1}^{10}x_i^2-2\\bar{x}\\times 10\\times\\frac{1}{10}\\sum_{i=1}^{10}x_i +10\\bar{x}^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right)-\\bar{x}\\left(\\sum_{i=1}^{10} y_i\\right)+10\\bar{x}\\bar{y}}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10}\\left(x_iy_i-x_i\\bar{y}-\\bar{x}y_i+\\bar{x}\\bar{y}\\right)}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\\\\\n\\end{align*}\\]\nTherefore, we have the following\n\\[\\begin{align*}\n     \\hat{\\beta_0}&=\\bar{y}-\\hat{\\beta_1}\\bar{x}\\\\\n     \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\n\\end{align*}\\]\nSimple Linear Regression slr is applicable for a single feature data set with contineous response variable.\n\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.linear_model import LinearRegression\n\n\n\n\nLinearity: The relationship between the feature set and the target variable has to be linear.\n\nHomoscedasticity: The variance of the residuals has to be constant.\n\nIndependence: All the observations are independent of each other.\n\nNormality: The distribution of the dependent variable \\(y\\) has to be normal.\n\n\n\n\nTo implement the algorithm, we need some synthetic data. To generate the synthetic data we use the linear equation \\(y(x)=2x+\\frac{1}{2}+\\xi\\) where \\(\\xi\\sim \\mathbf{N}(0,1)\\)\n\nX=np.random.random(100)\ny=2*X+0.5+np.random.randn(100)\n\nNote that we used two random number generators, np.random.random(n) and np.random.randn(n). The first one generates \\(n\\) random numbers of values from the range (0,1) and the second one generates values from the standard normal distribution with mean 0 and variance or standard deviation 1.\n\nplt.figure(figsize=(9,6))\nplt.scatter(X,y)\nplt.xlabel('$X$')\nplt.ylabel('y')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe want to fit a simple linear regression to the above data.\n\nslr=LinearRegression()\n\nNow to fit our data \\(X\\) and \\(y\\) we need to reshape the input variable. Because if we look at \\(X\\),\n\nX\n\narray([0.93528634, 0.26240381, 0.82384203, 0.86476408, 0.72637977,\n       0.54019726, 0.69040792, 0.17952609, 0.88594992, 0.73228639,\n       0.98426734, 0.53706925, 0.69608751, 0.72924121, 0.60035726,\n       0.28674558, 0.36866188, 0.45365048, 0.03612779, 0.9531276 ,\n       0.85982339, 0.76314655, 0.33068622, 0.31296773, 0.95376167,\n       0.4646125 , 0.4004289 , 0.994043  , 0.29408838, 0.67065374,\n       0.90744776, 0.25233995, 0.52881289, 0.46089207, 0.68134486,\n       0.53896879, 0.67813675, 0.64030128, 0.02083313, 0.13489822,\n       0.44778866, 0.09707126, 0.30994168, 0.11690619, 0.60102956,\n       0.41080095, 0.07980584, 0.3452269 , 0.48137141, 0.40464879,\n       0.82541363, 0.26015365, 0.67562027, 0.53942066, 0.00680632,\n       0.20340156, 0.69073198, 0.79983868, 0.85215857, 0.63729709,\n       0.56774736, 0.61766119, 0.91633813, 0.09266206, 0.24464372,\n       0.19896896, 0.78304676, 0.29707748, 0.29073494, 0.1748536 ,\n       0.14192913, 0.29974626, 0.75621903, 0.62437501, 0.69728233,\n       0.71681963, 0.34691551, 0.20993083, 0.16956837, 0.48612103,\n       0.96974364, 0.50447249, 0.40481321, 0.90606233, 0.73214932,\n       0.6033985 , 0.74859313, 0.43385089, 0.86794505, 0.42021071,\n       0.41074729, 0.99395604, 0.61613797, 0.48458593, 0.92947484,\n       0.30557625, 0.00368347, 0.6395773 , 0.13181331, 0.67659732])\n\n\nIt is a one-dimensional array/vector but the slr object accepts input variable as matrix or two-dimensional format.\n\nX=X.reshape(-1,1)\nX[:10]\n\narray([[0.93528634],\n       [0.26240381],\n       [0.82384203],\n       [0.86476408],\n       [0.72637977],\n       [0.54019726],\n       [0.69040792],\n       [0.17952609],\n       [0.88594992],\n       [0.73228639]])\n\n\nNow we fit the data to our model\n\nslr.fit(X,y)\nslr.predict([[2],[3]])\n\narray([5.14636359, 7.5940391 ])\n\n\nWe have our \\(X=2,3\\) and the corresponding \\(y\\) values are from the above cell output, which are pretty close to the model \\(y=2x+\\frac{1}{2}\\).\n\nintercept = round(slr.intercept_,4)\nslope = slr.coef_\n\nNow our model parameters are: intercept \\(\\beta_0=\\) 0.251 and slope \\(\\beta_1=\\) array([2.44767552]).\n\nplt.figure(figsize=(9,6))\nplt.scatter(X,y, alpha=0.7,label=\"Sample Data\")\nplt.plot(np.linspace(0,1,100),\n    slr.predict(np.linspace(0,1,100).reshape(-1,1)),\n    'k',\n    label='Model $\\hat{f}$'\n)\nplt.plot(np.linspace(0,1,100),\n    2*np.linspace(0,1,100)+0.5,\n    'r--',\n    label='$f$'\n)\nplt.xlabel('$X$')\nplt.ylabel('y')\nplt.legend(fontsize=10)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nSo the model fits the data almost perfectly.\nUp next multiple linear regression.\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/simplelinreg/index.html#assumptions-of-linear-regressions",
    "href": "dsandml/simplelinreg/index.html#assumptions-of-linear-regressions",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Linearity: The relationship between the feature set and the target variable has to be linear.\n\nHomoscedasticity: The variance of the residuals has to be constant.\n\nIndependence: All the observations are independent of each other.\n\nNormality: The distribution of the dependent variable \\(y\\) has to be normal."
  },
  {
    "objectID": "dsandml/simplelinreg/index.html#synthetic-data",
    "href": "dsandml/simplelinreg/index.html#synthetic-data",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "To implement the algorithm, we need some synthetic data. To generate the synthetic data we use the linear equation \\(y(x)=2x+\\frac{1}{2}+\\xi\\) where \\(\\xi\\sim \\mathbf{N}(0,1)\\)\n\nX=np.random.random(100)\ny=2*X+0.5+np.random.randn(100)\n\nNote that we used two random number generators, np.random.random(n) and np.random.randn(n). The first one generates \\(n\\) random numbers of values from the range (0,1) and the second one generates values from the standard normal distribution with mean 0 and variance or standard deviation 1.\n\nplt.figure(figsize=(9,6))\nplt.scatter(X,y)\nplt.xlabel('$X$')\nplt.ylabel('y')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()"
  },
  {
    "objectID": "dsandml/simplelinreg/index.html#model",
    "href": "dsandml/simplelinreg/index.html#model",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "We want to fit a simple linear regression to the above data.\n\nslr=LinearRegression()\n\nNow to fit our data \\(X\\) and \\(y\\) we need to reshape the input variable. Because if we look at \\(X\\),\n\nX\n\narray([0.93528634, 0.26240381, 0.82384203, 0.86476408, 0.72637977,\n       0.54019726, 0.69040792, 0.17952609, 0.88594992, 0.73228639,\n       0.98426734, 0.53706925, 0.69608751, 0.72924121, 0.60035726,\n       0.28674558, 0.36866188, 0.45365048, 0.03612779, 0.9531276 ,\n       0.85982339, 0.76314655, 0.33068622, 0.31296773, 0.95376167,\n       0.4646125 , 0.4004289 , 0.994043  , 0.29408838, 0.67065374,\n       0.90744776, 0.25233995, 0.52881289, 0.46089207, 0.68134486,\n       0.53896879, 0.67813675, 0.64030128, 0.02083313, 0.13489822,\n       0.44778866, 0.09707126, 0.30994168, 0.11690619, 0.60102956,\n       0.41080095, 0.07980584, 0.3452269 , 0.48137141, 0.40464879,\n       0.82541363, 0.26015365, 0.67562027, 0.53942066, 0.00680632,\n       0.20340156, 0.69073198, 0.79983868, 0.85215857, 0.63729709,\n       0.56774736, 0.61766119, 0.91633813, 0.09266206, 0.24464372,\n       0.19896896, 0.78304676, 0.29707748, 0.29073494, 0.1748536 ,\n       0.14192913, 0.29974626, 0.75621903, 0.62437501, 0.69728233,\n       0.71681963, 0.34691551, 0.20993083, 0.16956837, 0.48612103,\n       0.96974364, 0.50447249, 0.40481321, 0.90606233, 0.73214932,\n       0.6033985 , 0.74859313, 0.43385089, 0.86794505, 0.42021071,\n       0.41074729, 0.99395604, 0.61613797, 0.48458593, 0.92947484,\n       0.30557625, 0.00368347, 0.6395773 , 0.13181331, 0.67659732])\n\n\nIt is a one-dimensional array/vector but the slr object accepts input variable as matrix or two-dimensional format.\n\nX=X.reshape(-1,1)\nX[:10]\n\narray([[0.93528634],\n       [0.26240381],\n       [0.82384203],\n       [0.86476408],\n       [0.72637977],\n       [0.54019726],\n       [0.69040792],\n       [0.17952609],\n       [0.88594992],\n       [0.73228639]])\n\n\nNow we fit the data to our model\n\nslr.fit(X,y)\nslr.predict([[2],[3]])\n\narray([5.14636359, 7.5940391 ])\n\n\nWe have our \\(X=2,3\\) and the corresponding \\(y\\) values are from the above cell output, which are pretty close to the model \\(y=2x+\\frac{1}{2}\\).\n\nintercept = round(slr.intercept_,4)\nslope = slr.coef_\n\nNow our model parameters are: intercept \\(\\beta_0=\\) 0.251 and slope \\(\\beta_1=\\) array([2.44767552]).\n\nplt.figure(figsize=(9,6))\nplt.scatter(X,y, alpha=0.7,label=\"Sample Data\")\nplt.plot(np.linspace(0,1,100),\n    slr.predict(np.linspace(0,1,100).reshape(-1,1)),\n    'k',\n    label='Model $\\hat{f}$'\n)\nplt.plot(np.linspace(0,1,100),\n    2*np.linspace(0,1,100)+0.5,\n    'r--',\n    label='$f$'\n)\nplt.xlabel('$X$')\nplt.ylabel('y')\nplt.legend(fontsize=10)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nSo the model fits the data almost perfectly.\nUp next multiple linear regression.\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/polyreg/index.html",
    "href": "dsandml/polyreg/index.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Polynomial regression is an extension of linear regression that captures the relationship between the dependent and independent variables by fitting a polynomial equation. Unlike linear regression, where the model assumes a straight-line relationship, polynomial regression allows for more complex relationships, enabling the model to fit non-linear data more accurately.\n\n\n\nPolynomial regression is a type of regression where the relationship between the independent variable \\(X\\) and the dependent variable \\(y\\) is modeled as an \\(n\\)th degree polynomial. The general form of a polynomial regression model is:\n\\[\ny = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\dots + \\beta_n X^n + \\epsilon\n\\]\nWhere:\n\n\\(y\\) is the predicted output (dependent variable),\n\\(X\\) is the input feature (independent variable),\n\\(\\beta_0, \\beta_1, \\dots, \\beta_n\\) are the coefficients to be learned,\n\\(\\epsilon\\) is the error term (the difference between the actual and predicted values),\n\\(n\\) is the degree of the polynomial.\n\n\nPolynomial regression can model non-linear data by introducing polynomial terms (such as \\(X^2, X^3\\), etc.), but the model is still linear in terms of the coefficients, which is why it is often treated as a type of linear regression.\n\n\n\n\n\nThe objective of polynomial regression, like linear regression, is to minimize the sum of squared errors (SSE) between the observed values \\(y_i\\) and the predicted values \\(\\hat{y}_i\\). This can be done by applying the ordinary least squares (OLS) method.\n\nFor simplicity, let’s assume a second-degree polynomial regression model:\n\\[\n\\hat{y} = \\beta_0 + \\beta_1 X + \\beta_2 X^2\n\\]\nThe error for each data point is the difference between the actual value \\(y_i\\) and the predicted value \\(\\hat{y}_i\\):\n\\[\ne_i = y_i - \\hat{y}_i = y_i - (\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2)\n\\]\nWe aim to minimize the sum of squared errors (SSE):\n\\[\nSSE = \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{m} (y_i - (\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2))^2\n\\]\nWhere \\(m\\) is the number of data points.\nWe can represent this problem in matrix form to generalize for higher-degree polynomials and simplify the calculation:\nLet \\(X\\) represent the design matrix, where each column corresponds to a power of the independent variable \\(X\\):\n\\[\nX =\n\\begin{bmatrix}\n1 & X_1 & X_1^2 \\\\\n1 & X_2 & X_2^2 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & X_m & X_m^2\n\\end{bmatrix}\n\\]\nLet \\(\\beta\\) be the coefficient vector:\n\\[\n\\beta =\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\n\\end{bmatrix}\n\\]\nAnd \\(y\\) be the output vector:\n\\[\ny =\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_m\n\\end{bmatrix}\n\\]\nThe predicted values can be written as:\n\\[\n\\hat{y} = X \\beta\n\\]\nTo find the optimal coefficients \\(\\beta\\), we minimize the SSE, which can be rewritten in matrix form as:\n\\[\nSSE = (y - X\\beta)^T(y - X\\beta)\n\\]\nTo minimize this, we take the derivative of the SSE with respect to \\(\\beta\\) and set it to zero:\n\\[\n\\frac{\\partial}{\\partial \\beta} (y - X\\beta)^T(y - X\\beta) = -2X^T(y - X\\beta) = 0\n\\]\nSolving for \\(\\beta\\):\n\\[\n\\beta = (X^T X)^{-1} X^T y\n\\]\nThis gives the optimal solution for the coefficients \\(\\beta\\), which can be used to predict the output \\(\\hat{y}\\). The detail proof of this parameter \\(\\hat{\\beta}\\) can be found in the  multiple linear regression  page.\n\n\n\nWe use PolynomialFeatures from Scikit-learn to transform our input data \\(X\\) to include polynomial terms (e.g., \\(X^2, X^3\\), etc.).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nnp.random.seed(0)\nX = 2 - 3 * np.random.normal(0, 1, 100)\ny = X - 2 * (X ** 2) + np.random.normal(-3, 3, 100)\nX = X[:, np.newaxis]\nplt.scatter(X, y, color='blue')\nplt.title(\"Sample Data\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\nX_poly = poly.fit_transform(X)\n\nNow we fit a linear regression model on the transformed polynomial features.\n\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n\ny_pred = model.predict(X_poly)\n\nprint(f\"Coefficients: {model.coef_}\")\nprint(f\"Intercept: {model.intercept_}\")\nprint(f\"Mean Squared Error: {mean_squared_error(y, y_pred)}\")\n\nCoefficients: [ 0.          0.96597113 -2.02225052]\nIntercept: -2.414835667353632\nMean Squared Error: 9.44744195245028\n\n\nFinally, let’s plot the polynomial curve that fits the data.\n\nimport operator\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X, y_pred), key=sort_axis)\nX_sorted, y_pred_sorted = zip(*sorted_zip)\n\n# Plot the polynomial curve\nplt.scatter(X, y, color='blue')\nplt.plot(X_sorted, y_pred_sorted, color='red')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title(\"Polynomial Regression Fit\")\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nWe can evaluate the performance of the model by comparing the mean squared error (MSE) between the actual and predicted values:\n\nmse = mean_squared_error(y, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\nMean Squared Error: 9.44744195245028\n\n\n\n\n\nWe’ll generate some non-linear data and try to fit a polynomial regression model to it.\n\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\nnp.random.seed(0)\nx1 = 2 - 3 * np.random.normal(0, 1, 100)\nx2 = 4*np.random.normal(-2,2,100)\ny = 2+3*x1 -4*x1**2 + 2 * x2 + np.random.normal(-3, 3, 100)\ndf={\n    'x1':x1,\n    'x2':x2,\n    'y':y\n}\ndf = pd.DataFrame(df)\nprint(df.head())\nscatter_matrix(df)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n         x1         x2           y\n0 -3.292157   7.065206  -41.206797\n1  0.799528 -18.782072  -39.440680\n2 -0.936214 -18.163880  -40.343409\n3 -4.722680  -0.244826 -102.906711\n4 -3.602674 -17.384987  -96.574641\n\n\n\n\n\n\n\n\n\nSince it’s clear that the relationships are not linear. So if we fit a linear regression model, it won’t be a good fit.\n\nX = df.drop('y', axis=1)\ny = df.y\nmodel1 = LinearRegression()\nmodel1.fit(X,y)\npred1 = model1.predict(X)\nresidual1 = y - pred1\nsns.scatterplot(residual1)\nplt.axhline(y=0, c='r')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nTherefore, we generate some non linear features from the given data.\n\ndf['x1_squared']=df.x1**2\nscatter_matrix(df)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nFrom this plot, we see that \\(x1\\) is parabolic and \\(x2\\) is linear in relationship with \\(y\\). So, how about a model that combines a linear and quadratic model?\n\nX = df.drop('y', axis=1)\ny = df.y\nmodel2 = LinearRegression()\nmodel2.fit(X,y)\npred2 = model2.predict(X)\nresidual2 = y - pred2\nsns.scatterplot(residual2)\nplt.axhline(y=0, c='r')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nThis one is much better.\n\nPolynomial regression is a powerful technique that extends the basic linear regression model to capture non-linear relationships between variables. By transforming the input data into polynomial terms, the model becomes more flexible, allowing it to better fit data that doesn’t follow a linear pattern.  The mathematical derivation shows that polynomial regression is still linear in terms of its parameters, allowing us to use simple optimization techniques like ordinary least squares (OLS) for parameter estimation.  In Python, using Scikit-learn makes it easy to implement polynomial regression. We can increase the degree of the polynomial to improve model accuracy, but we need to be careful of overfitting, where the model becomes too complex and fits the noise in the data rather than the underlying pattern.\n\nYou may also like"
  },
  {
    "objectID": "dsandml/polyreg/index.html#introduction",
    "href": "dsandml/polyreg/index.html#introduction",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Polynomial regression is an extension of linear regression that captures the relationship between the dependent and independent variables by fitting a polynomial equation. Unlike linear regression, where the model assumes a straight-line relationship, polynomial regression allows for more complex relationships, enabling the model to fit non-linear data more accurately.\n\n\n\nPolynomial regression is a type of regression where the relationship between the independent variable \\(X\\) and the dependent variable \\(y\\) is modeled as an \\(n\\)th degree polynomial. The general form of a polynomial regression model is:\n\\[\ny = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\dots + \\beta_n X^n + \\epsilon\n\\]\nWhere:\n\n\\(y\\) is the predicted output (dependent variable),\n\\(X\\) is the input feature (independent variable),\n\\(\\beta_0, \\beta_1, \\dots, \\beta_n\\) are the coefficients to be learned,\n\\(\\epsilon\\) is the error term (the difference between the actual and predicted values),\n\\(n\\) is the degree of the polynomial.\n\n\nPolynomial regression can model non-linear data by introducing polynomial terms (such as \\(X^2, X^3\\), etc.), but the model is still linear in terms of the coefficients, which is why it is often treated as a type of linear regression.\n\n\n\n\n\nThe objective of polynomial regression, like linear regression, is to minimize the sum of squared errors (SSE) between the observed values \\(y_i\\) and the predicted values \\(\\hat{y}_i\\). This can be done by applying the ordinary least squares (OLS) method.\n\nFor simplicity, let’s assume a second-degree polynomial regression model:\n\\[\n\\hat{y} = \\beta_0 + \\beta_1 X + \\beta_2 X^2\n\\]\nThe error for each data point is the difference between the actual value \\(y_i\\) and the predicted value \\(\\hat{y}_i\\):\n\\[\ne_i = y_i - \\hat{y}_i = y_i - (\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2)\n\\]\nWe aim to minimize the sum of squared errors (SSE):\n\\[\nSSE = \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{m} (y_i - (\\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2))^2\n\\]\nWhere \\(m\\) is the number of data points.\nWe can represent this problem in matrix form to generalize for higher-degree polynomials and simplify the calculation:\nLet \\(X\\) represent the design matrix, where each column corresponds to a power of the independent variable \\(X\\):\n\\[\nX =\n\\begin{bmatrix}\n1 & X_1 & X_1^2 \\\\\n1 & X_2 & X_2^2 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & X_m & X_m^2\n\\end{bmatrix}\n\\]\nLet \\(\\beta\\) be the coefficient vector:\n\\[\n\\beta =\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\beta_2\n\\end{bmatrix}\n\\]\nAnd \\(y\\) be the output vector:\n\\[\ny =\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_m\n\\end{bmatrix}\n\\]\nThe predicted values can be written as:\n\\[\n\\hat{y} = X \\beta\n\\]\nTo find the optimal coefficients \\(\\beta\\), we minimize the SSE, which can be rewritten in matrix form as:\n\\[\nSSE = (y - X\\beta)^T(y - X\\beta)\n\\]\nTo minimize this, we take the derivative of the SSE with respect to \\(\\beta\\) and set it to zero:\n\\[\n\\frac{\\partial}{\\partial \\beta} (y - X\\beta)^T(y - X\\beta) = -2X^T(y - X\\beta) = 0\n\\]\nSolving for \\(\\beta\\):\n\\[\n\\beta = (X^T X)^{-1} X^T y\n\\]\nThis gives the optimal solution for the coefficients \\(\\beta\\), which can be used to predict the output \\(\\hat{y}\\). The detail proof of this parameter \\(\\hat{\\beta}\\) can be found in the  multiple linear regression  page.\n\n\n\nWe use PolynomialFeatures from Scikit-learn to transform our input data \\(X\\) to include polynomial terms (e.g., \\(X^2, X^3\\), etc.).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nnp.random.seed(0)\nX = 2 - 3 * np.random.normal(0, 1, 100)\ny = X - 2 * (X ** 2) + np.random.normal(-3, 3, 100)\nX = X[:, np.newaxis]\nplt.scatter(X, y, color='blue')\nplt.title(\"Sample Data\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)\nX_poly = poly.fit_transform(X)\n\nNow we fit a linear regression model on the transformed polynomial features.\n\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n\ny_pred = model.predict(X_poly)\n\nprint(f\"Coefficients: {model.coef_}\")\nprint(f\"Intercept: {model.intercept_}\")\nprint(f\"Mean Squared Error: {mean_squared_error(y, y_pred)}\")\n\nCoefficients: [ 0.          0.96597113 -2.02225052]\nIntercept: -2.414835667353632\nMean Squared Error: 9.44744195245028\n\n\nFinally, let’s plot the polynomial curve that fits the data.\n\nimport operator\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(X, y_pred), key=sort_axis)\nX_sorted, y_pred_sorted = zip(*sorted_zip)\n\n# Plot the polynomial curve\nplt.scatter(X, y, color='blue')\nplt.plot(X_sorted, y_pred_sorted, color='red')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title(\"Polynomial Regression Fit\")\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nWe can evaluate the performance of the model by comparing the mean squared error (MSE) between the actual and predicted values:\n\nmse = mean_squared_error(y, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\nMean Squared Error: 9.44744195245028\n\n\n\n\n\nWe’ll generate some non-linear data and try to fit a polynomial regression model to it.\n\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\nnp.random.seed(0)\nx1 = 2 - 3 * np.random.normal(0, 1, 100)\nx2 = 4*np.random.normal(-2,2,100)\ny = 2+3*x1 -4*x1**2 + 2 * x2 + np.random.normal(-3, 3, 100)\ndf={\n    'x1':x1,\n    'x2':x2,\n    'y':y\n}\ndf = pd.DataFrame(df)\nprint(df.head())\nscatter_matrix(df)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n         x1         x2           y\n0 -3.292157   7.065206  -41.206797\n1  0.799528 -18.782072  -39.440680\n2 -0.936214 -18.163880  -40.343409\n3 -4.722680  -0.244826 -102.906711\n4 -3.602674 -17.384987  -96.574641\n\n\n\n\n\n\n\n\n\nSince it’s clear that the relationships are not linear. So if we fit a linear regression model, it won’t be a good fit.\n\nX = df.drop('y', axis=1)\ny = df.y\nmodel1 = LinearRegression()\nmodel1.fit(X,y)\npred1 = model1.predict(X)\nresidual1 = y - pred1\nsns.scatterplot(residual1)\nplt.axhline(y=0, c='r')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nTherefore, we generate some non linear features from the given data.\n\ndf['x1_squared']=df.x1**2\nscatter_matrix(df)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nFrom this plot, we see that \\(x1\\) is parabolic and \\(x2\\) is linear in relationship with \\(y\\). So, how about a model that combines a linear and quadratic model?\n\nX = df.drop('y', axis=1)\ny = df.y\nmodel2 = LinearRegression()\nmodel2.fit(X,y)\npred2 = model2.predict(X)\nresidual2 = y - pred2\nsns.scatterplot(residual2)\nplt.axhline(y=0, c='r')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nThis one is much better.\n\nPolynomial regression is a powerful technique that extends the basic linear regression model to capture non-linear relationships between variables. By transforming the input data into polynomial terms, the model becomes more flexible, allowing it to better fit data that doesn’t follow a linear pattern.  The mathematical derivation shows that polynomial regression is still linear in terms of its parameters, allowing us to use simple optimization techniques like ordinary least squares (OLS) for parameter estimation.  In Python, using Scikit-learn makes it easy to implement polynomial regression. We can increase the degree of the polynomial to improve model accuracy, but we need to be careful of overfitting, where the model becomes too complex and fits the noise in the data rather than the underlying pattern.\n\nYou may also like"
  },
  {
    "objectID": "dsandml/knn/index.html",
    "href": "dsandml/knn/index.html",
    "title": "K Nearest Neighbors Regression",
    "section": "",
    "text": "Non-parametric model is a statistical model that does not make any assumptions about the underlying data distributions, meaning it does not require specifying functional form for the relationships between variables, instead learning directly from the data points without pre-defined parameters."
  },
  {
    "objectID": "dsandml/knn/index.html#introduction-non-parametric-models",
    "href": "dsandml/knn/index.html#introduction-non-parametric-models",
    "title": "K Nearest Neighbors Regression",
    "section": "",
    "text": "Non-parametric model is a statistical model that does not make any assumptions about the underlying data distributions, meaning it does not require specifying functional form for the relationships between variables, instead learning directly from the data points without pre-defined parameters."
  },
  {
    "objectID": "dsandml/knn/index.html#k-nearest-neighbors-knn-algorithm",
    "href": "dsandml/knn/index.html#k-nearest-neighbors-knn-algorithm",
    "title": "K Nearest Neighbors Regression",
    "section": "\\(K-\\)Nearest Neighbors (KNN) Algorithm",
    "text": "\\(K-\\)Nearest Neighbors (KNN) Algorithm\n\nK-Nearest Neighbors (KNN) is one of the simplest yet effective algorithms used in supervised learning for both classification and regression problems. It’s a lazy learner—meaning it does not perform any specific training of a model but memorizes the training dataset and makes predictions based on proximity in feature space.\n\nWe are given a set of data points \\((\\bar{x}_i,y_i)\\) with \\(\\bar{x}_i\\in \\mathbb{R}^d\\) and \\(y_i\\in \\mathbb{R}\\)\n1. Choose the number of neighbors \\(K\\)\n2. Compute the distance between the new data point and all the training samples\n3. Select the \\(K\\) nearest neighbors based on distance.\n4. For classification, the output is the most common class among the \\(K\\) neighbors.\n5. For regression, the output is the average of the target values of \\(K\\) neighbors\n\n\\(K-\\)Nearest Neighbors Classification\nThe KNN classification algorithm can be summarized with the following steps:\nGiven:\n\n\\(X_{train} = [x_1, x_2, \\ldots, x_n]\\) (the training data features)\n\n\\(y_{train} = [y_1, y_2, \\ldots, y_n]\\) (the training data labels)\n\n\\(x_{test}\\) (the new data point for which we want to predict the class)\n\nSteps\n1. Compute Distance: For each training point \\(x_i\\), calculate the distance \\(d(x_i, x_{test})\\) using a distance metric like Euclidean distance: \\[\n   d(x_i, x_{test}) = \\sqrt{\\sum_{j=1}^{m} (x_{i,j} - x_{test,j})^2}\n   \\] where \\(m\\) is the number of features.\n2. Find K Nearest Neighbors: Sort the distances and pick the K closest points.\n3. Majority Voting: Look at the labels \\(y_i\\) of the K nearest neighbors. The predicted label for \\(x_{test}\\) is the most frequent label (majority vote) among the neighbors.\nFor example, let’s say our data looks like this\n\n\n\n\n\n\n\nTraining Data\n\n\narea\nbedroom\nbathroom\nprice\ncondition\n\n\n\n\n7420\n4\n2\n1300000\n1\n\n\n7520\n3\n3\n1450000\n1\n\n\n6420\n2\n1\n1110000\n0\n\n\n5423\n3\n2\n1363400\n0\n\n\n5423\n3\n1\n1263400\n1\n\n\n\n\n\n\nTest Data\n\n\narea\nbedroom\nbathroom\nprice\ncondition\n\n\n\n\n5420\n3\n2.5\n1302000\n\n\n\n7120\n5\n4\n1453000\n\n\n\n\n\n\n\nFor the data points \\(x_i\\) from the training set and a single test data point \\(xt=[5420,3,2.5,1302000]\\)\n\\[\\begin{align*}\n    d(x_1, xt) & = \\sqrt{(x_{11}-xt_1)^2 + (x_{12}-xt_2)^2 + (x_{13}-xt_3)^2 + (x_{14}-xt_4)^2}\\\\\n               & = \\sqrt{(7420-5420)^2 + (4-5)^2 + (2-2.5)^2 + (1300000-1302000)^2} \\approx 2828.43\\\\\n    d(x_2,xt)  & = \\sqrt{(x_{21}-xt_1)^2 + (x_{22}-xt_2)^2 + (x_{23}-xt_3)^2 + (x_{24}-xt_4)^2} \\\\     \n               & = \\sqrt{(7520-5420)^2 + (3-5)^2 + (3-2.5)^2 + (1450000-1302000)^2} \\approx 14805.92\\\\\n    d(x_3,xt)  & = \\sqrt{(x_{31}-xt_1)^2 + (x_{32}-xt_2)^2 + (x_{33}-xt_3)^2 + (x_{34}-xt_4)^2}   \\\\\n               & = \\sqrt{(6420-5420)^2 + (2-5)^2 + (1-2.5)^2 + (1110000-1302000)^2} \\approx 19209.38\\\\\n    d(x_4,xt)  & = \\sqrt{(x_{41}-xt_1)^2 + (x_{42}-xt_2)^2 + (x_{43}-xt_3)^2 + (x_{44}-xt_4)^2}\\\\\n               & = \\sqrt{(6420-5420)^2 + (2-5)^2 + (1-2.5)^2 + (1110000-1302000)^2} \\approx 19209.38\\\\\n    d(x_5,xt)  & = \\sqrt{(x_{51}-xt_1)^2 + (x_{52}-xt_2)^2 + (x_{53}-xt_3)^2 + (x_{54}-xt_4)^2} \\\\\n               & = \\sqrt{(5423-5420)^2 + (3-5)^2 + (1-2.5)^2 + (1263400-1302000)^2} \\approx 38602.95\n\\end{align*}\\]\nSo the distances\n\n\\(d_1=d(x_1, xt) \\approx 2828.43\\)\n\\(d_2=d(x_2, xt) \\approx 14805.92\\)\n\\(d_3=d(x_3, xt) \\approx 19209.38\\)\n\\(d_4=d(x_4, xt) \\approx 61405.03\\)\n\\(d_5=d(x_5, xt) \\approx 38602.95\\)\n\nIf we sort the above distances, we get \\(d_1&lt;d_2&lt;d_3&lt;d_5&lt;d_4\\) and if we choose \\(K=3\\) nearest neighbors, then \\(d_1&lt;d_2&lt;d_3\\) and\n\nData point \\(x_1\\) has class label condition\\(=1\\)\n\nData point \\(x_2\\) has class label condition\\(=1\\)\n\nData point \\(x_3\\) has class label condition\\(=0\\)\n\nWe can clearly see that the majority class (2 out of 3) is condition\\(=1\\). Therefore, for the given test data, the label would be also condition\\(=1\\).\n\nKNN Classifier Using Python\nHere’s how to implement KNN for classification in Python from scratch:\n\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nclass CustomKNNclassifier:\n\n    def __init__(self, k=3):\n        self.k = k\n    \n    def fit(self, X, Y):\n        self.X = X\n        self.Y = Y\n    \n    def predict(self, X):\n        predictions = [self._predict(x) for x in X.to_numpy()] \n        return np.array(predictions)\n    \n    def _predict(self, x):\n        # Compute the Euclidean distances \n        distances = [np.linalg.norm(x - X_train) for X_train in self.X.to_numpy()]\n\n        # Get the indices of the k nearest neighbors\n        k_indices = np.argsort(distances)[:self.k]\n\n        # Get the labels of k nearest neighbors\n        k_nearest_neighbors = [self.Y[i] for i in k_indices]\n\n        # Return the most common label\n        common_label = Counter(k_nearest_neighbors).most_common(1)[0][0]\n        return common_label\n\n# Example usage\ntrain_data = pd.DataFrame(\n    {\n        'area': [7420, 7520, 6420, 5423, 5423],\n        'bedroom': [4, 3, 2, 3, 3],\n        'bathroom': [2, 3, 1, 2, 1],\n        'price': [1300000, 1450000, 1110000, 1363400, 1263400],\n        'condition': [1, 1, 0, 0, 1]\n    }\n)\ntest_data = pd.DataFrame(\n    {\n        'area': [5420, 7120],\n        'bedroom': [3, 5],\n        'bathroom': [2.5, 4],\n        'price': [1302000, 1453000]\n    }\n)\n\nX_train = train_data.drop('condition', axis=1)\ny_train = train_data['condition']\n\nX_test = test_data\n\n# Initialize and train the KNN model\nclassifier = CustomKNNclassifier(k=3)\nclassifier.fit(X_train, y_train)\n\n# Predict on test data\npredictions = classifier.predict(X_test)\nprint(predictions)\n\n[1 1]\n\n\nSo the complete test set would be\n\n\n\narea\nbedroom\nbathroom\nprice\ncondition\n\n\n\n\n5420\n3\n2.5\n1302000\n1\n\n\n7120\n5\n4\n1453000\n1\n\n\n\nNote: We did not scale the data before applying the classifier. If we scaled, the result might have been different (?). In practice, we need to scale the data before applying KNN algorithm. Because computing a large number of distances with big numbers may get us wrong order and also time cosuming.\n\n\n\n\\(K-\\)Nearest Neighbors Regression\nKNN regression is slightly different from classification. Instead of taking a majority vote, we predict the output by averaging the values of the K nearest neighbors.\nGiven:\n\n\\(X_{train} = [x_1, x_2, \\ldots, x_n]\\) (the training data features)\n\\(y_{train} = [y_1, y_2, \\ldots, y_n]\\) (the continuous target values)\n\\(x_{test}\\) (the new data point for which we want to predict the value)\n\nStep-by-Step:\n1. Compute Distance: Calculate the Euclidean distance between \\(x_{test}\\) and each training point \\(x_i\\).\n2. Find K Nearest Neighbors: Sort the distances and select the K nearest points.\n3. Averaging: The predicted value for \\(x_{test}\\) is the average of the target values \\(y_i\\) of the K nearest neighbors:\n\\[\n\\hat{y}_{test} = \\frac{1}{K} \\sum_{i=1}^{K} y_i\n\\]\n\nKNN Regressor Using Python\nNow we use the same training data and test data for this regression. But this time, our target variable is price and test data looks like this\n\n\n\narea\nbedroom\nbathroom\nCondition\nprice\n\n\n\n\n5420\n3\n2.5\n1\n\n\n\n7120\n5\n4\n1\n\n\n\n\nAfter scaling the data looks like this\n\n\n\n\n\n\n\nTraining Data\n\n\narea\nbedroom\nbathroom\ncondition\nprice\n\n\n\n\n1.213\n1.414\n0.267\n0.730\n1300000\n\n\n1.336\n0.000\n1.603\n0.730\n1450000\n\n\n-0.026\n-1.414\n-1.336\n-1.095\n1110000\n\n\n-1.261\n0.000\n0.267\n-1.095\n1363400\n\n\n-1.261\n0.000\n-1.336\n0.730\n1263400\n\n\n\n\n\n\nTest Data\n\n\narea\nbedroom\nbathroom\ncondition\nprice\n\n\n\n\n-1.266\n0.000\n0.803\n0.730\n\n\n\n0.854\n2.828\n3.876\n0.730\n\n\n\n\n\n\n\nNow we see that\n\\[\\begin{align*}\n    d_1=d(x_1, x_t) & = \\sqrt{(1.213 - (-1.266))^2 + (1.414 - 0)^2 + (0.267 - 0.803)^2 + (0.730 - 0.730)^2}  \\approx 2.904\\\\\n    d_2=d(x_2, x_t) & = \\sqrt{(1.336 - (-1.266))^2 + (0.000 - 0)^2 + (1.603 - 0.803)^2 + (0.730 - 0.730)^2} \\approx 2.721\\\\\n    d_3=d(x_3, x_t) & = \\sqrt{(-0.026 - (-1.266))^2 + (-1.414 - 0)^2 + (-1.336 - 0.803)^2 + (-1.095 - 0.730)^2}  \\approx 3.382\\\\\n    d_4=d(x_4, x_t) & = \\sqrt{(-1.261 - (-1.266))^2 + (0.000 - 0)^2 + (0.267 - 0.803)^2 + (-1.095 - 0.730)^2}  \\approx 1.902\\\\\n    d_5=d(x_5, x_t) & = \\sqrt{(-1.261 - (-1.266))^2 + (0.000 - 0)^2 + (-1.336 - 0.803)^2 + (0.730 - 0.730)^2} \\approx 2.140\n\\end{align*}\\]\nBut this time, the order is \\(d_4&lt;d_5&lt;d_2&lt;d_1&lt;d_3\\) and for \\(k=3\\) we have \\(d_4&lt;d_5&lt;d_2\\). The price for this distances\n\nFor data point \\(x_4\\), the price\\(=1363400\\)\n\nFor data point \\(x_5\\), the price\\(=1263400\\)\n\nFor data point \\(x_2\\), the price\\(=1450000\\)\n\nSo the predicted price should be the average of this three prices, that for \\(xt=[5420,3,2.5,1]\\) the price we expect\n\\[\nprice = \\frac{1363400+1263400+1450000}{3}=1358933.33\n\\]\nHere’s how to implement KNN for regression in Python from scratch and we see if we get the same as the hand calculation.\n\nfrom sklearn.preprocessing import StandardScaler\n\nclass CustomKNNRegressor:\n    def __init__(self, k=3):\n        self.k = k\n\n    def fit(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train.to_numpy()\n\n    def predict(self, X_test):\n        predictions = [self._predict(x) for x in X_test]\n        return np.array(predictions)\n    \n    def _predict(self, x):\n        distances = [np.linalg.norm(x-x_train) for x_train in self.X_train]\n        k_indices = np.argsort(distances)[:self.k]\n        k_nearest_values = [self.y_train[i] for i in k_indices]\n        return np.mean(k_nearest_values)\n\nX_train = train_data.drop('price', axis=1)\ny_train = train_data['price']\n\ntest_data = pd.DataFrame(\n    {\n        'area': [5420, 7120],\n        'bedroom': [3, 5],\n        'bathroom': [2.5, 4],\n        'condition': [1, 1]\n    }\n)\n\nX_test = test_data\n\nscaler = StandardScaler()\n\nX_train_sc = scaler.fit_transform(X_train)\nX_test_sc = scaler.transform(X_test)\n\n# Initialize and train the KNN regressor\nregressor = CustomKNNRegressor(k=3)\nregressor.fit(X_train_sc, y_train)\n\n# Predict on test data\npredictions = regressor.predict(X_test_sc)\nprint(np.round(predictions,2))\n\n[1358933.33 1371133.33]\n\n\n\n\n\n\nChoosing the Value of K\nThe value of K significantly affects the performance of the KNN algorithm:\n\nSmall K: If K is too small, the model is sensitive to noise, and the predictions can be unstable.\n\nLarge K: If K is too large, the model becomes more biased, and the predictions may be overly smoothed.\n\nA typical way to choose K is by trying different values and using cross-validation to see which value yields the best performance.\n\n\n\nDistance Metrics\nThe default metric for KNN is Euclidean distance, but depending on the dataset, other metrics like Manhattan distance or Minkowski distance might be more suitable.\n\nEuclidean Distance (L2 Norm): \\[\nd(x_i, x_j) = \\sqrt{\\sum_{k=1}^{m} (x_{i,k} - x_{j,k})^2}\n\\]\nManhattan Distance (L1 Norm): \\[\nd(x_i, x_j) = \\sum_{k=1}^{m} |x_{i,k} - x_{j,k}|\n\\]\n\n\n\nKNN Implementation\nIn this section we use KNN regression for Boston Housing dataset and find the optimal \\(K\\) using the KFold cross-validation.\n\ndf = pd.read_csv('HousingData.csv')\n\nNext we see if there is any missing values. If we have any, we will skip those observations.\n\nprint(df.isnull().sum())\ndf.dropna(axis=1,inplace=True)\ndf.head()\n\nCRIM       20\nZN         20\nINDUS      20\nCHAS       20\nNOX         0\nRM          0\nAGE        20\nDIS         0\nRAD         0\nTAX         0\nPTRATIO     0\nB           0\nLSTAT      20\nMEDV        0\ndtype: int64\n\n\n\n\n\n\n\n\n\nNOX\nRM\nDIS\nRAD\nTAX\nPTRATIO\nB\nMEDV\n\n\n\n\n0\n0.538\n6.575\n4.0900\n1\n296\n15.3\n396.90\n24.0\n\n\n1\n0.469\n6.421\n4.9671\n2\n242\n17.8\n396.90\n21.6\n\n\n2\n0.469\n7.185\n4.9671\n2\n242\n17.8\n392.83\n34.7\n\n\n3\n0.458\n6.998\n6.0622\n3\n222\n18.7\n394.63\n33.4\n\n\n4\n0.458\n7.147\n6.0622\n3\n222\n18.7\n396.90\n36.2\n\n\n\n\n\n\n\n\nThe data looks clean and ready to implement to the KNNRegressor. Note that, for predictive modeling we need a lot of things, such as exporatory data analysis (EDA), feature engineering, preprocessing and others. However, we will simply apply the KNNRegressor that we built from scratch and built-in library function from scikit-learn to explore the algorithm and find the optimal \\(K\\).\n\n\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt \n\n\nX = df.drop('MEDV',axis=1)\ny = df['MEDV']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,y, test_size=0.30, random_state=123\n)\nscaler = StandardScaler()\nX_train_sc = scaler.fit_transform(X_train)\nX_test_sc = scaler.transform(X_test)\n\nk_values = [5,15,30,40]\n\nkfold = KFold(n_splits=7, shuffle=True, random_state=123)\nmses = np.zeros((7,4))\n\nfor i,(train_index,test_index) in enumerate(kfold.split(X_train_sc)):\n    X_train_train = X_train_sc[train_index]\n    X_train_holdout = X_train_sc[test_index]\n\n    y_train_train = y_train.iloc[train_index]\n    y_train_holdout = y_train.iloc[test_index]\n\n    for j,k in enumerate(k_values):\n        regressor1 = CustomKNNRegressor(k=k)\n        regressor1.fit(X_train_train, y_train_train)\n        preds = regressor1.predict(X_train_holdout)\n        mses[i,j] = mean_squared_error(preds, y_train_holdout)\n\nplt.scatter(np.zeros(7),mses[:,0], s=60, c='white', edgecolors='black', label='Single Split')\nplt.scatter(np.ones(7),mses[:,1],s=60, c='white', edgecolors='black')\nplt.scatter(2*np.ones(7),mses[:,2],s=60, c='white', edgecolors='black')\nplt.scatter(3*np.ones(7),mses[:,3],s=60, c='white', edgecolors='black')\nplt.scatter([0,1,2,3], np.mean(mses, axis=0), s=60,c='r', marker='X', label='Mean')\nplt.legend(loc='upper right')\nplt.xticks([0,1,2,3],['K=5','K=15','K=30','K=40'])\nplt.ylabel('MSE')\nplt.gca().set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nSo, \\(K=5\\) seems optimal based on our custom built regressor. Now if we do the same thing using the scikit-learn library\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nmses = np.zeros((7,4))\n\nfor i,(train_index,test_index) in enumerate(kfold.split(X_train_sc)):\n    X_train_train = X_train_sc[train_index]\n    X_train_holdout = X_train_sc[test_index]\n\n    y_train_train = y_train.iloc[train_index]\n    y_train_holdout = y_train.iloc[test_index]\n\n    for j,k in enumerate(k_values):\n        regressor2 = KNeighborsRegressor(k)\n        regressor2.fit(X_train_train, y_train_train)\n        preds = regressor2.predict(X_train_holdout)\n        mses[i,j] = mean_squared_error(preds, y_train_holdout)\n\nplt.scatter(np.zeros(7),mses[:,0], s=60, c='white', edgecolors='black', label='Single Split')\nplt.scatter(np.ones(7),mses[:,1],s=60, c='white', edgecolors='black')\nplt.scatter(2*np.ones(7),mses[:,2],s=60, c='white', edgecolors='black')\nplt.scatter(3*np.ones(7),mses[:,3],s=60, c='white', edgecolors='black')\nplt.scatter([0,1,2,3], np.mean(mses, axis=0), s=60,c='r', marker='X', label='Mean')\nplt.legend(loc='upper right')\nplt.xticks([0,1,2,3],['K=5','K=15','K=30','K=40'])\nplt.ylabel('MSE')\nplt.gca().set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nIn both method, we got \\(K=5\\) is the optimal number of neighbors for KNN regression. Let’s apply this in our test dataset\n\nregressor = CustomKNNRegressor(k=5)\nregressor.fit(X_train_sc, y_train)\n\npredictions = regressor.predict(X_test_sc)\n\nmse = mean_squared_error(predictions,y_test)\nrsquared = r2_score(predictions,y_test)\nprint('MSE = {}'.format(np.round(mse,2)),' and R-square = {}'.format(np.round(rsquared,2)))\n\nMSE = 41.26  and R-square = 0.23\n\n\n\n\nConclusion\n\nK-Nearest Neighbors is a simple, intuitive algorithm that can be highly effective in both classification and regression problems. Its simplicity comes from the fact that it doesn’t make any assumptions about the underlying data distribution (it’s non-parametric). However, its performance can be sensitive to the choice of K and the distance metric.   Although it’s easy to implement, KNN can become computationally expensive for large datasets, as it requires calculating distances between the test point and all training samples.   If you need an efficient version, it’s always possible to use optimized libraries like scikit-learn, but writing the algorithm from scratch helps build a solid understanding.\n\n\n\nWhen to Use KNN Over Linear Regression?\nWe would consider using KNN regression over linear regression in the following situations:\n\nNon-linear relationships: When the data shows non-linear patterns or complex relationships between features and target variables that cannot be captured by a straight line.\n\nLocal behavior: When data has local patterns or clusters, and you believe that predictions should rely on the nearest data points.\n\nMinimal assumptions: If you do not want to assume a specific relationship between the features and target, KNN’s non-parametric nature might be more appropriate.\n\nSmaller datasets: KNN works well with smaller datasets and lower-dimensional data where calculating distances is feasible and efficient.\n\nHowever, KNN becomes less efficient and struggles in high dimensions or when the dataset is large. In those cases, linear regression or other more scalable models may be more appropriate"
  },
  {
    "objectID": "dsandml/knn/index.html#references",
    "href": "dsandml/knn/index.html#references",
    "title": "K Nearest Neighbors Regression",
    "section": "References",
    "text": "References\n\nKNN Regressor Overview:\n\nGéron, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media, 2019. This book provides an in-depth explanation of KNN, including its behavior in non-linear data and high-dimensionality challenges.\nBishop, Christopher M. Pattern Recognition and Machine Learning. Springer, 2006. This book covers non-parametric methods like KNN, highlighting the “curse of dimensionality” and distance-based approaches.\n\nKNN vs. Linear Regression (Model Assumptions & Complexity of Data):\n\nHastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2009. This source discusses the assumptions behind linear regression and the flexibility of non-parametric models like KNN.\nKuhn, Max, and Johnson, Kjell. Applied Predictive Modeling. Springer, 2013. The comparison between parametric (like linear regression) and non-parametric models (like KNN) is elaborated in this book.\n\nInterpretability:\n\nMolnar, Christoph. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. 2019. This book emphasizes the trade-offs between interpretable models like linear regression and more black-box models like KNN.\nMurdoch, W. James, et al. “Definitions, methods, and applications in interpretable machine learning.” Proceedings of the National Academy of Sciences 116.44 (2019): 22071-22080.\n\nSensitivity to Outliers:\n\nAggarwal, Charu C. Data Classification: Algorithms and Applications. Chapman and Hall/CRC, 2014. This discusses the impact of outliers on different models, including linear regression and KNN.\nFriedman, Jerome, et al. The Elements of Statistical Learning. Springer Series in Statistics, 2001. Sensitivity to outliers is compared across various regression techniques, including KNN.\n\nHandling High-Dimensional Data:\n\nDomingos, Pedro. “A few useful things to know about machine learning.” Communications of the ACM 55.10 (2012): 78-87. This paper discusses challenges like the curse of dimensionality in models like KNN.\nVerleysen, Michel, and François, Damien. “The curse of dimensionality in data mining and time series prediction.” International Work-Conference on Artificial Neural Networks. Springer, 2005.\n\nTraining and Prediction Time:\n\nShalev-Shwartz, Shai, and Ben-David, Shai. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014. Provides insights into the computational cost differences between linear and non-parametric models like KNN.\nLi, Zhe, et al. “Fast k-nearest neighbor search using GPU.” International Conference on Image and Graphics. Springer, 2015. This paper discusses computational complexity related to KNN.\n\nOverfitting and Flexibility:\n\nYao, Ying, et al. “Overfitting and Underfitting: A Visual Explanation.” Towards Data Science, 2019. Offers a visual and intuitive explanation of the bias-variance tradeoff in KNN and linear models.\nRasmussen, Carl E., and Williams, Christopher KI. Gaussian Processes for Machine Learning. MIT Press, 2006. Discusses overfitting in KNN due to small values of k and regularization techniques for linear models.\n\n\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/naivebayes/index.html",
    "href": "dsandml/naivebayes/index.html",
    "title": "Classification using Naive Bayes algorithm",
    "section": "",
    "text": "Naive Bayes is a family of simple yet powerful probabilistic classifiers based on Bayes’ Theorem, with the assumption of independence among predictors. It is widely used for tasks like spam detection, text classification, and sentiment analysis due to its efficiency and simplicity. Despite being called “naive” for its strong assumption of feature independence, it often performs remarkably well in real-world scenarios."
  },
  {
    "objectID": "dsandml/naivebayes/index.html#introduction",
    "href": "dsandml/naivebayes/index.html#introduction",
    "title": "Classification using Naive Bayes algorithm",
    "section": "",
    "text": "Naive Bayes is a family of simple yet powerful probabilistic classifiers based on Bayes’ Theorem, with the assumption of independence among predictors. It is widely used for tasks like spam detection, text classification, and sentiment analysis due to its efficiency and simplicity. Despite being called “naive” for its strong assumption of feature independence, it often performs remarkably well in real-world scenarios."
  },
  {
    "objectID": "dsandml/naivebayes/index.html#what-is-naive-bayes",
    "href": "dsandml/naivebayes/index.html#what-is-naive-bayes",
    "title": "Classification using Naive Bayes algorithm",
    "section": "What is Naive Bayes?",
    "text": "What is Naive Bayes?\n\nNaive Bayes is a probabilistic classifier that leverages Bayes’ Theorem to predict the class of a given data point. It belongs to the family of generative models and works by estimating the posterior probability of a class given a set of features. The term “Naive” refers to the assumption that features are conditionally independent given the class label, which simplifies computation.\n\n\nBayes’ Theorem: The Foundation\nBayes’ Theorem provides a way to update our beliefs about the probability of an event, based on new evidence. The formula for Bayes’ Theorem is:\n\\[\nP(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}\n\\]\nWhere:\n\n\\(P(y|X)\\): Posterior probability of class \\(y\\) given feature set \\(X\\)\n\n\\(P(X|y)\\): Likelihood of feature set \\(X\\) given class \\(y\\)\n\n\\(P(y)\\): Prior probability of class \\(y\\)\n\n\\(P(X)\\): Evidence or probability of feature set \\(X\\)\n\nIn the context of classification:\n\nThe goal is to predict \\(y\\) (the class) given \\(X\\) (the features).\n\n\\(P(y)\\) is derived from the distribution of classes in the training data.\n\n\\(P(X|y)\\) is derived from the distribution of features for each class.\n\n\\(P(X)\\) is a normalizing constant to ensure probabilities sum to 1, but it can be ignored for classification purposes because it is the same for all classes.\n\n\n\nAssumptions and Requirements\nThe key assumption in Naive Bayes is the conditional independence of features. Specifically, it assumes that the likelihood of each feature is independent of the others, given the class label:\n\\[\nP(X_1, X_2, \\dots, X_n | y) = P(X_1 | y) \\cdot P(X_2 | y) \\cdot \\dots \\cdot P(X_n | y)\n\\]\nWhile this assumption is often violated in real-world data, Naive Bayes can still perform well, especially when certain features dominate the prediction.\nRequirements:\n\nNumerical Data: Naive Bayes can handle both numerical and categorical data, though different versions (Gaussian, Multinomial, Bernoulli) of the algorithm handle specific types of data more effectively\nNon-Collinear Features: Highly correlated features can distort predictions since the model assumes independence.\n\nSufficient Data: Naive Bayes relies on probability estimates; thus, insufficient data might lead to unreliable predictions."
  },
  {
    "objectID": "dsandml/naivebayes/index.html#types-of-naive-bayes-classifiers",
    "href": "dsandml/naivebayes/index.html#types-of-naive-bayes-classifiers",
    "title": "Classification using Naive Bayes algorithm",
    "section": "Types of Naive Bayes Classifiers",
    "text": "Types of Naive Bayes Classifiers\nThere are several variants of Naive Bayes, depending on the nature of the data:\n\nGaussian Naive Bayes: Assumes features follow a Gaussian distribution (useful for continuous data).\n\nMultinomial Naive Bayes: Suitable for discrete data, often used in text classification (e.g., word counts).\n\nBernoulli Naive Bayes: Works well for binary/boolean data, often used in scenarios where the features represent the presence/absence of a characteristic."
  },
  {
    "objectID": "dsandml/naivebayes/index.html#mathematics-behind-the-process",
    "href": "dsandml/naivebayes/index.html#mathematics-behind-the-process",
    "title": "Classification using Naive Bayes algorithm",
    "section": "Mathematics behind the process",
    "text": "Mathematics behind the process\nTo understand the working of Naive Bayes, let’s start with the Bayes’s theorem\n\\[\nP(y_k | X) = \\frac{P(X|y_k) \\cdot P(y_k)}{P(X)}\n\\]\nWhere \\(y_k\\) is one of the possible classes. Due to the independence assumption, the likelihood term \\(P(X|y_k)\\) can be factorized as:\n\\[\nP(X|y_k) = P(x_1|y_k) \\cdot P(x_2|y_k) \\cdot \\dots \\cdot P(x_n|y_k)\n\\]\nWhere \\(x_1, x_2, \\dots, x_n\\) are the individual features in the feature set \\(X\\). For each class \\(y_k\\), compute the posterior probability:\n\\[\nP(y_k | X) \\propto P(y_k) \\cdot \\prod_{i=1}^n P(x_i|y_k)\n\\]\nThe denominator \\(P(X)\\) is constant for all classes, so we can ignore it during classification. Finally, the class \\(y_k\\) with the highest posterior probability is chosen as the predicted class:\n\\[\\begin{align*}\n\\hat{y} &= \\arg\\max_{y_k} P(y_k) \\cdot \\prod_{i=1}^n P(x_i|y_k)\\\\\n\\log{(\\hat{y})}&= \\log{ \\left(\\arg\\max_{y_k} P(y_k) \\cdot \\prod_{i=1}^n P(x_i|y_k)\\right)}\\\\\n\\implies \\hat{y} & = \\arg\\max_{y_k} \\left(\\log P(y_k)+\\sum_{i=1}^{n} P(x_i|y_k)\\right)\n\\end{align*}\\]\n\nComputing the probabilities\n\nPrior Probabilities\n\\(P(y_k)\\) is the prior probability, usually frequency of each class \\(k\\).\n\\[\n  P(y_k)=\\frac{\\text{number of instances in class }y_k}{\\text{total number of instances}}\n\\]\n\n\nClass Conditional Probabilities\n\\(P(x_i|y_k)\\) is the class conditional probability. For the\n\nGaussian Naive Bayes: when the features are continuous and assumed that the features follow a Gaussian distribution, the class conditional probability is given as \\[\nP(x_i|y_k) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_k}}\\exp{\\left(-\\frac{(x_i-\\mu_i)^2}{2\\sigma^2_k}\\right)}\n\\]\nMultinomial Naive Bayes: when the featrues (typically word frequencies) follow a multinomial distribution, the class conditional distribution is given as\n\\[\nP(x_i|y_k)=\\frac{N_{x_i,y_k}+\\alpha}{N_{y_k}+\\alpha V}\n\\]\nwhere,\n\n\\(N_{x_i,y_k}\\) is the count of the feature (e.g. word or term) \\(x_i\\) appearing in documents of class \\(y_k\\)\n\n\\(N_{y_k}\\) is the total count of all features (e.g. words) in all documents belonging to class \\(y_k\\)\n\n\\(\\alpha\\) is a smoothing parameter (often called Laplace smoothing), used to avoid zero probabilities. If not using smoothing, set \\(\\alpha=0\\)\n\n\\(V\\) is the size of the vocabulary (i.e., the number of unique words)\n\nBernoulli Naive Bayes: when features are binary/boolean data, often used in scenarios where the features represent the presence/absence of a characteristic, the class conditional distribution is given as\n\\[\nP(x_i|y_k)=\\begin{cases}\\frac{N_{x_i,y_k}+\\alpha}{N_{y_k}+2\\alpha }\\hspace{2mm}\\text{ if } x_i=1\\\\\n1-\\frac{N_{x_i,y_k}+\\alpha}{N_{y_k}+2\\alpha }\\hspace{2mm}\\text{ if } x_i=0\\end{cases}\n\\]"
  },
  {
    "objectID": "dsandml/naivebayes/index.html#python-implementation",
    "href": "dsandml/naivebayes/index.html#python-implementation",
    "title": "Classification using Naive Bayes algorithm",
    "section": "Python Implementation",
    "text": "Python Implementation\n\nGaussian Naive Bayes\nCode credit for the custom classifier goes to Assembly AI\n\nimport numpy as np\n\nclass GNaiveBayes:\n    def fit(self, X,y):\n        \"\"\"\n        n_samples: number of observed data n; int;\n        n_features: number of continueous features d; int;\n        _classes: unique classes\n        n_classes: number of unique classes\n        \"\"\"\n        n_samples, n_features = X.shape\n        self._classes = np.unique(y)\n        n_classes = len(self._classes)\n\n        # Calculate mean, variance, and prior for each class  \n        self._mean = np.zeros((n_classes,n_features),dtype=np.float64)\n        self._var = np.zeros((n_classes,n_features),dtype=np.float64)\n        self._prior = np.zeros(n_classes,dtype=np.float64)\n\n        for idx, c in enumerate(self._classes):\n            X_c = X[y==c]\n            self._mean[idx,:] = X_c.mean(axis=0)\n            self._var[idx,:] = X_c.var(axis=0)\n            self._prior[idx] = X_c.shape[0]/float(n_samples)\n    \n    def predict(self,X):\n        y_pred = [self._predict(x) for x in X]\n\n        return np.array(y_pred)\n\n    def _predict(self, x):\n        posteriors = []\n\n        # Calculate the posterior probability for each class  \n        for idx,c in enumerate(self._classes):\n            prior = np.log(self._prior[idx])\n            post = np.sum(np.log(self._pdf(idx,x)))\n            posterior = post + prior\n            posteriors.append(posterior)\n        # Return the class with the highest posterior\n        return self._classes[np.argmax(posteriors)]\n    \n    def _pdf(self, class_idx, x):\n        mean = self._mean[class_idx]\n        var = self._var[class_idx]\n        numerator = np.exp(-((x-mean)**2)/(2*var))\n        denominator = np.sqrt(2*np.pi*var)\n\n        return numerator/denominator\n\nLet’s apply this to the irish data set\n\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n\n# Load Iris dataset\ndata = load_iris()\nX = data.data  # Features\ny = data.target  # Target variable (Classes)\ndf = pd.DataFrame(X, columns=data.feature_names)\ndf['target'] = pd.Categorical.from_codes(y, data.target_names)\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report  \n\n# Split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\ngnb1 = GNaiveBayes()\ngnb1.fit(X_train, y_train)\npred1 = gnb1.predict(X_test)\n# Evaluate the model\nacc1 = accuracy_score(y_test, pred1)\n\ngnb2 = GaussianNB()\ngnb2.fit(X_train, y_train)\npred2 = gnb2.predict(X_test)\nacc2 = accuracy_score(y_test, pred2) \n\nprint('Accuracy from custom classifier = {:.2f}'.format(acc1*100))\n\n# Confusion matrix and classification report\nprint(confusion_matrix(y_test, pred1))\nprint(classification_report(y_test, pred1))\nprint('\\n')\nprint('Accuracy from sklearn classifier = {:.2f}'.format(acc2*100))\nprint(confusion_matrix(y_test, pred2))\nprint(classification_report(y_test, pred2))\n\nAccuracy from custom classifier = 97.78\n[[19  0  0]\n [ 0 12  1]\n [ 0  0 13]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        19\n           1       1.00      0.92      0.96        13\n           2       0.93      1.00      0.96        13\n\n    accuracy                           0.98        45\n   macro avg       0.98      0.97      0.97        45\nweighted avg       0.98      0.98      0.98        45\n\n\n\nAccuracy from sklearn classifier = 97.78\n[[19  0  0]\n [ 0 12  1]\n [ 0  0 13]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        19\n           1       1.00      0.92      0.96        13\n           2       0.93      1.00      0.96        13\n\n    accuracy                           0.98        45\n   macro avg       0.98      0.97      0.97        45\nweighted avg       0.98      0.98      0.98        45\n\n\n\n\n\nMultinomial Naive Bayes\n\nclass MNaiveBayes:\n    def __init__(self, alpha = 1):\n        self.alpha = alpha\n\n    def fit(self, X,y):\n        \"\"\"\n        Fit the Multinomial Naive Bayes model to the training data.  \n        X: input data (n_samples, n_features)\n        y: target labels (n_samples)\n        \"\"\"\n        n_samples, n_features = X.shape\n        self._classes = np.unique(y)\n        n_classes = len(self._classes)\n\n        # Initialize and count priors \n        self._class_feature_count = np.zeros((n_classes, n_features),dtype=np.float64)\n        self._class_count = np.zeros(n_classes, dtype=np.float64)\n        self._prior = np.zeros(n_classes, dtype=np.float64)\n\n        for idx,c in enumerate(self._classes):\n            X_c = X[y==c]\n            self._class_feature_count[idx,:] = X_c.sum(axis=0)\n            self._class_count[idx] = X_c.shape[0]\n            self._prior[idx] = X_c.shape[0]/float(n_samples)\n        \n        # Total count of all features accross all classes \n        self._total_feature_count = self._class_feature_count.sum(axis=1)\n    \n    def predict(self, X):\n        y_pred = [self._predict(x) for x in X]\n        return np.array(y_pred)\n    \n    def _predict(self,x):\n        posteriors = []\n        for idx, c in enumerate(self._classes):\n            prior = np.log(self._prior[idx])\n            likelihood = np.sum(np.log(self._likelihood(idx,x)))\n            posterior_prob = prior+ likelihood\n            posteriors.append(posterior_prob)\n        return self._classes[np.argmax(posteriors)]\n    \n    def _likelihood(self, class_idx, x):\n        alpha = self.alpha\n        V = len(self._class_feature_count[class_idx])\n        class_feature_count = self._class_feature_count[class_idx]\n        total_class_count = self._total_feature_count[class_idx]\n        likelihood = (class_feature_count+alpha)/(total_class_count + alpha * V)\n\n        return likelihood**x\n\nX = np.array([[2, 1, 0],\n              [1, 0, 1],\n              [0, 3, 0],\n              [2, 2, 1],\n              [0, 0, 2]])\n\n# Corresponding labels (2 classes: 0 and 1)\ny = np.array([0, 1, 0, 0, 1])\n\n# Create and train Multinomial Naive Bayes model\nmodel = MNaiveBayes()\nmodel.fit(X, y)\n\n# Predict for new sample\nX_test = np.array([[1, 1, 0], [0, 1, 1]])\npredictions = model.predict(X_test)\nprint(predictions)\n\n[0 0]"
  },
  {
    "objectID": "dsandml/naivebayes/index.html#pros-and-cons-of-naive-bayes",
    "href": "dsandml/naivebayes/index.html#pros-and-cons-of-naive-bayes",
    "title": "Classification using Naive Bayes algorithm",
    "section": "Pros and Cons of Naive Bayes",
    "text": "Pros and Cons of Naive Bayes\n\nPros:\n\nSimplicity: Easy to implement and computationally efficient.\n\nFast Training and Prediction: Naive Bayes is especially fast for both training and inference, even on large datasets.\n\nPerforms Well with Small Data: Despite its simplicity, Naive Bayes works well even with relatively small datasets.\n\nHandles Irrelevant Features: Naive Bayes can often ignore irrelevant features in the data since the independence assumption dilutes their influence.\n\nMulti-Class Classification: Naturally suited for multi-class classification problems.\n\n\n\nCons:\n\nStrong Assumption of Independence: The assumption that features are independent is rarely true in real-world data, which can limit the model’s effectiveness.\n\nPoor Estimation of Probabilities: When dealing with very small datasets or unseen feature combinations, Naive Bayes can yield inaccurate probability estimates.\n\nZero-Frequency Problem: If a feature value was not present in the training data, Naive Bayes will assign zero probability to the entire class, which can be addressed using Laplace smoothing."
  },
  {
    "objectID": "dsandml/lda/index.html",
    "href": "dsandml/lda/index.html",
    "title": "Classification: Linear Discriminant Analysis (LDA)",
    "section": "",
    "text": "Linear Discriminant Analysis (LDA) is a supervised machine learning algorithm commonly used for classification tasks. It is widely applied when dealing with datasets where the number of predictors (features) exceeds the number of observations, or when multicollinearity is a concern. LDA works by projecting data onto a lower-dimensional space, maximizing the separation between classes."
  },
  {
    "objectID": "dsandml/lda/index.html#introduction",
    "href": "dsandml/lda/index.html#introduction",
    "title": "Classification: Linear Discriminant Analysis (LDA)",
    "section": "",
    "text": "Linear Discriminant Analysis (LDA) is a supervised machine learning algorithm commonly used for classification tasks. It is widely applied when dealing with datasets where the number of predictors (features) exceeds the number of observations, or when multicollinearity is a concern. LDA works by projecting data onto a lower-dimensional space, maximizing the separation between classes."
  },
  {
    "objectID": "dsandml/lda/index.html#mathematical-foundation-of-lda",
    "href": "dsandml/lda/index.html#mathematical-foundation-of-lda",
    "title": "Classification: Linear Discriminant Analysis (LDA)",
    "section": "Mathematical Foundation of LDA",
    "text": "Mathematical Foundation of LDA\n\nLet’s assume we have a dataset \\(X \\in \\mathbb{R}^{n \\times p}\\) consisting of \\(n\\) data points and \\(p\\) features, and each data point belongs to one of \\(K\\) distinct classes. The goal of LDA is to find a new space (called a discriminant space) in which the classes are maximally separated, i.e. we want to maximize the separability between classes while minimizing the variation within each class. This can be mathematically expressed as finding a projection that maximizes the ratio of between-class variance to within-class variance.\n\nFor each class \\(C_k\\) (where \\(k \\in \\{1, 2, \\dots, K\\}\\)):\n\n\\(\\mu_k\\) is the mean vector of class \\(C_k\\).\n\n\\(\\mu\\) is the overall mean of the entire dataset.\n\nClass Mean: For each class \\(C_k\\), the mean is calculated as:\n\\[\n\\mu_k = \\frac{1}{N_k} \\sum_{x_i \\in C_k} x_i\n\\]\nwhere \\(N_k\\) is the number of data points in class \\(C_k\\), and \\(x_i\\) represents individual data points.\nOverall Mean: The mean of the entire dataset is:\n\\[\n\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nTo understand how well classes are separated, we need two key measures:\n\nWithin-Class Scatter Matrix \\(S_W\\)\nThe within-class scatter matrix measures how the data points of each class deviate from the class mean. It captures the spread of data points within each class. For class \\(C_k\\), the scatter matrix is calculated as:\n\\[\nS_W = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} (x_i - \\mu_k)(x_i - \\mu_k)^T\n\\]\nThis formula is saying that for each class \\(C_k\\), we calculate the distance of every point \\(x_i\\) from the mean of its class \\(\\mu_k\\), and then sum these squared distances across all classes.\nBetween-Class Scatter Matrix \\(S_B\\)\nThe between-class scatter matrix measures how the class means deviate from the overall mean. It captures how well-separated the classes are.\n\\[\nS_B = \\sum_{k=1}^{K} N_k (\\mu_k - \\mu)(\\mu_k - \\mu)^T\n\\]\nIn this case, for each class \\(C_k\\), we calculate the distance between the mean of class \\(\\mu_k\\) and the overall mean \\(\\mu\\), then scale this by the number of points in class \\(C_k\\).\n\n\n\nLDA aims to find a transformation that maximizes the separation between classes. This is done by finding a linear projection \\(\\mathbf{w}\\) such that the between-class scatter is maximized and the within-class scatter is minimized. Mathematically, the optimization problem becomes:\n\n\\[\nJ(\\mathbf{w}) = \\frac{\\mathbf{w}^T S_B \\mathbf{w}}{\\mathbf{w}^T S_W \\mathbf{w}}\n\\]\n\n\\(S_B \\mathbf{w}\\) captures the between-class variance (how well-separated the classes are in the new projection).\n\n\\(S_W \\mathbf{w}\\) captures the within-class variance (how tightly packed the points of the same class are in the new projection).\n\nThis ratio \\(J(\\mathbf{w})\\) is known as the Fisher’s discriminant ratio. The goal is to find \\(\\mathbf{w}\\) that maximizes this ratio. To maximize the Fisher’s discriminant ratio, we need to solve the following generalized eigenvalue problem:\n\\[\nS_W^{-1} S_B \\mathbf{w} = \\lambda \\mathbf{w}\n\\]\nHere, \\(\\mathbf{w}\\) is the vector that defines the linear combination of features that maximizes class separation, and \\(\\lambda\\) is an eigenvalue that represents how much variance is explained by that direction.\nThe solution to this equation gives us the eigenvectors (directions) and eigenvalues (variances) of the transformed space. We select the top eigenvectors corresponding to the largest eigenvalues to form the projection matrix \\(W\\)."
  },
  {
    "objectID": "dsandml/lda/index.html#dimensionality-reduction",
    "href": "dsandml/lda/index.html#dimensionality-reduction",
    "title": "Classification: Linear Discriminant Analysis (LDA)",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nThe LDA transformation reduces the dimensionality of the data by projecting it onto a subspace spanned by the eigenvectors with the largest eigenvalues. For a dataset with \\(K\\) classes, LDA can reduce the data to at most \\(K-1\\) dimensions because \\(S_B\\) has rank \\(K-1\\).  If we have two classes, LDA will reduce the data to a one-dimensional subspace. For three classes, LDA can project the data onto a two-dimensional subspace, and so on.\n\n\nNow before diving into the python code, let’s do some math by hand so that we can understand the skeleton of the process. Let’s create a small dataset with 6 features and 4 observations divided into 3 classes. We will use this dataset to manually go through the Linear Discriminant Analysis (LDA) process step by step.\n\nDataset\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nFeature 1\nFeature 2\nFeature 3\nFeature 4\nFeature 5\nFeature 6\nClass\n\n\n\n\n\\(x_1\\)\n2\n3\n4\n5\n6\n7\n\\(C_1\\)\n\n\n\\(x_2\\)\n3\n4\n5\n6\n7\n8\n\\(C_1\\)\n\n\n\\(x_3\\)\n6\n5\n4\n3\n2\n1\n\\(C_2\\)\n\n\n\\(x_4\\)\n7\n6\n5\n4\n3\n2\n\\(C_3\\)\n\n\n\nNow, we’ll walk through the mathematical steps of LDA for this small dataset.\n\n\n1. Compute Class Means \\(\\mu_k\\) for each class:\n\nClass \\(C_1\\) (mean of \\(x_1\\) and \\(x_2\\)): \\[\n\\mu_1 = \\frac{1}{2} \\left( \\begin{bmatrix} 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\end{bmatrix} \\right) = \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\\\ 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix}\n\\]\nClass \\(C_2\\) (only one observation \\(x_3\\)): \\[\n\\mu_2 = \\begin{bmatrix} 6 \\\\ 5 \\\\ 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}\n\\]\nClass \\(C_3\\) (only one observation \\(x_4\\)): \\[\n\\mu_3 = \\begin{bmatrix} 7 \\\\ 6 \\\\ 5 \\\\ 4 \\\\ 3 \\\\ 2 \\end{bmatrix}\n\\]\n\n\n\n2. Compute Overall Mean \\(\\mu\\):\nWe compute the overall mean \\(\\mu\\), which is the average of all observations from all classes:\n\\[\n\\mu = \\frac{1}{4} \\left( \\begin{bmatrix} 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\end{bmatrix} + \\begin{bmatrix} 6 \\\\ 5 \\\\ 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} 7 \\\\ 6 \\\\ 5 \\\\ 4 \\\\ 3 \\\\ 2 \\end{bmatrix} \\right)= \\frac{1}{4} \\begin{bmatrix} 18 \\\\ 18 \\\\ 18 \\\\ 18 \\\\ 18 \\\\ 18 \\end{bmatrix} = \\begin{bmatrix} 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\end{bmatrix}\n\\]\n\n\n3. Compute the Within-Class Scatter Matrix \\(S_W\\):\nFor each class \\(C_k\\), the within-class scatter matrix \\(S_W\\) is computed as:\n\\[\nS_W = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} (x_i - \\mu_k)(x_i - \\mu_k)^T\n\\]\nFor \\(C_1\\), the within-class scatter matrix is:\n\\[\n(x_1 - \\mu_1) = \\begin{bmatrix} 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\end{bmatrix} - \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\\\ 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix} = \\begin{bmatrix} -0.5 \\\\ -0.5 \\\\ -0.5 \\\\ -0.5 \\\\ -0.5 \\\\ -0.5 \\end{bmatrix}; \\hspace{6mm} (x_2 - \\mu_1) = \\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\end{bmatrix} - \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\\\ 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}\n\\]\nFor class \\(C_1\\), the scatter matrix is:\n\\[\\begin{align*}\nS_{W1} &= (x_1 - \\mu_1)(x_1 - \\mu_1)^T + (x_2 - \\mu_1)(x_2 - \\mu_1)^T\\\\\n&=\\begin{bmatrix} -0.5 \\\\ -0.5 \\\\ -0.5 \\\\ -0.5 \\\\ -0.5 \\\\ -0.5 \\end{bmatrix} \\begin{bmatrix} -0.5 & -0.5 & -0.5 & -0.5 & -0.5 & -0.5 \\end{bmatrix} + \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix} \\begin{bmatrix} 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\end{bmatrix}\\\\\n&= \\begin{bmatrix} 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\end{bmatrix}\n\\end{align*}\\]\nFor classes \\(C_2\\) and \\(C_3\\), there is only one data point in each, so there is no within-class scatter:\n\\[\nS_{W2} = 0, \\quad S_{W3} = 0\n\\]\nThus, the total within-class scatter matrix is:\n\\[\nS_W = S_{W1} + S_{W2} + S_{W3} = S_{W1}\n\\]\n\\[\nS_W = \\begin{bmatrix} 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\end{bmatrix}\n\\]\n\n\n4. Compute the Between-Class Scatter Matrix \\(S_B\\):\nFor each class \\(C_k\\), the between-class scatter matrix is computed as:\n\\[\nS_B = \\sum_{k=1}^{K} N_k (\\mu_k - \\mu)(\\mu_k - \\mu)^T\n\\]\nFor class \\(C_1\\) (where \\(N_1 = 2\\)):\n\\[\n(\\mu_1 - \\mu) = \\begin{bmatrix} 2.5 \\\\ 3.5 \\\\ 4.5 \\\\ 5.5 \\\\ 6.5 \\\\ 7.5 \\end{bmatrix} - \\begin{bmatrix} 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\n\\]\nThus, for \\(C_1\\):\n\\[\\begin{align*}\nS_{B1} &= 2 \\begin{bmatrix} -2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} -2 & -1 & 0 & 1 & 2 & 3 \\end{bmatrix}= 2 \\begin{bmatrix} 4 & 2 & 0 & -2 & -4 & -6 \\\\ 2 & 1 & 0 & -1 & -2 & -3 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ -2 & -1 & 0 & 1 & 2 & 3 \\\\ -4 & -2 & 0 & 2 & 4 & 6 \\\\ -6 & -3 & 0 & 3 & 6 & 9 \\end{bmatrix}\\\\\n&=\\begin{bmatrix} 8 & 4 & 0 & -4 & -8 & -12 \\\\ 4 & 2 & 0 & -2 & -4 & -6 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ -4 & -2 & 0 & 2 & 4 & 6 \\\\ -8 & -4 & 0 & 4 & 8 & 12 \\\\ -12 & -6 & 0 & 6 & 12 & 18 \\end{bmatrix}\n\\end{align*}\\]\nFor \\(C_2\\) (where \\(N_2 = 1\\)):\n\\[\n(\\mu_2 - \\mu) = \\begin{bmatrix} 6 \\\\ 5 \\\\ 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\end{bmatrix} = \\begin{bmatrix} 1.5 \\\\ 0.5 \\\\ -0.5 \\\\ -1.5 \\\\ -2.5 \\\\ -3.5 \\end{bmatrix}\n\\]\nThe between-class scatter matrix for \\(C_2\\) is:\n\\[\\begin{align*}\nS_{B2} &= \\begin{bmatrix} 1.5 \\\\ 0.5 \\\\ -0.5 \\\\ -1.5 \\\\ -2.5 \\\\ -3.5 \\end{bmatrix} \\begin{bmatrix} 1.5 & 0.5 & -0.5 & -1.5 & -2.5 & -3.5 \\end{bmatrix}\\\\\n&= \\begin{bmatrix} 2.25 & 0.75 & -0.75 & -2.25 & -3.75 & -5.25 \\\\ 0.75 & 0.25 & -0.25 & -0.75 & -1.25 & -1.75 \\\\ -0.75 & -0.25 & 0.25 & 0.75 & 1.25 & 1.75 \\\\ -2.25 & -0.75 & 0.75 & 2.25 & 3.75 & 5.25 \\\\ -3.75 & -1.25 & 1.25 & 3.75 & 6.25 & 8.75 \\\\ -5.25 & -1.75 & 1.75 & 5.25 & 8.75 & 12.25 \\end{bmatrix}\n\\end{align*}\\]\nFor \\(C_3\\) (where \\(N_3 = 1\\)):\n\\[\n(\\mu_3 - \\mu) = \\begin{bmatrix} 7 \\\\ 6 \\\\ 5 \\\\ 4 \\\\ 3 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\\\ 4.5 \\end{bmatrix} = \\begin{bmatrix} 2.5 \\\\ 1.5 \\\\ 0.5 \\\\ -0.5 \\\\ -1.5 \\\\ -2.5 \\end{bmatrix}\n\\]\nThe between-class scatter matrix for \\(C_3\\) is:\n\\[\\begin{align*}\nS_{B3} &= \\begin{bmatrix} 2.5 \\\\ 1.5 \\\\ 0.5 \\\\ -0.5 \\\\ -1.5 \\\\ -2.5 \\end{bmatrix} \\begin{bmatrix} 2.5 & 1.5 & 0.5 & -0.5 & -1.5 & -2.5 \\end{bmatrix}\\\\\n&=\\begin{bmatrix} 6.25 & 3.75 & 1.25 & -1.25 & -3.75 & -6.25 \\\\ 3.75 & 2.25 & 0.75 & -0.75 & -2.25 & -3.75 \\\\ 1.25 & 0.75 & 0.25 & -0.25 & -0.75 & -1.25 \\\\ -1.25 & -0.75 & -0.25 & 0.25 & 0.75 & 1.25 \\\\ -3.75 & -2.25 & -0.75 & 0.75 & 2.25 & 3.75 \\\\ -6.25 & -3.75 & -1.25 & 1.25 & 3.75 & 6.25 \\end{bmatrix}\n\\end{align*}\\]\nTotal Between-Class Scatter Matrix \\(S_B\\):\n\\[\\begin{align*}\nS_B &= S_{B1} + S_{B2} + S_{B3}\\\\\n&=\\begin{bmatrix} 8 & 4 & 0 & -4 & -8 & -12 \\\\ 4 & 2 & 0 & -2 & -4 & -6 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ -4 & -2 & 0 & 2 & 4 & 6 \\\\ -8 & -4 & 0 & 4 & 8 & 12 \\\\ -12 & -6 & 0 & 6 & 12 & 18 \\end{bmatrix} + \\begin{bmatrix} 2.25 & 0.75 & -0.75 & -2.25 & -3.75 & -5.25 \\\\ 0.75 & 0.25 & -0.25 & -0.75 & -1.25 & -1.75 \\\\ -0.75 & -0.25 & 0.25 & 0.75 & 1.25 & 1.75 \\\\ -2.25 & -0.75 & 0.75 & 2.25 & 3.75 & 5.25 \\\\ -3.75 & -1.25 & 1.25 & 3.75 & 6.25 & 8.75 \\\\ -5.25 & -1.75 & 1.75 & 5.25 & 8.75 & 12.25 \\end{bmatrix} \\\\\n&\\\\\n& + \\begin{bmatrix} 6.25 & 3.75 & 1.25 & -1.25 & -3.75 & -6.25 \\\\ 3.75 & 2.25 & 0.75 & -0.75 & -2.25 & -3.75 \\\\ 1.25 & 0.75 & 0.25 & -0.25 & -0.75 & -1.25 \\\\ -1.25 & -0.75 & -0.25 & 0.25 & 0.75 & 1.25 \\\\ -3.75 & -2.25 & -0.75 & 0.75 & 2.25 & 3.75 \\\\ -6.25 & -3.75 & -1.25 & 1.25 & 3.75 & 6.25 \\end{bmatrix}\n\\end{align*}\\]\nAdding the matrices gives:\n\\[\nS_B = \\begin{bmatrix} 16.5 & 8.5 & 0.5 & -7.5 & -15.5 & -23.5 \\\\ 8.5 & 4.5 & 0.5 & -3.5 & -7.5 & -11.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ -7.5 & -3.5 & 0.5 & 4.5 & 8.5 & 12.5 \\\\ -15.5 & -7.5 & 0.5 & 8.5 & 16.5 & 24.5 \\\\ -23.5 & -11.5 & 0.5 & 12.5 & 24.5 & 36.5 \\end{bmatrix}\n\\]\n\n\n5. Solve the Eigenvalue Problem:\nWe now solve the eigenvalue problem:\n\\[\nS_W^{-1} S_B \\mathbf{w} = \\lambda \\mathbf{w}\n\\]\n\nThe solution to this eigenvalue problem gives us the eigenvalues \\(\\lambda\\) (which quantify the amount of variance captured in each direction) and the eigenvectors \\(\\mathbf{w}\\) (which give the directions of maximum class separation). The eigenvector corresponding to the largest eigenvalue defines the direction of the first discriminant axis, which is the direction that maximally separates the classes.\n\n\\[\\begin{align*}\n\\begin{bmatrix}\n6.67 & 6.67 & 6.67 & 6.67 & 6.67 & 6.67 \\\\\n6.67 & 6.67 & 6.67 & 6.67 & 6.67 & 6.67 \\\\\n6.67 & 6.67 & 6.67 & 6.67 & 6.67 & 6.67 \\\\\n6.67 & 6.67 & 6.67 & 6.67 & 6.67 & 6.67 \\\\\n6.67 & 6.67 & 6.67 & 6.67 & 6.67 & 6.67 \\\\\n6.67 & 6.67 & 6.67 & 6.67 & 6.67 & 6.67 \\end{bmatrix}\\begin{bmatrix} 16.5 & 8.5 & 0.5 & -7.5 & -15.5 & -23.5 \\\\ 8.5 & 4.5 & 0.5 & -3.5 & -7.5 & -11.5 \\\\ 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\\\ -7.5 & -3.5 & 0.5 & 4.5 & 8.5 & 12.5 \\\\ -15.5 & -7.5 & 0.5 & 8.5 & 16.5 & 24.5 \\\\ -23.5 & -11.5 & 0.5 & 12.5 & 24.5 & 36.5 \\end{bmatrix}\\begin{bmatrix}w_1\\\\w_2\\\\w_3\\\\w_4\\\\w_5\\\\w_6\\end{bmatrix}&=\\lambda\\begin{bmatrix}w_1\\\\w_2\\\\w_3\\\\w_4\\\\w_5\\\\w_6\\end{bmatrix}\\\\\n\\implies \\begin{bmatrix}\n-2.33 & -1.00 & 0.33 & 1.67 & 3.00 & 4.33 \\\\\n-2.33 & -1.00 & 0.33 & 1.67 & 3.00 & 4.33 \\\\\n-2.33 & -1.00 & 0.33 & 1.67 & 3.00 & 4.33 \\\\\n-2.33 & -1.00 & 0.33 & 1.67 & 3.00 & 4.33 \\\\\n-2.33 & -1.00 & 0.33 & 1.67 & 3.00 & 4.33 \\\\\n-2.33 & -1.00 & 0.33 & 1.67 & 3.00 & 4.33 \\end{bmatrix}\\begin{bmatrix}w_1\\\\w_2\\\\w_3\\\\w_4\\\\w_5\\\\w_6\\end{bmatrix}&=\\lambda\\begin{bmatrix}w_1\\\\w_2\\\\w_3\\\\w_4\\\\w_5\\\\w_6\\end{bmatrix}\n\\end{align*}\\]\nThe eigenvalues of the matrix are:\n\\[\n\\lambda_1 = 6.00, \\quad \\lambda_2 = 1.78 \\times 10^{-15}, \\quad \\lambda_3 = 9.86 \\times 10^{-32}, \\quad \\lambda_4 = 0.00, \\quad \\lambda_5 = -5.47 \\times 10^{-48}, \\quad \\lambda_6 = -5.95 \\times 10^{-16}\n\\]\nThe two largest eigenvalues are:\n\n\\(\\lambda_1 = 6.00\\)\n\\(\\lambda_2 = 1.78 \\times 10^{-15}\\)\n\nThe corresponding eigenvectors for the two largest eigenvalues are:\n\\[\n\\mathbf{w_1} = \\begin{bmatrix}\n-0.408 \\\\\n-0.408 \\\\\n-0.408 \\\\\n-0.408 \\\\\n-0.408 \\\\\n-0.408 \\end{bmatrix}, \\quad\n\\mathbf{w_2} = \\begin{bmatrix}\n-0.848 \\\\\n-0.237 \\\\\n-0.237 \\\\\n-0.237 \\\\\n-0.237 \\\\\n-0.237 \\end{bmatrix}\n\\]\nBy projecting the data onto the eigenvector \\(\\mathbf{w}\\), we transform the original dataset into a lower-dimensional space where class separability is maximized. For this dataset, since there are 3 classes, LDA will find up to \\(K-1 = 2\\) discriminant axes. Let’s see how.\nThe matrix formed by the two largest eigenvectors is:\n\\[\nW=\\begin{bmatrix}\n-0.408 & -0.848 \\\\\n-0.408 & -0.237 \\\\\n-0.408 & -0.237 \\\\\n-0.408 & -0.237 \\\\\n-0.408 & -0.237 \\\\\n-0.408 & -0.237 \\end{bmatrix}\n\\]\n\nThis matrix represents the projection directions corresponding to the two largest eigenvalues in the Linear Discriminant Analysis process. With the eigenvectors \\(\\mathbf{w}_1\\) and \\(\\mathbf{w}_2\\), we can now project our original dataset onto the new 2D subspace.  Now, let \\(X\\) represent our original dataset (where each row corresponds to an observation and each column to a feature). The projection of the original data onto the new 2D subspace is given by:\n\n\\[\nY = X W\n\\]\nWhere:\n\n\\(X\\) is the \\(4 \\times 6\\) matrix (4 observations, 6 features),\n\\(W\\) is the \\(6 \\times 2\\) matrix of eigenvectors.\n\nAfter multiplying \\(X\\) by \\(W\\), we obtain the projected data matrix \\(Y\\), which is a \\(4 \\times 2\\) matrix (4 observations, 2 features):\n\\[\nY = \\begin{bmatrix}\ny_{11} & y_{12} \\\\\ny_{21} & y_{22} \\\\\ny_{31} & y_{32} \\\\\ny_{41} & y_{42}\n\\end{bmatrix}\n\\]\nThis matrix \\(Y\\) represents the data in the new 2D space where class separability is maximized. So for our data\n\\[\\begin{align*}\n\\begin{bmatrix}2&3&4&5&6&7\\\\3&4&5&6&7&8\\\\6&5&4&3&2&1\\\\7&6&5&4&3&2\\end{bmatrix}\\begin{bmatrix}\n-0.408 & -0.848 \\\\\n-0.408 & -0.237 \\\\\n-0.408 & -0.237 \\\\\n-0.408 & -0.237 \\\\\n-0.408 & -0.237 \\\\\n-0.408 & -0.237 \\end{bmatrix}&=\\begin{bmatrix}-11.016 & -7.621\\\\-13.464 & -9.654\\\\ -8.568 & -8.643\\\\-11.016 &-10.676\\end{bmatrix}\n\\end{align*}\\]\n\n\nStep 6: Visualizing the Results\nIf we were to plot the projected data in this new 2D space, we would see the observations from different classes are better separated, which is the ultimate goal of LDA. The two axes of this 2D space correspond to the two linear discriminants that maximize the separation between the classes.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = np.array([[2, 3, 4, 5, 6, 7],\n                  [3, 4, 5, 6, 7, 8],\n                  [6, 5, 4, 3, 2, 1],\n                  [7, 6, 5, 4, 3, 2]])\n\nW = np.array([[-0.408, -0.848],\n                         [-0.408, -0.237],\n                         [-0.408, -0.237],\n                         [-0.408, -0.237],\n                         [-0.408, -0.237],\n                         [-0.408, -0.237]])\n\nY = np.dot(X, W)\n\n# Visualize the projection\nplt.figure(figsize=(8, 6))\nfor i in range(Y.shape[0]):\n    plt.scatter(Y[i, 0], Y[i, 1], label=f'Obs {i+1}', s=100)\n    plt.text(Y[i, 0] + 0.02, Y[i, 1] + 0.02, f'Obs {i+1}', fontsize=12)\n\nplt.title(\"Projected Data after LDA\")\nplt.xlabel('LD1 (First Linear Discriminant)')\nplt.ylabel('LD2 (Second Linear Discriminant)')\nplt.axhline(0, color='gray', lw=1)\nplt.axvline(0, color='gray', lw=1)\nplt.grid(True)\nplt.legend(loc='upper right')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.gca().set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of the Process of Eigenvalue Problem\n\nEigenvalue Calculation: We found the eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) to be the largest, indicating the directions with the most class separability. We did find only two eigenvaleus since total class is 3.\nEigenvector Calculation: We computed the eigenvectors \\(\\mathbf{w}_1\\) and \\(\\mathbf{w}_2\\) corresponding to these eigenvalues. These eigenvectors define the directions in the original feature space along which the class separation is maximized.\nProjection: We projected the original dataset onto the new 2D subspace spanned by the eigenvectors. This resulted in a new dataset in 2D, where the different classes are more separable.\n\nThis completes the detailed walkthrough of solving the eigenvalue problem in LDA for our example dataset.\n\n\n\nFinal Summary\n\nWithin-class scatter matrix \\(S_W\\) quantifies the spread of data points within each class, and we calculated it for each class.\nBetween-class scatter matrix \\(S_B\\) quantifies the separation between the class means, and we calculated it using the mean of each class and the overall mean.\nSolving the eigenvalue problem \\(S_W^{-1} S_B \\mathbf{w} = \\lambda \\mathbf{w}\\) gives us the directions \\(\\mathbf{w}\\) (eigenvectors) that maximize class separation.\n\nThis is how LDA works step by step, using a small dataset as an example."
  },
  {
    "objectID": "dsandml/lda/index.html#python-code-example",
    "href": "dsandml/lda/index.html#python-code-example",
    "title": "Classification: Linear Discriminant Analysis (LDA)",
    "section": "Python Code Example",
    "text": "Python Code Example\nLet’s now revisit the Python code, with an understanding of the math behind LDA. First build our own classifier\n\nclass CustomLDA:\n    def __init__(self,n_components = None) -&gt; None:\n        \"\"\"\n        Parameters:\n        n_components: int, optional (default=None)\n                      Number of components to keep. If None, all components are kept\n        \"\"\"\n        self.n_components = n_components\n        self.eigenvalues = None\n        self.eigenvectors = None \n        self.mean_vectors = None \n        self.class_means = None\n    \n    def fit(self, X, y):\n        \"\"\"\n        Parameters:\n        X: ndarray of shape (n_samples, n_features)\n        y: ndarray of shape (n_samples,)\n           Target labels (must be categorical)\n        \"\"\"\n        n_features = X.shape[1]\n        class_labels = np.unique(y)\n\n        # Step1: Compute the class means mu_k for each class \n        self.mean_vectors = []\n        for c in class_labels:\n            self.mean_vectors.append(np.mean(X[y==c], axis=0))\n        \n        # Step 2: Compute the within-class scatter matrix S_W \n        S_W = np.zeros((n_features, n_features))\n        for c in class_labels:\n            class_scatter = np.cov(X[y==c].T, bias=True) # Covariance matrix for each class\n            S_W += class_scatter * (X[y==c].shape[0])\n\n        # Step 3: Compute the between-class scatter matrix S_B\n        overall_mean = np.mean(X, axis=0)\n        S_B = np.zeros((n_features, n_features))\n\n\n        for i,mean_vector in enumerate(self.mean_vectors):\n            n = X[y == class_labels[i]].shape[0]\n            mean_differences = (mean_vector -overall_mean).reshape(n_features,1)\n            S_B += n*(mean_differences).dot(mean_differences.T)\n        \n        # Step 4: Solve the Eigenvalue problem \n        eigvalues, eigvectors = np.linalg.eig(np.linalg.pinv(S_W).dot(S_B))\n\n        # Step 5: Sort the Eigenvalues and corresponding eigenvectors \n        eigvalues_sort_idx = np.argsort(np.abs(eigvalues))[::-1]\n        self.eigenvalues = eigvalues[eigvalues_sort_idx]\n        self.eigenvectors = eigvectors[:,eigvalues_sort_idx]\n\n        # Step 6: Keep only the top n_components\n        if self.n_components:\n            self.eigenvectors = self.eigenvectors[:,:self.n_components]\n        \n        self.class_means = np.dot(self.mean_vectors, self.eigenvectors)\n    \n    def transform(self,X):\n        \"\"\"\n        Project the data onto the LDA components \n\n        Parameters:\n        X: ndarray of shape (n_samples, n_features)\n\n        Returns:\n        X_transformed: ndarray of shape (n_samples, n_features)\n        \"\"\"\n        return np.dot(X,self.eigenvectors)\n    \n    def fit_transform(self, X, y):\n        \"\"\"\n        Fit the LDA model and transform the data.\n        \n        Parameters:\n        X : ndarray of shape (n_samples, n_features)\n            Training data.\n        y : ndarray of shape (n_samples,)\n            Target labels (must be categorical).\n        \n        Returns:\n        X_transformed : ndarray of shape (n_samples, n_components)\n            Transformed data after fitting.\n        \"\"\"\n        self.fit(X, y)\n        return self.transform(X)\n    \n    def predict(self, X):\n        \"\"\"\n        Predict the class labels for new data points.\n\n        Parameters:\n        X : ndarray of shape (n_samples, n_features)\n            New data to classify.\n\n        Returns:\n        Predictions: ndarray of shape (n_samples,)\n                     Predicted class labels\n        \"\"\"\n        X_projected = self.transform(X)\n\n        predictions = []\n        for x in X_projected:\n            distances = np.linalg.norm(x-self.class_means, axis=1)\n            predictions.append(np.argmin(distances))\n        \n        return np.array(predictions)\n\n    \n    def explained_variance_ratio(self):\n        \"\"\"\n        Return the percentage of variance explained by each of the selected components\n\n        Returns:\n        explained_variance: ndarray of shape (n_components,)\n                            Percentage of variance explained by each selected components\n        \"\"\"\n        total = np.sum(self.eigenvalues)\n\n        return [(i/total) for i in self.eigenvalues[:self.n_components]]\n\nNext we apply both the custom classifier and the classifier from the scikit-learn library.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import accuracy_score\n\n# Load the dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Standardize the dataset (optional but often improves performance)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\n# Apply LDA from the scikit-learn library\nlda1 = LDA(n_components=2)  # Reduce to 2 dimensions\nX_train_lda1 = lda1.fit_transform(X_train, y_train)\nX_test_lda1 = lda1.transform(X_test)\n\n# Apply LDA from the custom built classifier\nlda2 = CustomLDA(n_components=2)  # Reduce to 2 dimensions\nX_train_lda2 = lda2.fit_transform(X_train, y_train)\nX_test_lda2 = lda2.transform(X_test)\n\n# Visualize the LDA-transformed data\nfig, axes = plt.subplots(1,2, figsize=(9.5,4))\n\naxes[0].scatter(X_train_lda1[:, 0], X_train_lda1[:, 1], c=y_train, cmap='rainbow', edgecolor='k', s=100)\naxes[0].set_xlabel('LD1')\naxes[0].set_ylabel('LD2')\naxes[0].set_title('Scikit-learn')\naxes[1].scatter(X_train_lda2[:, 0], X_train_lda2[:, 1], c=y_train, cmap='rainbow', edgecolor='k', s=100)\naxes[1].set_xlabel('LD1')\naxes[1].set_ylabel('LD2')\naxes[1].set_title('Custom')\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nfig.suptitle('LDA: Projection of the Iris Dataset')\nplt.show()\n\n\n\n\n\n\n\n\nNext, apply LDA as a classifiers for the actual classification\n\nlda_classifier1 = LDA()\nlda_classifier1.fit(X_train, y_train)\ny_pred1 = lda_classifier1.predict(X_test)\n\nlda_classifier2 = CustomLDA()\nlda_classifier2.fit(X_train, y_train)\ny_pred2 = lda_classifier2.predict(X_test)\n\n\n# Check accuracy\naccuracy1 = accuracy_score(y_test, y_pred1)\naccuracy2 = accuracy_score(y_test, y_pred2)\nprint(f'sklearn LDA Classifier Accuracy: {accuracy1 * 100:.2f}% and \\ncustom LDA Classifier Accuracy: {accuracy2 * 100:.2f}%')\n\nsklearn LDA Classifier Accuracy: 100.00% and \ncustom LDA Classifier Accuracy: 95.56%\n\n\nNot too bad, huh! Let’s see the confusion matrix for our custom classifier\n\nfrom sklearn.metrics import confusion_matrix\n\nconf_mat = confusion_matrix(y_test, y_pred2)\n\nprint(pd.DataFrame(\n    conf_mat, \n    columns=['Pred: Setosa','Pred: Virginica', 'Pred: Versicolor'],\n    index=['Actual: Setosa','Actual: Virginica', 'Actual: Versicolor']\n))\n\n                    Pred: Setosa  Pred: Virginica  Pred: Versicolor\nActual: Setosa                19                0                 0\nActual: Virginica              0               11                 2\nActual: Versicolor             0                0                13"
  },
  {
    "objectID": "dsandml/lda/index.html#conclusion",
    "href": "dsandml/lda/index.html#conclusion",
    "title": "Classification: Linear Discriminant Analysis (LDA)",
    "section": "Conclusion",
    "text": "Conclusion\n\nLinear Discriminant Analysis (LDA) is a powerful technique for dimensionality reduction and classification. Its goal is to find directions (linear combinations of the original features) that best separate the classes by maximizing between-class variance while minimizing within-class variance.\n\n\nDisclaimer\n\nFor the mathematical explanation, I used generative AI to produce the matrices and vectors and their manipulations. So it won’t be surprising if a calculation mistake is found. The custom python class was created by the help of ChatGPT4"
  },
  {
    "objectID": "dsandml/lda/index.html#references",
    "href": "dsandml/lda/index.html#references",
    "title": "Classification: Linear Discriminant Analysis (LDA)",
    "section": "References",
    "text": "References\n\nFisher, R.A. (1936). “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics, 7(2), 179–188.\n\nMurphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n\nStrang, G. (2016). Introduction to Linear Algebra (5th ed.). Wellesley-Cambridge Press.\n\nLay, D. C. (2011). Linear Algebra and Its Applications (4th ed.). Pearson.\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/biasvariance/index.html",
    "href": "dsandml/biasvariance/index.html",
    "title": "Model Fine Tuning: Bias-Variance Trade Off",
    "section": "",
    "text": "The bias-variance tradeoff is a fundamental concept in machine learning that helps us understand the balance between underfitting and overfitting. It describes how different sources of error contribute to a model’s overall prediction error and how we can optimize model complexity for better generalization.   To understand the bias-variance tradeoff, let’s first define bias and variance in the context of machine learning models:   Bias is the error introduced by approximating a real-world problem (often complex) by a simplified model. High bias occurs when a model is too simple and can’t capture the underlying patterns, leading to underfitting.   Variance is the model’s sensitivity to small fluctuations in the training data. High variance indicates that the model is too complex, fitting the noise in the training data rather than the actual signal, leading to overfitting.   The goal is to strike a balance between bias and variance to minimize the overall error, often called the expected prediction error."
  },
  {
    "objectID": "dsandml/biasvariance/index.html#introduction",
    "href": "dsandml/biasvariance/index.html#introduction",
    "title": "Model Fine Tuning: Bias-Variance Trade Off",
    "section": "",
    "text": "The bias-variance tradeoff is a fundamental concept in machine learning that helps us understand the balance between underfitting and overfitting. It describes how different sources of error contribute to a model’s overall prediction error and how we can optimize model complexity for better generalization.   To understand the bias-variance tradeoff, let’s first define bias and variance in the context of machine learning models:   Bias is the error introduced by approximating a real-world problem (often complex) by a simplified model. High bias occurs when a model is too simple and can’t capture the underlying patterns, leading to underfitting.   Variance is the model’s sensitivity to small fluctuations in the training data. High variance indicates that the model is too complex, fitting the noise in the training data rather than the actual signal, leading to overfitting.   The goal is to strike a balance between bias and variance to minimize the overall error, often called the expected prediction error."
  },
  {
    "objectID": "dsandml/biasvariance/index.html#mathematical-derivation",
    "href": "dsandml/biasvariance/index.html#mathematical-derivation",
    "title": "Model Fine Tuning: Bias-Variance Trade Off",
    "section": "Mathematical Derivation",
    "text": "Mathematical Derivation\nThe expected mean squared error (MSE) between the true function \\(f(x)\\) and the model’s predictions \\(\\hat{f}(x)\\) is given by:\n\\[\\begin{align*}\n\\text{MSE}(x) &= \\mathbb{E}\\left[\\left(f(x) - \\hat{f}(x)\\right)^2\\right]\\\\\n& = \\mathbb{E}\\left[\\left(f(x) - \\mathbb{E}[\\hat{f}(x)] + \\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x)\\right)^2\\right]\\\\\n& = \\mathbb{E}\\left[\\left(f(x) - \\mathbb{E}[\\hat{f}(x)]\\right)^2+2\\left(f(x) - \\mathbb{E}[\\hat{f}(x)]\\right)\\left(\\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x)\\right)+\\left(\\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x)\\right)^2\\right]\\\\\n& = \\mathbb{E}\\left[\\left(f(x) - \\mathbb{E}[\\hat{f}(x)]\\right)^2\\right] + \\mathbb{E}\\left[\\left(\\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x)\\right)^2\\right] + 2\\mathbb{E}\\left[\\left(f(x) - \\mathbb{E}[\\hat{f}(x)]\\right)\\left(\\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x)\\right)\\right]\n\\end{align*}\\]\nWhere:\n\n\\(f(x)\\) is the true function.\n\\(\\hat{f}(x)\\) is the estimated function (the model).\n\nThe third term, \\(2\\mathbb{E}[(f(x) - \\mathbb{E}[\\hat{f}(x)])(\\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x))]\\), vanishes because the errors \\(f(x) - \\mathbb{E}[\\hat{f}(x)]\\) and \\(\\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x)\\) are independent. This is a key step in the decomposition.\n\n\\(f(x) - \\mathbb{E}[\\hat{f}(x)]\\) is the bias-related error.\n\\(\\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x)\\) is the variance-related error.\n\nSince these two terms are uncorrelated, their cross-product expectation equals zero:\n\\[\n2\\mathbb{E}[(f(x) - \\mathbb{E}[\\hat{f}(x)])(\\mathbb{E}[\\hat{f}(x)] - \\hat{f}(x))] = 0\n\\]\nWhat is \\(\\sigma^2\\)?\nNow, \\(\\sigma^2\\), the irreducible error, is the variance of the noise in the data:\n\\[\n\\sigma^2 = \\mathbb{E}[(y - f(x))^2] = \\mathbb{E}[\\epsilon^2]\n\\]\n\nwhere \\(y = f(x) + \\epsilon\\), and \\(\\epsilon\\) is the noise term with variance \\(\\sigma^2\\). This noise is independent of both the bias and variance components and does not interact with them in the decomposition. It is the part of the error that remains no matter how good the model is.\n\n\\[\n\\text{MSE}(x) = (\\text{Bias}[\\hat{f}(x)])^2 + \\text{Variance}[\\hat{f}(x)] + \\sigma^2\n\\]"
  },
  {
    "objectID": "dsandml/biasvariance/index.html#bias-variance-tradeoff-intuition",
    "href": "dsandml/biasvariance/index.html#bias-variance-tradeoff-intuition",
    "title": "Model Fine Tuning: Bias-Variance Trade Off",
    "section": "Bias-Variance Tradeoff Intuition",
    "text": "Bias-Variance Tradeoff Intuition\n\nA high bias model makes strong assumptions about the data and fails to capture the underlying patterns, resulting in underfitting.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(0)\nX = np.linspace(-3,3,100)\ny = X**2 + np.random.randn(100)*2\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X.reshape(-1,1),y)\ny_pred = model.predict(X.reshape(-1,1))\n\nplt.scatter(X,y, color='blue', label = 'Data Points')\nplt.plot(X,y_pred, color='red', label='High Bias Model (linear)')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('High Bias Model Overfitting the Data')\nplt.legend()\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nA high variance model is highly flexible, capturing not only the signal but also the noise in the data, leading to overfitting.\n\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nmodel2 = DecisionTreeRegressor(max_depth=10)\nmodel2.fit(X.reshape(-1,1),y)\ny_pred = model2.predict(X.reshape(-1,1))\n\nplt.scatter(X,y, color='blue', label = 'Data Points')\nplt.plot(X,y_pred, color='red', label='High Variance Model (linear)')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('High Variance Model Overfitting the Data')\nplt.legend()\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe tradeoff arises because increasing model complexity reduces bias but increases variance, while simplifying the model reduces variance but increases bias.\n\nThe key is to find a sweet spot where the model has low enough bias and variance to generalize well to unseen data."
  },
  {
    "objectID": "dsandml/biasvariance/index.html#more-visualization-of-bias-variance-tradeoff",
    "href": "dsandml/biasvariance/index.html#more-visualization-of-bias-variance-tradeoff",
    "title": "Model Fine Tuning: Bias-Variance Trade Off",
    "section": "More Visualization of Bias-Variance Tradeoff",
    "text": "More Visualization of Bias-Variance Tradeoff\nNow let’s use Python to visualize the bias-variance tradeoff by generating models of varying complexity on a synthetic dataset.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Generate synthetic data\nnp.random.seed(0)\nn_samples = 100\nX = np.random.rand(n_samples, 1) * 10\ny = np.sin(X).ravel() + np.random.randn(n_samples) * 0.5\n\n# Function to plot the results\ndef plot_bias_variance(X, y, degrees):\n    X_plot = np.linspace(0, 10, 100).reshape(-1, 1)\n    fig, axs = plt.subplots(2,2, figsize=(8.2,5.5))\n    fig.patch.set_facecolor('#f4f4f4')\n\n    for i, degree in enumerate(degrees):\n        poly = PolynomialFeatures(degree)\n        X_poly = poly.fit_transform(X)\n        X_plot_poly = poly.transform(X_plot)\n        \n        # Train a linear regression model\n        model = LinearRegression()\n        model.fit(X_poly, y)\n        \n        # Predict on the plot points\n        y_plot_pred = model.predict(X_plot_poly)\n        \n        ax = axs[i//2, i%2]\n        ax.set_facecolor('#f4f4f4')\n        ax.scatter(X,y, color='red', label='Data')\n        ax.plot(X_plot, y_plot_pred, label=f'Polynomial degree {degree}')\n        ax.set_title(f'Polynomial Degree {degree}')\n        ax.legend()\n        \n        # Calculate and display training error\n        y_pred = model.predict(X_poly)\n        mse = mean_squared_error(y, y_pred)\n        ax.text(0, -1.5, f'MSE: {mse:.2f}', fontsize=12)\n\n    plt.tight_layout()\n    plt.savefig('bv.png')\n    plt.show()\n\n# Visualize bias-variance tradeoff\ndegrees = [1, 3, 5, 9]\nplot_bias_variance(X, y, degrees)\n\n\n\n\n\n\n\n\nWe create a synthetic dataset where \\(y = \\sin(x)\\) with added Gaussian noise.\n\nFor low-degree polynomials (e.g., degree 1), the model has high bias. It is too simple to capture the nonlinear relationship in the data, leading to underfitting.\nFor high-degree polynomials (e.g., degree 9), the model has high variance. It fits the training data too closely, even capturing the noise, leading to overfitting.\nA moderate-degree polynomial (e.g., degree 3 or 5) balances bias and variance, achieving the lowest error on unseen data.\n\n\nThe bias-variance tradeoff is a crucial concept for building machine learning models that generalize well. By understanding how bias and variance contribute to the total error, we can make informed decisions about model complexity. In practice, techniques like cross-validation and regularization are often used to find the optimal balance between bias and variance.   Understanding and visualizing this tradeoff helps machine learning practitioners fine-tune their models to achieve the best possible performance."
  },
  {
    "objectID": "dsandml/biasvariance/index.html#references",
    "href": "dsandml/biasvariance/index.html#references",
    "title": "Model Fine Tuning: Bias-Variance Trade Off",
    "section": "References",
    "text": "References\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.\nGéron, A. (2019). Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow. O’Reilly Media.\nKohavi, R. & Wolpert, D. (1996). Bias plus variance decomposition for zero-one loss functions. Proceedings of the 13th International Conference on Machine Learning.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R. Springer.\n\n\nShare on\n\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/randomforest/index.html",
    "href": "dsandml/randomforest/index.html",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "",
    "text": "Random Forest is one of the most popular machine learning algorithms, known for its simplicity, versatility, and ability to perform both classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputs the mode of the classes (for classification) or the mean prediction (for regression) of the individual trees."
  },
  {
    "objectID": "dsandml/randomforest/index.html#introduction",
    "href": "dsandml/randomforest/index.html#introduction",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "",
    "text": "Random Forest is one of the most popular machine learning algorithms, known for its simplicity, versatility, and ability to perform both classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputs the mode of the classes (for classification) or the mean prediction (for regression) of the individual trees."
  },
  {
    "objectID": "dsandml/randomforest/index.html#what-is-random-forest",
    "href": "dsandml/randomforest/index.html#what-is-random-forest",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "What is Random Forest?",
    "text": "What is Random Forest?\n\nRandom Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to obtain a more accurate and stable result. Each tree is built using a different random subset of the data, and at each node, a random subset of features is considered when splitting the data.\n\n\nClassification: The final output is determined by majority voting from all the decision trees\nRegression: The output is the average of all tree predictions."
  },
  {
    "objectID": "dsandml/randomforest/index.html#mathematics-behind-random-forest",
    "href": "dsandml/randomforest/index.html#mathematics-behind-random-forest",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "Mathematics Behind Random Forest",
    "text": "Mathematics Behind Random Forest\nTo understand Random Forest, we first need to recap how a decision tree works and then explore how Random Forest extends this idea.\n\nDecision Tree Recap\nA decision tree is a tree-structured model where each internal node represents a “test” on an attribute (e.g., whether the feature value is above or below a threshold), each branch represents the outcome of the test, and each leaf node represents a class label (classification) or a value (regression).\n\nFor classification, the goal is to partition the data such that the class labels in each partition are as homogeneous as possible.\n\nFor regression, the goal is to minimize the variance of the predicted values.\n\nMathematically, the decision tree makes decisions by minimizing the Gini Index or Entropy for classification tasks and minimizing the Mean Squared Error (MSE) for regression tasks.\n\n\nRandom Forest Algorithm\nRandom Forest enhances decision trees by employing two key concepts:\n\nRandom Sampling (Bootstrap Sampling): From the training set of size \\(N\\), randomly draw \\(N\\) samples with replacement.\n\nFeature Subsampling: At each node of the decision tree, a random subset of the features is selected, and the best split is chosen only from these features.\n\nThe process for building a Random Forest can be summarized as follows:\n\nDraw \\(B\\) bootstrap samples from the original dataset.\nFor each bootstrap sample, grow an unpruned decision tree using a random subset of features at each node.\nFor classification, combine the predictions of all the trees by majority voting.\nFor regression, combine the predictions by averaging the outputs of all trees.\n\n\n\nRandom Forest for Classification\nFor classification tasks, Random Forest works by constructing multiple decision trees, each built on a different subset of the data and a random subset of the features.\nGiven a dataset \\(D = \\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\\}\\), where \\(x_i\\) is a feature vector and \\(y_i\\) is the class label, Random Forest generates \\(B\\) decision trees \\(T_1, T_2, ..., T_B\\).\nFor each test point \\(x\\), each tree \\(T_b\\) gives a class prediction: \\[\n\\hat{y}_b(x) = T_b(x)\n\\] The final prediction is determined by majority voting: \\[\n\\hat{y}(x) = \\text{argmax}_k \\sum_{b=1}^{B} I(\\hat{y}_b(x) = k)\n\\] where \\(I(\\cdot)\\) is an indicator function that equals 1 if the condition is true and 0 otherwise.\n\n\n\nRandom Forest for Regression\nIn regression tasks, Random Forest builds trees that predict continuous values and averages the results.\nGiven a dataset \\(D = \\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\\}\\), where \\(x_i\\) is a feature vector and \\(y_i\\) is the continuous target variable, Random Forest generates \\(B\\) decision trees \\(T_1, T_2, ..., T_B\\).\nFor each test point \\(x\\), each tree \\(T_b\\) gives a predicted value: \\[\n\\hat{y}_b(x) = T_b(x)\n\\] The final prediction is the average of all the tree predictions: \\[\n\\hat{y}(x) = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{y}_b(x)\n\\]"
  },
  {
    "objectID": "dsandml/randomforest/index.html#assumptions-of-random-forest",
    "href": "dsandml/randomforest/index.html#assumptions-of-random-forest",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "Assumptions of Random Forest",
    "text": "Assumptions of Random Forest\nRandom Forest makes few assumptions about the data, making it highly flexible. Some assumptions include:\n\nIndependent Features: While Random Forest does not explicitly assume that features are independent, correlated features can reduce its performance slightly.\n\nNoisy Data: Random Forest is robust to noise due to its ensemble nature.\n\nNon-linearity: Random Forest can handle non-linear relationships between features and the target."
  },
  {
    "objectID": "dsandml/randomforest/index.html#advantages-of-random-forest",
    "href": "dsandml/randomforest/index.html#advantages-of-random-forest",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "Advantages of Random Forest",
    "text": "Advantages of Random Forest\n\nReduction of Overfitting: Random Forest reduces overfitting by averaging the predictions of multiple trees.\nHandles Missing Data: It can handle missing values by assigning them to the most frequent class (classification) or mean value (regression).\nRobust to Noise: It is relatively resistant to outliers and noise due to its ensemble nature.\nWorks with Categorical & Continuous Variables: Random Forest can handle both categorical and continuous data types.\nFeature Importance: It provides an estimate of feature importance, allowing for better interpretability of models."
  },
  {
    "objectID": "dsandml/randomforest/index.html#disadvantages-of-random-forest",
    "href": "dsandml/randomforest/index.html#disadvantages-of-random-forest",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "Disadvantages of Random Forest",
    "text": "Disadvantages of Random Forest\n\nComplexity: The algorithm is computationally intensive, especially with a large number of trees.\nInterpretability: While decision trees are interpretable, Random Forest is a “black-box” model where it’s hard to understand individual predictions.\nMemory Usage: Random Forest can require more memory to store multiple decision trees.\nBias in Imbalanced Data: For classification tasks with imbalanced data, Random Forest may be biased toward the majority class."
  },
  {
    "objectID": "dsandml/randomforest/index.html#python-implementation",
    "href": "dsandml/randomforest/index.html#python-implementation",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "Python Implementation",
    "text": "Python Implementation\nHere is a Python code example of how to implement Random Forest for both classification and regression using scikit-learn.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.datasets import load_iris\n\n# Classification Example: Iris dataset\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize RandomForest Classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Classification Accuracy: {accuracy}\")\n\n# Regression Example: Boston Housing dataset\ndata_url = \"http://lib.stat.cmu.edu/datasets/boston\"\nraw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\ndata = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\ntarget = raw_df.values[1::2, 2]\n\nX = data\ny = target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize RandomForest Regressor\nreg = RandomForestRegressor(n_estimators=100, random_state=42)\nreg.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = reg.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Regression Mean Squared Error: {mse}\")\n\nClassification Accuracy: 1.0\nRegression Mean Squared Error: 9.711591381578941"
  },
  {
    "objectID": "dsandml/randomforest/index.html#hyperparameter-tuning-for-random-forest",
    "href": "dsandml/randomforest/index.html#hyperparameter-tuning-for-random-forest",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "Hyperparameter Tuning for Random Forest",
    "text": "Hyperparameter Tuning for Random Forest\nTuning the hyperparameters of a Random Forest can significantly improve its performance. Here are some important hyperparameters to consider:\n\nImportant Hyperparameters\n\nn_estimators: This is the number of trees in the forest. Increasing this number usually improves performance but also increases computational cost.\n\nTip: Start with a default value of 100 and increase as needed.\n\n\nmax_depth: The maximum depth of each tree. Deeper trees can model more complex relationships, but they also increase the risk of overfitting.\n\nTip: Use cross-validation to find the optimal depth that balances bias and variance\n\n\nmin_samples_split: The minimum number of samples required to split an internal node. Higher values prevent the tree from becoming too specific (overfitting).\n\nTip: Use higher values (e.g., 5 or 10) to reduce overfitting in noisy datasets.\n\nmin_samples_leaf: The minimum number of samples required to be at a leaf node. Larger leaf sizes reduce model complexity and can help generalization.\nmax_features: The number of features to consider when looking for the best split. Randomly selecting fewer features can reduce correlation between trees and improve generalization.\n\nTip: For classification, a common choice is sqrt(number_of_features). For regression, max_features = number_of_features / 3 is often effective.\n\nbootstrap: Whether to use bootstrap samples when building trees. Set this to True for Random Forest (default) or False for extremely randomized trees (also known as ExtraTrees).\n\n\n\nGrid Search for Hyperparameter Tuning\nTo fine-tune the hyperparameters of a Random Forest, we can use GridSearchCV or RandomizedSearchCV in scikit-learn. Here’s an example of how to use GridSearchCV for tuning a Random Forest Classifier:\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2', None]\n}\n\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize Random Forest Classifier\nclf = RandomForestClassifier(random_state=42)\n\n# Perform grid search\ngrid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=0)\ngrid_search.fit(X_train, y_train)\n\n# Best parameters from grid search\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\n\n# Evaluate with best parameters\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy with Best Parameters: {accuracy}\")\n\nBest Hyperparameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\nAccuracy with Best Parameters: 1.0\n\n\nUsing this technique, we can find the combination of hyperparameters that yields the best model performance."
  },
  {
    "objectID": "dsandml/randomforest/index.html#feature-importance-in-random-forest",
    "href": "dsandml/randomforest/index.html#feature-importance-in-random-forest",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "Feature Importance in Random Forest",
    "text": "Feature Importance in Random Forest\nOne of the appealing aspects of Random Forest is that it provides a measure of feature importance, which indicates how much each feature contributes to the model’s predictions.\n\nComputing Feature Importance\nIn Random Forest, feature importance is computed by measuring the average reduction in impurity (e.g., Gini impurity or MSE) brought by each feature across all trees. Features that lead to larger reductions are considered more important.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclf.fit(X_train,y_train)\n# Get feature importance from the RandomForest model\nimportances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# Plot the feature importance\nplt.figure(figsize=(10, 6))\nplt.title(\"Feature Importance\")\nplt.bar(range(X.shape[1]), importances[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), iris.feature_names, rotation=90)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart is showing the relative importance of each feature, making it easier to understand which features have the most predictive power."
  },
  {
    "objectID": "dsandml/randomforest/index.html#out-of-bag-oob-error-estimate",
    "href": "dsandml/randomforest/index.html#out-of-bag-oob-error-estimate",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "Out-of-Bag (OOB) Error Estimate",
    "text": "Out-of-Bag (OOB) Error Estimate\nRandom Forest uses Out-of-Bag (OOB) samples as an alternative to cross-validation. Since each tree is trained on a bootstrap sample, about one-third of the data is left out in each iteration. These “out-of-bag” samples can be used to estimate the model’s performance without the need for a separate validation set.\n\nEnabling OOB in Python\nYou can enable the out-of-bag error estimate by setting oob_score=True in the RandomForestClassifier or RandomForestRegressor.\n\nclf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\nclf.fit(X_train, y_train)\n\n# Access the OOB score\nprint(f\"OOB Score: {clf.oob_score_}\")\n\nOOB Score: 0.9428571428571428\n\n\nThe OOB score is an unbiased estimate of the model’s performance, which is particularly useful when the dataset is small and splitting it further into training/validation sets might reduce training effectiveness."
  },
  {
    "objectID": "dsandml/randomforest/index.html#dealing-with-imbalanced-data",
    "href": "dsandml/randomforest/index.html#dealing-with-imbalanced-data",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "Dealing with Imbalanced Data",
    "text": "Dealing with Imbalanced Data\nFor imbalanced classification tasks (where one class is much more frequent than the others), Random Forest may be biased toward predicting the majority class. Several techniques can help mitigate this issue:\n\nClass Weights: You can assign higher weights to the minority class to force the model to pay more attention to it.\n\n\nclf = RandomForestClassifier(class_weight='balanced', random_state=42)\nclf.fit(X_train, y_train)\n\nRandomForestClassifier(class_weight='balanced', random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(class_weight='balanced', random_state=42) \n\n\n\nResampling: You can either oversample the minority class or undersample the majority class.\n\n\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=42)\nX_resampled, y_resampled = sm.fit_resample(X_train, y_train)"
  },
  {
    "objectID": "dsandml/randomforest/index.html#random-forest-in-practice-best-practices",
    "href": "dsandml/randomforest/index.html#random-forest-in-practice-best-practices",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "Random Forest in Practice: Best Practices",
    "text": "Random Forest in Practice: Best Practices\n\nCross-Validation: Always perform cross-validation to ensure the model generalizes well\nParallelization: Random Forest naturally supports parallelization. If using scikit-learn, set n_jobs=-1 to utilize all CPU cores for training.\n\nEnsemble Methods: For better results, you can combine Random Forest with other ensemble methods, such as boosting (e.g., XGBoost or Gradient Boosting) to further improve performance.\n\nRandom Forest is a highly flexible, non-parametric machine learning algorithm that can be used for both classification and regression tasks. Its ensemble-based approach reduces overfitting, improves predictive performance, and provides valuable insights like feature importance. Despite its many advantages, Random Forest is computationally intensive and may not be the best choice for real-time applications or datasets with extremely high dimensionality."
  },
  {
    "objectID": "dsandml/randomforest/index.html#references",
    "href": "dsandml/randomforest/index.html#references",
    "title": "Ensemble Methods: Random Forest - A detailed overview",
    "section": "References",
    "text": "References\n\nBreiman, L. (2001). “Random Forests”. Machine Learning, 45(1), 5-32.\nPedregosa, F., et al. (2011). “Scikit-learn: Machine Learning in Python”. Journal of Machine Learning Research, 12, 2825-2830.\nHastie, T., Tibshirani, R., & Friedman, J. (2009). “The Elements of Statistical Learning”. Springer Series in Statistics."
  },
  {
    "objectID": "dsandml/svm/index.html",
    "href": "dsandml/svm/index.html",
    "title": "Support Vector Machine (SVM) Algorithm",
    "section": "",
    "text": "Support Vector Machines (SVM) is a powerful non-parametric supervised machine learning algorithm used for classification and, less commonly, regression tasks. Support Vector Machines are designed to find an optimal hyperplane that best separates data points into classes. The key idea behind SVMs is to maximize the margin between data points of different classes while minimizing classification errors. This leads to a robust decision boundary that generalizes well to unseen data."
  },
  {
    "objectID": "dsandml/pytorch/index.html",
    "href": "dsandml/pytorch/index.html",
    "title": "PyTorch Basics",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(torch.__version__)\n\n2.4.0\n\n\n\n\n\nScaler\n\nscaler = torch.tensor(7)\nprint(scaler)\nprint(scaler.ndim)\n\ntensor(7)\n0\n\n\nVector\n\nvec = torch.tensor([2,3,4])\nprint(vec.ndim)\nprint(vec.shape)\n\n1\ntorch.Size([3])\n\n\nMatrix\n\nMAT = torch.tensor([[2,3,4],\n                    [3,2,6]])\nMAT\n\ntensor([[2, 3, 4],\n        [3, 2, 6]])\n\n\nTensor\n\nTEN = torch.tensor([[[2,3,5],\n                     [5,4,3]]])\nTEN.shape\n\ntorch.Size([1, 2, 3])"
  },
  {
    "objectID": "dsandml/pytorch/index.html#introduction-to-tensors",
    "href": "dsandml/pytorch/index.html#introduction-to-tensors",
    "title": "PyTorch Basics",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(torch.__version__)\n\n2.4.0\n\n\n\n\n\nScaler\n\nscaler = torch.tensor(7)\nprint(scaler)\nprint(scaler.ndim)\n\ntensor(7)\n0\n\n\nVector\n\nvec = torch.tensor([2,3,4])\nprint(vec.ndim)\nprint(vec.shape)\n\n1\ntorch.Size([3])\n\n\nMatrix\n\nMAT = torch.tensor([[2,3,4],\n                    [3,2,6]])\nMAT\n\ntensor([[2, 3, 4],\n        [3, 2, 6]])\n\n\nTensor\n\nTEN = torch.tensor([[[2,3,5],\n                     [5,4,3]]])\nTEN.shape\n\ntorch.Size([1, 2, 3])"
  },
  {
    "objectID": "dsandml/bayesianclassification/index.html",
    "href": "dsandml/bayesianclassification/index.html",
    "title": "Bayesian Probabilistic Models for Classification",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields including machine learning, to make predictions and decisions under uncertainty.\nBayes’s theorem is based on the definition of conditional probability. For two events \\(A\\) and \\(B\\) with \\(\\mathbb{P}(B) \\neq 0\\), we define the conditional probability of occurring \\(A\\) given that \\(B\\) has already occurred.\n\n\n\n\n\n\n\\(\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\)\nSimilarly, the conditional probability of occuring \\(B\\) given that \\(A\\) has already occured with \\(\\mathbb{P}(A) \\ne 0\\) is\n\\[\n\\mathbb{P}(B|A)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(A)}\n\\]\nFrom this equation, we can derive that the joint probability of \\(A\\cap B\\) is \\[\n\\mathbb{P}(A\\cap B) = \\mathbb{P}(B | A) \\mathbb{P} (A) = \\mathbb{P}(A | B) \\mathbb{P} (B)\n\\]"
  },
  {
    "objectID": "dsandml/kmeans/index.html",
    "href": "dsandml/kmeans/index.html",
    "title": "Unsupervised Learning: K-Means Clustering",
    "section": "",
    "text": "Clustering is a fundamental technique in unsupervised learning where the goal is to group similar data points into clusters. One of the most popular algorithms for clustering is K-Means. K-Means is a centroid-based algorithm that partitions the dataset into \\(k\\) clusters. The algorithm iterates over data points, assigning each to one of \\(k\\) centroids (cluster centers), and then updates the centroids based on the current assignments. The objective is to minimize the sum of squared distances (also known as inertia) between each data point and its assigned centroid."
  },
  {
    "objectID": "dsandml/kmeans/index.html#introduction",
    "href": "dsandml/kmeans/index.html#introduction",
    "title": "Unsupervised Learning: K-Means Clustering",
    "section": "",
    "text": "Clustering is a fundamental technique in unsupervised learning where the goal is to group similar data points into clusters. One of the most popular algorithms for clustering is K-Means. K-Means is a centroid-based algorithm that partitions the dataset into \\(k\\) clusters. The algorithm iterates over data points, assigning each to one of \\(k\\) centroids (cluster centers), and then updates the centroids based on the current assignments. The objective is to minimize the sum of squared distances (also known as inertia) between each data point and its assigned centroid."
  },
  {
    "objectID": "dsandml/kmeans/index.html#mathematics-behind-k-means",
    "href": "dsandml/kmeans/index.html#mathematics-behind-k-means",
    "title": "Unsupervised Learning: K-Means Clustering",
    "section": "Mathematics Behind K-Means",
    "text": "Mathematics Behind K-Means\nThe K-Means algorithm works through the following key steps:\n\nInitialization: Randomly select \\(k\\) points from the dataset as initial centroids.\nAssignment Step: For each data point, assign it to the closest centroid based on the Euclidean distance:\n\\[\n\\text{distance}(x_i, \\mu_j) = \\sqrt{\\sum_{d=1}^{D} (x_i^d - \\mu_j^d)^2}\n\\]\n\nwhere:\n\n\\(x_i\\) is the i-th data point.\n\\(\\mu_j\\) is the j-th centroid.\n\\(D\\) is the number of features (dimensions).\n\n\nUpdate Step: After all data points are assigned, recalculate the centroid of each cluster as the mean of all data points assigned to it:\n\\[\n\\mu_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} x_i\n\\] where \\(n_j\\) is the number of points in cluster j.\nRepeat: The assignment and update steps are repeated until the centroids no longer change or the maximum number of iterations is reached.\n\n\nObjective Function (Inertia)\nThe objective of K-Means is to minimize the following cost function, also called inertia or within-cluster sum of squares:\n\\[\nJ = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} \\|x_i - \\mu_j\\|^2\n\\]\nThis measures how compact the clusters are, i.e., how close the points within each cluster are to their centroid.\n\n\nHow to Choose the Best \\(k\\) Value?\nOne of the critical tasks in K-Means clustering is selecting the optimal number of clusters (\\(k\\)). Several methods can be used:\n\n1. The Elbow Method\nThe most common way to determine the best \\(k\\) is the elbow method. It involves plotting the inertia (the sum of squared distances from each point to its assigned cluster centroid) for different values of \\(k\\). The point where the inertia starts to flatten out (forming an elbow) is considered a good choice for \\(k\\).\n\n\n2. Silhouette Score\nThe silhouette score measures how similar each point is to its own cluster (cohesion) compared to other clusters (separation). It ranges from -1 to 1:\n\n\\(1\\) indicates that the point is well inside its cluster.\n\n\\(0\\) means the point is on the boundary between two clusters.\n\nNegative values indicate the point may have been assigned to the wrong cluster.\n\n\n\n3. Gap Statistic\nThe gap statistic compares the total within-cluster variation for different values of \\(k\\) with the expected value under null reference distribution. The optimal number of clusters is where the gap statistic is the largest."
  },
  {
    "objectID": "dsandml/kmeans/index.html#python-implementation-of-k-means",
    "href": "dsandml/kmeans/index.html#python-implementation-of-k-means",
    "title": "Unsupervised Learning: K-Means Clustering",
    "section": "Python Implementation of K-Means",
    "text": "Python Implementation of K-Means\n\nSynthetic Data\nLet’s implement K-Means clustering using Python with visualizations and explore how to choose the best value of \\(k\\) using the elbow method.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_score\n\n# For plotting purposes\nimport seaborn as sns\nsns.set()\n\nWe’ll create a simple dataset with 4 distinct clusters for visualization.\n\n# Create a dataset with 4 clusters\nX, y = make_blobs(n_samples=500, centers=4, cluster_std=0.60, random_state=0)\n\n# Visualize the dataset\nplt.scatter(X[:, 0], X[:, 1], s=50)\nplt.title('Dataset with 4 Clusters')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nWe can now apply K-Means clustering with different values of \\(k\\) and observe how the clusters are formed.\n\n# Fit KMeans with k=4 (since we know we generated 4 clusters)\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\n\n# Predict clusters\ny_kmeans = kmeans.predict(X)\n\n# Plot the clustered data\nplt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n\n# Plot the centroids\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75)\nplt.title('K-Means Clustering with k=4')\nplt.savefig('kmeans.png')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nTo determine the optimal number of clusters, we’ll plot the inertia for different values of \\(k\\) using the elbow method.\n\n# Test multiple k values\ninertia = []\nk_values = range(1, 10)\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    inertia.append(kmeans.inertia_)\n\n# Plot the inertia vs. k values\nplt.plot(k_values, inertia, marker='o')\nplt.title('Elbow Method: Choosing the Optimal k')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nWe see that the curve starts to flatten at \\(k=4\\), suggesting this is a good choice for the number of clusters. Let’s also compute the silhouette score for different values of \\(k\\) to confirm our choice.\n\nsil_scores = []\nfor k in range(2, 10):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    labels = kmeans.predict(X)\n    sil_scores.append(silhouette_score(X, labels))\n\n# Plot Silhouette Score vs. k\nplt.plot(range(2, 10), sil_scores, marker='o')\nplt.title('Silhouette Score for Different k Values')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nReal Data\nDescription:\n\n\nThis dataset contains information about customers of a shopping mall, including their annual income, spending score, gender, and age.\n\nGoal: Our goal is to segment customers into different groups based on their spending behavior and income.\nColumns:\n- CustomerID: Unique identifier for each customer.\n- Gender: The gender of the customer (Male or Female).\n- Age: Age of the customer.\n- Annual Income: Annual income of the customer in thousands of dollars.\n- Spending Score: A score assigned by the mall based on customer behavior and spending patterns.\nData Source: You can find the Mall Customer Segmentation data on Kaggle.\n\nimport pandas as pd \nmall = pd.read_csv('Mall_Customers.csv')\nmall.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\n\n# Data Information\nprint(mall.info())\nprint('\\n')\n# Check for Missing Data\nprint(mall.isnull().sum())\nprint('\\n')\n# Data Description\nmall.rename(columns={'CustomerID':'ID','Annual Income (k$)':'Income','Spending Score (1-100)':'SpendingScore'},inplace=True)\ncmall = mall.drop('ID',axis=1)\nprint(cmall.describe().loc[['mean','std','min','max']].T)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Gender                  200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\nNone\n\n\nCustomerID                0\nGender                    0\nAge                       0\nAnnual Income (k$)        0\nSpending Score (1-100)    0\ndtype: int64\n\n\n                mean        std   min    max\nAge            38.85  13.969007  18.0   70.0\nIncome         60.56  26.264721  15.0  137.0\nSpendingScore  50.20  25.823522   1.0   99.0\n\n\nPre-Process: Since our data contains categorical variable Gender, we need to encode this column and scale the numerical features like Age, Annual Income, and Spending Score.\n\nfrom sklearn.preprocessing import StandardScaler\nX = mall[['Age','Income','SpendingScore']]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nprint(X_scaled[:5])\n\n[[-1.42456879 -1.73899919 -0.43480148]\n [-1.28103541 -1.73899919  1.19570407]\n [-1.3528021  -1.70082976 -1.71591298]\n [-1.13750203 -1.70082976  1.04041783]\n [-0.56336851 -1.66266033 -0.39597992]]\n\n\nNext we use the Elbow method to find the best \\(k\\), the number of clusters\n\nk_values = range(1,15)\ninertia = []\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=123)\n    kmeans.fit(X_scaled)\n    inertia.append(kmeans.inertia_)\nplt.plot(k_values,inertia, marker='o')\nplt.title('Elbow method to find $k$')\nplt.xlabel('Number of Clusters $k$')\nplt.ylabel('Inertia')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show() \n\n\n\n\n\n\n\n\nThe elbow point in the plot (where the decrease in inertia starts to slow) helps determine the optimal number of clusters. Let’s say we find that \\(k=5\\) looks like a reasonable choice from the plot.\nTo further validate the choice of \\(k\\), let’s compute the silhouette score for different cluster numbers. A higher silhouette score indicates better-defined clusters\n\nsil_scores = []\nfor k in range(2,15):\n    kmeans = KMeans(n_clusters=k, random_state=123)\n    labels = kmeans.fit_predict(X_scaled)\n    sil_scores.append(silhouette_score(X_scaled,labels))\nplt.plot(range(2,15),sil_scores, marker='o')\nplt.title('Silhoutte method to find $k$')\nplt.xlabel('Number of Clusters $k$')\nplt.ylabel('Silhoutte Score')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show() \n\n\n\n\n\n\n\n\nNext, we apply \\(k=5\\) clusters\n\nplt.figure(figsize=(9.5,6))\nkmeans = KMeans(n_clusters=5, random_state=123)\nmall['Cluster'] = kmeans.fit_predict(X_scaled)\nprint(mall.head())\nsns.scatterplot(\n    x='Income', y='SpendingScore', hue='Cluster',\n    data=mall, palette='viridis', s=100, alpha=0.7\n    )\nplt.title('Customer Segmentation Based on Income and Spending Score')\nplt.legend()\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n   ID  Gender  Age  Income  SpendingScore  Cluster\n0   1    Male   19      15             39        2\n1   2    Male   21      15             81        2\n2   3  Female   20      16              6        4\n3   4  Female   23      16             77        2\n4   5  Female   31      17             40        2\n\n\n\n\n\n\n\n\n\nAnalyze the segments\n\ncluster_summary = mall.drop(columns=['Gender','ID']).groupby('Cluster').mean()\nprint(cluster_summary)\n\n               Age     Income  SpendingScore\nCluster                                     \n0        32.875000  86.100000      81.525000\n1        55.638298  54.382979      48.851064\n2        25.185185  41.092593      62.240741\n3        39.871795  86.102564      19.358974\n4        46.250000  26.750000      18.350000\n\n\nNow say we have two new customers\n\nnew_customer = {'ID':[201,202],'Gender':['Male','Female'],'Age': [30,50],'Income':[40,70],'SpendingScore':[70,20]}\nnew_customer = pd.DataFrame(new_customer)\nprint(new_customer)\n\n    ID  Gender  Age  Income  SpendingScore\n0  201    Male   30      40             70\n1  202  Female   50      70             20\n\n\nWe would like to know in which cluster they belong.\n\nX_new = new_customer[['Age', 'Income','SpendingScore']]\nX_new_sc = scaler.transform(X_new)\ncluster_labels = kmeans.predict(X_new_sc)\nprint(cluster_labels)\n\n[2 3]\n\n\nK-Means is a powerful and widely used clustering algorithm, but it has limitations, such as assuming spherical clusters of equal sizes."
  },
  {
    "objectID": "dsandml/kmeans/index.html#limitations-of-k-means-clustering",
    "href": "dsandml/kmeans/index.html#limitations-of-k-means-clustering",
    "title": "Unsupervised Learning: K-Means Clustering",
    "section": "Limitations of K-Means Clustering",
    "text": "Limitations of K-Means Clustering\nWhile K-Means is a widely used clustering algorithm due to its simplicity and scalability, it has several notable limitations:\n\n1. Assumption of Spherical Clusters\nK-Means assumes that clusters are spherical and have roughly the same size. This assumption may not hold true in real-world datasets, where clusters may have different shapes and densities. For example, if clusters are elongated or irregularly shaped, K-Means may not perform well.\n\nSolution: Use algorithms like DBSCAN (Density-Based Spatial Clustering of Applications with Noise) or Spectral Clustering, which do not assume any specific shape for the clusters.\n\n\n\n2. Sensitivity to Initialization\nK-Means is sensitive to the initial selection of centroids. Different initializations can lead to different final clusters, and in some cases, the algorithm may converge to suboptimal solutions. To address this, the algorithm is often run multiple times with different initializations (e.g., using the k-means++ initialization method).\n\nSolution: Use the k-means++ initialization, which ensures that centroids are chosen in a way that increases the likelihood of converging to an optimal solution.\n\n\n\n3. Needs to Specify k in Advance\nOne of the main limitations is that K-Means requires the number of clusters (k) to be specified in advance. This can be a challenge when the number of clusters is unknown, and choosing the wrong k can lead to poor clustering results.\n\nSolution: Use the Elbow Method, Silhouette Score, or the Gap Statistic to estimate the best value for k.\n\n\n\n4. Outliers and Noise Sensitivity\nK-Means is highly sensitive to outliers, as they can significantly affect the position of centroids. An outlier will either form its own cluster or distort the positions of nearby centroids, leading to incorrect clustering.\n\nSolution: Preprocess your data by removing outliers or use clustering methods like DBSCAN, which can handle outliers more effectively by considering them as noise.\n\n\n\n5. Equal Cluster Size Assumption\nThe algorithm tends to assign roughly equal-sized clusters because it minimizes variance. This can be a problem if clusters in your data have highly varying sizes. Small clusters might be absorbed into larger ones.\n\nSolution: Use Hierarchical Clustering, which can naturally handle different cluster sizes.\n\n\n\n6. Non-Convex Shapes\nK-Means struggles with data where clusters have non-convex shapes, such as two overlapping rings or crescent shapes. It partitions the space into Voronoi cells, which are convex, leading to poor clustering results in non-convex structures.\n\nSolution: Algorithms like Spectral Clustering or Gaussian Mixture Models (GMM) can better handle non-convex clusters."
  },
  {
    "objectID": "dsandml/kmeans/index.html#references",
    "href": "dsandml/kmeans/index.html#references",
    "title": "Unsupervised Learning: K-Means Clustering",
    "section": "References",
    "text": "References\n\nK-Means Algorithm:\n\nMacQueen, J. B. (1967). “Some Methods for Classification and Analysis of Multivariate Observations”. Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics.\nHartigan, J. A., & Wong, M. A. (1979). “Algorithm AS 136: A K-means clustering algorithm”. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1), 100-108.\n\nChoosing k (Elbow Method & Silhouette Score):\n\nRousseeuw, P. J. (1987). “Silhouettes: A graphical aid to the interpretation and validation of cluster analysis”. Journal of Computational and Applied Mathematics, 20, 53-65.\n\nInertia and the Elbow Method:\n\nTibshirani, R., Walther, G., & Hastie, T. (2001). “Estimating the number of clusters in a dataset via the gap statistic”. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2), 411-423.\n\n\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/machinelearning/index.html",
    "href": "posts/machinelearning/index.html",
    "title": "Data Science & Machine Learning Basics",
    "section": "",
    "text": "This page is my personal repository of most common and useful machine learning algorithms using Python and other data science tricks and tips."
  },
  {
    "objectID": "posts/machinelearning/index.html#texttextcolor782f40data-science",
    "href": "posts/machinelearning/index.html#texttextcolor782f40data-science",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{\\textcolor{#782F40}{Data Science}}\\)",
    "text": "\\(\\text{\\textcolor{#782F40}{Data Science}}\\)\nData science involves extracting knowledge from structured and unstructured data. It combines principle from statistics, machine learning, data analysis, and domain knoledge to understand and interpret the data\n\nData Collection & Accuisition\n\nWeb srcaping: Data collection through Webscraping\n\nAPI integration\n\nData Lakes, Data Warehouses\n\n\n\nData Cleaning & Preprocessing\n\nHandling Missing Values\n\nData Transformation\n\nFeature Engineering and Selection\n\nEncoding Categorical Variables\n\nHandling Outliers\n\n\n\nExploratory Data Analysis (EDA)\n\nDescriptive Statistics\n\nData Visualization\n\nIdentifying Patterns, Trends, Correlations\n\n\n\nStatistical Methods\n\nANOVA - Categorical Features’: How do we treat the categorical features for our data science project?\nHypothesis Testing\n\nProbability Distributions\n\nInferential Statistics\n\nSampling Methods\n\n\n\nBig Data Techniques\n\nHadoop, Spark\n\nDistributed Data Storage (e.g., HDFS, NoSQL)\nData PipeLines, ETL (Extract, Transform, Load)"
  },
  {
    "objectID": "posts/machinelearning/index.html#texttextcolor782f40machine-learning-algorithms",
    "href": "posts/machinelearning/index.html#texttextcolor782f40machine-learning-algorithms",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{\\textcolor{#782F40}{Machine Learning Algorithms}}\\)",
    "text": "\\(\\text{\\textcolor{#782F40}{Machine Learning Algorithms}}\\)\n\n\\(\\text{Supervised Learning}\\)\n(Training with labeled data: input-output pairs)\n\nRegression\n\n\nParametric\n\nSimple Linear Regression\nMultiple Linear Regression\nPolynomial Regression\n\n\n\nNon-Parametric\n\nK-Nearest Neighbor (KNN) Regression\nDecesion Trees Regression\nRandom Forest Regression\nSupport Vector Machine (SVM) Regression\n\n\n\n\n\nClassification\n\n\n\nParametric\n\nLogistic Regression\nNaive Bayes\nLinear Discriminant Analysis (LDA)\n\nQuadratic Discriminant Analysis (QDA)\n\n\n\n\nNon-Parametric\n\nKNN Classification\nDecision Tree Classification\nRandom Forest Classification\nSupport Vector Machine (SVM) Classification\n\n\n\n\nMulti-Class Classification\n\nMulti-class Classification\n\n\n\n\nBayesian or Probabilistic Classification\n\nWhat is Bayesian or Probabilistic Classification?\n\nLinear Discriminant Analysis (LDA)\n\nQuadratic Discriminant Analysis (QDA)\n\nNaive Bayes\nBayesian Network Classifier (Tree Augmented Naive Bayes (TAN))\n\n\n\n\nNon-probabilistic Classification\n\nSupport Vector Machine (SVM) Classification\n\nDecision Tree Classification\n\nRandom Forest Classification\n\nKNN Classification\n\nPerceptron\n\n\n\n\n\n\n\n\\(\\text{Unsupervised Learning}\\)\n(Training with unlabeled data)\n\n\n\nClustering\n\nk-Means Clustering\n\nHierarchical Clustering\n\nDBSCAN (Density-Based Spatial Clustering)\n\nGaussian Mixture Models (GMM)\n\n\n\n\nDimensionality Reduction\n\nPrincipal Component Analysis\n\nLatent Dirichlet Allocation (LDA)\nt-SNE (t-distributed Stochastic Neihbor Embedding)\n\nFactor Analysis\n\nAutoencoders\n\n\n\n\n\nAnomaly Detection\n\nIsolation Forests\n\nOne-Class SVM\n\n\n\n\n\n\n\\(\\text{Semi-Supervised Learning}\\)\n(Combination of labeled and unlabeled data)\n\nSelf-training\n\nCo-training\n\nLabel Propagation\n\n\n\n\\(\\text{Reinforcement Learning}\\)\n(Learning via rewards and penalties)\n\nMarkov Decision Process (MDP)\n\nQ-Learning\n\nDeep Q-Networks (DQN)\n\nPolicy Gradient Method"
  },
  {
    "objectID": "posts/machinelearning/index.html#texttextcolor782f40deep-learnings",
    "href": "posts/machinelearning/index.html#texttextcolor782f40deep-learnings",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{\\textcolor{#782F40}{Deep Learnings}}\\)",
    "text": "\\(\\text{\\textcolor{#782F40}{Deep Learnings}}\\)\n\nPyTorch\n\nArtificial Neural Networks (ANN)\n\nConvolutional Neural Networks (CNN)\n\nRecurrent Neural Networks (RNN)\n\nLong Short-Term Memory (LSTM)\n\nGenerative Adversarial Networks (GAN)"
  },
  {
    "objectID": "posts/machinelearning/index.html#texttextcolor782f40model-evaluation-and-fine-tuning",
    "href": "posts/machinelearning/index.html#texttextcolor782f40model-evaluation-and-fine-tuning",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{\\textcolor{#782F40}{Model Evaluation and Fine Tuning}}\\)",
    "text": "\\(\\text{\\textcolor{#782F40}{Model Evaluation and Fine Tuning}}\\)\n\nModel Evaluation Metrics\n\nFor Regression: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), \\(R^2\\) score\n\nFor Classification: Accuracy, Precision, Recall, F1 Score, ROC-AUC\n\nCross-validation: kFold, Stratified k-fold, leave-one-out\n\n\n\nModel Optimization\n\nBias-Variance: Bias Variance Trade off\n\nHyperparameter Tuning: Grid Search, Random Search, Bayesian Optimization\n\nFeatures Selection Techniques: Recursive Feature Elimination (RFE), L1 or Rasso Regurlarization, L2 or Ridge Regularization\n\nModel Interpretability: SHAP (Shapley values), LIME (Local Interpretable Model-agnostic Explanations)\n\n\n\nEnsemble Methods\n\nBagging: Random Forest, Bootstrap Aggregating\n\nBoosting: Gradient Boosting, AdaBoost, XGBoost, CatBoost\n\nStacking: Stacked Generalization\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/bengalitrial/index.html",
    "href": "posts/bengalitrial/index.html",
    "title": "বাংলা ভাষায় আমার লেখা || My Blog in Benglali Language",
    "section": "",
    "text": "“বাংলায় ব্লগিং করতে পারলে ভালই হতো” এমন ভাবনা থেকেই ঘাটাঘটি শুরু করলাম কিভাবে নিজের ব্লগে বাংলায় লিখতে পারি। বাংলায় মনের ভাব প্রকাশের অসংখ্য মাধ্যম রয়েছে। সামাজিক যোগাযোগের মাধ্যম, পত্রিকা, কিংবা অন্যান্য প্রতিষ্ঠিত ব্লগ। কিন্তু নিজের ব্লগে নিজে বাংলায় লিখতে পারবো কিনা তা নিয়ে একটু সংশয় ছিল কারিগরি দিকটা নিয়ে। কোয়ার্তো দিয়ে আমার এই ব্লগ সাইট বানানো। তাই কোয়ার্তোর ওয়েবসাইট ঘাঁটতে ঘাঁটতে আজ পেয়ে গেলাম কিভাবে ইউনিকোড দিয়ে লিখা যায়। এখন থেকে মাঝে মধ্যেই এখানে বাংলায় পোষ্ট করবো। দেখা যাক।"
  },
  {
    "objectID": "posts/bengalitrial/index.html#আপনক-সবগতম",
    "href": "posts/bengalitrial/index.html#আপনক-সবগতম",
    "title": "বাংলা ভাষায় আমার লেখা || My Blog in Benglali Language",
    "section": "",
    "text": "“বাংলায় ব্লগিং করতে পারলে ভালই হতো” এমন ভাবনা থেকেই ঘাটাঘটি শুরু করলাম কিভাবে নিজের ব্লগে বাংলায় লিখতে পারি। বাংলায় মনের ভাব প্রকাশের অসংখ্য মাধ্যম রয়েছে। সামাজিক যোগাযোগের মাধ্যম, পত্রিকা, কিংবা অন্যান্য প্রতিষ্ঠিত ব্লগ। কিন্তু নিজের ব্লগে নিজে বাংলায় লিখতে পারবো কিনা তা নিয়ে একটু সংশয় ছিল কারিগরি দিকটা নিয়ে। কোয়ার্তো দিয়ে আমার এই ব্লগ সাইট বানানো। তাই কোয়ার্তোর ওয়েবসাইট ঘাঁটতে ঘাঁটতে আজ পেয়ে গেলাম কিভাবে ইউনিকোড দিয়ে লিখা যায়। এখন থেকে মাঝে মধ্যেই এখানে বাংলায় পোষ্ট করবো। দেখা যাক।"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "research.html#current-research",
    "href": "research.html#current-research",
    "title": "",
    "section": "Current Research",
    "text": "Current Research\n\n\n\n\n\n\n\n\n\n\nHigher-order Langevin Algorithms\n\n\n\nThanh L. Dang, Mert Gurbuzbalaban, Mohammad Rafiqul Islam, Nihan Yao, Lingjiong Zhu\n\n\nJul 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflected Underdamped Langevin Monte Carlo\n\n\n\nHengrong Du, Qi Feng, Mert Gurbuzbalaban, Mohammad Rafiqul Islam, Lingjiong Zhu\n\n\nMar 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized EXTRA stochastic gradient Langevin dynamics\n\n\n\nMert Gurbuzbalaban, Mohammad Rafiqul Islam, Xiaoyu Wang, Lingjiong Zhu\n\n\nFeb 12, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html#research-interest",
    "href": "research.html#research-interest",
    "title": "",
    "section": "Research Interest",
    "text": "Research Interest\n\nMachine Learning: Centralized and Decentralized Stochastic Gradient Descent (SGD);Algorithmic Stability in SGD; Differential Privacy in machine learning algorithms\nApplied Data Science\nFinancial Mathematics"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "",
    "section": "Publications",
    "text": "Publications\n\nGeneralized EXTRA stochastic gradient Langevin dynamics\nMert, Gurbuzbalaban; Islam, Mohammad Rafiqul; Wang, Xiaoyu; Zhu, Lingjiong (2024) “Generalized EXTRA stochastic gradient Langevin dynamics.” arXiv preprint arXiv.2412.01993\nGJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies \nMostafa, F; Saha, P; Islam, Mohammad R.; Nguyen, N. (2020) “GJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies.” Journal of Risk and Financial Management.\nComparison of Financial Models for Stock Price Prediction \nIslam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management."
  },
  {
    "objectID": "research.html#course-projects",
    "href": "research.html#course-projects",
    "title": "",
    "section": "Course Projects",
    "text": "Course Projects\n\nOption pricing techniques: A performance-based comparative study of the randomized quasi-Monte Carlo method and Fourier cosine method\nAdvisor: Prof. Giray Ökten\n\nPricing financial derivatives such as options with desired accuracy can be hard due to the nature of the functions and complicated integrals required by the pricing techniques. In this paper we investigate the pricing methodology of the European style options using two advanced numerical methods, namely, Quasi-Monte Carlo and Fourier Cosine (COS). For the RQMC method, we use the random-start Halton sequence. We use the Black-Scholes-Merton model to measure the pricing quality of both of the methods. For the numerical results we compute the option price of the call option and we found a few reasons to prefer the RQMC method over the COS method to approximate the European style options.\n\nThe Relationship Between Forced Sexual Activities And Suicidal Attempts Of The Victims\nAdvisor: Dr. Andy Chang\n\nIn project, we apply data-analytic methods to further explore the relationship between forced sexual activities and suicidal behavior among adolescents in the United States. Our findings build on existing literature that explores this relationship. The sample of the study was taken from the Youth Risk Behavior Surveillance System survey 2017. We used a chi-squared test to find the association of forced sexual activities and suicidal behavior, and we found a strong association. Then we used bi-variate logistic regression analysis to ascertain the association of race, age, sex, and education with suicidal attempts after experiencing forced sexual activity (sexual assault). The results of the following paper provide greater insight into the relationship between forced sexual activities and suicide attempts by the adolescents.\n\nStudy of Runge-Kutta Method of Higher orders and its Applications\nAdvisor: Dr. Md. Abdus Samad \n\nThis project is concerned with the study on Runge-Kutta method to apply on different order of differential equation and solve different types of problem such as initial value problem and boundary value problem in ordinary differential equation. At first we discuss about the definition and generation of differential equation specially based on partial differential equation and then definition of Runge-kutta method and the derivation of midpoint method and the formula of Runge-Kutta metod of fourth order and sixth order. We also write FORTRAN 90/95 program for different order of Runge-Kutta methods. We have solved some examples of fourth order R-K method and sixth order R-K method to get the application of R-K method. We also compared the solution of R-K method with exact solution for different step sizes. Then we have given simultaneous first order differential equation and second order differential equation and then solved them by fourth order Runge-Kutta method. At last we have discussed the boundary value problem which we have solved by fourth and sixth order R-K method. After that we have written the algorithm of shooting method and showed computer results with the difference between two answer along with percentages of error."
  },
  {
    "objectID": "research.html#talks-and-presentations",
    "href": "research.html#talks-and-presentations",
    "title": "",
    "section": "Talks and Presentations",
    "text": "Talks and Presentations"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Download a PDF"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\n\nPh.D in Mathematics, Florida State University; Florida, USA 2026 (expected)\nM.S. in Mathematics, Youngstown State University; Ohio, USA 2020\nM.S. in Applied Mathematics, University of Dhaka; Dhaka, Bangladesh 2016\nB.S. in Mathematics, University of Dhaka; Dhaka, Bangladesh 2014"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "CV",
    "section": "Work experience",
    "text": "Work experience\n\nGraduate Teaching Assistant (Fall 2021- To Date)\n\nFlorida State University\nDuties includes: Teaching, Proctoring, and Grading\nSupervisor: Penelope Kirby, Ph.D\n\nGraduate Teaching Assistant (Fall 2018 - Spring 2020)\n\nYoungstown State University University\nDuties included: Teaching, Proctoring, and Grading\nSupervisor: G. Jay Kerns, Ph.D\n\nAssistant Vice President (September 2017 - July 2018)\n\nDelta Life Insurance Company Ltd. Dhaka, Bangladesh\nDuties included: Calculated all types of claims (death, surrender, and maturity) using excel spreadsheets.\nProcessed approximately 500 claims each week and submitted corresponding statistical reports to the higher authority.\nWorked in a team to develop a new short-term endowment assurance product which played an important role to increase the company’s new business.\nRefurbished a without risk endowment product which was out of the sale. Priced insurance premiums based on different risk factors for bigger clients which impacted our life fund significantly.\nCalculated reserves for group endowment, term and premium back policies which was a vital part of the final valuation report.\nLiaised directly with the consulting actuary and provided all sorts of technical and documental supports during actuarial valuation\nSupervisor: Md. Salahuddin Soud, VP"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "CV",
    "section": "Skills",
    "text": "Skills\n\nLanguage\n\nBengali: Native\nEnglish: Fluent\n\nComputer Literacy\n\nProgramming Languages: Python, FORTRAN, Julia, R, MATLAB, Mathematica\nSoftware Development Tools: Git, GitHub, PyPi\n\nMusical Instrument: Amateur/Novice Bamboo flute player"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "CV",
    "section": "Publications",
    "text": "Publications\n\nGeneralized EXTRA stochastic gradient Langevin dynamics\nMert, Gurbuzbalaban; Islam, Mohammad Rafiqul; Wang, Xiaoyu; Zhu, Lingjiong (2024) “Generalized EXTRA stochastic gradient Langevin dynamics.” arXiv preprint arXiv.2412.01993\n\nGJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies \nMostafa, F; Saha, P; Islam, Mohammad R.; Nguyen, N. (2020) “GJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies.” Journal of Risk and Financial Management.\nComparison of Financial Models for Stock Price Prediction \nIslam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management."
  },
  {
    "objectID": "cv.html#talks-and-presentations",
    "href": "cv.html#talks-and-presentations",
    "title": "CV",
    "section": "Talks and Presentations",
    "text": "Talks and Presentations\n\n The Heavy-Tail Phenomenon in Decentralized Stochastic Gradient Descent\nNovember 20, 2023\nPresentation at James J Love Building, Florida State University, Tallahassee, Florida\n\n Decentralized Stochastic Gradient Langevin Dynamics and Hamiltonian Monte Carlo\nOctober 05, 2023\nPresentation at James J Love Building, Florida State University, Tallahassee, Florida\n\n Sensitivity analysis for Monte Carlo and Quasi Monte Carlo option pricing\nApril 28, 2020\nPresentation at Cafaro Hall, Youngstown State University, Youngstown, Ohio"
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "CV",
    "section": "Teaching",
    "text": "Teaching\n\n Spring 2025: MAC2311 Calculus and Analytic Geometry I\n Spring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n Fall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n Spring 2023: MAC1140 PreCalculus Algebra\n\n Fall 2022: MAC2311 Calculus and Analytic Geometry I\n\n Fall 2021 and Spring 2022: PreCalculus and Algebra\n\n Fall 2018 to Spring 2020: College Algebra, Trigonometry"
  },
  {
    "objectID": "cv.html#awards-and-affiliations",
    "href": "cv.html#awards-and-affiliations",
    "title": "CV",
    "section": "Awards and Affiliations",
    "text": "Awards and Affiliations\n\nAwards\n\nBettye Anne Busbee Case Graduate Fellowship & Doctoral Mentorship Recognition 2024\n\nOutstanding Graduate Student in Statistics Award for the 2019-2020 academic year, Youngstown State University.\n\nGraduate College Premiere Scholarship, Youngstown State University.\n\nMetLife Bangladesh Actuarial Study Program 2015\n\n\n\nAffiliations\n\nBangladesh Mathematical Society: Life Member\nSociety of Actuaries, SOA: Student Member\nAmerican Mathematical Society, AMS\nSociety for Industrial and Applied Mathematics"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "portfolio.html#data-science-and-machine-learning-projects",
    "href": "portfolio.html#data-science-and-machine-learning-projects",
    "title": "",
    "section": "Data Science and Machine Learning Projects",
    "text": "Data Science and Machine Learning Projects\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nAuto Loan Decision Model\n\n\n\n\n\n\nRafiq Islam\n\n\nNov 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information\n\n\n\n\n\n\nRafiq Islam\n\n\nOct 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisease diagnosis using classification and NLP\n\n\n\n\n\n\nRebecca Ceppas de Castro, Fulya Tastan, Philip Barron, Mohammad Rafiqul Islam, Nina Adhikari, Viraj Meruliya\n\n\nJun 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Selection: A linear regression approach to find the impact of the features of e-commerce sales data\n\n\n\n\n\n\nRafiq Islam\n\n\nAug 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsurance Cost Forecast by using Linear Regression\n\n\n\n\n\n\nRafiq Islam\n\n\nAug 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLendingclub’s loan default prediction\n\n\n\n\n\n\nRafiq Islam\n\n\nFeb 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Product Success Using Customer Reviews and Sales Data\n\n\n\n\n\n\nRafiq Islam\n\n\nOct 11, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio.html#software-package-and-development",
    "href": "portfolio.html#software-package-and-development",
    "title": "",
    "section": "Software, Package, and Development",
    "text": "Software, Package, and Development\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nStreamlit Web App\n\n\n\nFriday, August 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Application Library: desgld packaging\n\n\n\nFriday, May 3, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio.html#teaching",
    "href": "portfolio.html#teaching",
    "title": "",
    "section": "Teaching",
    "text": "Teaching\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\nSpring 2025: MAC2311 Calculus With Analytic Geometry I\n\n\n\n\nSpring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\nFall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\nSpring 2023: MAC1140 PreCalculus Algebra\n\n\n\n\nFall 2022: MAC2311 Calculus and Analytic Geometry I\n\n\n\n\nFall 2021 and Spring 2022: PreCalculus and Algebra\n\n\n\n\nFall 2018 to Spring 2020: College Algebra, Trigonometry\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dsandml/bayesianclassification/index.html#introduction",
    "href": "dsandml/bayesianclassification/index.html#introduction",
    "title": "Bayesian Probabilistic Models for Classification",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields including machine learning, to make predictions and decisions under uncertainty.\nBayes’s theorem is based on the definition of conditional probability. For two events \\(A\\) and \\(B\\) with \\(\\mathbb{P}(B) \\neq 0\\), we define the conditional probability of occurring \\(A\\) given that \\(B\\) has already occurred.\n\n\n\n\n\n\n\\(\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\)\nSimilarly, the conditional probability of occuring \\(B\\) given that \\(A\\) has already occured with \\(\\mathbb{P}(A) \\ne 0\\) is\n\\[\n\\mathbb{P}(B|A)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(A)}\n\\]\nFrom this equation, we can derive that the joint probability of \\(A\\cap B\\) is \\[\n\\mathbb{P}(A\\cap B) = \\mathbb{P}(B | A) \\mathbb{P} (A) = \\mathbb{P}(A | B) \\mathbb{P} (B)\n\\]"
  },
  {
    "objectID": "dsandml/bayesianclassification/index.html#bayess-theorem",
    "href": "dsandml/bayesianclassification/index.html#bayess-theorem",
    "title": "Bayesian Probabilistic Models for Classification",
    "section": "Bayes’s Theorem",
    "text": "Bayes’s Theorem\n\nFor Two Events or Random Variables\nBayes’s theorem is based on these conditional probabilities. It states that the likelihood of occuring the event \\(A\\) given that the event \\(B\\) has occured is given as\n\\[\n\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(B | A)\\mathbb{P}(A)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B | A)\\mathbb{P}(A)}{\\mathbb{P}(B\\cap A)+\\mathbb{P}(B\\cap A^c)} = \\frac{\\mathbb{P}(B | A)\\mathbb{P}(A)}{\\mathbb{P}(B | A)\\mathbb{P}(A)+\\mathbb{P}(A | B)\\mathbb{P}(B)}\n\\]\nwhere, in Bayesin terminology,\n\n\\(\\mathbb{P}(A|B)\\) is called posterior probability of \\(A\\) given the event \\(B\\) or simply, posterior distribution.\n\n\\(\\mathbb{P}(B|A)\\) is the likelihood: the probability of evidence \\(B\\) given that \\(A\\) is true.\n\n\\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B)\\) are the probabilities of occuring \\(A\\) and \\(B\\) respectively, without any dependence on each other.\n\n\\(\\mathbb{P}(A)\\) is called the prior probability or prior distribution and \\(\\mathbb{P}(B)\\) is called the marginal likelihood or marginal probabilities.\n\nFor two continuous random variable \\(X\\) and \\(Y\\), the conditional probability density function of \\(X\\) given the occurence of the value \\(y\\) of \\(Y\\) can be given as\n\\[\nf_{X|Y} (x | y) =\\frac{f_{X,Y}(x,y)}{f_Y(y)}\n\\]\nor the otherway around,\n\\[\nf_{Y|X} (y | x) =\\frac{f_{X,Y}(x,y)}{f_X(x)}\n\\]\nTherefore, the continuous version of Bayes’s theorem is given as follows\n\\[\nf_{Y|X}(y) = \\frac{f_{X|Y}(x)f_Y(y)}{f_X(x)}\n\\]\n\n\nGeneralization of Bayes’s Theorem\nFor \\(n\\) disjoint set of discrete events \\(B_1,B_2\\dots, B_n\\) where \\(\\Omega = \\cup_{i}^{n}B_i\\) and for any event \\(A\\in \\Omega\\), we will have\n\\[\n\\mathbb{P}(A) = \\sum_{i=1}^{n}\\mathbb{P}(A\\cap B_i)\n\\]\nand this is true by the law of total probability.\n\n\n\n\n\nThen the Bayes’s rule extends to the following\n\\[\n\\mathbb{P}(B_i|A) = \\frac{\\mathbb{P}(A|B_i)\\mathbb{P}(B_i)}{\\mathbb{P}(A)}=\\frac{\\mathbb{P}(A|B_i)\\mathbb{P}(B_i)}{\\sum_{i=1}^{n}\\mathbb{P}(A|B_i)\\mathbb{P}(B_i)}\n\\]\nThe continuous version would be \\[\nf_{Y=y|X=x}(y|x) = \\frac{f_{X|Y=y}(x)f_Y(y)}{\\sum_{i=1}^{n}\\int_{-\\infty}^{\\infty}f_{X|Y=y}(x|u)f_{Y}(u)du}\n\\]"
  },
  {
    "objectID": "posts/jobandintern/index.html",
    "href": "posts/jobandintern/index.html",
    "title": "Basic Statistics and Probability",
    "section": "",
    "text": "Bayesian Inference in Machine Learning: Part 1\n\n\n\nRafiq Islam\n\n\nSunday, July 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation, Bivariate, and Regression Analysis\n\n\n\nRafiq Islam\n\n\nWednesday, December 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Structure and Algorithms: Basic Programming Hacks\n\n\n\nRafiq Islam\n\n\nWednesday, September 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonte-Carlo Methods: PRNGs\n\n\n\nRafiq Islam\n\n\nSunday, August 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPricing Derivatives Using Black-Scholes-Merton Model\n\n\n\nRafiq Islam\n\n\nSunday, November 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview probabilities\n\n\n\nRafiq Islam\n\n\nThursday, August 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Key Statistical Concepts for Interview Prep\n\n\n\nRafiq Islam\n\n\nThursday, September 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "jobandintern/statisticaltalk/index.html",
    "href": "jobandintern/statisticaltalk/index.html",
    "title": "Some Key Statistical Concepts for Interview Prep",
    "section": "",
    "text": "In the world of data analysis and machine learning, statistics plays a vital role in making sense of the data. Whether you’re estimating parameters, testing hypotheses, or understanding relationships between variables, statistical concepts guide how we interpret data. In this post, I want to summarise and collect some fundamental statistical ideas that are quite common and asked in many data science, machine learning, and quant interviews"
  },
  {
    "objectID": "jobandintern/statisticaltalk/index.html#basic-statistical-terminologies",
    "href": "jobandintern/statisticaltalk/index.html#basic-statistical-terminologies",
    "title": "Some Key Statistical Concepts for Interview Prep",
    "section": "Basic Statistical Terminologies",
    "text": "Basic Statistical Terminologies\n\nThe mean\nThe mean is one of the most basic statistical concepts and represents the average value of a dataset. It’s calculated by summing all the values in a dataset and then dividing by the number of observations.\nMathematically, for a set of discrete observations \\(x_1, x_2, ..., x_n\\), the mean \\(\\mu\\) or Expected Value is defined as:\n\\[\\begin{align*}\n\\mu &=  \\frac{1}{n} \\sum_{i=1}^{n} x_i\\\\\n\\implies \\mathbb{E}[X] &= \\sum_{i=1}^{n} x_i\\mathbb{P}(X=x_i)\n\\end{align*}\\]\nFor a continuous random variable \\(X\\), the mean\n\\[\n\\mu = \\mathbb{E}[X]=\\int_{-\\infty}^{\\infty}xf_X(x)dx\n\\]\n\nwhere, \\(\\mathbb{P}(X=x)\\) is the probability mass function (pmf) and \\(f_X(x)\\) is the probability density function (pdf) of the random variable \\(X\\), depending on whether it is discrete or contineous type. The mean helps describe the central tendency of data, but it can be sensitive to outliers.\n\n\n\nVariance\n\nVariance measures the spread or dispersion of a dataset relative to its mean. It tells us how far the individual data points are from the mean. A small variance indicates that data points are clustered closely around the mean, while a large variance means they are spread out.\n\nThe formula for variance \\(\\sigma^2\\) is:\n\\[\\begin{align*}\n    \\sigma^2=Var(X)&=\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2\\\\\n    &=\\mathbb{E}\\left[(X-\\mathbb{E}[X])^2\\right]\\\\\n    &=\\mathbb{E}\\left[(X^2-2X\\mathbb{E}[X]+(\\mathbb{E}[X])^2)\\right]\\\\\n    &=\\mathbb{E}[X^2]-2\\mathbb{E}[X]\\mathbb{E}[X]+(\\mathbb{E}[X])^2\\\\\n    &=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2\\\\\n\\end{align*}\\]\nHowever, the population and sample variance formula are slightly different. For discrete observations, the sample variance is given as\n\\[ s= \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]\nInstead of dividing by \\(n\\) we devide by \\(n-1\\) to have the sample variance unbiased and bigger than the population variance so that it contains the true population variance.\nExamples\n\nNormal Distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) has the pdf \\(f_{X}(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]}\\)\n\nStandard Normal Distribution with mean \\(0\\) and variance \\(1\\) has the pdf \\(f_{X}(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{x^2}{2}\\right]}\\)\n\nNow if \\(\\log X\\sim \\mathbfcal{N}(0,1)\\) then what is the distribution of \\(X\\)?\n\n\n\nCovariance\n\nCovariance measures how two variables move together. If the covariance is positive, the two variables tend to increase or decrease together. If negative, one variable tends to increase when the other decreases.\n\nThe formula for covariance between two variables \\(X\\) and \\(Y\\) is:\n\\[\n\\text{Cov}(X, Y) = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\mu_X)(Y_i - \\mu_Y)\n\\]\nHowever, covariance doesn’t indicate the strength of the relationship, which brings us to correlation.\n\n\nCorrelation\n\nCorrelation is a standardized measure of the relationship between two variables. It ranges from \\(-1\\) to \\(1\\), where \\(1\\) indicates a perfect positive relationship, \\(-1\\) a perfect negative relationship, and \\(0\\) no relationship.\n\nThe most common correlation metric is Pearson correlation, defined as:\n\\[\n\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\\]\nUnlike covariance, correlation gives a clearer picture of the strength and direction of a linear relationship between variables.\n\n\nP-Values and Hypothesis Testing\n\nP-values and hypothesis testing form the backbone of inferential statistics. Hypothesis testing is used to determine if a given assumption (the null hypothesis \\(H_0\\)) about a population parameter is true or not.\n\n\nThe null hypothesis \\(H_0\\) typically suggests no effect or no difference.\nThe alternative hypothesis \\(H_1\\) is the claim you want to test.\n\nThe p-value is the probability of observing a result as extreme as, or more extreme than, the one obtained, assuming the null hypothesis is true. A small p-value (usually less than 0.05) indicates that the null hypothesis is unlikely, and we may reject it in favor of the alternative hypothesis.\n\n\nMaximum Likelihood Estimation (MLE)\n\nMaximum Likelihood Estimation (MLE) is a method for estimating the parameters of a statistical model. The idea behind MLE is to find the parameter values that maximize the likelihood function, which represents the probability of observing the given data under a particular model.\n\nGiven a parameter \\(\\theta\\) and observed data \\(X\\), the likelihood function is:\n\\[\nL(\\theta | X) = P(X | \\theta)\n\\]\nMLE finds the parameter \\(\\hat{\\theta}\\) that maximizes this likelihood:\n\\[\n\\hat{\\theta} = \\arg\\max_{\\theta} L(\\theta | X)\n\\]\nMLE is widely used in statistical modeling, from simple linear regression to complex machine learning algorithms.\n\n7. Maximum A Posteriori (MAP)\nWhile MLE focuses on maximizing the likelihood, Maximum A Posteriori (MAP) estimation incorporates prior information about the parameters. MAP is rooted in Bayesian statistics, where the goal is to find the parameter that maximizes the posterior distribution.\nThe posterior is given by Bayes’ Theorem:\n\\[\nP(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{P(X)}\n\\]\nMAP finds the parameter \\(\\hat{\\theta}_{\\text{MAP}}\\) that maximizes the posterior probability:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} P(\\theta | X)\n\\]\nUnlike MLE, MAP estimation incorporates the prior distribution \\(P(\\theta)\\), making it more robust when prior knowledge is available."
  },
  {
    "objectID": "jobandintern/dsa/index.html",
    "href": "jobandintern/dsa/index.html",
    "title": "Data Structure and Algorithms: Basic Programming Hacks",
    "section": "",
    "text": "Code\nimport time\n\ndef time_required(func):\n    def wrapper(*args, **kwargs):\n        starting = time.perf_counter()\n        output = func(*args, **kwargs)\n        ending = time.perf_counter()\n        elapsed = ending - starting\n        print(f'Time required: {elapsed:.6f} seconds')\n        return output\n    return wrapper"
  },
  {
    "objectID": "jobandintern/dsa/index.html#linked-list",
    "href": "jobandintern/dsa/index.html#linked-list",
    "title": "Data Structure and Algorithms: Basic Programming Hacks",
    "section": "Linked List",
    "text": "Linked List\n\nclass Node:\n    def __init__(self, value, next=None) -&gt; None:\n        self.value = value\n        self.next = next\n\ndef linklist(arr):\n    if not arr:\n        return None \n    head = Node(arr[0])\n    current = head \n    for value in arr[1:]:\n        current.next = Node(value)\n        current = current.next \n    return head \n\ndef print_linklist(head):\n    current = head\n    print(\"[\", end=\"\")\n    while current:\n        print(current.value, end=\", \" if current.next else \"]\")\n        current = current.next\n    print()\n\n\n1. Reverse a linked list: Type I\n\n\n\n\ndef reverse(head):\n    prev = None \n    curr = head \n    while curr:\n        next = curr.next \n        curr.next = prev \n        prev = curr \n        curr = next \n    return prev \n\nh = linklist([1,2,3,4,5])\nprint('Original List:')\nprint_linklist(h)\n\nh_reversed = reverse(h)\nprint('Reversed List')\nprint_linklist(h_reversed)\n\nOriginal List:\n[1, 2, 3, 4, 5]\nReversed List\n[5, 4, 3, 2, 1]\n\n\n\n\n2. Reverse a linked list: Type II\n\ndef reverse_in_between(head, left, right):\n    dummy = Node(0, head)\n    leftPrev = dummy\n    curr = head \n\n    for _ in range(left-1):\n        leftPrev = curr \n        curr = curr.next \n    \n    prev = None \n    tail = curr \n\n    for _ in range(right - left + 1):\n        next = curr.next \n        curr.next  = prev \n        prev = curr \n        curr = next \n    \n    leftPrev.next = prev \n    tail.next = curr \n\n    return dummy.next if left != 1 else prev\n\nh = linklist([1,2,3,4,5])\nprint('Original List:')\nprint_linklist(h)  \n\nh_reversed = reverse_in_between(h,2,4)\nprint('Reversed List between 2 and 4')\nprint_linklist(h_reversed)\n\nOriginal List:\n[1, 2, 3, 4, 5]\nReversed List between 2 and 4\n[1, 4, 3, 2, 5]"
  },
  {
    "objectID": "jobandintern/dsa/index.html#arrays-lists-and-strings",
    "href": "jobandintern/dsa/index.html#arrays-lists-and-strings",
    "title": "Data Structure and Algorithms: Basic Programming Hacks",
    "section": "Arrays, Lists, and Strings",
    "text": "Arrays, Lists, and Strings\n\n1. Intersection of two arrays\nSay you have two arrays. Write a function to get the intersection of the two. For example, if \\(A=[2,3,5,6,8]\\) and \\(B=[4,6,8]\\), then the function should return \\([6,8]\\)\nBrute Force\n\n\nOne way to solve this problem is using brute force solution, that is using two nested loops. But this method takes the time complexity of \\(O(n\\times m)\\) given that the lenght of set A is \\(n\\) and set B is \\(m\\). And here is how it is:\n\n\n@time_required\ndef intersection_of_two_sets(A,B):\n    set_A = set(A)\n    set_B = set(B)\n    intersection = []\n    for a in set_A:\n        for b in set_B:\n            if a==b:\n                intersection.append(a)\n    return intersection\nA = [2,3,5,6,8]\nB = [4,6,8]\nprint(intersection_of_two_sets(A,B))\n\nTime required: 0.000005 seconds\n[6, 8]\n\n\nHash Map Approach: In hash map approach, we can solve the same problem but in this case the time and space complexity is \\(O(n+m)\\)\n\n@time_required\ndef intersection_of_two_sets(A,B):\n    set_A = set(A)\n    set_B = set(B)\n    if len(set_A) &lt; len(set_B):\n        return [a for a in set_A if a in set_B]\n    return [b for b in set_B if b in set_A]\n\nA = [2,3,5,6,8]\nB = [4,6,8]\nprint(intersection_of_two_sets(A,B))\n\nTime required: 0.000004 seconds\n[8, 6]\n\n\nThe reason we’re getting \\([8,6]\\) instead of \\([6,8]\\) is because sets in Python are unordered collections, meaning that when you convert the lists \\(A\\) and \\(B\\) to sets, the order of elements is not preserved. So, when we iterate over set_A or set_A, the order can change.\nBetter Approach: If we want to maintain the order of the elements in the original list \\(A\\) or \\(B\\), we can iterate over the original list directly rather than converting it to a set. Here’s how:\n\n@time_required\ndef intersection_of_two_sets(A, B):\n    set_B = set(B)  \n    return [a for a in A if a in set_B]\n\nA = [2, 3, 5, 6, 8]\nB = [4, 6, 8]\nprint(intersection_of_two_sets(A, B))\n\nTime required: 0.000002 seconds\n[6, 8]\n\n\n\n\n2. Max product of \\(k\\) elements from an array of \\(n\\) elements\n\nSay we have an array of size \\(n\\). We want to find the maximum of the products of \\(k\\) elements from the array where \\(k &lt; n\\). For example, if we set \\(k=3\\) and if we have \\(A=[1,2,3,4,5,6]\\) then the answer is 120, if we have \\(B=[-3,-4,3,5]\\) then the answer is 60.\n\nSolution\nnlargest and nsmallest are two functions from the heapq library that returns \\(n\\) largest and \\(n\\) smallest numbers in decreasing and increasing order, respectively. For example,\n\nimport heapq\n\nA = [1,2,3,4,5,6]\nB = [-3,-4,3,5]\nprint('For set {} \\n largest 3 numbers {} \\n smallest 2 numbers'.format(A,heapq.nlargest(3,A)),heapq.nsmallest(2,A))\nprint('\\n')\nprint('For set {} \\n largest 3 numbers {} \\n smallest 2 numbers'.format(B,heapq.nlargest(3,B)),heapq.nsmallest(2,B))\n\nFor set [1, 2, 3, 4, 5, 6] \n largest 3 numbers [6, 5, 4] \n smallest 2 numbers [1, 2]\n\n\nFor set [-3, -4, 3, 5] \n largest 3 numbers [5, 3, -3] \n smallest 2 numbers [-4, -3]\n\n\nNow if all the elements are positive, then the maximum product of \\(k=3\\) elements would just be the product of the largest three element. However, if the set contains negative numbers like the one in the example, product of the smallest two negative numbers and the first element from the nlargest element that would be the largest.\n\nk = 3\n\ndef max_of_three_element_product(arr):\n    m = heapq.nlargest(k, arr)\n    n = heapq.nsmallest(k-1, arr)\n    return max(m[0]*m[1]*m[2], m[0]*n[0]*n[1])\nA = [1,2,3,4,5,6]\nB = [-3,-4,3,5]\n\nprint('Max product of {} elements from set A={} is'.format(k,A), max_of_three_element_product(A))\nprint('Max product of {} elements from set B={} is'.format(k,B), max_of_three_element_product(B))\n\nMax product of 3 elements from set A=[1, 2, 3, 4, 5, 6] is 120\nMax product of 3 elements from set B=[-3, -4, 3, 5] is 60\n\n\n\n\n3. Find the \\(k\\) nearest points from a given point\n\n@time_required\ndef knearest(points: list[list[int]], k: int) -&gt; list[list[int]]:\n    dis = []\n    for x in points:\n        d = pow(pow(x[0],2)+pow(x[1],2),0.5)\n        dis.append((x,d))\n    dis.sort(key= lambda item: item[1])\n    return [x for x,_ in dis[:k]]\npts = [[2,-1],[3,2],[4,1],[-1,-1],[-2,2]]\nk = 3\nprint(knearest(pts,k))\n\nTime required: 0.000010 seconds\n[[-1, -1], [2, -1], [-2, 2]]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "index.html#rafiq-islam",
    "href": "index.html#rafiq-islam",
    "title": "",
    "section": "Rafiq Islam",
    "text": "Rafiq Islam\nPh.D. Candidate in Mathematics\nFlorida State University\nContact\nEmail:  rislam@fsu.edu\nOffice:  Love Building: Room 331A\nOffice Hours:  M, W: 12:00 pm -1:00 pm  T: 11:30 am - 12:30 pm\nFind me on"
  },
  {
    "objectID": "dsandml/bayesianclassification/index.html#probabilistic-models",
    "href": "dsandml/bayesianclassification/index.html#probabilistic-models",
    "title": "Bayesian Probabilistic Models for Classification",
    "section": "Probabilistic Models",
    "text": "Probabilistic Models\n\nBayes’s theorem gets us the posterior probability given the data with a prior. Therefore, for classification tasks in machine learning, we can use Bayesin style models for classification by maximizing the numerator and minimizing the denominator in the previous equation, for any given class. For instance, say we have a \\(d-\\) dimensional data collected as a random matrix \\(X\\) and the response variable \\(y\\) is a categorical one with \\(c\\) categories. Then for a given data vector \\(X'\\), the posterior distibution that it falls for category \\(j\\) is given as\n\n\\[\n\\mathbb{P}(y=j|X=X')=\\frac{\\pi_j f_j(X')}{\\sum_{i=1}^{c}\\pi_i f_i(X')}\n\\]\nwhere,\n\n\\(f_i(X)\\) is the probability density function of the features conditioned on \\(y\\) being class \\(i\\)\n\n\\(\\pi_i =\\mathbb{P}(y=i)\\)\n\nWe can estimate \\(\\pi_i\\) as the fraction of observations which belong to class \\(i\\).\n\nLinear Discriminant Analysis (LDA)\n\nTo connect Linear Discriminant Analysis (LDA) with the Bayesian probabilistic classification, we start by considering the Bayes Theorem and the assumptions made in LDA. We adapt the Bayes theorem for classification as follows\n\n\\[\nP(C_k | \\mathbf{x}) = \\frac{P(\\mathbf{x} | C_k) P(C_k)}{P(\\mathbf{x})}\n\\]\nWhere:\n\n\\(P(C_k | \\mathbf{x})\\) is the posterior probability that \\(\\mathbf{x}\\) belongs to class \\(C_k\\),\n\\(P(\\mathbf{x} | C_k)\\) is the likelihood (the probability of observing \\(\\mathbf{x}\\) given class \\(C_k\\)),\n\\(P(C_k)\\) is the prior probability of class \\(C_k\\),\n\\(P(\\mathbf{x})\\) is the marginal likelihood (normalizing constant).\n\n\nGaussian Assumption in LDA\nLDA assumes that:\n\nThe likelihood for each class follows a Gaussian distribution with a common covariance matrix \\(\\Sigma\\), i.e.,\n\n\\[\nP(\\mathbf{x} | C_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\\right)\n\\]\nwhere \\(\\boldsymbol{\\mu}_k\\) is the mean of class \\(C_k\\) and \\(\\Sigma\\) is the shared covariance matrix. Now let’s talk about \\(\\boldsymbol{\\mu}_k\\) and \\(\\Sigma\\).\nOne feature or dimension\nFor a single feature \\(x\\) and \\(N_k\\) samples \\(x_{k,1},x_{k,2},\\dots, x_{k,N}\\) for class \\(C_k\\), the mean \\(\\mu_k\\):\n\\[\n\\mu_k = \\frac{1}{N_k}\\sum_{i=1}^{N_k} x_{k,i}\n\\]\nand variance \\(\\sigma^2\\) is calculated as the variance within-class variance \\(\\sigma_k^2\\) for each class\n\\[\n\\sigma_k^2 = \\frac{1}{N_k-1}\\sum_{i=1}^{N_k}(x_{k,i}-\\mu_k)^2\n\\]\nand then the pooled variance \\(\\sigma^2\\) is calculated by averaging these variances, weighted by the degrees of freedom in each class:\n\\[\n\\sigma^2 = \\frac{1}{n-\\mathcal{C}}\\sum_{k=1}^{\\mathcal{C}}\\sum_{i=1}^{N_k}(x_{k,i}-\\mu_k)^2\n\\]\nwhere, \\(n\\) is the total number of samples accross all classes, \\(\\mathcal{C}\\) is the number of classes, and \\(x_{k,i}\\) are samples from each class \\(C_k\\).\nFor multi-dimensional data\nIf we have \\(d\\) features (e.g., if \\(\\mathbf{x}\\) is a \\(d-\\)dimensional vector), we calculate the mean vector \\(\\boldsymbol{\\mu}_k\\) for each feature across the \\(N_k\\) samples in class \\(C_k\\) as follows\n\\[\n\\boldsymbol{\\mu}_k = \\frac{1}{N_k}\\sum_{i=1}^{N_k}\\mathbf{x}_{k,i}\n\\]\nand the covariance matrix for each class \\(C_k\\):\n\\[\n\\Sigma_k = \\frac{1}{N_k}\\sum_{i=1}^{N_k} (\\mathbf{x}_{k,i}-\\boldsymbol{\\mu}_k)(\\mathbf{x}_{k,i}-\\boldsymbol{\\mu}_k)^T\n\\]\nTherefore, the pooled variance\n\\[\n\\Sigma = \\frac{1}{n-\\mathcal{C}}\\sum_{k=1}^{\\mathcal{C}}\\sum_{i=1}^{N_k} (\\mathbf{x}_{k,i}-\\boldsymbol{\\mu}_k)(\\mathbf{x}_{k,i}-\\boldsymbol{\\mu}_k)^T\n\\]\n\n\nLog Likelihood Ratio\n\nFor simplicity, let’s say we have only two classes \\(C_1\\) and \\(C_2\\). To derive a decision boundary, we take the ratio of the posterior probabilities for two classes \\(C_1\\) and \\(C_2\\), and then take the logarithm. The rationality behind this approach is when we divide a relatively bigger number by a smaller number we get a larger number and smaller number if we reverse the divison. Since we are working with the probabilities, therefore, we take logarithm.\n\n\\[\\begin{align*}\n\\log\\left(\\frac{P(C_1 | \\mathbf{x})}{P(C_2 | \\mathbf{x})}\\right) &= \\log\\left(\\frac{P(\\mathbf{x} | C_1) P(C_1)}{P(\\mathbf{x} | C_2) P(C_2)}\\right)\\\\\n& =\\log\\left(\\frac{P(\\mathbf{x} | C_1)}{P(\\mathbf{x} | C_2)}\\right) + \\log\\left(\\frac{P(C_1)}{P(C_2)}\\right)\n\\end{align*}\\]\nUsing the Gaussian likelihood assumption, we expand the terms \\(P(\\mathbf{x} | C_1)\\) and \\(P(\\mathbf{x} | C_2)\\):\n\\[\\begin{align*}\n\\log\\left(\\frac{P(\\mathbf{x} | C_1)}{P(\\mathbf{x} | C_2)}\\right) &=\\log{\\left(\\frac{\\frac{1}{(2\\pi)^{\\frac{d}{2}}\\sqrt{|\\Sigma|}}e^{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_1)^T\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_1)}}{\\frac{1}{(2\\pi)^{\\frac{d}{2}}\\sqrt{|\\Sigma|}}e^{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_2)^T\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_2)}}\\right)}\\\\\n&= -\\frac{1}{2} \\left[ (\\mathbf{x} - \\boldsymbol{\\mu}_1)^T \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_1) - (\\mathbf{x} - \\boldsymbol{\\mu}_2)^T \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_2) \\right]\\\\\n& = -\\frac{1}{2} \\left[\\mathbf{x}^T\\Sigma^{-1}\\mathbf{x} - 2 \\mathbf{x}^T\\Sigma^{-1}\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_1^T\\Sigma^{-1}\\boldsymbol{\\mu_1} - \\mathbf{x}^T\\Sigma^{-1}\\mathbf{x} + 2 \\mathbf{x}^T\\Sigma^{-1}\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_2^T\\Sigma^{-1}\\boldsymbol{\\mu_2}\\right]\\\\\n& = -\\frac{1}{2} \\left[ - 2 \\mathbf{x}^T\\Sigma^{-1}\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_1^T\\Sigma^{-1}\\boldsymbol{\\mu_1}  + 2 \\mathbf{x}^T\\Sigma^{-1}\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_2^T\\Sigma^{-1}\\boldsymbol{\\mu_2}\\right]\\\\\n& = \\mathbf{x}^T\\Sigma^{-1}(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2)-\\frac{1}{2}(\\boldsymbol{\\mu}_1^T\\Sigma^{-1}\\boldsymbol{\\mu}_1+\\boldsymbol{\\mu}_2^T\\Sigma^{-1}\\boldsymbol{\\mu}_2)\\\\\n& = \\mathbf{x}^T\\mathbf{w}+\\text{constant};\\hspace{4mm}\\text{where, }\\hspace{4mm}\\mathbf{w} = \\Sigma^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\\\\\n\\end{align*}\\]\nTherefore, we can write\n\\[\n\\log\\left(\\frac{P(\\mathbf{x} | C_1)}{P(\\mathbf{x} | C_2)}\\right) = \\mathbf{w}^T\\mathbf{x}+\\text{constant}\n\\]\nsince \\(\\mathbf{w}^T\\mathbf{x}=\\mathbf{x}^T\\mathbf{w}\\), as inner product is commutative. This is the linear projection vector \\(\\mathbf{w}\\) that LDA uses.\n\n\nFisher’s Discriminant Ratio\n\nNow, we derive the Fisher’s Discriminant Ratio. The goal is to find a projection \\(\\mathbf{w}\\) that maximizes the separation between classes (between-class variance) and minimizes the spread within each class (within-class variance).\n\n\nBetween-class scatter \\(S_B\\) is defined as:\n\n\\[\nS_B = (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T\n\\]\n\nWithin-class scatter \\(S_W\\) is the covariance matrix \\(\\Sigma\\), assuming equal covariance for both classes.\n\nThe Fisher’s discriminant ratio is the objective function to maximize:\n\\[\nJ(\\mathbf{w}) = \\frac{\\mathbf{w}^T S_B \\mathbf{w}}{\\mathbf{w}^T S_W \\mathbf{w}}\n\\]\nSubstituting \\(S_B\\) and \\(S_W\\) into this expression, we get:\n\\[\nJ(\\mathbf{w}) = \\frac{\\mathbf{w}^T (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^T \\mathbf{w}}{\\mathbf{w}^T \\Sigma \\mathbf{w}}\n\\]\nThus, maximizing this ratio gives the direction \\(\\mathbf{w} = \\Sigma^{-1} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)\\), which is the same as the result from the Bayesian classification.\n\n\nSummary\nThe Fisher’s Discriminant Ratio arises as a byproduct of maximizing the posterior probability ratios between two classes under Gaussian assumptions. It captures the optimal linear projection to maximize the separation between classes (via between-class scatter) and minimize the spread within classes (via within-class scatter).\n\n\n\nQuadratic Discriminant Analysis (QDA)\n\nUnlike LDA, we allow each class \\(C_k\\) to have its own covariance matrix \\(\\Sigma_k\\), leading to a more flexible model capable of handling classes with different shapes and orientations in feature space. Here’s how we can derive the discriminant function for QDA.\n\n\nDiscriminant Function for QDA\nIn QDA, we aim to classify a sample \\(\\mathbf{x}\\) based on the probability that it belongs to class \\(C_k\\), given by \\(P(C_k|\\mathbf{x})\\). Using Bayes’ theorem, we have:\n\\[\nP(C_k | \\mathbf{x}) = \\frac{P(\\mathbf{x} | C_k) P(C_k)}{P(\\mathbf{x})}\n\\]\nSince we’re primarily interested in maximizing this value to classify \\(\\mathbf{x}\\), we can focus on maximizing the posterior probability \\(P(\\mathbf{x} | C_k) P(C_k)\\).\n\n\nLikelihood of \\(\\mathbf{x}\\) in Class \\(C_k\\)\nAssuming that the feature vector \\(\\mathbf{x}\\) follows a Gaussian distribution within each class \\(C_k\\), the likelihood \\(P(\\mathbf{x} | C_k)\\) is given by:\n\\[\nP(\\mathbf{x} | C_k) = \\frac{1}{(2 \\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp \\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\Sigma_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right)\n\\]\nwhere:\n\n\\(\\boldsymbol{\\mu}_k\\) is the mean vector for class \\(C_k\\),\n\\(\\Sigma_k\\) is the covariance matrix for class \\(C_k\\),\n\\(d\\) is the dimensionality of \\(\\mathbf{x}\\).\n\n\n\nLog of the Posterior (Quadratic Discriminant)\nTo simplify the computation, we take the logarithm of the posterior probability. Ignoring constant terms that do not depend on \\(k\\), we have:\n\\[\n\\ln P(\\mathbf{x} | C_k) P(C_k) = -\\frac{1}{2} \\left( (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\Sigma_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) + \\ln |\\Sigma_k| \\right) + \\ln P(C_k)\n\\]\nThe discriminant function for QDA can then be expressed as:\n\\[\n\\delta_k(\\mathbf{x}) = -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\Sigma_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) - \\frac{1}{2} \\ln |\\Sigma_k| + \\ln P(C_k)\n\\]\n\n\nExpanding the Quadratic Term\nLet’s expand the quadratic term:\n\\[\n(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\Sigma_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\n\\]\nExpanding this gives:\n\\[\n(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\Sigma_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) = \\mathbf{x}^T \\Sigma_k^{-1} \\mathbf{x} - 2 \\mathbf{x}^T \\Sigma_k^{-1} \\boldsymbol{\\mu}_k + \\boldsymbol{\\mu}_k^T \\Sigma_k^{-1} \\boldsymbol{\\mu}_k\n\\]\nSubstituting this expansion into the discriminant function:\n\\[\n\\delta_k(\\mathbf{x}) = -\\frac{1}{2} \\left( \\mathbf{x}^T \\Sigma_k^{-1} \\mathbf{x} - 2 \\mathbf{x}^T \\Sigma_k^{-1} \\boldsymbol{\\mu}_k + \\boldsymbol{\\mu}_k^T \\Sigma_k^{-1} \\boldsymbol{\\mu}_k \\right) - \\frac{1}{2} \\ln |\\Sigma_k| + \\ln P(C_k)\n\\]\n\n\nFinal Form of the QDA Discriminant Function\nRearranging terms, we get:\n\\[\n\\delta_k(\\mathbf{x}) = -\\frac{1}{2} \\mathbf{x}^T \\Sigma_k^{-1} \\mathbf{x} + \\mathbf{x}^T \\Sigma_k^{-1} \\boldsymbol{\\mu}_k - \\frac{1}{2} \\boldsymbol{\\mu}_k^T \\Sigma_k^{-1} \\boldsymbol{\\mu}_k - \\frac{1}{2} \\ln |\\Sigma_k| + \\ln P(C_k)\n\\]\n\n\nKey Points in QDA\n\nQuadratic term: Unlike LDA, QDA includes a quadratic term in \\(\\mathbf{x}\\), \\(-\\frac{1}{2} \\mathbf{x}^T \\Sigma_k^{-1} \\mathbf{x}\\), which allows QDA to model classes with different covariances.\nLinear term: \\(\\mathbf{x}^T \\Sigma_k^{-1} \\boldsymbol{\\mu}_k\\) is a linear term in \\(\\mathbf{x}\\).\nConstant term: The remaining terms \\(-\\frac{1}{2} \\boldsymbol{\\mu}_k^T \\Sigma_k^{-1} \\boldsymbol{\\mu}_k - \\frac{1}{2} \\ln |\\Sigma_k| + \\ln P(C_k)\\) are independent of \\(\\mathbf{x}\\).\n\nBecause of the quadratic term, the decision boundaries in QDA are generally quadratic surfaces, allowing it to handle more complex class separations than LDA, which has linear boundaries."
  },
  {
    "objectID": "posts/machinelearning/index.html#texttextcolorreddata-science",
    "href": "posts/machinelearning/index.html#texttextcolorreddata-science",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{\\textcolor{red}{Data Science}}\\)",
    "text": "\\(\\text{\\textcolor{red}{Data Science}}\\)\nData science involves extracting knowledge from structured and unstructured data. It combines principle from statistics, machine learning, data analysis, and domain knoledge to understand and interpret the data\n\nData Collection & Accuisition\n\nWeb srcaping: Data collection through Webscraping\n\nAPI integration\n\nData Lakes, Data Warehouses\n\n\n\nData Cleaning & Preprocessing\n\nHandling Missing Values\n\nData Transformation\n\nFeature Engineering and Selection\n\nEncoding Categorical Variables\n\nHandling Outliers\n\n\n\nExploratory Data Analysis (EDA)\n\nDescriptive Statistics\n\nData Visualization\n\nIdentifying Patterns, Trends, Correlations\n\n\n\nStatistical Methods\n\nANOVA - Categorical Features’: How do we treat the categorical features for our data science project?\nHypothesis Testing\n\nProbability Distributions\n\nInferential Statistics\n\nSampling Methods\n\n\n\nBig Data Techniques\n\nHadoop, Spark\n\nDistributed Data Storage (e.g., HDFS, NoSQL)\nData PipeLines, ETL (Extract, Transform, Load)"
  },
  {
    "objectID": "posts/machinelearning/index.html#texttextcoloryellowdata-science",
    "href": "posts/machinelearning/index.html#texttextcoloryellowdata-science",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{\\textcolor{yellow}{Data Science}}\\)",
    "text": "\\(\\text{\\textcolor{yellow}{Data Science}}\\)\nData science involves extracting knowledge from structured and unstructured data. It combines principle from statistics, machine learning, data analysis, and domain knoledge to understand and interpret the data\n\nData Collection & Accuisition\n\nWeb srcaping: Data collection through Webscraping\n\nAPI integration\n\nData Lakes, Data Warehouses\n\n\n\nData Cleaning & Preprocessing\n\nHandling Missing Values\n\nData Transformation\n\nFeature Engineering and Selection\n\nEncoding Categorical Variables\n\nHandling Outliers\n\n\n\nExploratory Data Analysis (EDA)\n\nDescriptive Statistics\n\nData Visualization\n\nIdentifying Patterns, Trends, Correlations\n\n\n\nStatistical Methods\n\nANOVA - Categorical Features’: How do we treat the categorical features for our data science project?\nHypothesis Testing\n\nProbability Distributions\n\nInferential Statistics\n\nSampling Methods\n\n\n\nBig Data Techniques\n\nHadoop, Spark\n\nDistributed Data Storage (e.g., HDFS, NoSQL)\nData PipeLines, ETL (Extract, Transform, Load)"
  },
  {
    "objectID": "posts/machinelearning/index.html#texttextcolorbluedata-science",
    "href": "posts/machinelearning/index.html#texttextcolorbluedata-science",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{\\textcolor{blue}{Data Science}}\\)",
    "text": "\\(\\text{\\textcolor{blue}{Data Science}}\\)\nData science involves extracting knowledge from structured and unstructured data. It combines principle from statistics, machine learning, data analysis, and domain knoledge to understand and interpret the data\n\nData Collection & Accuisition\n\nWeb srcaping: Data collection through Webscraping\n\nAPI integration\n\nData Lakes, Data Warehouses\n\n\n\nData Cleaning & Preprocessing\n\nHandling Missing Values\n\nData Transformation\n\nFeature Engineering and Selection\n\nEncoding Categorical Variables\n\nHandling Outliers\n\n\n\nExploratory Data Analysis (EDA)\n\nDescriptive Statistics\n\nData Visualization\n\nIdentifying Patterns, Trends, Correlations\n\n\n\nStatistical Methods\n\nANOVA - Categorical Features’: How do we treat the categorical features for our data science project?\nHypothesis Testing\n\nProbability Distributions\n\nInferential Statistics\n\nSampling Methods\n\n\n\nBig Data Techniques\n\nHadoop, Spark\n\nDistributed Data Storage (e.g., HDFS, NoSQL)\nData PipeLines, ETL (Extract, Transform, Load)"
  },
  {
    "objectID": "jobandintern/bayesianinference/index.html",
    "href": "jobandintern/bayesianinference/index.html",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields including machine learning, to make predictions and decisions under uncertainty.\n\nBayes’s theorem is a fundamental result in probability theory that relates the conditional and marginal probabilities of random events. Mathematically,\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\hspace{4mm} \\implies \\mathbb{P}(A|B) \\propto \\mathbb{P}(B|A)\\mathbb{P}(A)\\]\nwhere, \\(A\\) and \\(B\\) are events and \\(\\mathbb{P}(B)\\ne 0\\).\n\n\\(\\mathbb{P}(A|B)\\) is a conditional probability which states the probability of occuring the event \\(A\\) when the event \\(B\\) is given or true. The other name of this quantity is called posterior probability of \\(A\\) given the event \\(B\\) or simply, posterior distribution.\n\n\\(\\mathbb{P}(B|A)\\) is a conditional probability which states the probability of occuring the event \\(B\\) when the event \\(A\\) is given or true. In other terms, \\(\\mathbb{P}(B|A)\\) is the likelihood: the probability of evidence \\(B\\) given that \\(A\\) is true.\n\n\\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B)\\) are the probabilities of occuring \\(A\\) and \\(B\\) respectively, without any dependence on each other. \\(\\mathbb{P}(A)\\) is called the prior probability or prior distribution and \\(\\mathbb{P}(B)\\) is called the marginal likelihood or marginal probabilities.\n\nExample 1\nConsider a medical example where we want to diagnose a disease based on a test result. Let:\n\n\\(D\\) be the event that a patient has the disease.\n\n\\(T\\) be the event that the test result is positive.\n\nWe are interested in finding the probability that a patient has the disease given a positive test result, \\(\\mathbb{P}(D|T)\\).\nGiven:\n\n\\(\\mathbb{P}(T|D) = 0.99\\) (the probability of a positive test result given the patient has the disease).\n\n\\(\\mathbb{P}(D) = 0.01\\) (the prior probability of the disease).\n\n\\(\\mathbb{P}(T|D') = 0.05\\) (the probability of a positive test result given the patient does not have the disease).\n\nFirst, we need to calculate the marginal likelihood \\(P(T)\\): \\[\\begin{align*}\n    \\mathbb{P}(T) &= \\mathbb{P}(T|D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(T|D') \\cdot \\mathbb{P}(D') \\\\\n    \\mathbb{P}(T) &= 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99\\\\\n    \\mathbb{P}(T) &= 0.0099 + 0.0495 \\\\\n    \\mathbb{P}(T) &= 0.0594\n\\end{align*}\\]\nNow, we can apply Bayes’s theorem:\n\\[\\begin{align*}\n    \\mathbb{P}(D|T) &= \\frac{\\mathbb{P}(T|D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(T)}\\\\\n    \\mathbb{P}(D|T) &= \\frac{0.99 \\cdot 0.01}{0.0594}\\\\\n    \\mathbb{P}(D|T) &\\approx 0.1667\n\\end{align*}\\]\nSo, the probability that the patient has the disease given a positive test result is approximately \\(16.67\\%\\).\nExample 2\n\n Assume that you are in a restuarant and you ordered a plate of 3 pancakes. The chef made three pancakes with one in perfect condition, that is not burnt in any side, one with one side burnt, and the last one burnt in both sides. The waiter wanted to stack the pancakes so that the burnt side does not show up when served. However, the chef recommended not to hide the burnt side and asked her to stack the pancakes randomly. What is the likelyhood that the fully burnt pancake will be on the top?  To solve this problem, we can use Bayesian approach. We denote the event \\(X\\) as the pancake without any burnt, \\(Y\\) with one side burnt, and \\(Z\\) both side burnt. Then we have the following conditional probabilities\n\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt} | X)&=0\\\\\n    \\mathbb{P}(\\text{top-burnt} | Y)&=\\frac{1}{2}\\\\\n    \\mathbb{P}(\\text{top-burnt} | Z)&=1\\\\\n\\end{align*}\\]\nThe probability of picking a pancake irrespective of their burnt condition is \\(\\frac{1}{3}\\). So,\n\\[\\begin{equation}\n    \\mathbb{P}(X)=\\mathbb{P}(Y)=\\mathbb{P}(Z)=\\frac{1}{3}\n\\end{equation}\\]\nThe marginal probability of having burnt side in the top position\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt})&=\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)\\\\\n    &=0\\cdot \\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}+1\\cdot\\frac{1}{3}\\\\\n    &=\\frac{1}{2}\n\\end{align*}\\]\nNow, we can only have a burnt side on top if either \\(Z\\) is placed in the top or the burnt side of \\(Y\\) is placed in the top. \\[\\begin{align*}\n    \\mathbb{P}(Y|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{2}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}\\\\\n    \\mathbb{P}(Z|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}\n\\end{align*}\\]\nSo the probability of having the fully burnt pancake on the top is \\(\\frac{2}{3}\\)."
  },
  {
    "objectID": "jobandintern/bayesianinference/index.html#introduction",
    "href": "jobandintern/bayesianinference/index.html#introduction",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields including machine learning, to make predictions and decisions under uncertainty.\n\nBayes’s theorem is a fundamental result in probability theory that relates the conditional and marginal probabilities of random events. Mathematically,\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\hspace{4mm} \\implies \\mathbb{P}(A|B) \\propto \\mathbb{P}(B|A)\\mathbb{P}(A)\\]\nwhere, \\(A\\) and \\(B\\) are events and \\(\\mathbb{P}(B)\\ne 0\\).\n\n\\(\\mathbb{P}(A|B)\\) is a conditional probability which states the probability of occuring the event \\(A\\) when the event \\(B\\) is given or true. The other name of this quantity is called posterior probability of \\(A\\) given the event \\(B\\) or simply, posterior distribution.\n\n\\(\\mathbb{P}(B|A)\\) is a conditional probability which states the probability of occuring the event \\(B\\) when the event \\(A\\) is given or true. In other terms, \\(\\mathbb{P}(B|A)\\) is the likelihood: the probability of evidence \\(B\\) given that \\(A\\) is true.\n\n\\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B)\\) are the probabilities of occuring \\(A\\) and \\(B\\) respectively, without any dependence on each other. \\(\\mathbb{P}(A)\\) is called the prior probability or prior distribution and \\(\\mathbb{P}(B)\\) is called the marginal likelihood or marginal probabilities.\n\nExample 1\nConsider a medical example where we want to diagnose a disease based on a test result. Let:\n\n\\(D\\) be the event that a patient has the disease.\n\n\\(T\\) be the event that the test result is positive.\n\nWe are interested in finding the probability that a patient has the disease given a positive test result, \\(\\mathbb{P}(D|T)\\).\nGiven:\n\n\\(\\mathbb{P}(T|D) = 0.99\\) (the probability of a positive test result given the patient has the disease).\n\n\\(\\mathbb{P}(D) = 0.01\\) (the prior probability of the disease).\n\n\\(\\mathbb{P}(T|D') = 0.05\\) (the probability of a positive test result given the patient does not have the disease).\n\nFirst, we need to calculate the marginal likelihood \\(P(T)\\): \\[\\begin{align*}\n    \\mathbb{P}(T) &= \\mathbb{P}(T|D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(T|D') \\cdot \\mathbb{P}(D') \\\\\n    \\mathbb{P}(T) &= 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99\\\\\n    \\mathbb{P}(T) &= 0.0099 + 0.0495 \\\\\n    \\mathbb{P}(T) &= 0.0594\n\\end{align*}\\]\nNow, we can apply Bayes’s theorem:\n\\[\\begin{align*}\n    \\mathbb{P}(D|T) &= \\frac{\\mathbb{P}(T|D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(T)}\\\\\n    \\mathbb{P}(D|T) &= \\frac{0.99 \\cdot 0.01}{0.0594}\\\\\n    \\mathbb{P}(D|T) &\\approx 0.1667\n\\end{align*}\\]\nSo, the probability that the patient has the disease given a positive test result is approximately \\(16.67\\%\\).\nExample 2\n\n Assume that you are in a restuarant and you ordered a plate of 3 pancakes. The chef made three pancakes with one in perfect condition, that is not burnt in any side, one with one side burnt, and the last one burnt in both sides. The waiter wanted to stack the pancakes so that the burnt side does not show up when served. However, the chef recommended not to hide the burnt side and asked her to stack the pancakes randomly. What is the likelyhood that the fully burnt pancake will be on the top?  To solve this problem, we can use Bayesian approach. We denote the event \\(X\\) as the pancake without any burnt, \\(Y\\) with one side burnt, and \\(Z\\) both side burnt. Then we have the following conditional probabilities\n\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt} | X)&=0\\\\\n    \\mathbb{P}(\\text{top-burnt} | Y)&=\\frac{1}{2}\\\\\n    \\mathbb{P}(\\text{top-burnt} | Z)&=1\\\\\n\\end{align*}\\]\nThe probability of picking a pancake irrespective of their burnt condition is \\(\\frac{1}{3}\\). So,\n\\[\\begin{equation}\n    \\mathbb{P}(X)=\\mathbb{P}(Y)=\\mathbb{P}(Z)=\\frac{1}{3}\n\\end{equation}\\]\nThe marginal probability of having burnt side in the top position\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt})&=\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)\\\\\n    &=0\\cdot \\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}+1\\cdot\\frac{1}{3}\\\\\n    &=\\frac{1}{2}\n\\end{align*}\\]\nNow, we can only have a burnt side on top if either \\(Z\\) is placed in the top or the burnt side of \\(Y\\) is placed in the top. \\[\\begin{align*}\n    \\mathbb{P}(Y|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{2}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}\\\\\n    \\mathbb{P}(Z|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}\n\\end{align*}\\]\nSo the probability of having the fully burnt pancake on the top is \\(\\frac{2}{3}\\)."
  },
  {
    "objectID": "jobandintern/bayesianinference/index.html#why-bayesian-inference-in-machine-learning",
    "href": "jobandintern/bayesianinference/index.html#why-bayesian-inference-in-machine-learning",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "Why Bayesian Inference in Machine Learning?",
    "text": "Why Bayesian Inference in Machine Learning?\nBayesian inference plays a crucial role in machine learning, particularly in areas involving uncertainty and probabilistic reasoning. It allows us to incorporate prior knowledge and update beliefs based on new data, which is especially useful in the following applications:\n\nBayesian Networks\nBayesian networks are graphical models that represent the probabilistic relationships among a set of variables. Each node in the network represents a random variable, and the edges represent conditional dependencies. Bayesian networks are used for various tasks such as classification, prediction, and anomaly detection.\n\n\nBayesian Regression\nBayesian regression extends linear regression by incorporating prior distributions on the model parameters. This approach provides a probabilistic framework for regression analysis, allowing for uncertainty in the parameter estimates. The posterior distribution of the parameters is computed using Bayes’s theorem, and predictions are made by averaging over this distribution.\n\n\nSampling Methods\nIn Bayesian inference, exact computation of the posterior distribution is often intractable. Therefore, sampling methods such as Markov Chain Monte Carlo (MCMC) and Variational Inference are used to approximate the posterior distribution. These methods generate samples from the posterior distribution, allowing us to estimate various statistical properties and make inferences.\nMarkov Chain Monte Carlo (MCMC)\nMCMC methods generate a sequence of samples from the posterior distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution. Common MCMC algorithms include the Underdamped and Overdamped Langevin dynamics, Metropolis-Hastings algorithm and the Gibbs sampler.\nExample: Metropolis-Hastings Algorithm\nConsider a posterior distribution \\(P(\\theta|D)\\) where \\(\\theta\\) represents the model parameters and \\(D\\) represents the data. The Metropolis-Hastings algorithm proceeds as follows:\n\nInitialize the parameters \\(\\theta_0\\).\nFor \\(t = 1\\) to \\(T\\):\n\nPropose a new state \\(\\theta'\\) from a proposal distribution \\(Q(\\theta'|\\theta_t)\\).\nCompute the acceptance ratio \\(\\alpha = \\frac{P(\\theta'|D) \\cdot Q(\\theta_t|\\theta')}{P(\\theta_t|D) \\cdot Q(\\theta'|\\theta_t)}\\).\nAccept the new state with probability \\(\\min(1, \\alpha)\\). If accepted, set \\(\\theta_{t+1} = \\theta'\\); otherwise, set \\(\\theta_{t+1} = \\theta_t\\).\n\n\nThe samples \\(\\theta_1, \\theta_2, \\ldots, \\theta_T\\) form a Markov chain whose stationary distribution is the posterior distribution \\(P(\\theta|D)\\).\n\n\nBayesian Inference in Neural Networks\nBayesian methods are also applied to neural networks, resulting in Bayesian Neural Networks (BNNs). BNNs incorporate uncertainty in the network weights by placing a prior distribution over them and using Bayes’s theorem to update this distribution based on the observed data. This allows BNNs to provide not only point estimates but also uncertainty estimates for their predictions.\nIn the next parts, we will talk about different applications of the Bayesian inferences, specifically, sampling problem using Langevin dynamics.\n\n\nReference\n\nPancake problems on mathstackexchance\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/machinelearning/index.html#texttextcolorredmachine-learning-algorithms",
    "href": "posts/machinelearning/index.html#texttextcolorredmachine-learning-algorithms",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{\\textcolor{red}{Machine Learning Algorithms}}\\)",
    "text": "\\(\\text{\\textcolor{red}{Machine Learning Algorithms}}\\)\n\n\\(\\text{Supervised Learning}\\)\n(Training with labeled data: input-output pairs)\n\nRegression\n\n\nParametric\n\nSimple Linear Regression\nMultiple Linear Regression\nPolynomial Regression\n\n\n\nNon-Parametric\n\nK-Nearest Neighbor (KNN) Regression\nDecesion Trees Regression\nRandom Forest Regression\nSupport Vector Machine (SVM) Regression\n\n\n\n\n\nClassification\n\n\n\nParametric\n\nLogistic Regression\nNaive Bayes\nLinear Discriminant Analysis (LDA)\n\nQuadratic Discriminant Analysis (QDA)\n\n\n\n\nNon-Parametric\n\nKNN Classification\nDecision Tree Classification\nRandom Forest Classification\nSupport Vector Machine (SVM) Classification\n\n\n\n\nMulti-Class Classification\n\nMulti-class Classification\n\n\n\n\nBayesian or Probabilistic Classification\n\nWhat is Bayesian or Probabilistic Classification?\n\nLinear Discriminant Analysis (LDA)\n\nQuadratic Discriminant Analysis (QDA)\n\nNaive Bayes\nBayesian Network Classifier (Tree Augmented Naive Bayes (TAN))\n\n\n\n\nNon-probabilistic Classification\n\nSupport Vector Machine (SVM) Classification\n\nDecision Tree Classification\n\nRandom Forest Classification\n\nKNN Classification\n\nPerceptron\n\n\n\n\n\n\n\n\\(\\text{Unsupervised Learning}\\)\n(Training with unlabeled data)\n\n\n\nClustering\n\nk-Means Clustering\n\nHierarchical Clustering\n\nDBSCAN (Density-Based Spatial Clustering)\n\nGaussian Mixture Models (GMM)\n\n\n\n\nDimensionality Reduction\n\nPrincipal Component Analysis\n\nLatent Dirichlet Allocation (LDA)\nt-SNE (t-distributed Stochastic Neihbor Embedding)\n\nFactor Analysis\n\nAutoencoders\n\n\n\n\n\nAnomaly Detection\n\nIsolation Forests\n\nOne-Class SVM\n\n\n\n\n\n\n\\(\\text{Semi-Supervised Learning}\\)\n(Combination of labeled and unlabeled data)\n\nSelf-training\n\nCo-training\n\nLabel Propagation\n\n\n\n\\(\\text{Reinforcement Learning}\\)\n(Learning via rewards and penalties)\n\nMarkov Decision Process (MDP)\n\nQ-Learning\n\nDeep Q-Networks (DQN)\n\nPolicy Gradient Method"
  },
  {
    "objectID": "posts/machinelearning/index.html#texttextcolorreddeep-learnings",
    "href": "posts/machinelearning/index.html#texttextcolorreddeep-learnings",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{\\textcolor{red}{Deep Learnings}}\\)",
    "text": "\\(\\text{\\textcolor{red}{Deep Learnings}}\\)\n\nPyTorch\n\nArtificial Neural Networks (ANN)\n\nConvolutional Neural Networks (CNN)\n\nRecurrent Neural Networks (RNN)\n\nLong Short-Term Memory (LSTM)\n\nGenerative Adversarial Networks (GAN)"
  },
  {
    "objectID": "posts/machinelearning/index.html#texttextcolorredmodel-evaluation-and-fine-tuning",
    "href": "posts/machinelearning/index.html#texttextcolorredmodel-evaluation-and-fine-tuning",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{\\textcolor{red}{Model Evaluation and Fine Tuning}}\\)",
    "text": "\\(\\text{\\textcolor{red}{Model Evaluation and Fine Tuning}}\\)\n\nModel Evaluation Metrics\n\nFor Regression: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), \\(R^2\\) score\n\nFor Classification: Accuracy, Precision, Recall, F1 Score, ROC-AUC\n\nCross-validation: kFold, Stratified k-fold, leave-one-out\n\n\n\nModel Optimization\n\nBias-Variance: Bias Variance Trade off\n\nHyperparameter Tuning: Grid Search, Random Search, Bayesian Optimization\n\nFeatures Selection Techniques: Recursive Feature Elimination (RFE), L1 or Rasso Regurlarization, L2 or Ridge Regularization\n\nModel Interpretability: SHAP (Shapley values), LIME (Local Interpretable Model-agnostic Explanations)\n\n\n\nEnsemble Methods\n\nBagging: Random Forest, Bootstrap Aggregating\n\nBoosting: Gradient Boosting, AdaBoost, XGBoost, CatBoost\n\nStacking: Stacked Generalization\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/machinelearning/index.html#textdeep-learnings",
    "href": "posts/machinelearning/index.html#textdeep-learnings",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{Deep Learnings}\\)",
    "text": "\\(\\text{Deep Learnings}\\)\n\nPyTorch\n\nArtificial Neural Networks (ANN)\n\nConvolutional Neural Networks (CNN)\n\nRecurrent Neural Networks (RNN)\n\nLong Short-Term Memory (LSTM)\n\nGenerative Adversarial Networks (GAN)"
  },
  {
    "objectID": "posts/machinelearning/index.html#textmodel-evaluation-and-fine-tuning",
    "href": "posts/machinelearning/index.html#textmodel-evaluation-and-fine-tuning",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{Model Evaluation and Fine Tuning}\\)",
    "text": "\\(\\text{Model Evaluation and Fine Tuning}\\)\n\nModel Evaluation Metrics\n\nFor Regression: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), \\(R^2\\) score\n\nFor Classification: Accuracy, Precision, Recall, F1 Score, ROC-AUC\n\nCross-validation: kFold, Stratified k-fold, leave-one-out\n\n\n\nModel Optimization\n\nBias-Variance: Bias Variance Trade off\n\nHyperparameter Tuning: Grid Search, Random Search, Bayesian Optimization\n\nFeatures Selection Techniques: Recursive Feature Elimination (RFE), L1 or Rasso Regurlarization, L2 or Ridge Regularization\n\nModel Interpretability: SHAP (Shapley values), LIME (Local Interpretable Model-agnostic Explanations)\n\n\n\nEnsemble Methods\n\nBagging: Random Forest, Bootstrap Aggregating\n\nBoosting: Gradient Boosting, AdaBoost, XGBoost, CatBoost\n\nStacking: Stacked Generalization\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/machinelearning/index.html#textmachine-learning-algorithms",
    "href": "posts/machinelearning/index.html#textmachine-learning-algorithms",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{Machine Learning Algorithms}\\)",
    "text": "\\(\\text{Machine Learning Algorithms}\\)\n\n\\(\\text{Supervised Learning}\\)\n(Training with labeled data: input-output pairs)\n\nRegression\n\n\nParametric\n\nSimple Linear Regression\nMultiple Linear Regression\nPolynomial Regression\n\n\n\nNon-Parametric\n\nK-Nearest Neighbor (KNN) Regression\nDecesion Trees Regression\nRandom Forest Regression\nSupport Vector Machine (SVM) Regression\n\n\n\n\n\nClassification\n\n\n\nParametric\n\nLogistic Regression\nNaive Bayes\nLinear Discriminant Analysis (LDA)\n\nQuadratic Discriminant Analysis (QDA)\n\n\n\n\nNon-Parametric\n\nKNN Classification\nDecision Tree Classification\nRandom Forest Classification\nSupport Vector Machine (SVM) Classification\n\n\n\n\nMulti-Class Classification\n\nMulti-class Classification\n\n\n\n\nBayesian or Probabilistic Classification\n\nWhat is Bayesian or Probabilistic Classification?\n\nLinear Discriminant Analysis (LDA)\n\nQuadratic Discriminant Analysis (QDA)\n\nNaive Bayes\nBayesian Network Classifier (Tree Augmented Naive Bayes (TAN))\n\n\n\n\nNon-probabilistic Classification\n\nSupport Vector Machine (SVM) Classification\n\nDecision Tree Classification\n\nRandom Forest Classification\n\nKNN Classification\n\nPerceptron\n\n\n\n\nEnsemble Methods\n\nBagging: Decision Tree Classification\n\nBagging: Random Forest Classification\n\nBoosting: Adaptive Boosting\n\n\n\n\n\n\n\n\n\\(\\text{Unsupervised Learning}\\)\n(Training with unlabeled data)\n\n\n\nClustering\n\nk-Means Clustering\n\nHierarchical Clustering\n\nDBSCAN (Density-Based Spatial Clustering)\n\nGaussian Mixture Models (GMM)\n\n\n\n\nDimensionality Reduction\n\nPrincipal Component Analysis\n\nLatent Dirichlet Allocation (LDA)\nt-SNE (t-distributed Stochastic Neihbor Embedding)\n\nFactor Analysis\n\nAutoencoders\n\n\n\n\n\nAnomaly Detection\n\nIsolation Forests\n\nOne-Class SVM\n\n\n\n\n\n\n\\(\\text{Semi-Supervised Learning}\\)\n(Combination of labeled and unlabeled data)\n\nSelf-training\n\nCo-training\n\nLabel Propagation\n\n\n\n\\(\\text{Reinforcement Learning}\\)\n(Learning via rewards and penalties)\n\nMarkov Decision Process (MDP)\n\nQ-Learning\n\nDeep Q-Networks (DQN)\n\nPolicy Gradient Method"
  },
  {
    "objectID": "posts/machinelearning/index.html#textdata-science",
    "href": "posts/machinelearning/index.html#textdata-science",
    "title": "Data Science & Machine Learning Basics",
    "section": "\\(\\text{Data Science}\\)",
    "text": "\\(\\text{Data Science}\\)\nData science involves extracting knowledge from structured and unstructured data. It combines principle from statistics, machine learning, data analysis, and domain knoledge to understand and interpret the data\n\nData Collection & Accuisition\n\nWeb srcaping: Data collection through Webscraping\n\nAPI integration\n\nData Lakes, Data Warehouse\n\n\n\nData Cleaning & Preprocessing\nThis involves Handling Missing Values, Data Transformation, Feature Engineering, Encoding Categorical Variables, Handling Outliers\n\n\nExploratory Data Analysis (EDA)\nThis usually includes the Descriptive Statistics, Data Visualization, Identifying Patterns, Trends, Correlations of the features and labels.\n\n\nStatistical Methods\n\nANOVA - Categorical Features’: How do we treat the categorical features for our data science project?\nHypothesis Testing\n\nProbability Distributions\n\nInferential Statistics\n\nSampling Methods\n\n\n\nBig Data Techniques\n\nHadoop, Spark\n\nDistributed Data Storage (e.g., HDFS, NoSQL)\nData PipeLines, ETL (Extract, Transform, Load)"
  },
  {
    "objectID": "dsandml/bayesianclassification/index.html#references",
    "href": "dsandml/bayesianclassification/index.html#references",
    "title": "Bayesian Probabilistic Models for Classification",
    "section": "References",
    "text": "References\n\n“The Elements of Statistical Learning” by Trevor Hastie, Robert Tibshirani, and Jerome Friedman\n\nThis book is an excellent resource for both Linear and Quadratic Discriminant Analysis, including mathematical derivations, explanations of Gaussian discriminant analysis, and the context for using LDA and QDA.\nSee Chapter 4: Linear Methods for Classification.\n\n“Pattern Recognition and Machine Learning” by Christopher M. Bishop\n\nBishop’s book offers a clear introduction to probabilistic classification, Bayes theorem, and discriminant analysis.\nSee Chapter 4: Linear Models for Classification.\n\n“Machine Learning: A Probabilistic Perspective” by Kevin P. Murphy\n\nThis text provides derivations and explanations of LDA and QDA from a probabilistic and Bayesian perspective.\nSee Chapter 7: Linear Discriminant Analysis.\n\n“Applied Multivariate Statistical Analysis” by Richard A. Johnson and Dean W. Wichern\n\nThis book goes deeper into the statistical foundation behind discriminant analysis, including pooled variance, unbiased estimators, and the assumptions behind LDA and QDA.\nSee Chapter 11: Discrimination and Classification.\n\n“Introduction to the Theory of Statistics” by Alexander M. Mood, Franklin A. Graybill, and Duane C. Boes\n\nThis text provides a theoretical foundation on statistical concepts, including unbiased estimators and quadratic forms, which underlie LDA and QDA derivations.\nRelevant for concepts of unbiased estimation and quadratic forms.\n\n\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/svm/index.html#introduction",
    "href": "dsandml/svm/index.html#introduction",
    "title": "Support Vector Machine (SVM) Algorithm",
    "section": "",
    "text": "Support Vector Machines (SVM) is a powerful non-parametric supervised machine learning algorithm used for classification and, less commonly, regression tasks. Support Vector Machines are designed to find an optimal hyperplane that best separates data points into classes. The key idea behind SVMs is to maximize the margin between data points of different classes while minimizing classification errors. This leads to a robust decision boundary that generalizes well to unseen data."
  },
  {
    "objectID": "dsandml/svm/index.html#the-mathematical-foundation-of-svm",
    "href": "dsandml/svm/index.html#the-mathematical-foundation-of-svm",
    "title": "Support Vector Machine (SVM) Algorithm",
    "section": "The Mathematical Foundation of SVM",
    "text": "The Mathematical Foundation of SVM\n\nConsider a classification problem. Given a dataset \\((\\mathbf{x}_i, y_i)\\) where \\(i = 1, 2, \\dots, N\\), \\(x_i\\in \\mathbb{R}^d\\) represents the feature vector of the \\(i\\)-th sample, and \\(y_i \\in \\{-1, 1\\}\\) represents the class label. The goal of SVM is to find a hyperplane that maximally separates the classes.\n\n\nHyperplane and Dicision Boundary\n\nDefinition (Hyperplane)\n\nA hyperplane in an \\(n\\)-dimensional space is defined by: \\[\nw^T \\mathbf{x} + b = 0\n\\]\n\n\nwhere:\n\n\\(w\\) is the weight vector,\n\\(b\\) is the bias term,\n\\(x\\) is any point on the hyperplane.\n\nFor a two-dimensional space, this hyperplane is simply a line. \\[\nw^T\\mathbf{x}+b=0;\\hspace{4mm}\\implies w_0x+w_1y+b=0;\\hspace{4mm}\\implies y=\\frac{-w_0x-b}{w_1}\n\\]\nand for a three-dimensional space, this hyperplane is simply a 2D plane\n\\[\nw^T\\mathbf{x}+b=0;\\hspace{4mm}\\implies w_0x+w_1y+w_2z+b=0;\\hspace{4mm}\\implies z=\\frac{-w_0x-w_1y-b}{w_2}\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nw_2d = np.array([1,1])\nb_2d = -0.5\n\nw_3d = np.array([1,1,1])\nb_3d = -1\n\ndef decision_boundary_2d(x):\n    return (-w_2d[0]*x-b_2d) / w_2d[1]\n\ndef decision_boundary_3d(x, y):\n    return (-w_3d[0]*x-w_3d[1]*y-b_3d) / w_3d[2]\n\nnp.random.seed(0)\nclass1x_2d = np.random.normal(loc=[1,1],scale=0.5, size=(30,2))\nclass2x_2d = np.random.normal(loc=[-1,-1],scale=0.5, size=(30,2))\n\nclass1x_3d = np.random.normal(loc=[1,1,1],scale=0.5, size=(90,3))\nclass2x_3d = np.random.normal(loc=[-1,-1,-1],scale=0.5, size=(90,3))\n\nfig = plt.figure( figsize=(7.9,4))\nax1 = fig.add_subplot(121)\nx_vals_2d = np.linspace(-2,3,100)\nplt.plot(\n    x_vals_2d, decision_boundary_2d(x_vals_2d),\n    'k-', label = \"Decision Boundary (Hyperplane)\"\n    )\nax1.scatter(\n    class1x_2d[:,0], class1x_2d[:,1], color='blue',\n    marker='o', label = 'Class +1'\n    )\nax1.scatter(\n    class2x_2d[:,0], class2x_2d[:,1], color='red',\n    marker='o', label = 'Class -1'\n    )\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('Hyperplane (a line) in 2D Space')\nax1.axhline(0, color='grey', lw = 0.5)\nax1.axvline(0, color='grey', lw = 0.5)\n\n\nax2 = fig.add_subplot(122, projection = '3d')\nx_vals_3d = np.linspace(-2,2,30)\ny_vals_3d = np.linspace(-2,2,30)\nX, Y = np.meshgrid(x_vals_3d, y_vals_3d)\nZ = decision_boundary_3d(X, Y)\n\nax2.plot_surface(X, Y, Z, color='k', alpha = 0.3, rstride=100, cstride=100, edgecolor='none')\nax2.scatter(class1x_3d[:,0], class1x_3d[:,1],class1x_3d[:,2], color = 'blue', marker='o', label='Class +1')\nax2.scatter(class2x_3d[:,0], class2x_3d[:,1],class2x_3d[:,2], color = 'red', marker='o', label='Class -1')\nax2.set_xlabel('X')\nax2.set_ylabel('Y')\nax2.set_zlabel('Z')\nax2.set_title('Hyperplane (a 2D plate) in 3D Space')\n\nplt.tight_layout()\naxes = [ax1,ax2]\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMargin and the Optimal Hyperplane\n\nDefinition (Margin)\n\nThe margin is the distance between the hyperplane and the nearest data points from either class. SVM aims to maximize this margin to achieve better separation, which makes the classifier more robust.\n\n\nTo define the margin mathematically, we impose that for all points: \\[\ny_i (w^T \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i\n\\]\nFor a data vector \\(\\mathbf{x}_i\\) with label \\(y_i\\):\n\nIf \\(y_i = +1\\): we want \\(w^T \\mathbf{x}_i + b\\ge 1\\) (to be on the correct side of the hyperplane)\n\nIf \\(y_i = -1\\): we want \\(w^T \\mathbf{x}_i + b\\le 1\\) (to be on the correct side of the hyperplane)\n\n\nThese two conditions combaine the equation mention above. That is all points must be at least a unit distance from the hyperplane on the correct side. The data points that satisfy \\(y_i (w^T x_i + b) = 1\\) or \\(y_i (w^T x_i + b) = -1\\) lie on the “support vectors,” or the points closest to the hyperplane.\n\nWe know from the elementary geometry that the distance between two parallel lines \\(ax+by+c_1=0\\) and \\(ax+by+c_2=0\\) is given by\n\\[\n\\frac{|c_1-c_2|}{\\sqrt{a^2+b^2}}\n\\]\nand the distance between two 2D parallel planes \\(ax+by+cz+d_1=0\\) and \\(ax+by+cz+d_2=0\\) in 3D space is given as\n\\[\n\\frac{|d_1-d_2|}{\\sqrt{a^2+b^2+c^2}}\n\\]\n\n\nCode\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.renderers\n\nz1 = np.array([\n    [8.83,8.89,8.81,8.87,8.9,8.87],\n    [8.89,8.94,8.85,8.94,8.96,8.92],\n    [8.84,8.9,8.82,8.92,8.93,8.91],\n    [8.79,8.85,8.79,8.9,8.94,8.92],\n    [8.79,8.88,8.81,8.9,8.95,8.92],\n    [8.8,8.82,8.78,8.91,8.94,8.92],\n    [8.75,8.78,8.77,8.91,8.95,8.92],\n    [8.8,8.8,8.77,8.91,8.95,8.94],\n    [8.74,8.81,8.76,8.93,8.98,8.99],\n    [8.89,8.99,8.92,9.1,9.13,9.11],\n    [8.97,8.97,8.91,9.09,9.11,9.11],\n    [9.04,9.08,9.05,9.25,9.28,9.27],\n    [9,9.01,9,9.2,9.23,9.2],\n    [8.99,8.99,8.98,9.18,9.2,9.19],\n    [8.93,8.97,8.97,9.18,9.2,9.18]\n])\n\nz2 = z1 + 1\nz3 = z1 - 1\n\nfig = go.Figure(data=[\n    go.Surface(z=z1),\n    go.Surface(z=z2, showscale=False, opacity=0.9),\n    go.Surface(z=z3, showscale=False, opacity=0.9)\n\n])\nfig.update_layout(\n    scene=dict(\n        xaxis=dict(backgroundcolor='#f4f4f4'),\n        yaxis=dict(backgroundcolor='#f4f4f4'),\n        zaxis=dict(backgroundcolor='#f4f4f4')\n    ),\n    paper_bgcolor = '#f4f4f4',\n    title = \"Hyperplanes in higher dimension\"\n)\nfig.show()\n\n\n                                                \n\n\nFor the hyperplanes in higher dimensions, the distance between two parallel hyperplanes \\(w^T\\mathbf{x}+b=1\\) and \\(w^T\\mathbf{x}+b=-1\\) is given as\n\\[\n\\text{Distance: }M= \\frac{|1-(-1)|}{\\|w\\|}=\\frac{2}{\\|w\\|}\n\\]\nThis distance, \\(M\\) is the margin and our objective is to maximize \\(M\\), or equivalently, minimize \\(\\|w\\|\\) subject to the constraints \\(y_i (w^T x_i + b) \\geq 1\\).\n\n\nOptimization of the SVM\nThe optimization problem can be formulated as follows:\nPrimal Form: \\[\n\\min_{w, b} \\frac{1}{2} \\|w\\|^2\n\\]\nsubject to: \\[\ny_i (w^T x_i + b) \\geq 1, \\quad \\forall i\n\\]\nThis is a convex optimization problem because the objective function \\(\\frac{1}{2} \\|w\\|^2\\) is convex, and the constraints are linear.\n\n\nThe Dual Form of SVM\nTo solve the optimization problem, it is often more efficient to use the dual form. By introducing Lagrange multipliers \\(\\alpha_i \\geq 0\\), we can construct the Lagrangian:\n\\[\nL(w, b, \\alpha) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^n \\alpha_i \\left( y_i (w^T x_i + b) - 1 \\right)\n\\]\nTaking the partial derivatives of \\(L\\) with respect to \\(w\\) and \\(b\\) and setting them to zero yields:\n\\[\\begin{align*}\n\\frac{\\partial L}{\\partial w} &= w - \\sum_{i=1}^n \\alpha_i y_i x_i = 0\\\\\n\\implies w &= \\sum_{i=1}^n \\alpha_i y_i x_i\\\\\n\\text{ and } &\\\\\n\\frac{\\partial L}{\\partial b} &= -\\sum_{i=1}^n \\alpha_i y_i = 0\\\\\n\\sum_{i=1}^n \\alpha_i y_i &= 0\n\\end{align*}\\]\nThis tells us that \\(w\\) can be expressed as a linear combination of the training points \\(x_i\\) with weights given by \\(\\alpha_i y_i\\) and the sum of the weighted labels is zero.\nNow we substitute \\(w = \\sum_{i=1}^n \\alpha_i y_i x_i\\) back into the Lagrangian \\(L(w, b, \\alpha)\\). The primal objective function \\(\\frac{1}{2} \\|w\\|^2\\) becomes:\n\\[\\begin{align*}\n\\frac{1}{2} \\|w\\|^2 &= \\frac{1}{2} \\left( \\sum_{i=1}^n \\alpha_i y_i x_i \\right)^T \\left( \\sum_{j=1}^n \\alpha_j y_j x_j \\right)\\\\\n&= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n\\end{align*}\\]\nSubstituting back into the Lagrangian,\n\\[\nL(w, b, \\alpha) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^n \\alpha_i  y_i (w^T x_i + b) + \\sum_{i=1}^n \\alpha_i\n\\]\nwe get the dual form as:\n\\[\n\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^T x_j\n\\]\nsubject to:\n\\[\n\\alpha_i \\geq 0 \\quad \\forall i, \\quad \\text{and} \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\nThe solution to the dual form gives the values of \\(\\alpha_i\\), which are used to construct the optimal hyperplane. The final decision boundary is then:\n\\[\nf(x) = \\text{sign} \\left( \\sum_{i=1}^N \\alpha_i y_i x_i^T x + b \\right)\n\\]"
  },
  {
    "objectID": "dsandml/svm/index.html#nonlinear-support-vector-machines",
    "href": "dsandml/svm/index.html#nonlinear-support-vector-machines",
    "title": "Support Vector Machine (SVM) Algorithm",
    "section": "Nonlinear Support Vector Machines",
    "text": "Nonlinear Support Vector Machines\nImagine we have a dataset that looks like this.\n\n\nCode\nfrom sklearn.datasets import make_moons\nX,y = make_moons(n_samples=300, noise=0.2, random_state=42)\nplt.figure(figsize=(8,6))\nplt.scatter(X[y==0][:,0], X[y==0][:,1], color='red', label='Class 0')\nplt.scatter(X[y==1][:,0], X[y==1][:,1], color='blue', label='Class 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.gca().set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThere is now way that a linear hyperplane seperates the data. Therefore, when the data is not linearly separable, SVMs use the kernel trick to map the data into a higher-dimensional space where a linear separation is possible. The idea is to map the original data points \\(\\mathbf{x}\\) from the input space to a higher-dimensional feature space using a *feature transformation function \\(\\phi(x)\\).\n\nFor example,\n\\[\n\\phi: \\mathbb{R}^n\\mapsto \\mathbb{R}^m, \\hspace{4mm} \\text{where } m&gt;n\n\\]\n\nIn the higher-dimensional space, it’s often easier to find a hyperplane that separates the two classes linearly.  However, explicitly calculating and working with this higher-dimensional transformation \\(\\phi(x)\\) can be computationally expensive, especially when the dimensionality \\(m\\) is very high or infinite. This is where the kernel trick comes in.\n\n\nThe Kernel Trick\n\nThe kernel trick is a method that allows us to compute the inner product between two transformed data points \\(\\phi(x_i)\\) and \\(\\phi(x_j)\\) in the higher-dimensional space without explicitly computing the transformation \\(\\phi(x)\\).  Instead of computing \\(\\phi(x_i)\\) and \\(\\phi(x_j)\\) separately and then taking their inner product, we define a kernel function \\(K(x_i, x_j)\\) that directly computes this inner product in the higher-dimensional space:\n\n\\[\nK(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\n\\]\n\nBy substituting this kernel function into the SVM optimization problem, we can work in the higher-dimensional space implicitly, without ever explicitly mapping the data points to that space. This allows us to handle complex, nonlinear decision boundaries with a more computationally efficient approach.\n\n\nPolynomial Kernel\nThe polynomial kernel allows us to model nonlinear decision boundaries using polynomial functions. It is defined as:\n\\[\nK(x_i, x_j) = (x_i^T x_j + c)^d\n\\]\nwhere:\n\n\\(c\\) is a constant that controls the influence of higher-order terms.\n\\(d\\) is the degree of the polynomial.\n\n\nThe polynomial kernel creates a feature space that corresponds to all monomials up to degree \\(d\\). It can model interactions between features, allowing the SVM to classify data with polynomial decision boundaries.\n\nFor example, when we have 1-D data and it is linearly inseperable, we can use polynomial kernel with degree 2 or higher. Say \\(c=1/2\\) and \\(d=2\\),\n\\[\\begin{align*}\nK(x_1,x_2)& = \\left(x_1x_2+\\frac{1}{2}\\right)^2\\\\\n& = \\left(x_1x_2+\\frac{1}{2}\\right)\\left(x_1x_2+\\frac{1}{2}\\right)\\\\\n& = x_1^2x_2^2+\\frac{1}{2}x_1x_2+\\frac{1}{4}\\\\\n& = x_1x_2+x_1^2x_2^2+\\frac{1}{4}\\\\\n& = (x_1,x_1^2,\\frac{1}{2})\\cdot (x_2,x_2^2,\\frac{1}{2})\n\\end{align*}\\]\n\n\nCode\nx1 = list(range(1,11))\nx2 = list(range(14,24))\nx3 = list(range(27,37))\nx = [j for sub in [x1,x2,x3] for j in sub]\ny = [0]*30\ncolors = ['blue']*10+['red']*10+ ['blue']*10\ny_squared = [i**2 for i in x]\nslope = (197-529.5)/(14-23)\nline_x = np.linspace(0,40,100)\nline_y = slope* (line_x - 14) + 197\n\nfig = plt.figure(figsize=(7.9,4))\nax1 = fig.add_subplot(121)\nax1.scatter(x,y, c=colors, label='Actual Data in 1D')\nax1.set_xlabel(r'$feature$')\nax1.set_xlim(0,40)\nax1.set_ylim(-50,50)\n\nax2 = fig.add_subplot(122)\nax2.scatter(x,y, c=colors, label='Actual Data in 1D')\nax2.scatter(x,y_squared, c= colors, marker='o', label='Transformed Data in 2D')\nax2.plot(line_x,line_y, color='green',label='1D Hyperplane')\nax2.set_xlim(0,40)\nax2.set_ylim(-100,1400)\nax2.set_xlabel(r'$feature$')\nax2.set_ylabel(r'$feature^2$')\nax2.legend()\n\nplt.tight_layout()\naxes = [ax1,ax2]\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nor for a 2D data to 3D transformation along with 2D hyperplane\n\n\nCode\nfrom sklearn.datasets import make_circles\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Generate a dataset that is not linearly separable\nX, y = make_circles(n_samples=300, factor=0.3, noise=0.1, random_state=42)\n\n# Plot the original dataset\nfig = plt.figure(figsize=(7.9, 4))\nax1 = fig.add_subplot(121)\nax1.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', label='Class 0')\nax1.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')\nax1.set_title('Original Data')\nax1.set_xlabel('feature 1')\nax1.set_ylabel('feature 2')\nax1.legend()\n\n# Apply polynomial kernel transformation\nX_transformed = np.hstack((X, (X[:, 0]**2 + X[:, 1]**2).reshape(-1, 1)))\n\n# Plot the transformed dataset in 3D\nax2 = fig.add_subplot(122, projection='3d')\nax2.scatter(X_transformed[y == 0][:, 0], X_transformed[y == 0][:, 1], X_transformed[y == 0][:, 2], color='red', label='Class 0')\nax2.scatter(X_transformed[y == 1][:, 0], X_transformed[y == 1][:, 1], X_transformed[y == 1][:, 2], color='blue', label='Class 1')\nax2.set_title('2D to 3D transformed')\nax2.set_xlabel('Feature 1')\nax2.set_ylabel('Feature 2')\nax2.set_zlabel('Poly Feature')\nax2.legend()\n\naxes = [ax1, ax2]\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nRadial Basis Function (RBF) Kernel (Gaussian Kernel)\n\nThe RBF kernel, also known as the Gaussian kernel, is one of the most popular kernels because it can map the data to an infinite-dimensional space, allowing the model to capture highly complex relationships. It’s defined as:\n\n\\[\nK(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\n\\]\nor equivalently:\n\\[\nK(x_i, x_j) = \\exp\\left(-\\gamma \\|x_i - x_j\\|^2\\right)\n\\]\nwhere:\n\n\\(\\|x_i - x_j\\|^2\\) is the squared Euclidean distance between the points \\(x_i\\) and \\(x_j\\).\n\\(\\sigma\\) (or \\(\\gamma = \\frac{1}{2\\sigma^2}\\)) controls the width of the Gaussian function and, thus, the influence of each training example.\n\n\nThe RBF kernel is particularly effective when the relationship between classes is highly nonlinear. It maps each data point to an infinite-dimensional space, allowing the SVM to capture fine-grained patterns.\n\n\n\nSigmoid Kernel\nThe sigmoid kernel is related to neural networks and is defined as:\n\\[\nK(x_i, x_j) = \\tanh(\\kappa x_i^T x_j + \\theta)\n\\]\nwhere:\n\n\\(\\kappa\\) and \\(\\theta\\) are parameters that control the shape of the kernel.\n\nThis kernel can be interpreted as simulating a neural network with a single hidden layer, where \\(\\tanh\\) serves as the activation function.\n\n\n\nDual Formulation with the Kernel Trick\n\nIn the dual form of the SVM optimization problem, we only require the inner products \\(x_i^T x_j\\) between data points. By replacing these inner products with \\(K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\\), we obtain the dual form of the optimization problem for a kernelized SVM:\n\n\\[\n\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n\\]\nsubject to:\n\\[\n\\alpha_i \\geq 0 \\quad \\forall i, \\quad \\text{and} \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\nUsing the kernel function \\(K(x_i, x_j)\\), we can compute the decision boundary in the original space without explicitly mapping to the higher-dimensional space.\n\n\nDecision Function with the Kernel Trick\nOnce we solve for \\(\\alpha\\) and determine the support vectors, the decision function for a new point \\(x\\) becomes:\n\\[\nf(x) = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n\\]\nwhere:\n\n\\(\\alpha_i\\) are the Lagrange multipliers found from the optimization.\n\\(y_i\\) are the labels of the support vectors.\n\\(K(x_i, x)\\) is the kernel function that calculates the inner product between the support vector \\(x_i\\) and the new data point \\(x\\).\n\nThis decision function allows us to classify new data points by evaluating their relationship with the support vectors in the original input space, using the kernel to measure similarity.\n\n\nSoft Margin SVM\n\nThe concept of soft margin SVM extends the hard margin SVM approach to handle cases where data is not perfectly separable. In real-world datasets, it’s often impossible to perfectly separate classes without allowing some misclassification or overlap. Soft margin SVM addresses this by introducing a margin of tolerance—it allows some data points to lie within the margin or even on the wrong side of the decision boundary.\n\nIn hard margin SVM, we strictly enforced that: \\[\ny_i (w^T x_i + b) \\ge 1, \\quad \\forall i\n\\]\nwhich means that each point is correctly classified and outside the margin.\nIn soft margin SVM, we introduce slack variables \\(\\xi_i\\), which allow some points to violate this constraint. The constraints become: \\[\ny_i (w^T x_i + b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0\n\\]\nwhere:\n\n\\(\\xi_i\\) measures the degree of misclassification for each data point \\(x_i\\).\nIf \\(\\xi_i = 0\\), then \\(x_i\\) lies on or outside the margin (correct classification).\nIf \\(0 &lt; \\xi_i \\le 1\\), then \\(x_i\\) lies within the margin but is still correctly classified.\nIf \\(\\xi_i &gt; 1\\), then \\(x_i\\) is misclassified.\n\n\nTo find the optimal hyperplane with a soft margin, we modify the objective function to include a penalty for misclassifications. The goal is to balance maximizing the margin and minimizing the misclassification error. The objective function becomes:\n\n\\[\n\\min_{w, b, \\xi} \\quad \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i\n\\]\nwhere:\n\nThe term \\(\\frac{1}{2} \\|w\\|^2\\) encourages a large margin, just as in hard margin SVM.\nThe term \\(C \\sum_{i=1}^n \\xi_i\\) penalizes misclassified points, where \\(C\\) is a regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n\nThe parameter \\(C\\):\n\nIf \\(C\\) is large, the optimization emphasizes minimizing misclassifications (more sensitive to individual data points), which leads to a narrower margin with fewer violations.\nIf \\(C\\) is small, the optimization focuses more on maximizing the margin, allowing more misclassifications.\n\nThe optimization problem for soft margin SVM can be written as:\n\\[\n\\min_{w, b, \\xi} \\quad \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i\n\\] subject to: \\[\ny_i (w^T x_i + b) \\ge 1 - \\xi_i, \\quad \\xi_i \\ge 0 \\quad \\forall i\n\\]\nThis problem is still convex and can be solved using Lagrange multipliers, though it becomes slightly more complex due to the introduction of slack variables \\(\\xi_i\\).\nThe dual form of the soft margin SVM, similar to the hard margin case, can be derived using Lagrange multipliers. The dual problem becomes:\n\\[\n\\max_{\\alpha} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n\\] subject to: \\[\n0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n\\]\nThe main difference here is that each \\(\\alpha_i\\) is now bounded by \\(C\\) instead of being unrestricted, which introduces a balance between the margin maximization and error tolerance.\n\nIn soft margin SVM, the margin is not strict. Some points are allowed to lie within the margin or even be misclassified. Points that lie on the wrong side of the margin are called support vectors with non-zero slack values \\(\\xi_i\\).\n\n\nHigh \\(C\\): A larger \\(C\\) results in a narrower margin with fewer violations, meaning fewer points within the margin or misclassified. This leads to a more complex model that might overfit if \\(C\\) is too high.\nLow \\(C\\): A smaller \\(C\\) results in a wider margin with more allowed violations, meaning more tolerance to misclassifications. This generally leads to a simpler, more robust model that might underfit if \\(C\\) is too low.\n\n\nThe regularization parameter \\(C\\) controls the trade-off between margin width and classification accuracy. Cross-validation is commonly used to select the optimal value of \\(C\\) by evaluating the model’s performance across different values of \\(C\\) and choosing the one that generalizes best to unseen data.\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic dataset\nX, y = make_blobs(n_samples=40, centers=2, random_state=6)\ny = np.where(y == 0, -1, 1)  # Transform labels to -1 and +1 for SVM\n\n# Different values of C for comparison\nC_values = [0.1, 1, 100]\n\n# Plotting\nplt.figure(figsize=(6, 15))  # Adjust figure size for vertical layout\nplt.gcf().patch.set_facecolor('#f4f4f4')  # Set background color for the figure\n\nfor i, C in enumerate(C_values):\n    # Fit SVM model with the given C value\n    model = svm.SVC(kernel='linear', C=C)\n    model.fit(X, y)\n    \n    # Create a mesh to plot decision boundaries\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n    \n    # Plot decision boundary and margin\n    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    ax = plt.subplot(len(C_values), 1, i + 1)  # Adjust to create vertical subplots\n    ax.set_facecolor('#f4f4f4')  # Set background color for the plot area\n    plt.contourf(xx, yy, Z, levels=[-1, 0, 1], colors=['#FFAAAA', '#AAAAFF', '#AAAAFF'], alpha=0.3)\n    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='k')\n    \n    # Plot training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr, s=30, edgecolors='k')\n    plt.title(f\"SVM with Soft Margin (C={C})\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    \n    # Mark support vectors\n    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')\n\nplt.suptitle(\"Effect of Regularization Parameter C on Soft Margin SVM\", y=0.96)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()"
  },
  {
    "objectID": "dsandml/svm/index.html#references",
    "href": "dsandml/svm/index.html#references",
    "title": "Support Vector Machine (SVM) Algorithm",
    "section": "References",
    "text": "References\n\nBooks\n\nBishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd Edition. Springer.\n\nBoyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.\n\nSchölkopf, B., & Smola, A. J. (2002). Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press.\n\nMurphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT Press.\n\nVapnik, V. (1998). Statistical Learning Theory. Wiley-Interscience.\n\n\n\nLecture Notes\n\nAndrew Ng’s Machine Learning course on Coursera, particularly the lectures on Support Vector Machines, covering linear SVMs, geometric interpretation, and constraints.\n\nStatQuest with Josh Starmer\nData Science Bootcamp by The Erdos Institute\n\n\n\nJournals and Articles\n\nCortes, C., & Vapnik, V. (1995). “Support-vector networks.” Machine Learning, 20(3), 273-297.\n\nAizerman, M. A., Braverman, E. M., & Rozonoer, L. I. (1964). “Theoretical foundations of the potential function method in pattern recognition learning.” Automation and Remote Control, 25, 821-837.\n\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/svm/index.html#python-implementation-of-svm",
    "href": "dsandml/svm/index.html#python-implementation-of-svm",
    "title": "Support Vector Machine (SVM) Algorithm",
    "section": "Python Implementation of SVM",
    "text": "Python Implementation of SVM\n\nLinearSVC\nLet’s first create the data\n\nnp.random.seed(123)\nn_rows = 200\ndiff = 0.1\nX1 = np.random.random((n_rows,2))\nX_1 = X1[(X1[:,1]-X1[:,0])&lt;= -diff,:]\nX_2 = X1[(X1[:,1]-X1[:,0])&gt;= diff,:]\n\nX = np.append(X_1, X_2, axis=0)\ny = np.empty(np.shape(X)[0])\ny[(X[:,1]-X[:,0])&lt;= -diff] = -1\ny[(X[:,1]-X[:,0])&gt;= diff] = 1\nplt.scatter(X[y==-1,0], X[y==-1,1], c='blue', label=-1)\nplt.scatter(X[y==1,0], X[y==1,1], c='red', label=1)\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.grid(True)\nplt.legend()\nplt.gca().set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nNow we apply linear SVM classifier\n\nfrom sklearn.svm import LinearSVC\n\nmaximum_margin_SVC = LinearSVC(C= 1000, max_iter=10000, dual=\"auto\")\nmaximum_margin_SVC.fit(X,y)\nx1 = np.linspace(0,1,100)\nx2 = np.linspace(0,1,100)\n\nx1,x2 = np.meshgrid(x1,x2)\nx1x2 = np.vstack([x1.ravel(),x2.ravel()]).T\nz = maximum_margin_SVC.decision_function(x1x2).reshape(x1.shape)\n\nplt.scatter(X[y==-1,0],X[y==-1,1], c='blue', label='Training -1')\nplt.scatter(X[y==1,0],X[y==1,1], c='red', label='Training 1')\nplt.contour(x1,x2,z, colors='k',levels=[-1,0,1], alpha=0.7, linestyles=['--','-','--'])\nplt.legend()\nplt.xlim(0,1)\nplt.ylim(0,1)\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.grid(True)\nplt.legend()\nplt.gca().set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nThe data that we used to explain the polynomial kernels\n\nfrom sklearn.svm import SVC\nX = 2*np.random.random(50)-1\ny = np.ones(len(X))\ny[(X&gt;0.35) | (X&lt;-0.35)] = -1\n\nsvc = SVC(kernel='poly', degree=2, C=1000)\nsvc.fit(X.reshape(-1,1),y)\nplt.scatter(X[y==-1],np.ones(sum(y==-1)), c='blue',label='class -1')\nplt.scatter(X[y==1],np.ones(sum(y==1)), c='red',label='class 1')\ndcsns = svc.decision_function(np.linspace(-1,1,10000).reshape(-1,1)).round(1)\nplt.scatter(\n    np.linspace(-1,1,10000)[dcsns==0],\n    np.ones(10000)[dcsns==0],\n    marker='|',\n    s= 400,\n    c='green',\n    label='Decision Boundary'\n    )\nplt.legend()\nplt.gca().set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nFor the higher dimensions\n\nX, y = make_circles(n_samples=300, factor=0.3, noise=0.1, random_state=42)\n\n# Create SVM models with polynomial and RBF kernels\nmodel_poly = svm.SVC(kernel='poly', degree=2, C=1000)\nmodel_rbf = svm.SVC(kernel='rbf', gamma=1, C=1000)\n\n# Fit the models\nmodel_poly.fit(X, y)\nmodel_rbf.fit(X, y)\n\n# Create a mesh to plot decision boundaries\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n\n# Plotting\nplt.figure(figsize=(7.9, 4))\n\n# Polynomial Kernel\nplt.subplot(1, 2, 1)\nZ_poly = model_poly.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ_poly = Z_poly.reshape(xx.shape)\nplt.contourf(xx, yy, Z_poly, levels=[-1, 0, 1], colors=['#FFAAAA', '#AAAAFF', '#AAAAFF'], alpha=0.3)\nplt.contour(xx, yy, Z_poly, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='k')\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr, s=20, edgecolors='k')\nplt.title(\"SVM with Polynomial Kernel\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.gca().set_facecolor('#f4f4f4')\n\n# RBF Kernel\nplt.subplot(1, 2, 2)\nZ_rbf = model_rbf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ_rbf = Z_rbf.reshape(xx.shape)\nplt.contourf(xx, yy, Z_rbf, levels=[-1, 0, 1], colors=['#FFAAAA', '#AAAAFF', '#AAAAFF'], alpha=0.3)\nplt.contour(xx, yy, Z_rbf, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='k')\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr, s=20, edgecolors='k')\nplt.title(\"SVM with RBF Kernel\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.gca().set_facecolor('#f4f4f4')\n\nplt.suptitle(\"Polynomial and RBF Kernels on Nonlinear Data\")\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.gca().set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()"
  },
  {
    "objectID": "codepages/salesforecasting/index.html",
    "href": "codepages/salesforecasting/index.html",
    "title": "Time Series Forecasting of Future Sales Using ARIMA and SARIMA",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "portfolio/dsp/salesforecasting/index.html",
    "href": "portfolio/dsp/salesforecasting/index.html",
    "title": "Time Series Forecasting of Future Sales Using ARIMA and SARIMA",
    "section": "",
    "text": "Notebook\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2024,\n  author = {Islam, Rafiq},\n  title = {Time {Series} {Forecasting} of {Future} {Sales} {Using}\n    {ARIMA} and {SARIMA}},\n  date = {2024-11-08},\n  url = {https://mrislambd.github.io/portfolio/dsp/salesforecasting/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2024. “Time Series Forecasting of Future Sales Using\nARIMA and SARIMA.” November 8, 2024. https://mrislambd.github.io/portfolio/dsp/salesforecasting/."
  },
  {
    "objectID": "codepages/salesforecasting/index.html#data-cleaning-and-preprocessing",
    "href": "codepages/salesforecasting/index.html#data-cleaning-and-preprocessing",
    "title": "Time Series Forecasting of Future Sales Using ARIMA and SARIMA",
    "section": "Data Cleaning and Preprocessing",
    "text": "Data Cleaning and Preprocessing\n\ndf.columns = ['Month', 'Sales']\ndf = df.drop(df.index[-2:])\ndf.tail()\n\n\n\n\n\n\n\n\nMonth\nSales\n\n\n\n\n100\n1972-05\n4618.0\n\n\n101\n1972-06\n5312.0\n\n\n102\n1972-07\n4298.0\n\n\n103\n1972-08\n1413.0\n\n\n104\n1972-09\n5877.0\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 105 entries, 0 to 104\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Month   105 non-null    object \n 1   Sales   105 non-null    float64\ndtypes: float64(1), object(1)\nmemory usage: 1.8+ KB\n\n\nSince the column, month is a string object, so we need to convert it into date and time format in order to apply time series models\n\ndf['Month'] = pd.to_datetime(df['Month'])\ndf.head()\n\n\n\n\n\n\n\n\nMonth\nSales\n\n\n\n\n0\n1964-01-01\n2815.0\n\n\n1\n1964-02-01\n2672.0\n\n\n2\n1964-03-01\n2755.0\n\n\n3\n1964-04-01\n2721.0\n\n\n4\n1964-05-01\n2946.0\n\n\n\n\n\n\n\nNow looking at the data, we see that it’s a monthly sales data. So we set the month as our index, instead of the regular index.\n\ndf.set_index('Month', inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nSales\n\n\nMonth\n\n\n\n\n\n1964-01-01\n2815.0\n\n\n1964-02-01\n2672.0\n\n\n1964-03-01\n2755.0\n\n\n1964-04-01\n2721.0\n\n\n1964-05-01\n2946.0"
  },
  {
    "objectID": "codepages/salesforecasting/index.html#visualize-the-data",
    "href": "codepages/salesforecasting/index.html#visualize-the-data",
    "title": "Time Series Forecasting of Future Sales Using ARIMA and SARIMA",
    "section": "Visualize the data",
    "text": "Visualize the data\n\ndf.plot()\n\n\n\n\n\n\n\n\nFrom the plot we see that there is some seasonality in the data which needs to be taken care off. Let’s first check the stationarity of the data. Stationarity means the statistical properites such as mean, variance remains the same over the time for a time series data. We define the null and alternative hypothesis like this\n\n\\(H_0\\): The data is not stationary\n\n\\(H_1\\): The data is stationary\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\ndef adfuler_test(sales):\n    adf_test = adfuller(sales)\n    labels = ['ADF Test Statistic','p-value', 'Number of observation used']\n    for value,label in zip(adf_test, labels):\n        print(label+' : '+str(value))\n    if adf_test[1] &lt;= 0.05:\n        print(\"Strong Evidence against the null hypothesis (H0), so reject the null hypothesis\")\n    else:\n        print(\"There is not enough evidence against the null hypothesis, so we accept the alternative\")\nadfuler_test(df['Sales'])\n\nADF Test Statistic : -1.8335930563276166\np-value : 0.36391577166024813\nNumber of observation used : 11\nThere is not enough evidence against the null hypothesis, so we accept the alternative"
  },
  {
    "objectID": "dsandml/eda/index.html",
    "href": "dsandml/eda/index.html",
    "title": "Exploratory Data Analysis (EDA) and Data Visualization",
    "section": "",
    "text": "In this Exploratory Data Analysis and Visualization notebook, we want to explore the 911 call data from Kaggle.com"
  },
  {
    "objectID": "dsandml/eda/index.html#the-dataset",
    "href": "dsandml/eda/index.html#the-dataset",
    "title": "Exploratory Data Analysis (EDA) and Data Visualization",
    "section": "The Dataset",
    "text": "The Dataset\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('911.csv')"
  },
  {
    "objectID": "dsandml/eda/index.html#discriptive-statistics",
    "href": "dsandml/eda/index.html#discriptive-statistics",
    "title": "Exploratory Data Analysis (EDA) and Data Visualization",
    "section": "Discriptive Statistics",
    "text": "Discriptive Statistics\nWe first check the data information to see the number of observations, datatype, memory usages etc.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 105957 entries, 0 to 105956\nData columns (total 9 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   lat        105957 non-null  float64\n 1   lng        105957 non-null  float64\n 2   desc       105957 non-null  object \n 3   zip        92735 non-null   float64\n 4   title      105957 non-null  object \n 5   timeStamp  105957 non-null  object \n 6   twp        105924 non-null  object \n 7   addr       105957 non-null  object \n 8   e          105957 non-null  int64  \ndtypes: float64(3), int64(1), object(5)\nmemory usage: 7.3+ MB\n\n\nA first look of the data\n\ndf.head()\n\n\n\n\n\n\n\n\nlat\nlng\ndesc\nzip\ntitle\ntimeStamp\ntwp\naddr\ne\n\n\n\n\n0\n40.297876\n-75.581294\nREINDEER CT & DEAD END; NEW HANOVER; Station ...\n19525.0\nEMS: BACK PAINS/INJURY\n12/10/15 17:10\nNEW HANOVER\nREINDEER CT & DEAD END\n1\n\n\n1\n40.258061\n-75.264680\nBRIAR PATH & WHITEMARSH LN; HATFIELD TOWNSHIP...\n19446.0\nEMS: DIABETIC EMERGENCY\n12/10/15 17:29\nHATFIELD TOWNSHIP\nBRIAR PATH & WHITEMARSH LN\n1\n\n\n2\n40.121182\n-75.351975\nHAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...\n19401.0\nFire: GAS-ODOR/LEAK\n12/10/15 14:39\nNORRISTOWN\nHAWS AVE\n1\n\n\n3\n40.116153\n-75.343513\nAIRY ST & SWEDE ST; NORRISTOWN; Station 308A;...\n19401.0\nEMS: CARDIAC EMERGENCY\n12/10/15 16:47\nNORRISTOWN\nAIRY ST & SWEDE ST\n1\n\n\n4\n40.251492\n-75.603350\nCHERRYWOOD CT & DEAD END; LOWER POTTSGROVE; S...\nNaN\nEMS: DIZZINESS\n12/10/15 16:56\nLOWER POTTSGROVE\nCHERRYWOOD CT & DEAD END\n1\n\n\n\n\n\n\n\nSome data related questions. For example,\n\nWhat are the top 10 zipcodes for 911 calls?\n\ndf.zip.value_counts().head(10)\n\nzip\n19401.0    7445\n19464.0    7122\n19403.0    5189\n19446.0    5060\n19406.0    3404\n19002.0    3238\n19468.0    3202\n19454.0    2984\n19090.0    2832\n19046.0    2779\nName: count, dtype: int64\n\n\nWhat are the top 10 twonships for the 911 calls?\n\ndf.twp.value_counts().head(10)\n\ntwp\nLOWER MERION        9069\nABINGTON            6403\nNORRISTOWN          6265\nUPPER MERION        5551\nCHELTENHAM          4882\nPOTTSTOWN           4448\nUPPER MORELAND      3658\nLOWER PROVIDENCE    3435\nPLYMOUTH            3371\nHORSHAM             3142\nName: count, dtype: int64"
  },
  {
    "objectID": "dsandml/eda/index.html#feature-engineering",
    "href": "dsandml/eda/index.html#feature-engineering",
    "title": "Exploratory Data Analysis (EDA) and Data Visualization",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nSometimes creating new features from the existing features helps understand the data better. For example, for this dataset, we can create a new column called Reason for emergency 911 call.\n\ndf['reason'] = df.title.apply(lambda title: title.split(':')[0])\ndf.head(3)\n\n\n\n\n\n\n\n\nlat\nlng\ndesc\nzip\ntitle\ntimeStamp\ntwp\naddr\ne\nreason\n\n\n\n\n0\n40.297876\n-75.581294\nREINDEER CT & DEAD END; NEW HANOVER; Station ...\n19525.0\nEMS: BACK PAINS/INJURY\n12/10/15 17:10\nNEW HANOVER\nREINDEER CT & DEAD END\n1\nEMS\n\n\n1\n40.258061\n-75.264680\nBRIAR PATH & WHITEMARSH LN; HATFIELD TOWNSHIP...\n19446.0\nEMS: DIABETIC EMERGENCY\n12/10/15 17:29\nHATFIELD TOWNSHIP\nBRIAR PATH & WHITEMARSH LN\n1\nEMS\n\n\n2\n40.121182\n-75.351975\nHAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...\n19401.0\nFire: GAS-ODOR/LEAK\n12/10/15 14:39\nNORRISTOWN\nHAWS AVE\n1\nFire\n\n\n\n\n\n\n\nwhat are top reasons for the emergency calls?\n\ndf.reason.value_counts()\n\nreason\nEMS        52515\nTraffic    37505\nFire       15937\nName: count, dtype: int64\n\n\nvisualization of the reason column\n\nsns.countplot(x=df.reason, hue=df.reason, palette='viridis')\n\n\n\n\n\n\n\n\nThe timeStamp column contains time information year-month-day hour:minute:second format but in string value/object. So we can convert this column to obtain new features.\n\ndf['timeStamp'] = pd.to_datetime(df.timeStamp)\ntime = df.timeStamp.iloc[0]\n\n/var/folders/53/8y5n2fl55p3g3r5_pk9yfwk40000gn/T/ipykernel_7643/1994586768.py:1: UserWarning:\n\nCould not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n\n\n\nLet’s create new features called hour, month, and day of the calls.\n\ndf['hour'] = df.timeStamp.apply(lambda time: time.hour)\ndf['month'] = df.timeStamp.apply(lambda time: time.month)\ndf['day'] = df.timeStamp.apply(lambda time: time.dayofweek)\ndays = {\n    0:'Monday', 1:'Tuesday', 2:'Wednesday', \n    3:'Thursday', 4:'Friday', 5:'Saturday',\n    6:'Sunday'\n    }\ndf.day = df.day.map(days)\ndf = df[\n    ['lat','lng','zip','twp','e','reason',\n    'month','day','hour','title','timeStamp',\n    'desc','addr']\n    ]\ndf.head(3)\n\n\n\n\n\n\n\n\nlat\nlng\nzip\ntwp\ne\nreason\nmonth\nday\nhour\ntitle\ntimeStamp\ndesc\naddr\n\n\n\n\n0\n40.297876\n-75.581294\n19525.0\nNEW HANOVER\n1\nEMS\n12\nThursday\n17\nEMS: BACK PAINS/INJURY\n2015-12-10 17:10:00\nREINDEER CT & DEAD END; NEW HANOVER; Station ...\nREINDEER CT & DEAD END\n\n\n1\n40.258061\n-75.264680\n19446.0\nHATFIELD TOWNSHIP\n1\nEMS\n12\nThursday\n17\nEMS: DIABETIC EMERGENCY\n2015-12-10 17:29:00\nBRIAR PATH & WHITEMARSH LN; HATFIELD TOWNSHIP...\nBRIAR PATH & WHITEMARSH LN\n\n\n2\n40.121182\n-75.351975\n19401.0\nNORRISTOWN\n1\nFire\n12\nThursday\n14\nFire: GAS-ODOR/LEAK\n2015-12-10 14:39:00\nHAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...\nHAWS AVE\n\n\n\n\n\n\n\nNow that we have almost a clean dataset, we can analyze the reason column based on the days of the week or months of a year.\n\nsns.countplot(x='day', data= df, hue='reason', palette='viridis')\nplt.legend(bbox_to_anchor=(1.05,1), loc=2, borderaxespad=0.0)\n\n\n\n\n\n\n\n\nFor the month column\n\nsns.countplot(x='month', data= df, hue='reason', palette='viridis')\nplt.legend(bbox_to_anchor=(1.05,1), loc=2, borderaxespad=0.0)\n\n\n\n\n\n\n\n\nTo create a time series data\n\ndf['date'] = df['timeStamp'].apply(lambda time: time.date())\ndf.groupby('date').count()['twp'].plot()\n\n\n\n\n\n\n\n\nNow to see for each reason\n\nstart_date = pd.to_datetime('2019-01-01')\n\n\ndf['date'] = pd.to_datetime(df['date'])\n\n\nfig = plt.figure(figsize=(7.9,6))\n\nax1 = fig.add_subplot(311)\ndf[(df['reason'] == 'Traffic') & (df['date'] &gt;= start_date)].groupby('date').count()['twp'].plot(ax=ax1)\n\nax2 = fig.add_subplot(312)\ndf[(df['reason'] == 'Fire') & (df['date'] &gt;= start_date)].groupby('date').count()['twp'].plot(ax=ax2)\n\nax3 = fig.add_subplot(313)\ndf[(df['reason'] == 'EMS') & (df['date'] &gt;= start_date)].groupby('date').count()['twp'].plot(ax=ax3)\n\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "dsandml/dataviz/index.html",
    "href": "dsandml/dataviz/index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "For this data visualization project we use top 5 bank stock price data.\nimport pandas as pd\nimport yfinance as yf\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')"
  },
  {
    "objectID": "dsandml/dataviz/index.html#data",
    "href": "dsandml/dataviz/index.html#data",
    "title": "Data Visualization",
    "section": "Data",
    "text": "Data\n\nstart = pd.to_datetime('2020-01-01')\nend = pd.to_datetime('today')\ndf = yf.download(['WFC','BAC','JPM','C','GS'], start=start, end=end)\ndf.index = df.index.date\ndf.tail()\n\n[                       0%                       ][*******************   40%                       ]  2 of 5 completed[**********************60%****                   ]  3 of 5 completed[**********************80%*************          ]  4 of 5 completed[*********************100%***********************]  5 of 5 completed\n\n\n\n\n\n\n\n\nPrice\nAdj Close\nClose\n...\nOpen\nVolume\n\n\nTicker\nBAC\nC\nGS\nJPM\nWFC\nBAC\nC\nGS\nJPM\nWFC\n...\nBAC\nC\nGS\nJPM\nWFC\nBAC\nC\nGS\nJPM\nWFC\n\n\n\n\n2025-02-10\n46.669998\n80.730003\n650.530029\n271.040009\n79.099998\n46.669998\n80.730003\n650.530029\n271.040009\n79.099998\n...\n47.480000\n81.629997\n659.020020\n276.149994\n80.459999\n26725200\n10681600\n2351600\n8627400\n18357400\n\n\n2025-02-11\n46.790001\n81.110001\n647.239990\n274.989990\n79.639999\n46.790001\n81.110001\n647.239990\n274.989990\n79.639999\n...\n46.549999\n80.459999\n646.780029\n270.260010\n78.769997\n20314900\n9993800\n2711400\n7196400\n16682800\n\n\n2025-02-12\n46.209999\n81.269997\n649.000000\n275.450012\n79.250000\n46.209999\n81.269997\n649.000000\n275.450012\n79.250000\n...\n46.570000\n80.169998\n643.760010\n274.079987\n79.199997\n29777100\n10448100\n2295800\n6683900\n13440500\n\n\n2025-02-13\n46.330002\n82.099998\n648.950012\n276.320007\n78.849998\n46.330002\n82.099998\n648.950012\n276.320007\n78.849998\n...\n46.520000\n81.400002\n651.830017\n275.500000\n79.430000\n26815400\n11108400\n2031800\n8398700\n10093400\n\n\n2025-02-14\n46.959999\n84.610001\n660.549988\n276.589996\n79.980003\n46.959999\n84.610001\n660.549988\n276.589996\n79.980003\n...\n46.480000\n82.410004\n650.320007\n277.369995\n79.230003\n25938000\n16023000\n2246000\n5685600\n17516800\n\n\n\n\n5 rows × 30 columns\n\n\n\nNow we compute the maximum closing prices of all these 5 banks during this time period\n\ndf.xs(key='Close', axis=1, level='Price').max()\n\nTicker\nBAC     49.380001\nC       84.610001\nGS     660.549988\nJPM    276.899994\nWFC     81.419998\ndtype: float64\n\n\nNow we compute the returns for each of the stock\n\n# Retrieve the 'Close' prices for each ticker directly\nclose_prices = df.xs(key='Close', axis=1, level=0)\n\n# Calculate the daily percentage change (returns) for all tickers\nreturns = close_prices.pct_change()\nreturns.index = pd.to_datetime(returns.index)\n\n# Display the first few rows of returns\nreturns.head()\n\n\n\n\n\n\n\nTicker\nBAC\nC\nGS\nJPM\nWFC\n\n\n\n\n2020-01-02\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2020-01-03\n-0.020763\n-0.018835\n-0.011693\n-0.019491\n-0.006140\n\n\n2020-01-06\n-0.001433\n-0.003137\n0.010234\n-0.000795\n-0.005990\n\n\n2020-01-07\n-0.006600\n-0.008685\n0.006583\n-0.017001\n-0.008286\n\n\n2020-01-08\n0.010110\n0.007618\n0.009639\n0.007801\n0.003038"
  },
  {
    "objectID": "posts/socialshare/index.html",
    "href": "posts/socialshare/index.html",
    "title": "How to generate social share buttons",
    "section": "",
    "text": "If you want to share any blog posts on social media, you can have a share button at the bottom of each post so that the reader can easily share this on their preferred social media such as Facebook, LinkedIn, and X. Here I am showing only three but can be added more.\n\n\n\n# Library you need\nimport urllib.parse\n\n# Define the function to parse facebook sharable link\ndef fblink(link):\n    base_url = \"https://www.facebook.com/sharer/sharer.php\"\n    encoded_url = urllib.parse.quote(link, safe='')\n    full_url= f\"{base_url}?u={encoded_url}&amp;src=sdkpreparse\"\n    return full_url  \n\n# Suppose this is the link you want to share. Replace with your own link  \nlink=\"https://mrislambd.github.io/posts/social-share/\"\n\n# Then you can use this template\nprint('&lt;div id=\"fb-root\"&gt;&lt;/div&gt;')\nprint('&lt;script async defer crossorigin=\"anonymous\"\\n src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"&gt;&lt;/script&gt;')\nprint(' ')\nprint('&lt;div class=\"share-buttons\"&gt;')\nprint('&lt;div class=\"fb-share-button\" data-href=\"{}\"'.format(link))\nprint('data-layout=\"button_count\" data-size=\"small\"&gt;&lt;a target=\"_blank\" \\n href=\"{}\" \\n class=\"fb-xfbml-parse-ignore\"&gt;Share&lt;/a&gt;&lt;/div&gt;'.format(fblink(link)))\nprint('')\nprint('&lt;script src=\"https://platform.linkedin.com/in.js\" type=\"text/javascript\"&gt;lang: en_US&lt;/script&gt;')\nprint('&lt;script type=\"IN/Share\" data-url=\"{}\"&gt;&lt;/script&gt; '.format(link)) \nprint(' ')\nprint('&lt;a href=\"https://twitter.com/share?ref_src=twsrc%5Etfw\" class=\"twitter-share-button\" \\n data-url=\"{}\" data-show-count=\"true\"&gt;Tweet&lt;/a&gt;'.format(link))\nprint('&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;')\nprint('&lt;/div&gt;')\nprint('')\nprint('&lt;div class=\"fb-comments\" data-href=\"{}\"\\n data-width=\"\" data-numposts=\"5\"&gt;&lt;/div&gt;'.format(link)) \n# Then you can directly post the following output at the bottom of your page \n\n&lt;div id=\"fb-root\"&gt;&lt;/div&gt;\n&lt;script async defer crossorigin=\"anonymous\"\n src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"&gt;&lt;/script&gt;\n \n&lt;div class=\"share-buttons\"&gt;\n&lt;div class=\"fb-share-button\" data-href=\"https://mrislambd.github.io/posts/social-share/\"\ndata-layout=\"button_count\" data-size=\"small\"&gt;&lt;a target=\"_blank\" \n href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fmrislambd.github.io%2Fposts%2Fsocial-share%2F&amp;src=sdkpreparse\" \n class=\"fb-xfbml-parse-ignore\"&gt;Share&lt;/a&gt;&lt;/div&gt;\n\n&lt;script src=\"https://platform.linkedin.com/in.js\" type=\"text/javascript\"&gt;lang: en_US&lt;/script&gt;\n&lt;script type=\"IN/Share\" data-url=\"https://mrislambd.github.io/posts/social-share/\"&gt;&lt;/script&gt; \n \n&lt;a href=\"https://twitter.com/share?ref_src=twsrc%5Etfw\" class=\"twitter-share-button\" \n data-url=\"https://mrislambd.github.io/posts/social-share/\" data-show-count=\"true\"&gt;Tweet&lt;/a&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n&lt;/div&gt;\n\n&lt;div class=\"fb-comments\" data-href=\"https://mrislambd.github.io/posts/social-share/\"\n data-width=\"\" data-numposts=\"5\"&gt;&lt;/div&gt;\n\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science & Machine Learning Basics\n\n\n9 min\n\n\n\nRafiq Islam\n\n\nFriday, September 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)\n\n\n8 min\n\n\n\nRafiq Islam\n\n\nThursday, September 19, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to topCitationBibTeX citation:@online{islam2024,\n  author = {Islam, Rafiq},\n  title = {How to Generate Social Share Buttons},\n  date = {2024-07-17},\n  url = {https://mrislambd.github.io/posts/socialshare/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2024. “How to Generate Social Share Buttons.”\nJuly 17, 2024. https://mrislambd.github.io/posts/socialshare/."
  },
  {
    "objectID": "dsandml/dataviz/index.html#visualization",
    "href": "dsandml/dataviz/index.html#visualization",
    "title": "Data Visualization",
    "section": "Visualization",
    "text": "Visualization\nLet’s create a pairplot of the returns\n\nsns.pairplot(returns[1:])\n\n\n\n\n\n\n\n\nNow to check the maximum and minimum return and on what dates that happened\n\nprint('Minimum Return')\nprint(' ')\nprint(returns.idxmin())\nprint(' ')\nprint('Maximum Return')\nprint(' ')\nprint(returns.idxmax())\n\nMinimum Return\n \nTicker\nBAC   2020-03-16\nC     2020-03-16\nGS    2020-03-16\nJPM   2020-03-16\nWFC   2020-03-12\ndtype: datetime64[ns]\n \nMaximum Return\n \nTicker\nBAC   2020-03-13\nC     2020-03-13\nGS    2020-03-13\nJPM   2020-03-13\nWFC   2020-03-24\ndtype: datetime64[ns]\n\n\nTo find which bank is more risky we can simply check the standard deviations of the returns of each bank\n\nreturns.std()\n\nTicker\nBAC    0.022338\nC      0.024614\nGS     0.020780\nJPM    0.020345\nWFC    0.024490\ndtype: float64\n\n\nIt seems like CITI na groop has the maximum value in the standared deviations.\nNow let’s check the distribution of Wells Fargo’s return in 2023\n\nreturn_2023_wf = returns.loc['2023-01-01':'2023-12-31','WFC']\nsns.displot(return_2023_wf, color='blue', bins=80, kde=True)\n\n\n\n\n\n\n\n\nNext, we create the timeseries plot of the closing prices\n\ndf.xs(key='Close', axis=1, level='Price').plot(figsize=(9,5))\n\n\n\n\n\n\n\n\n\nTrend\nNext we plot moving average for Wells Fargo\n\nwfc = df['Close']['WFC']\nstart_date = pd.to_datetime('2023-10-10').date()\nend_date = pd.to_datetime('2024-10-20').date()\nwfc.loc[start_date:end_date].rolling(window=30).mean().plot(\n    figsize=(9,5),label='30 Day Avg'\n    )\nwfc.loc[start_date:end_date].plot(\n    label='WFC Close',figsize=(9,5)\n    )\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nPrice Movement Correlation\n\nsns.heatmap(df.xs(key='Close', axis=1, level='Price').corr(), annot=True)\n\n\n\n\n\n\n\n\nShare on"
  },
  {
    "objectID": "codepages/ecommerce/index.html",
    "href": "codepages/ecommerce/index.html",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nsalesdata = pd.read_csv('Ecommerce Customers')\nsalesdata.head()\n\n\n\n\n\n\n\n\nEmail\nAddress\nAvatar\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\n\n0\nmstephenson@fernandez.com\n835 Frank Tunnel\\nWrightmouth, MI 82180-9605\nViolet\n34.497268\n12.655651\n39.577668\n4.082621\n587.951054\n\n\n1\nhduke@hotmail.com\n4547 Archer Common\\nDiazchester, CA 06566-8576\nDarkGreen\n31.926272\n11.109461\n37.268959\n2.664034\n392.204933\n\n\n2\npallen@yahoo.com\n24645 Valerie Unions Suite 582\\nCobbborough, D...\nBisque\n33.000915\n11.330278\n37.110597\n4.104543\n487.547505\n\n\n3\nriverarebecca@gmail.com\n1414 David Throughway\\nPort Jason, OH 22070-1220\nSaddleBrown\n34.305557\n13.717514\n36.721283\n3.120179\n581.852344\n\n\n4\nmstephens@davidson-herman.com\n14023 Rodriguez Passage\\nPort Jacobville, PR 3...\nMediumAquaMarine\n33.330673\n12.795189\n37.536653\n4.446308\n599.406092"
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html",
    "href": "portfolio/dsp/ecommerce/index.html",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "",
    "text": "Notebook"
  },
  {
    "objectID": "codepages/ecommerce/index.html#load-the-data",
    "href": "codepages/ecommerce/index.html#load-the-data",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nsalesdata = pd.read_csv('Ecommerce Customers')\nsalesdata.head()\n\n\n\n\n\n\n\n\nEmail\nAddress\nAvatar\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\n\n0\nmstephenson@fernandez.com\n835 Frank Tunnel\\nWrightmouth, MI 82180-9605\nViolet\n34.497268\n12.655651\n39.577668\n4.082621\n587.951054\n\n\n1\nhduke@hotmail.com\n4547 Archer Common\\nDiazchester, CA 06566-8576\nDarkGreen\n31.926272\n11.109461\n37.268959\n2.664034\n392.204933\n\n\n2\npallen@yahoo.com\n24645 Valerie Unions Suite 582\\nCobbborough, D...\nBisque\n33.000915\n11.330278\n37.110597\n4.104543\n487.547505\n\n\n3\nriverarebecca@gmail.com\n1414 David Throughway\\nPort Jason, OH 22070-1220\nSaddleBrown\n34.305557\n13.717514\n36.721283\n3.120179\n581.852344\n\n\n4\nmstephens@davidson-herman.com\n14023 Rodriguez Passage\\nPort Jacobville, PR 3...\nMediumAquaMarine\n33.330673\n12.795189\n37.536653\n4.446308\n599.406092"
  },
  {
    "objectID": "codepages/ecommerce/index.html#eda",
    "href": "codepages/ecommerce/index.html#eda",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "EDA",
    "text": "EDA\n\nDescriptive Statistics\n\nsalesdata.describe()\n\n\n\n\n\n\n\n\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\n\ncount\n500.000000\n500.000000\n500.000000\n500.000000\n500.000000\n\n\nmean\n33.053194\n12.052488\n37.060445\n3.533462\n499.314038\n\n\nstd\n0.992563\n0.994216\n1.010489\n0.999278\n79.314782\n\n\nmin\n29.532429\n8.508152\n33.913847\n0.269901\n256.670582\n\n\n25%\n32.341822\n11.388153\n36.349257\n2.930450\n445.038277\n\n\n50%\n33.082008\n11.983231\n37.069367\n3.533975\n498.887875\n\n\n75%\n33.711985\n12.753850\n37.716432\n4.126502\n549.313828\n\n\nmax\n36.139662\n15.126994\n40.005182\n6.922689\n765.518462\n\n\n\n\n\n\n\n\nsalesdata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 8 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   Email                 500 non-null    object \n 1   Address               500 non-null    object \n 2   Avatar                500 non-null    object \n 3   Avg. Session Length   500 non-null    float64\n 4   Time on App           500 non-null    float64\n 5   Time on Website       500 non-null    float64\n 6   Length of Membership  500 non-null    float64\n 7   Yearly Amount Spent   500 non-null    float64\ndtypes: float64(5), object(3)\nmemory usage: 31.4+ KB\n\n\n\n\nVisualization\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.9, 5))\n\n# Scatter plot with regression line for 'Time on Website' vs 'Yearly Amount Spent'\nsns.scatterplot(\n    x='Time on Website', y='Yearly Amount Spent', \n    data=salesdata, ax=ax1\n    )\nsns.regplot(\n    x='Time on Website', y='Yearly Amount Spent', \n    data=salesdata, ax=ax1, scatter=False, color='blue'\n    )\nax1.set_title('Time on Website vs Yearly Amount Spent')\n\n# Scatter plot with regression line for 'Time on App' vs 'Yearly Amount Spent'\nsns.scatterplot(\n    x='Time on App', y='Yearly Amount Spent', \n    data=salesdata, ax=ax2\n    )\nsns.regplot(\n    x='Time on App', y='Yearly Amount Spent',\n    data=salesdata, ax=ax2, scatter=False, color='blue'\n    )\nax2.set_title('Time on App vs Yearly Amount Spent')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSo, from this plot, we see that Time on Website has no significant trend or pattern on Yearly Amount Spent variable. However, Time on App seems to have a linear relationship on Yearly Amount Spent.\nNext, we see the relationship between Avg. Session Length vs Yearly Amount Spent, and Length of Membership vs Yearly Amount Spent.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.9, 5))\n\n# Scatter plot with regression line for 'Time on Website' vs 'Yearly Amount Spent'\nsns.scatterplot(\n    x='Avg. Session Length', y='Yearly Amount Spent', \n    data=salesdata, ax=ax1\n    )\nsns.regplot(\n    x='Avg. Session Length', y='Yearly Amount Spent', \n    data=salesdata, ax=ax1, scatter=False, color='blue'\n    )\nax1.set_title('Avg. Session Length vs Yearly Amount Spent')\n\n# Scatter plot with regression line for 'Time on App' vs 'Yearly Amount Spent'\nsns.scatterplot(\n    x='Length of Membership', y='Yearly Amount Spent', \n    data=salesdata, ax=ax2\n    )\nsns.regplot(\n    x='Length of Membership', y='Yearly Amount Spent', \n    data=salesdata, ax=ax2, scatter=False, color='blue'\n    )\nax2.set_title('Length of Membership vs Yearly Amount Spent')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBoth of these features have impact on the dependent variable. However, Length of Membership seems to have the most significant impact on Yearly Amount Spent.\n\nsns.pairplot(salesdata)"
  },
  {
    "objectID": "codepages/ecommerce/index.html#modeling",
    "href": "codepages/ecommerce/index.html#modeling",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Modeling",
    "text": "Modeling\n\nTraining\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nX = salesdata[\n    ['Avg. Session Length', 'Time on App',\n    'Time on Website', 'Length of Membership']\n    ]\ny = salesdata['Yearly Amount Spent']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size = 0.30, random_state=123\n)\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\nprint('Coefficients: \\n', linreg.coef_)\n\nCoefficients: \n [25.36266491 38.82367921  0.80356799 61.54905291]\n\n\n\n\nTesting\n\npred = linreg.predict(X_test)\nplt.scatter(y_test, pred)\nplt.xlabel('y test')\nplt.ylabel('predicted y')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nModel Evaluation\n\nfrom sklearn import metrics\n\nprint('MAE', metrics.mean_absolute_error(y_test, pred))\nprint('MSE', metrics.mean_squared_error(y_test, pred))\nprint('RMSE', metrics.root_mean_squared_error(y_test, pred))\nprint('R-squared:', metrics.r2_score(y_test, pred))\n\nMAE 7.9880791942450875\nMSE 102.72313941865983\nRMSE 10.135242444986693\nR-squared: 0.9845789607829496\n\n\n\n\nResidual Analysis\n\nsns.displot(y_test-pred, bins= 60, kde=True)"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html",
    "href": "portfolio/dsp/medicalcost/index.html",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "Notebook GitHub WebApp"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#project-overview",
    "href": "portfolio/dsp/medicalcost/index.html#project-overview",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Project Overview",
    "text": "Project Overview\n\nThis predictive modeling project involves personal medical data to predict the medical insurance charge by using a linear regression model."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#dataset",
    "href": "portfolio/dsp/medicalcost/index.html#dataset",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Dataset",
    "text": "Dataset\nThe dataset used in this project is collected from Kaggle\nColumns\nage: age of primary beneficiary\nsex: insurance contractor gender, female, male\nbmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight \\(\\frac{kg}{m^ 2}\\) using the ratio of height to weight, ideally \\(18.5\\) to \\(24.9\\)\nchildren: Number of children covered by health insurance / Number of dependents\nsmoker: Smoking\nregion: the beneficiary’s residential area in the US, northeast, southeast, southwest, northwest.\ncharges: Individual medical costs billed by health insurance\nAcknowledgements\nThe dataset is available on GitHub here."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#stakeholders",
    "href": "portfolio/dsp/medicalcost/index.html#stakeholders",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Stakeholders",
    "text": "Stakeholders\nCan we accurately predict insurance costs?"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#key-performance-indicators-kpis",
    "href": "portfolio/dsp/medicalcost/index.html#key-performance-indicators-kpis",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Key Performance Indicators (KPIs)",
    "text": "Key Performance Indicators (KPIs)\n\nAll the features were considered for the modeling purposes. However, from the exploratory data analysis and mathematical analysis, it was found that the charges usually goes up for the factors such as increase in age, living in certain region, having certain number of children. But this is not always the same depending on the smoker variable. Also, there is a strong correlation between age and bmi variable. Age a result new features such as age_bmi and age_bmi_smoker features were created to see how the charges interact."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#modeling",
    "href": "portfolio/dsp/medicalcost/index.html#modeling",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Modeling",
    "text": "Modeling\n\nModeling Approaches\nWe consider the following models\n\nBaseline model: Assumption that the charges variable can be modeled with the mean value of this charges variable.\n\\[\n\\text{charges}=\\mathbb{E}[\\text{charges}]+\\xi\n\\]\nLinear Regression with age-bmi-smoke interaction\n\\[\n\\text{charges}=\\beta_0+\\beta_1 (\\text{age\\_bmi})+\\beta_2 (\\text{male})+\\beta_3 (\\text{smoke})+\\beta_4 (\\text{children})+\\beta_5 (\\text{region})+\\beta_6 (\\text{age-bmi-smoke})+\\xi\n\\]\nK-Neighbor Regression\n\\(k\\)NN using all the original feature with \\(k=10\\)\n\n\n\nFinal Model\nFinally the modeling was done based on the lowest MSE value found from the 5-fold cross validation and the model has the following form\n\\[\\begin{align*}\n\\text{charges} &=10621.25+ 3346.14\\times \\text{Age\\_BMI}+4570.76\\times \\text{Male}+ 479.61\\times \\text{Smoke}-315.12\\times \\text{Children}\\\\\n&+13274.48\\times \\text{Region}-212.22\\times \\text{Age\\_BMI\\_Smoke}\n\\end{align*}\\]"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#results-and-outcomes",
    "href": "portfolio/dsp/medicalcost/index.html#results-and-outcomes",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Results and Outcomes",
    "text": "Results and Outcomes\n\nModel Accuracy\nThe model above returns an RMSE of \\(5853.0\\) on the training set and an RMSE of \\(5600.0\\) on the test set with an \\(R^2=80\\%\\).\n\n\nWeb Application\nThe final model was developed and deployed using Streamlit. To try a single instance, fill out the following form and then click predict charges."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#future-directions",
    "href": "portfolio/dsp/medicalcost/index.html#future-directions",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Future Directions",
    "text": "Future Directions\nFuture project on the same data could be adding a neural network and compare the relative performances of the two models.\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet"
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#project-overview",
    "href": "portfolio/dsp/ecommerce/index.html#project-overview",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Project Overview",
    "text": "Project Overview\n\nThis is a preliminary level linear regression based machine learning project to investigate the feature importance for an e-commerce based company or simply building a predictive model to generate insights on different features."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#dataset",
    "href": "portfolio/dsp/ecommerce/index.html#dataset",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Dataset",
    "text": "Dataset\nThe data is collected from kaggle.com. It contains 500 observations with the following columns\n\nEmail: Email address of the customers\nAddress: Physical mailing address of the customers\n\nAvatar: The fancy avater of the customers\n\nAvg. Session Length: Average session lenght spent either on app or web\n\nLength of Membership: Length of the membership of the customers with the e-commerce company\n\nTime on App: Time spent on the mobile app\n\nTime on Website: Time spent on web based browser\n\nYearly Amount Spent: This is the dependent variable."
  },
  {
    "objectID": "codepages/ecommerce/index.html#conclusion",
    "href": "codepages/ecommerce/index.html#conclusion",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Conclusion",
    "text": "Conclusion\n\ncoeff = pd.DataFrame({\n    'Feature': ['Intercept'] + list(X.columns), \n    'Coefficient': [linreg.intercept_] + list(linreg.coef_) \n})\n\ncoeff\n\n\n\n\n\n\n\n\nFeature\nCoefficient\n\n\n\n\n0\nIntercept\n-1054.215476\n\n\n1\nAvg. Session Length\n25.362665\n\n\n2\nTime on App\n38.823679\n\n\n3\nTime on Website\n0.803568\n\n\n4\nLength of Membership\n61.549053"
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#stakeholders",
    "href": "portfolio/dsp/ecommerce/index.html#stakeholders",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Stakeholders",
    "text": "Stakeholders\nIf the company wants to decide whether to focus their efforts on the mobile app or the website."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#key-performance-indicators-kpis",
    "href": "portfolio/dsp/ecommerce/index.html#key-performance-indicators-kpis",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Key Performance Indicators (KPIs)",
    "text": "Key Performance Indicators (KPIs)\n\nAll the quantitative features were considered to find their importance on the Yearly Amount Spent variable. However, it was found that Length of Membership, Time on App, Avg. Session Length have the highest impact on the dependent variable in decreasing order."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#modeling",
    "href": "portfolio/dsp/ecommerce/index.html#modeling",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Modeling",
    "text": "Modeling\n\\[\\begin{align*}\n\\text{Yearly Amount Spent}&=-1054.215476+25.362665\\times (\\text{Avg. Session Length})\\\\\n& +38.823679\\times (\\text{Time on App})+0.803568\\times (\\text{Time on Website})\\\\\n& + 61.549053\\times (\\text{Length of Membership})\n\\end{align*}\\]"
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#results-and-outcome",
    "href": "portfolio/dsp/ecommerce/index.html#results-and-outcome",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Results and Outcome",
    "text": "Results and Outcome\n\nModel Explanation\nBased on the model above, we can sumerize as follows\n\nIf everything else remain unchanged, a 1 unit increase in Avg. Session Length is associated with an increase of \\(25.36\\) in total Yearly Amount Spent\n\nIf everything else remain unchanged, a 1 unit increase in Time on App is associated with an increase of \\(38.82\\) in total Yearly Amount Spent\n\nIf everything else remain unchanged, a 1 unit increase in Time on Website is associated with an increase of \\(0.80\\) in total Yearly Amount Spent\n\nIf everything else remain unchanged, a 1 unit increase in Length of Membership is associated with an increase of \\(61.55\\) in total Yearly Amount Spent\n\nNow the key question, should the company focus more on Time on App more?\n\nThe answer to the question above is a little bit tricky. Based on the modeling approach, appearantly it may seems that time on app has more impact than the time on web. However, the most significant factor seems the Length of Memberhsip. So we need further analysis of this two features to properly answer if the company should focus more on app.\n\n\n\nModel Accuracy\nThe model above returns a MAE of 7.99, MSE of 102.72, RMSE of 10.14, and \\(R^2=98.46\\%\\)"
  },
  {
    "objectID": "codepages/titanic/index.html",
    "href": "codepages/titanic/index.html",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\n\ntitanic = pd.read_csv('titanic_train.csv')"
  },
  {
    "objectID": "portfolio/dsp/titanic/index.html",
    "href": "portfolio/dsp/titanic/index.html",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "",
    "text": "Notebook\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Classification {Probelm:} {Predict} the Chance of Survival of\n    a Voager on {Titanic} Based on the Voager’s Information},\n  date = {2021-10-15},\n  url = {https://mrislambd.github.io/portfolio/dsp/titanic/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Classification Probelm: Predict the Chance of\nSurvival of a Voager on Titanic Based on the Voager’s\nInformation.” October 15, 2021. https://mrislambd.github.io/portfolio/dsp/titanic/."
  },
  {
    "objectID": "codepages/titanic/index.html#the-data",
    "href": "codepages/titanic/index.html#the-data",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\n\ntitanic = pd.read_csv('titanic_train.csv')"
  },
  {
    "objectID": "codepages/titanic/index.html#exploratory-data-analysis",
    "href": "codepages/titanic/index.html#exploratory-data-analysis",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nDescriptive Statistics\n\ntitanic.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ntitanic.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nSeems like there are some missing data for the Age, Cabin, and Emberked features. To see with visualization\n\nsns.heatmap(titanic.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n\n\n\n\n\n\n\n\nApproximately \\(20\\%\\) of the Age variable is missing. For the feature Cabin, it’s too many observations missing. For the Emberked, there are only two missing observations. So, we need to take extra care of these features in the data cleaning and preparation stage.\n\n\n\nData Visualization\n\nsns.countplot(x='Survived', hue='Sex', data= titanic, palette='RdBu_r')\n\n\n\n\n\n\n\n\nLooks like maximum of the passenger who didn’t survived are male.\n\nsns.countplot(x='Survived', hue='Pclass', data=titanic, palette='rainbow')\n\n\n\n\n\n\n\n\nFrom this plot we see that people from class 3 has the highest proportion who didn’t survive. In the survival class, passenger class 1 has the highest proportion.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.9, 4))\ntitanic['Age'].hist(bins=35, color='darkred', alpha=0.6, ax=ax1)\nax1.set_xlabel('Age')\nax1.set_title('Age Distribution')\ntitanic['Fare'].hist(bins=30, color='darkred', alpha=0.6, ax=ax2)\nax2.set_xlabel('Fare')\nax2.set_title('Fare Distribution')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSeems like Age is almost normally distributed. However, the the Fare is positively skewed. Other categorical features\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.9, 4))\n\n\nsns.countplot(\n    x='SibSp',data=titanic, ax=ax1\n    )\nax1.set_title('Number of Siblings/Spouse')\n\n\nsns.countplot(\n    x='Parch', data=titanic, ax=ax2\n    )\nax2.set_title('Number of Parents/Children')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "codepages/titanic/index.html#data-cleaning-and-preparation",
    "href": "codepages/titanic/index.html#data-cleaning-and-preparation",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "Data Cleaning and Preparation",
    "text": "Data Cleaning and Preparation\n\nHandling Missing Data\n\nHere, the Age feature is a continuous feature and almost normally distributed. So we can impute this by the mean of the Age variable. However, this feature can be classified by other categorical features such as Sex, Pclass, SibSp, or Perch. But we can be smarter by taking consideration of greater and homogeneously diversified categorical feature. In this case, Pclass is the perfect one.\n\n\nsns.boxplot(\n    x='Pclass', y='Age', hue='Pclass',\n    data=titanic, palette='winter'\n    )\n\n\n\n\n\n\n\n\nSo, whenever a passenger is in the 1st class, the mean Age is around 37 and for the 2nd class and 3rd class the mean Age are 29 and 24, respectively.\n\ndef age_imputation(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    if pd.isnull(Age):\n        if Pclass==1:\n            return 37\n        elif Pclass==2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age\ntitanic.Age = titanic[['Age','Pclass']].apply(age_imputation, axis=1)\nsns.heatmap(titanic.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n\n\n\n\n\n\n\nSince there are too many missing in Cabin, so we can drop it along with two missing values from the Emberked feature.\n\ntitanic.drop('Cabin', axis=1, inplace=True)\ntitanic.dropna(inplace=True)\nsns.heatmap(titanic.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n\n\n\n\n\n\n\nSo there is no missing value in any column. Next we convert the categorical features\n\n\nConverting the Categorical Features\n\ntitanic['Male'] = pd.get_dummies(titanic.Sex,dtype=int)['male']\nemb = pd.get_dummies(titanic['Embarked'],drop_first=True, dtype=int)\ntitanic = pd.concat([titanic, emb], axis=1)\ntitanic.drop(['Sex','Embarked','Name','Ticket'], axis = 1, inplace=True)\ntitanic.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nMale\nQ\nS\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n1\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n1\n0\n1"
  },
  {
    "objectID": "codepages/medicalcost/index.html",
    "href": "codepages/medicalcost/index.html",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "import pandas as pd\n\ninsurance = pd.read_csv('insurance.csv')\n\ninsurance.sample(5, random_state=111)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n1000\n30\nmale\n22.99\n2\nyes\nnorthwest\n17361.7661\n\n\n53\n36\nmale\n34.43\n0\nyes\nsoutheast\n37742.5757\n\n\n432\n42\nmale\n26.90\n0\nno\nsouthwest\n5969.7230\n\n\n162\n54\nmale\n39.60\n1\nno\nsouthwest\n10450.5520\n\n\n1020\n51\nmale\n37.00\n0\nno\nsouthwest\n8798.5930\n\n\n\n\n\n\n\n\n\n\n\n\n\ninsurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\nNo missing data. Total 1338 observations.\n\n\n\nStatistical properties of the non-categorical variables\n\nprint(insurance.describe())\n\n               age          bmi     children       charges\ncount  1338.000000  1338.000000  1338.000000   1338.000000\nmean     39.207025    30.663397     1.094918  13270.422265\nstd      14.049960     6.098187     1.205493  12110.011237\nmin      18.000000    15.960000     0.000000   1121.873900\n25%      27.000000    26.296250     0.000000   4740.287150\n50%      39.000000    30.400000     1.000000   9382.033000\n75%      51.000000    34.693750     2.000000  16639.912515\nmax      64.000000    53.130000     5.000000  63770.428010\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2,3,figsize = (15,9))\n\nsns.histplot(insurance['age'], color='red', kde=True, ax= axes[0, 0]).set_title('Age Distribution')\n\nsns.histplot(insurance['bmi'], color='green', kde=True, ax= axes[0,1]).set_title('BMI Distribution')\n\nsns.histplot(insurance['charges'],color='blue', kde=True, ax= axes[0,2]).set_title('Charge Distribution')\n\nsns.countplot(x='smoker', data=insurance, hue='sex', palette='Set2', ax=axes[1,0]).set_title('Smoker vs Gender')\n\nsns.countplot(x=insurance['region'], hue=insurance['region'], palette='Set1', ax=axes[1,1]).set_title('Region Distribution')\n\nsns.countplot(x=insurance['children'], hue=insurance['children'],legend=False,palette='Set2', ax=axes[1,2]).set_title('Children Distribution')\n\nplt.gcf().patch.set_facecolor('#f4f4f4')\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1,2, figsize=(9.5,4))\ng=sns.stripplot(data=insurance, x='smoker', y='charges', hue='smoker' ,palette=['blue', 'orange'], legend=True, ax=axes[0])\ng.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].scatter(insurance.loc[insurance.smoker=='yes'].bmi,\n            insurance.loc[insurance.smoker=='yes'].charges, label=\"yes\", marker='o',\n            s=60,edgecolors='black', c='orange'\n            )\naxes[1].set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].scatter(insurance.loc[insurance.smoker=='no'].bmi,\n            insurance.loc[insurance.smoker=='no'].charges, label=\"no\", marker='v',\n            s=60,edgecolors='black', c='lightblue'\n            )\naxes[1].set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].set_xlabel('bmi')\naxes[1].set_ylabel('charges')\naxes[1].legend()\n\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nClearly from the plots above we can see that the somoking status has effect on the insurance charges in relation with bmi\n\nfig, axes = plt.subplots(1,2,figsize=(9.5,4))\n\ng1=sns.stripplot(x='region', y='charges', data=insurance, ax=axes[0])\ng1.set_xticklabels(['SW', 'SE', 'NW','NE'])\ng1.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng2=sns.scatterplot(x='age', y='charges', data=insurance, hue='smoker' ,ax=axes[1])\ng2.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\n\ng1=sns.stripplot(x='children', y='charges',data=insurance,hue='children',palette='Set1', ax=axes[0])\ng1.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng1.set_facecolor('#f4f4f4')\ng2=sns.boxplot(x='sex', y='charges', data=insurance, hue='sex', palette='Set2', ax=axes[1])\ng2.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng2.set_facecolor('#f4f4f4')\n\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nTo see the combined effect of all the features\n\nplt.figure(figsize=(12,6))\ng = sns.FacetGrid(insurance, col='smoker', row='sex',hue='region', margin_titles=True, height=2.4, aspect=1.5)\ng.map(sns.scatterplot, 'age','charges')\n\ng.fig.patch.set_facecolor('#f4f4f4')\ng.add_legend()\nplt.show()\n\n&lt;Figure size 1152x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above plots, we can see that age feature stacks in three layers for charges. It maybe depending on other categorical features such as smoking status.\n\n\n\n\n\n\n\ncorr_matrix = insurance[['age','bmi','charges']].corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Matrix')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats as st\nanova_sex, p_value1 = st.f_oneway(\n    insurance[insurance['sex']=='male']['charges'],\n    insurance[insurance['sex']=='female']['charges']\n)\n\nanova_smoker, p_value2 = st.f_oneway(\n    insurance[insurance['smoker']=='yes']['charges'],\n    insurance[insurance['smoker']=='no']['charges']\n)\n\nanova_region, p_value3 = st.f_oneway(\n    insurance[insurance['region']=='southwest']['charges'],\n    insurance[insurance['region']=='southeast']['charges'],\n    insurance[insurance['region']=='northwest']['charges'],\n    insurance[insurance['region']=='northeast']['charges']\n)\n\nanova_children, p_value4 = st.f_oneway(\n    insurance[insurance['children']==0]['charges'],\n    insurance[insurance['children']==1]['charges'],\n    insurance[insurance['children']==2]['charges'],\n    insurance[insurance['children']==3]['charges'],\n    insurance[insurance['children']==4]['charges'],\n    insurance[insurance['children']==5]['charges']\n)\n\nanova_results = {\n    'feature_name': ['sex', 'smoker', 'region', 'children'],\n    'F-Statistic':[anova_sex, anova_smoker,anova_region,anova_children],\n    'p-value':[p_value1, p_value2, p_value3, p_value4]\n}\n\nanova = pd.DataFrame(anova_results)\nprint(anova)\n\n  feature_name  F-Statistic        p-value\n0          sex     4.399702   3.613272e-02\n1       smoker  2177.614868  8.271436e-283\n2       region     2.969627   3.089336e-02\n3     children     3.296920   5.785681e-03\n\n\n\n\n\nBoth age and bmi features are positively correlated to charges with correlation coefficients \\(0.3\\) and \\(0.2\\), respectively. Since the \\(p\\)-values are less thatn \\(0.05\\), therefore, all the categorical features have impact on the target features.\n\n\n\n\n\n\n\n\n# Binary Encoding for the variables with two categories\nfrom sklearn.preprocessing import LabelEncoder\n\ninsurance['male'] = pd.get_dummies(insurance.sex, dtype=int)['male']\ninsurance['smoke'] = pd.get_dummies(insurance.smoker, dtype=int)['yes']\ninsurance.drop(['sex','smoker'],axis=1, inplace=True)\n\nlabel_encoder = LabelEncoder()\ninsurance['region']=label_encoder.fit_transform(insurance['region'])\n\nnew_order = ['age', 'bmi', 'male', 'smoke','children','region', 'charges']\ninsurance = insurance[new_order]\ninsurance['charges'] = insurance['charges'].round(2)\ninsurance.sample(5)\n\n\n\n\n\n\n\n\nage\nbmi\nmale\nsmoke\nchildren\nregion\ncharges\n\n\n\n\n993\n38\n28.27\n1\n0\n1\n2\n5484.47\n\n\n1288\n20\n39.40\n1\n1\n2\n3\n38344.57\n\n\n1105\n54\n31.24\n0\n0\n0\n2\n10338.93\n\n\n636\n19\n24.51\n0\n0\n1\n1\n2709.11\n\n\n117\n29\n27.94\n0\n1\n1\n2\n19107.78\n\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = insurance.drop('charges', axis=1)\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values,i) for i in range(len(X.columns))]\nprint(vif_data)\n\n    feature        VIF\n0       age   7.551348\n1       bmi  10.371829\n2      male   2.001061\n3     smoke   1.256837\n4  children   1.801245\n5    region   2.924528\n\n\nSince BMI and Age have higher values for the multicolinearity, therefore we adopt the following methods\n\n\n\n\n\n\nplt.scatter(insurance.age,insurance.bmi)\nplt.xlabel('AGE')\nplt.ylabel('BMI')\nplt.title('BMI vs AGE')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nSince there is no clear linear relationship or any pattern, the Multicollinearity issue can be ignored. However, older individuals with a certain BMI range might have different risks or costs associated with their health. We could explore interaction terms like age * bmi in our model to capture any potential synergistic effects.\n\n\ninsurance.insert(6,'age_bmi',insurance.age*insurance.bmi)\ninsurance.insert(7,'age_bmi_smoke',insurance.age_bmi*insurance.smoke)\ninsurance.sample(5,random_state=111)\n\n\n\n\n\n\n\n\nage\nbmi\nmale\nsmoke\nchildren\nregion\nage_bmi\nage_bmi_smoke\ncharges\n\n\n\n\n1000\n30\n22.99\n1\n1\n2\n1\n689.70\n689.70\n17361.77\n\n\n53\n36\n34.43\n1\n1\n0\n2\n1239.48\n1239.48\n37742.58\n\n\n432\n42\n26.90\n1\n0\n0\n3\n1129.80\n0.00\n5969.72\n\n\n162\n54\n39.60\n1\n0\n1\n3\n2138.40\n0.00\n10450.55\n\n\n1020\n51\n37.00\n1\n0\n0\n3\n1887.00\n0.00\n8798.59\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX = insurance.drop('charges',axis=1)\ny = insurance['charges'].to_frame()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42)\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\nconts_features = ['age','bmi','age_bmi']\ncateg_features = ['male','smoke', 'children','region']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), conts_features)\n    ],\n    remainder= 'passthrough'\n)\nX_train_sc = preprocessor.fit_transform(X_train)\nX_test_sc = preprocessor.fit(X_test)"
  },
  {
    "objectID": "codepages/medicalcost/index.html#data-loading",
    "href": "codepages/medicalcost/index.html#data-loading",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "import pandas as pd\n\ninsurance = pd.read_csv('insurance.csv')\n\ninsurance.sample(5, random_state=111)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n1000\n30\nmale\n22.99\n2\nyes\nnorthwest\n17361.7661\n\n\n53\n36\nmale\n34.43\n0\nyes\nsoutheast\n37742.5757\n\n\n432\n42\nmale\n26.90\n0\nno\nsouthwest\n5969.7230\n\n\n162\n54\nmale\n39.60\n1\nno\nsouthwest\n10450.5520\n\n\n1020\n51\nmale\n37.00\n0\nno\nsouthwest\n8798.5930"
  },
  {
    "objectID": "codepages/medicalcost/index.html#exploratory-data-analysis-eda",
    "href": "codepages/medicalcost/index.html#exploratory-data-analysis-eda",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "insurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\nNo missing data. Total 1338 observations.\n\n\n\nStatistical properties of the non-categorical variables\n\nprint(insurance.describe())\n\n               age          bmi     children       charges\ncount  1338.000000  1338.000000  1338.000000   1338.000000\nmean     39.207025    30.663397     1.094918  13270.422265\nstd      14.049960     6.098187     1.205493  12110.011237\nmin      18.000000    15.960000     0.000000   1121.873900\n25%      27.000000    26.296250     0.000000   4740.287150\n50%      39.000000    30.400000     1.000000   9382.033000\n75%      51.000000    34.693750     2.000000  16639.912515\nmax      64.000000    53.130000     5.000000  63770.428010\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2,3,figsize = (15,9))\n\nsns.histplot(insurance['age'], color='red', kde=True, ax= axes[0, 0]).set_title('Age Distribution')\n\nsns.histplot(insurance['bmi'], color='green', kde=True, ax= axes[0,1]).set_title('BMI Distribution')\n\nsns.histplot(insurance['charges'],color='blue', kde=True, ax= axes[0,2]).set_title('Charge Distribution')\n\nsns.countplot(x='smoker', data=insurance, hue='sex', palette='Set2', ax=axes[1,0]).set_title('Smoker vs Gender')\n\nsns.countplot(x=insurance['region'], hue=insurance['region'], palette='Set1', ax=axes[1,1]).set_title('Region Distribution')\n\nsns.countplot(x=insurance['children'], hue=insurance['children'],legend=False,palette='Set2', ax=axes[1,2]).set_title('Children Distribution')\n\nplt.gcf().patch.set_facecolor('#f4f4f4')\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1,2, figsize=(9.5,4))\ng=sns.stripplot(data=insurance, x='smoker', y='charges', hue='smoker' ,palette=['blue', 'orange'], legend=True, ax=axes[0])\ng.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].scatter(insurance.loc[insurance.smoker=='yes'].bmi,\n            insurance.loc[insurance.smoker=='yes'].charges, label=\"yes\", marker='o',\n            s=60,edgecolors='black', c='orange'\n            )\naxes[1].set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].scatter(insurance.loc[insurance.smoker=='no'].bmi,\n            insurance.loc[insurance.smoker=='no'].charges, label=\"no\", marker='v',\n            s=60,edgecolors='black', c='lightblue'\n            )\naxes[1].set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].set_xlabel('bmi')\naxes[1].set_ylabel('charges')\naxes[1].legend()\n\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nClearly from the plots above we can see that the somoking status has effect on the insurance charges in relation with bmi\n\nfig, axes = plt.subplots(1,2,figsize=(9.5,4))\n\ng1=sns.stripplot(x='region', y='charges', data=insurance, ax=axes[0])\ng1.set_xticklabels(['SW', 'SE', 'NW','NE'])\ng1.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng2=sns.scatterplot(x='age', y='charges', data=insurance, hue='smoker' ,ax=axes[1])\ng2.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\n\ng1=sns.stripplot(x='children', y='charges',data=insurance,hue='children',palette='Set1', ax=axes[0])\ng1.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng1.set_facecolor('#f4f4f4')\ng2=sns.boxplot(x='sex', y='charges', data=insurance, hue='sex', palette='Set2', ax=axes[1])\ng2.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng2.set_facecolor('#f4f4f4')\n\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nTo see the combined effect of all the features\n\nplt.figure(figsize=(12,6))\ng = sns.FacetGrid(insurance, col='smoker', row='sex',hue='region', margin_titles=True, height=2.4, aspect=1.5)\ng.map(sns.scatterplot, 'age','charges')\n\ng.fig.patch.set_facecolor('#f4f4f4')\ng.add_legend()\nplt.show()\n\n&lt;Figure size 1152x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above plots, we can see that age feature stacks in three layers for charges. It maybe depending on other categorical features such as smoking status.\n\n\n\n\n\n\n\ncorr_matrix = insurance[['age','bmi','charges']].corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Matrix')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats as st\nanova_sex, p_value1 = st.f_oneway(\n    insurance[insurance['sex']=='male']['charges'],\n    insurance[insurance['sex']=='female']['charges']\n)\n\nanova_smoker, p_value2 = st.f_oneway(\n    insurance[insurance['smoker']=='yes']['charges'],\n    insurance[insurance['smoker']=='no']['charges']\n)\n\nanova_region, p_value3 = st.f_oneway(\n    insurance[insurance['region']=='southwest']['charges'],\n    insurance[insurance['region']=='southeast']['charges'],\n    insurance[insurance['region']=='northwest']['charges'],\n    insurance[insurance['region']=='northeast']['charges']\n)\n\nanova_children, p_value4 = st.f_oneway(\n    insurance[insurance['children']==0]['charges'],\n    insurance[insurance['children']==1]['charges'],\n    insurance[insurance['children']==2]['charges'],\n    insurance[insurance['children']==3]['charges'],\n    insurance[insurance['children']==4]['charges'],\n    insurance[insurance['children']==5]['charges']\n)\n\nanova_results = {\n    'feature_name': ['sex', 'smoker', 'region', 'children'],\n    'F-Statistic':[anova_sex, anova_smoker,anova_region,anova_children],\n    'p-value':[p_value1, p_value2, p_value3, p_value4]\n}\n\nanova = pd.DataFrame(anova_results)\nprint(anova)\n\n  feature_name  F-Statistic        p-value\n0          sex     4.399702   3.613272e-02\n1       smoker  2177.614868  8.271436e-283\n2       region     2.969627   3.089336e-02\n3     children     3.296920   5.785681e-03\n\n\n\n\n\nBoth age and bmi features are positively correlated to charges with correlation coefficients \\(0.3\\) and \\(0.2\\), respectively. Since the \\(p\\)-values are less thatn \\(0.05\\), therefore, all the categorical features have impact on the target features."
  },
  {
    "objectID": "codepages/medicalcost/index.html#pre-processing",
    "href": "codepages/medicalcost/index.html#pre-processing",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "# Binary Encoding for the variables with two categories\nfrom sklearn.preprocessing import LabelEncoder\n\ninsurance['male'] = pd.get_dummies(insurance.sex, dtype=int)['male']\ninsurance['smoke'] = pd.get_dummies(insurance.smoker, dtype=int)['yes']\ninsurance.drop(['sex','smoker'],axis=1, inplace=True)\n\nlabel_encoder = LabelEncoder()\ninsurance['region']=label_encoder.fit_transform(insurance['region'])\n\nnew_order = ['age', 'bmi', 'male', 'smoke','children','region', 'charges']\ninsurance = insurance[new_order]\ninsurance['charges'] = insurance['charges'].round(2)\ninsurance.sample(5)\n\n\n\n\n\n\n\n\nage\nbmi\nmale\nsmoke\nchildren\nregion\ncharges\n\n\n\n\n993\n38\n28.27\n1\n0\n1\n2\n5484.47\n\n\n1288\n20\n39.40\n1\n1\n2\n3\n38344.57\n\n\n1105\n54\n31.24\n0\n0\n0\n2\n10338.93\n\n\n636\n19\n24.51\n0\n0\n1\n1\n2709.11\n\n\n117\n29\n27.94\n0\n1\n1\n2\n19107.78\n\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = insurance.drop('charges', axis=1)\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values,i) for i in range(len(X.columns))]\nprint(vif_data)\n\n    feature        VIF\n0       age   7.551348\n1       bmi  10.371829\n2      male   2.001061\n3     smoke   1.256837\n4  children   1.801245\n5    region   2.924528\n\n\nSince BMI and Age have higher values for the multicolinearity, therefore we adopt the following methods\n\n\n\n\n\n\nplt.scatter(insurance.age,insurance.bmi)\nplt.xlabel('AGE')\nplt.ylabel('BMI')\nplt.title('BMI vs AGE')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nSince there is no clear linear relationship or any pattern, the Multicollinearity issue can be ignored. However, older individuals with a certain BMI range might have different risks or costs associated with their health. We could explore interaction terms like age * bmi in our model to capture any potential synergistic effects.\n\n\ninsurance.insert(6,'age_bmi',insurance.age*insurance.bmi)\ninsurance.insert(7,'age_bmi_smoke',insurance.age_bmi*insurance.smoke)\ninsurance.sample(5,random_state=111)\n\n\n\n\n\n\n\n\nage\nbmi\nmale\nsmoke\nchildren\nregion\nage_bmi\nage_bmi_smoke\ncharges\n\n\n\n\n1000\n30\n22.99\n1\n1\n2\n1\n689.70\n689.70\n17361.77\n\n\n53\n36\n34.43\n1\n1\n0\n2\n1239.48\n1239.48\n37742.58\n\n\n432\n42\n26.90\n1\n0\n0\n3\n1129.80\n0.00\n5969.72\n\n\n162\n54\n39.60\n1\n0\n1\n3\n2138.40\n0.00\n10450.55\n\n\n1020\n51\n37.00\n1\n0\n0\n3\n1887.00\n0.00\n8798.59\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX = insurance.drop('charges',axis=1)\ny = insurance['charges'].to_frame()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42)\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\nconts_features = ['age','bmi','age_bmi']\ncateg_features = ['male','smoke', 'children','region']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), conts_features)\n    ],\n    remainder= 'passthrough'\n)\nX_train_sc = preprocessor.fit_transform(X_train)\nX_test_sc = preprocessor.fit(X_test)"
  },
  {
    "objectID": "codepages/medicalcost/index.html#modelling-approaches",
    "href": "codepages/medicalcost/index.html#modelling-approaches",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Modelling Approaches",
    "text": "Modelling Approaches\nWe consider the following models\n\nBaseline model: Assumption that the charges variable can be modeled with the mean value of this charges variable.\n\\[\n\\text{charges}=\\mathbb{E}[\\text{charges}]+\\xi\n\\]\nLinear Regression with age-bmi-smoke interaction\n\\[\n\\text{charges}=\\beta_0+\\beta_1 (\\text{age\\_bmi})+\\beta_2 (\\text{male})+\\beta_3 (\\text{smoke})+\\beta_4 (\\text{children})+\\beta_5 (\\text{region})+\\beta_6 (\\text{age-bmi-smoke})+\\xi\n\\]\nK-Neighbor Regression\n\\(k\\)NN using all the original feature\n\n\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\n\nkfold = KFold(n_splits=5,shuffle=True, random_state=111)\n\nmses = np.zeros((3,5))\n\nk = 10\n\nfor i, (train_index, test_index) in enumerate(kfold.split(X_train_sc)):\n    X_train_sc_train = X_train_sc[train_index]\n    X_train_sc_holdout = X_train_sc[test_index]\n\n    y_train_train = y_train.iloc[train_index]\n    y_train_holdout = y_train.iloc[test_index]\n\n    pred0 = y_train_train.charges.mean()*np.ones(len(test_index))\n\n    model1 = LinearRegression()\n    model2 = KNeighborsRegressor(k)\n\n    model1.fit(X_train_sc_train[:,2:], y_train_train)\n    model2.fit(X_train_sc_train[:,:6], y_train_train)\n\n    pred1 = model1.predict(X_train_sc_holdout[:,2:])\n    pred2 = model2.predict(X_train_sc_holdout[:,:6])\n\n\n    mses[0,i] = mean_squared_error(y_train_holdout, pred0)\n    mses[1,i] = mean_squared_error(y_train_holdout, pred1)\n    mses[2,i] = mean_squared_error(y_train_holdout, pred2)\n\nplt.scatter(np.zeros(5), mses[0,:],s=60, c='white', edgecolors='black', label='Single Split')\nplt.scatter(np.ones(5), mses[1,:], s=60, c='white', edgecolors='black')\nplt.scatter(2*np.ones(5), mses[1,:], s=60, c='white', edgecolors='black')\nplt.scatter([0,1,2],np.mean(mses, axis=1),s=60, c='r', marker='X', label='Mean')\nplt.legend(loc='upper right', fontsize=12)\nplt.xticks([0,1,2],['Baseline','LinReg','KNN Reg'])\nplt.yticks(fontsize=10)\nplt.ylabel('MSE',fontsize=12)\nplt.gca().set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(np.mean(np.sqrt(mses),axis=1))\nprint('\\n')\nprint('Minimum RMSE={} \\n Model {}'.format(min(np.mean(np.sqrt(mses),axis=1)),np.argmin(np.mean(np.sqrt(mses),axis=1))) )\n\n[12101.66177575  5938.57275531  7120.2471585 ]\n\n\nMinimum RMSE=5938.5727553087745 \n Model 1"
  },
  {
    "objectID": "codepages/medicalcost/index.html#final-model",
    "href": "codepages/medicalcost/index.html#final-model",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Final Model",
    "text": "Final Model\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\ndata = pd.read_csv('insurance.csv')\n\nclass FeatureEngineering(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        # Initialize OneHotEncoder for 'smoker' and 'sex'\n        self.ohe_smoker_sex = OneHotEncoder(\n            drop='first', dtype=int, sparse_output=False)\n        self.label_encoder = LabelEncoder()\n\n    def fit(self, X, y=None):\n        # Fit the OneHotEncoder on smoker and sex\n        self.ohe_smoker_sex.fit(X[['smoker', 'sex']])\n        self.label_encoder.fit(X['region'])\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n\n        # Apply OneHotEncoder to 'smoker' and 'sex'\n        smoker_sex_encoded = self.ohe_smoker_sex.transform(\n            X[['smoker', 'sex']])\n        smoker_sex_columns = ['smoker_yes', 'sex_male']\n\n        # Create DataFrame for encoded variables and merge with original data\n        smoker_sex_df = pd.DataFrame(\n            smoker_sex_encoded, columns=smoker_sex_columns, index=X.index)\n        X = pd.concat([X, smoker_sex_df], axis=1)\n\n        # Label encode the 'region' column\n        X['region'] = self.label_encoder.transform(X['region'])\n\n        # Create new features\n        X['age_bmi'] = X['age'] * X['bmi']\n        X['age_bmi_smoker'] = X['age_bmi'] * X['smoker_yes']\n\n        # Drop original columns\n        X = X.drop(columns=['age', 'bmi', 'smoker', 'sex'])\n\n        return X\n\ndata = pd.read_csv('insurance.csv')\ndata['charges'] = data['charges'].round(2)\n\nX = data.drop('charges', axis=1)\ny = data['charges']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('scale', StandardScaler(), ['age_bmi', 'age_bmi_smoker'])\n    ],\n    remainder='passthrough'\n)\n\npipe = Pipeline(steps=[\n    ('feature_engineering', FeatureEngineering()),\n    ('preprocess', preprocessor),\n    ('model', LinearRegression())\n])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, shuffle=True, random_state=111\n)\npipe.fit(X_train, y_train)\nprint(np.round(pipe['model'].intercept_,2))\n\n10621.25"
  },
  {
    "objectID": "codepages/medicalcost/index.html#model-validation",
    "href": "codepages/medicalcost/index.html#model-validation",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Model Validation",
    "text": "Model Validation\n\nRoot Mean Squared Error (RMSE)\n\ntrain_prediction = pipe.predict(X_train)\ntest_prediction = pipe.predict(X_test)\n\nprint(\"Training set RMSE:\",\n    np.round(np.sqrt(mean_squared_error(train_prediction,y_train)))\n)\nprint(\"Test set RMSE:\",\n    np.round(np.sqrt(mean_squared_error(test_prediction,y_test)))\n)\n\nTraining set RMSE: 5853.0\nTest set RMSE: 5600.0\n\n\n\n\nR-Squared (\\(R^2\\))\n\nfrom sklearn.metrics import r2_score\ny_pred = pipe.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nprint(f'R-squared: {r2:.4f}')\n\nR-squared: 0.8008\n\n\n\n\nResiduals\n\nres = y_test - y_pred\n\nplt.scatter(y_pred, res)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals Plot')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.displot(res,kind='kde')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()"
  },
  {
    "objectID": "codepages/titanic/index.html#modeling",
    "href": "codepages/titanic/index.html#modeling",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nX_train, X_test, y_train, y_test = train_test_split(\n    titanic.drop('Survived', axis=1),\n    titanic.Survived, test_size=0.30,\n    random_state=123\n    )\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\npred = logreg.predict(X_test)\n\n/Users/macpc/Library/CloudStorage/OneDrive-FloridaStateUniversity/OnlineLearning/python_environments/pytorch-env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
  },
  {
    "objectID": "codepages/titanic/index.html#evaluation",
    "href": "codepages/titanic/index.html#evaluation",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "Evaluation",
    "text": "Evaluation\n\nprint(metrics.classification_report(y_test,pred))\n\n              precision    recall  f1-score   support\n\n           0       0.79      0.89      0.84       161\n           1       0.79      0.64      0.71       106\n\n    accuracy                           0.79       267\n   macro avg       0.79      0.76      0.77       267\nweighted avg       0.79      0.79      0.79       267"
  },
  {
    "objectID": "codepages/lendingclub/index.html",
    "href": "codepages/lendingclub/index.html",
    "title": "Lendingclub’s loan default prediction",
    "section": "",
    "text": "Load the data and primary library\n\n\nData\nkaggle.api.authenticate()\nkaggle.api.dataset_download_files(\n    'jeandedieunyandwi/lending-club-dataset', path='.', unzip=True\n)\n\n\nPrimary Libraries\n\nfrom mywebstyle import plot_style\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport kaggle\nplot_style('#f4f4f4')\nloandata = pd.read_csv('lending_club_loan_two.csv')\nfirst_13_cols = loandata.iloc[:, :13]\nfirst_13_cols.head()\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nint_rate\ninstallment\ngrade\nsub_grade\nemp_title\nemp_length\nhome_ownership\nannual_inc\nverification_status\nissue_d\nloan_status\n\n\n\n\n0\n10000\n36 months\n11.44\n329.48\nB\nB4\nMarketing\n10+ years\nRENT\n117000.0\nNot Verified\nJan-15\nFully Paid\n\n\n1\n8000\n36 months\n11.99\n265.68\nB\nB5\nCredit analyst\n4 years\nMORTGAGE\n65000.0\nNot Verified\nJan-15\nFully Paid\n\n\n2\n15600\n36 months\n10.49\n506.97\nB\nB3\nStatistician\n&lt; 1 year\nRENT\n43057.0\nSource Verified\nJan-15\nFully Paid\n\n\n3\n7200\n36 months\n6.49\n220.65\nA\nA2\nClient Advocate\n6 years\nRENT\n54000.0\nNot Verified\nNov-14\nFully Paid\n\n\n4\n24375\n60 months\n17.27\n609.33\nC\nC5\nDestiny Management Inc.\n9 years\nMORTGAGE\n55000.0\nVerified\nApr-13\nCharged Off\n\n\n\n\n\n\n\n\nsecond_13_cols = loandata.iloc[:, 13:]\nsecond_13_cols.head()\n\n\n\n\n\n\n\n\npurpose\ntitle\ndti\nearliest_cr_line\nopen_acc\npub_rec\nrevol_bal\nrevol_util\ntotal_acc\ninitial_list_status\napplication_type\nmort_acc\npub_rec_bankruptcies\naddress\n\n\n\n\n0\nvacation\nVacation\n26.24\nJun-90\n16\n0\n36369\n41.8\n25\nw\nINDIVIDUAL\n0.0\n0.0\n0174 Michelle Gateway\\r\\nMendozaberg, OK 22690\n\n\n1\ndebt_consolidation\nDebt consolidation\n22.05\nJul-04\n17\n0\n20131\n53.3\n27\nf\nINDIVIDUAL\n3.0\n0.0\n1076 Carney Fort Apt. 347\\r\\nLoganmouth, SD 05113\n\n\n2\ncredit_card\nCredit card refinancing\n12.79\nAug-07\n13\n0\n11987\n92.2\n26\nf\nINDIVIDUAL\n0.0\n0.0\n87025 Mark Dale Apt. 269\\r\\nNew Sabrina, WV 05113\n\n\n3\ncredit_card\nCredit card refinancing\n2.60\nSep-06\n6\n0\n5472\n21.5\n13\nf\nINDIVIDUAL\n0.0\n0.0\n823 Reid Ford\\r\\nDelacruzside, MA 00813\n\n\n4\ncredit_card\nCredit Card Refinance\n33.95\nMar-99\n13\n0\n24584\n69.8\n43\nf\nINDIVIDUAL\n1.0\n0.0\n679 Luna Roads\\r\\nGreggshire, VA 11650\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\nDescriptive Statistics\n\nloandata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 395900 entries, 0 to 395899\nData columns (total 27 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   loan_amnt             395900 non-null  int64  \n 1   term                  395900 non-null  object \n 2   int_rate              395900 non-null  float64\n 3   installment           395900 non-null  float64\n 4   grade                 395900 non-null  object \n 5   sub_grade             395900 non-null  object \n 6   emp_title             372982 non-null  object \n 7   emp_length            377608 non-null  object \n 8   home_ownership        395900 non-null  object \n 9   annual_inc            395900 non-null  float64\n 10  verification_status   395900 non-null  object \n 11  issue_d               395900 non-null  object \n 12  loan_status           395900 non-null  object \n 13  purpose               395900 non-null  object \n 14  title                 394145 non-null  object \n 15  dti                   395900 non-null  float64\n 16  earliest_cr_line      395900 non-null  object \n 17  open_acc              395900 non-null  int64  \n 18  pub_rec               395900 non-null  int64  \n 19  revol_bal             395900 non-null  int64  \n 20  revol_util            395624 non-null  float64\n 21  total_acc             395900 non-null  int64  \n 22  initial_list_status   395900 non-null  object \n 23  application_type      395900 non-null  object \n 24  mort_acc              358117 non-null  float64\n 25  pub_rec_bankruptcies  395365 non-null  float64\n 26  address               395900 non-null  object \ndtypes: float64(7), int64(5), object(15)\nmemory usage: 81.6+ MB\n\n\n\nsns.heatmap(loandata.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n\n\n\n\n\n\n\n\nSeems like there are some missing data in the emp_length, emp_title, and mort_acc columns. Around 4.62\\(\\%\\) missing data in emp_length column, 5.79\\(\\%\\) missing data in emp_title column, and 9.54\\(\\%\\) in the mort_acc coulumn. Also, there is very small proportion, 0.07\\(\\%\\) of missing data in the revol_util column. &lt; br &gt;  We need to take care of this missing observations. We may drop the missing values if it is insignificantly missing or doesn’t seem to be very strongly related to the predictive/dependent variable. &lt; /p &gt;\n\nloandata.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nloan_amnt\n395900.0\n14114.249305\n8357.637338\n500.00\n8000.00\n12000.00\n20000.00\n40000.00\n\n\nint_rate\n395900.0\n13.639385\n4.472112\n5.32\n10.49\n13.33\n16.49\n30.99\n\n\ninstallment\n395900.0\n431.859947\n250.733444\n16.08\n250.33\n375.43\n567.30\n1533.81\n\n\nannual_inc\n395900.0\n74206.819251\n61645.032777\n0.00\n45000.00\n64000.00\n90000.00\n8706582.00\n\n\ndti\n395900.0\n17.379187\n18.021550\n0.00\n11.28\n16.91\n22.98\n9999.00\n\n\nopen_acc\n395900.0\n11.311081\n5.137591\n0.00\n8.00\n10.00\n14.00\n90.00\n\n\npub_rec\n395900.0\n0.178204\n0.530716\n0.00\n0.00\n0.00\n0.00\n86.00\n\n\nrevol_bal\n395900.0\n15844.331435\n20589.846553\n0.00\n6026.00\n11181.00\n19620.00\n1743266.00\n\n\nrevol_util\n395624.0\n53.793449\n24.452575\n0.00\n35.80\n54.80\n72.90\n892.30\n\n\ntotal_acc\n395900.0\n25.414622\n11.887279\n2.00\n17.00\n24.00\n32.00\n151.00\n\n\nmort_acc\n358117.0\n1.814091\n2.148006\n0.00\n0.00\n1.00\n3.00\n34.00\n\n\npub_rec_bankruptcies\n395365.0\n0.121647\n0.356176\n0.00\n0.00\n0.00\n0.00\n8.00\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\nLoan Status\nFirst, let’s look at the class balance in the dependent variable\n\nsns.countplot(x='loan_status', data=loandata)\nfp = np.round(\n    len(loandata[loandata['loan_status'] == 'Fully Paid'])/len(loandata)*100, 2\n)\n\nco = np.round(\n    len(loandata[loandata['loan_status'] == 'Charged Off']) /\n    len(loandata)*100, 2\n)\n\n\n\n\n\n\n\n\nSo, 80.39\\(\\%\\) are Fully Paid category where as 19.61\\(\\%\\) is labeled as Charged Off or defaulter.\n\nInsights:  With 80.39 % labeled as “Fully Paid” and 19.61 % as “Charged Off,” this dataset qualifies as moderately imbalanced. The minority class (“Charged Off”) is well underrepresented, but not severely so. This level of imbalance can still impact model performance, especially if the model is biased towards predicting the majority class . Techniques like resampling(oversampling the minority class or undersampling the majority), using metrics like F1-score or AUC-ROC, or even experimenting with algorithms specifically designed for imbalanced datasets(such as XGBoost or balanced random forests) could be effective ways to address this imbalance in the predictive modeling process. &lt; /p &gt;\n\n\nLoan Amount and Installment Interaction With Loan Status\nNext, let’s see how the features interacts with the dependent variable\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nloandata[loandata['loan_status'] == 'Charged Off']['loan_amnt'].hist(\n    alpha=0.5, color='red', bins=30, ax=ax1,\n    label='Charged Off'\n)\nloandata[loandata['loan_status'] == 'Fully Paid']['loan_amnt'].hist(\n    alpha=0.5, color='blue', bins=30, ax=ax1,\n    label='Fully Paid'\n)\nax1.set_title('Loan Amount Distribution')\nax1.set_xlabel('Loan Amount')\nax1.legend()\n\nax2 = fig.add_subplot(122)\nloandata[loandata['loan_status'] == 'Charged Off']['installment'].hist(\n    alpha=0.5, color='red', bins=30, ax=ax2,\n    label='Charged Off'\n)\nloandata[loandata['loan_status'] == 'Fully Paid']['installment'].hist(\n    alpha=0.5, color='blue', bins=30, ax=ax2,\n    label='Fully Paid'\n)\nax2.set_title('Installment Distribution')\nax2.set_xlabel('Installment')\nax2.legend()\n\n\n\n\n\n\n\n\nBoth the loan_amnt and installment are slightly positively skewed. How about their mean, upper quartile, lower quartile based on loan_status?\n\nfig = plt.figure(figsize=(8.8, 4))\nax1 = fig.add_subplot(121)\nsns.boxplot(\n    x='loan_status', y='loan_amnt', hue='loan_status',\n    data=loandata, ax=ax1, palette='winter'\n)\nax1.set_title('Loan Amount Boxplot')\n\nax2 = fig.add_subplot(122)\nsns.boxplot(\n    x='loan_status', y='installment', hue='loan_status',\n    data=loandata, ax=ax2, palette='winter'\n)\nax2.set_title('Installment Boxplot')\n\nText(0.5, 1.0, 'Installment Boxplot')\n\n\n\n\n\n\n\n\n\nInsights:  From the above plot we see that mean loan amount of the fully paid and charged off categories are \\(\\$12, 500\\) and \\(\\$14, 000\\), respectively.\n\n\nTerm, Grade, and Sub-grade Interaction With Loan Status\n\nfig = plt.figure(figsize=(8.8, 4))\nax1 = fig.add_subplot(121)\nsns.countplot(\n    x='loan_status',\n    hue='term', data=loandata,\n    palette='RdBu_r', ax=ax1\n)\nax2 = fig.add_subplot(122)\nsns.countplot(\n    x='loan_status',\n    hue='grade', data=loandata,\n    palette='winter', ax=ax2\n)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.histplot(x='sub_grade', hue='loan_status', data=loandata, ax=ax)\n\n\n\n\n\n\n\n\n\n\nEmployment Title and Employment Length\n\nloandata['emp_title'] = loandata['emp_title'].str.lower()\nloandata.emp_title.value_counts()[:25]\n\nemp_title\nmanager                     5635\nteacher                     5426\nregistered nurse            2626\nsupervisor                  2589\nsales                       2381\ndriver                      2306\nowner                       2200\nrn                          2072\nproject manager             1776\noffice manager              1638\ngeneral manager             1460\ntruck driver                1288\ndirector                    1192\nengineer                    1187\npolice officer              1041\nvice president               961\nsales manager                961\noperations manager           960\nstore manager                941\npresident                    877\nadministrative assistant     865\naccountant                   845\naccount manager              845\ntechnician                   839\nmechanic                     753\nName: count, dtype: int64\n\n\nLet’s work with the employment length column\n\nloandata['emp_length'] = loandata['emp_length'].replace({\n    '&lt; 1 year': 0,\n    '1 year': 1,\n    '2 years': 2,\n    '3 years': 3,\n    '4 years': 4,\n    '5 years': 5,\n    '6 years': 6,\n    '7 years': 7,\n    '8 years': 8,\n    '9 years': 9,\n    '10+ years': 10\n}\n).infer_objects(copy=False)\n\nloandata['emp_length_group'] = pd.cut(\n    loandata['emp_length'],\n    bins=[-1, 2, 7, 10],  # Bins: &lt;3 years, 3-7 years, &gt; 7 years\n    labels=['Short-term', 'Mid-term', 'Long-term']\n)\n\nsns.countplot(\n    x='emp_length_group',\n    hue='loan_status',\n    data=loandata,\n    palette='winter',\n    stat='count'\n)\n\n/var/folders/53/8y5n2fl55p3g3r5_pk9yfwk40000gn/T/ipykernel_34790/3832649202.py:1: FutureWarning:\n\nDowncasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n\n\n\n\n\n\n\n\n\n\n\nInsights:  So, from this plot we can see a trend. As employment length goes up, the chances of fully paid gets higher, but the charged of situation remains almost same, irrespective of the employment length. Therefore, it may not be very useful when we may think of data imputation for the missing values. &lt; /p &gt;\n\n\nHome Ownership and Annual Income\n\nfig = plt.figure(figsize=(8, 10))\nax1 = fig.add_subplot(211)\nsns.boxplot(\n    x='loan_status', y='annual_inc',\n    hue='loan_status', palette='winter',\n    data=loandata, ax=ax1\n)\nax1.set_title('Income Distribution')\nax1.set_xlabel('Loan Status')\nax1.set_ylabel('Annual Income')\n\nax2 = fig.add_subplot(212)\nsns.countplot(\n    x='home_ownership', hue='loan_status',\n    data=loandata, ax=ax2\n)\n\n\n\n\n\n\n\n\nInsights:  From the Annual Income column, there is not enough insights based on the plot. But for the Home Ownership plot shows that, if the house is owned, it’s less likely to be charged off.\n\n\nIssue Date and Verification Status\n\nloandata['issue_d'] = pd.to_datetime(\n    loandata['issue_d'], format='%b-%y'\n)\nloandata = loandata.sort_values('issue_d')\nloan_status_trend = loandata.groupby(\n    ['issue_d', 'loan_status']).size().unstack()\n\nfig = plt.figure(figsize=(8, 10))\nax1 = fig.add_subplot(211)\nloan_status_trend.plot(\n    kind='line', marker='o', ax=ax1\n)\nax1.set_title('Loan Status Over Time by Issue Date')\nax1.set_xlabel('Issue Date(mm-yyyy)')\nax1.set_ylabel('Number of Loans')\nax1.legend(title='Loan Status')\n\nax2 = fig.add_subplot(212)\nsns.countplot(\n    x='verification_status', hue='loan_status',\n    data=loandata, palette='winter', ax=ax2\n)\n\n\n\n\n\n\n\n\n\nInsights:  From issue date plot we see that most of loans that were marked as charged off happened during the year 2012 to 2016, with the pick in in 2015. For the verification status, we can see the less charged off incidents when the source was not varified. &lt; /p &gt;\n\n\nPurpose and Debt-to-Income Ratio\n\nloandata['purpose'].value_counts()\n\npurpose\ndebt_consolidation    234420\ncredit_card            82998\nhome_improvement       24024\nother                  21177\nmajor_purchase          8788\nsmall_business          5701\ncar                     4696\nmedical                 4194\nmoving                  2853\nvacation                2452\nhouse                   2201\nwedding                 1811\nrenewable_energy         328\neducational              257\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2023,\n  author = {Islam, Rafiq},\n  title = {Lendingclub’s Loan Default Prediction},\n  date = {2023-02-23},\n  url = {https://mrislambd.github.io/codepages/lendingclub/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2023. “Lendingclub’s Loan Default\nPrediction.” February 23, 2023. https://mrislambd.github.io/codepages/lendingclub/."
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html",
    "href": "portfolio/dsp/lendingclub/index.html",
    "title": "Lendingclub’s loan default prediction",
    "section": "",
    "text": "Notebook"
  },
  {
    "objectID": "codepages/lendingclub/index.html#load-the-data-and-primary-library",
    "href": "codepages/lendingclub/index.html#load-the-data-and-primary-library",
    "title": "Lendingclub’s loan default prediction",
    "section": "",
    "text": "import kaggle\nkaggle.api.authenticate()\nkaggle.api.dataset_download_files(\n    'jeandedieunyandwi/lending-club-dataset', path='.', unzip=True\n    )\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nloandata = pd.read_csv('lending_club_loan_two.csv')\nfirst_13_cols = loandata.iloc[:,:13]\nfirst_13_cols.head()\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nint_rate\ninstallment\ngrade\nsub_grade\nemp_title\nemp_length\nhome_ownership\nannual_inc\nverification_status\nissue_d\nloan_status\n\n\n\n\n0\n10000\n36 months\n11.44\n329.48\nB\nB4\nMarketing\n10+ years\nRENT\n117000.0\nNot Verified\nJan-15\nFully Paid\n\n\n1\n8000\n36 months\n11.99\n265.68\nB\nB5\nCredit analyst\n4 years\nMORTGAGE\n65000.0\nNot Verified\nJan-15\nFully Paid\n\n\n2\n15600\n36 months\n10.49\n506.97\nB\nB3\nStatistician\n&lt; 1 year\nRENT\n43057.0\nSource Verified\nJan-15\nFully Paid\n\n\n3\n7200\n36 months\n6.49\n220.65\nA\nA2\nClient Advocate\n6 years\nRENT\n54000.0\nNot Verified\nNov-14\nFully Paid\n\n\n4\n24375\n60 months\n17.27\n609.33\nC\nC5\nDestiny Management Inc.\n9 years\nMORTGAGE\n55000.0\nVerified\nApr-13\nCharged Off\n\n\n\n\n\n\n\n\nsecond_13_cols = loandata.iloc[:,13:]\nsecond_13_cols.head()\n\n\n\n\n\n\n\n\npurpose\ntitle\ndti\nearliest_cr_line\nopen_acc\npub_rec\nrevol_bal\nrevol_util\ntotal_acc\ninitial_list_status\napplication_type\nmort_acc\npub_rec_bankruptcies\naddress\n\n\n\n\n0\nvacation\nVacation\n26.24\nJun-90\n16\n0\n36369\n41.8\n25\nw\nINDIVIDUAL\n0.0\n0.0\n0174 Michelle Gateway\\r\\nMendozaberg, OK 22690\n\n\n1\ndebt_consolidation\nDebt consolidation\n22.05\nJul-04\n17\n0\n20131\n53.3\n27\nf\nINDIVIDUAL\n3.0\n0.0\n1076 Carney Fort Apt. 347\\r\\nLoganmouth, SD 05113\n\n\n2\ncredit_card\nCredit card refinancing\n12.79\nAug-07\n13\n0\n11987\n92.2\n26\nf\nINDIVIDUAL\n0.0\n0.0\n87025 Mark Dale Apt. 269\\r\\nNew Sabrina, WV 05113\n\n\n3\ncredit_card\nCredit card refinancing\n2.60\nSep-06\n6\n0\n5472\n21.5\n13\nf\nINDIVIDUAL\n0.0\n0.0\n823 Reid Ford\\r\\nDelacruzside, MA 00813\n\n\n4\ncredit_card\nCredit Card Refinance\n33.95\nMar-99\n13\n0\n24584\n69.8\n43\nf\nINDIVIDUAL\n1.0\n0.0\n679 Luna Roads\\r\\nGreggshire, VA 11650"
  },
  {
    "objectID": "codepages/lendingclub/index.html#exploratory-data-analysis",
    "href": "codepages/lendingclub/index.html#exploratory-data-analysis",
    "title": "Lendingclub’s loan default prediction",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nDescriptive Statistics\n\nloandata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 395900 entries, 0 to 395899\nData columns (total 27 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   loan_amnt             395900 non-null  int64  \n 1   term                  395900 non-null  object \n 2   int_rate              395900 non-null  float64\n 3   installment           395900 non-null  float64\n 4   grade                 395900 non-null  object \n 5   sub_grade             395900 non-null  object \n 6   emp_title             372982 non-null  object \n 7   emp_length            377608 non-null  object \n 8   home_ownership        395900 non-null  object \n 9   annual_inc            395900 non-null  float64\n 10  verification_status   395900 non-null  object \n 11  issue_d               395900 non-null  object \n 12  loan_status           395900 non-null  object \n 13  purpose               395900 non-null  object \n 14  title                 394145 non-null  object \n 15  dti                   395900 non-null  float64\n 16  earliest_cr_line      395900 non-null  object \n 17  open_acc              395900 non-null  int64  \n 18  pub_rec               395900 non-null  int64  \n 19  revol_bal             395900 non-null  int64  \n 20  revol_util            395624 non-null  float64\n 21  total_acc             395900 non-null  int64  \n 22  initial_list_status   395900 non-null  object \n 23  application_type      395900 non-null  object \n 24  mort_acc              358117 non-null  float64\n 25  pub_rec_bankruptcies  395365 non-null  float64\n 26  address               395900 non-null  object \ndtypes: float64(7), int64(5), object(15)\nmemory usage: 81.6+ MB\n\n\n\nsns.heatmap(loandata.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n\n\n\n\n\n\n\n\nSeems like there are some missing data in the emp_length, emp_title, and mort_acc columns. Around 4.62\\(\\%\\) missing data in emp_length column, 5.79\\(\\%\\) missing data in emp_title column, and 9.54\\(\\%\\) in the mort_acc coulumn. Also, there is very small proportion, 0.07\\(\\%\\) of missing data in the revol_util column.   We need to take care of this missing observations. We may drop the missing values if it is insignificantly missing or doesn’t seem to be very strongly related to the predictive/dependent variable.\n\n\nloandata.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nloan_amnt\n395900.0\n14114.249305\n8357.637338\n500.00\n8000.00\n12000.00\n20000.00\n40000.00\n\n\nint_rate\n395900.0\n13.639385\n4.472112\n5.32\n10.49\n13.33\n16.49\n30.99\n\n\ninstallment\n395900.0\n431.859947\n250.733444\n16.08\n250.33\n375.43\n567.30\n1533.81\n\n\nannual_inc\n395900.0\n74206.819251\n61645.032777\n0.00\n45000.00\n64000.00\n90000.00\n8706582.00\n\n\ndti\n395900.0\n17.379187\n18.021550\n0.00\n11.28\n16.91\n22.98\n9999.00\n\n\nopen_acc\n395900.0\n11.311081\n5.137591\n0.00\n8.00\n10.00\n14.00\n90.00\n\n\npub_rec\n395900.0\n0.178204\n0.530716\n0.00\n0.00\n0.00\n0.00\n86.00\n\n\nrevol_bal\n395900.0\n15844.331435\n20589.846553\n0.00\n6026.00\n11181.00\n19620.00\n1743266.00\n\n\nrevol_util\n395624.0\n53.793449\n24.452575\n0.00\n35.80\n54.80\n72.90\n892.30\n\n\ntotal_acc\n395900.0\n25.414622\n11.887279\n2.00\n17.00\n24.00\n32.00\n151.00\n\n\nmort_acc\n358117.0\n1.814091\n2.148006\n0.00\n0.00\n1.00\n3.00\n34.00\n\n\npub_rec_bankruptcies\n395365.0\n0.121647\n0.356176\n0.00\n0.00\n0.00\n0.00\n8.00\n\n\n\n\n\n\n\n\n\nData Visualization\n\nLoan Status\nFirst, let’s look at the class balance in the dependent variable\n\nsns.countplot(x='loan_status', data=loandata)\nfp = np.round(\n    len(loandata[loandata['loan_status'] == 'Fully Paid'])/len(loandata)*100, 2\n)\n\nco = np.round(\n    len(loandata[loandata['loan_status'] == 'Charged Off']) /\n    len(loandata)*100, 2\n)\n\n\n\n\n\n\n\n\nSo, 80.39\\(\\%\\) are Fully Paid category where as 19.61\\(\\%\\) is labeled as Charged Off or defaulter.\n\nInsights:  With 80.39 % labeled as “Fully Paid” and 19.61 % as “Charged Off,” this dataset qualifies as moderately imbalanced. The minority class (“Charged Off”) is well underrepresented, but not severely so. This level of imbalance can still impact model performance, especially if the model is biased towards predicting the majority class . Techniques like resampling(oversampling the minority class or undersampling the majority), using metrics like F1-score or AUC-ROC, or even experimenting with algorithms specifically designed for imbalanced datasets(such as XGBoost or balanced random forests) could be effective ways to address this imbalance in the predictive modeling process.\n\n\n\nLoan Amount and Installment Interaction With Loan Status\nNext, let’s see how the features interacts with the dependent variable\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nloandata[loandata['loan_status'] == 'Charged Off']['loan_amnt'].hist(\n    alpha=0.5, color='red', bins=30, ax=ax1,\n    label='Charged Off'\n)\nloandata[loandata['loan_status'] == 'Fully Paid']['loan_amnt'].hist(\n    alpha=0.5, color='blue', bins=30, ax=ax1,\n    label='Fully Paid'\n)\nax1.set_title('Loan Amount Distribution')\nax1.set_xlabel('Loan Amount')\nax1.legend()\n\nax2 = fig.add_subplot(122)\nloandata[loandata['loan_status'] == 'Charged Off']['installment'].hist(\n    alpha=0.5, color='red', bins=30, ax=ax2,\n    label='Charged Off'\n)\nloandata[loandata['loan_status'] == 'Fully Paid']['installment'].hist(\n    alpha=0.5, color='blue', bins=30, ax=ax2,\n    label='Fully Paid'\n)\nax2.set_title('Installment Distribution')\nax2.set_xlabel('Installment')\nax2.legend()\n\n\n\n\n\n\n\n\nBoth the loan_amnt and installment are slightly positively skewed. How about their mean, upper quartile, lower quartile based on loan_status?\n\nfig = plt.figure(figsize=(8.8, 4))\nax1 = fig.add_subplot(121)\nsns.boxplot(\n    x='loan_status', y='loan_amnt', hue='loan_status',\n    data=loandata, ax=ax1, palette='winter'\n)\nax1.set_title('Loan Amount Boxplot')\n\nax2 = fig.add_subplot(122)\nsns.boxplot(\n    x='loan_status', y='installment', hue='loan_status',\n    data=loandata, ax=ax2, palette='winter'\n)\nax2.set_title('Installment Boxplot')\n\nText(0.5, 1.0, 'Installment Boxplot')\n\n\n\n\n\n\n\n\n\nInsights:  From the above plot we see that mean loan amount of the fully paid and charged off categories are \\(\\$12, 500\\) and \\(\\$14, 000\\), respectively.\n\n\nTerm, Grade, and Sub-grade Interaction With Loan Status\n\nfig = plt.figure(figsize=(8.8, 4))\nax1 = fig.add_subplot(121)\nsns.countplot(\n    x='loan_status',\n    hue='term', data=loandata,\n    palette='RdBu_r', ax=ax1\n)\nax2 = fig.add_subplot(122)\nsns.countplot(\n    x='loan_status',\n    hue='grade', data=loandata,\n    palette='winter', ax=ax2\n)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.histplot(x='sub_grade', hue='loan_status', data=loandata, ax=ax)\n\n\n\n\n\n\n\n\n\n\nEmployment Title and Employment Length\n\nloandata['emp_title'] = loandata['emp_title'].str.lower()\nloandata.emp_title.value_counts()[:25]\n\nemp_title\nmanager                     5635\nteacher                     5426\nregistered nurse            2626\nsupervisor                  2589\nsales                       2381\ndriver                      2306\nowner                       2200\nrn                          2072\nproject manager             1776\noffice manager              1638\ngeneral manager             1460\ntruck driver                1288\ndirector                    1192\nengineer                    1187\npolice officer              1041\nvice president               961\nsales manager                961\noperations manager           960\nstore manager                941\npresident                    877\nadministrative assistant     865\naccountant                   845\naccount manager              845\ntechnician                   839\nmechanic                     753\nName: count, dtype: int64\n\n\nLet’s work with the employment length column\n\nloandata['emp_length'] = loandata['emp_length'].replace({\n    '&lt; 1 year': 0,\n    '1 year': 1,\n    '2 years': 2,\n    '3 years': 3,\n    '4 years': 4,\n    '5 years': 5,\n    '6 years': 6,\n    '7 years': 7,\n    '8 years': 8,\n    '9 years': 9,\n    '10+ years': 10\n}\n).infer_objects(copy=False)\n\nloandata['emp_length_group'] = pd.cut(\n    loandata['emp_length'],\n    bins=[-1, 2, 7, 10],  # Bins: &lt;3 years, 3-7 years, &gt; 7 years\n    labels=['Short-term', 'Mid-term', 'Long-term']\n)\n\nsns.countplot(\n    x='emp_length_group',\n    hue='loan_status',\n    data=loandata,\n    palette='winter',\n    stat='count'\n)\n\n/var/folders/53/8y5n2fl55p3g3r5_pk9yfwk40000gn/T/ipykernel_87761/3832649202.py:1: FutureWarning:\n\nDowncasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n\n\n\n\n\n\n\n\n\n\n\nInsights:  So, from this plot we can see a trend. As employment length goes up, the chances of fully paid gets higher, but the charged of situation remains almost same, irrespective of the employment length. Therefore, it may not be very useful when we may think of data imputation for the missing values.\n\n\n\nHome Ownership and Annual Income\n\nfig = plt.figure(figsize=(8, 10))\nax1 = fig.add_subplot(211)\nsns.boxplot(\n    x='loan_status', y='annual_inc',\n    hue='loan_status', palette='winter',\n    data=loandata, ax=ax1\n)\nax1.set_title('Income Distribution')\nax1.set_xlabel('Loan Status')\nax1.set_ylabel('Annual Income')\n\nax2 = fig.add_subplot(212)\nsns.countplot(\n    x='home_ownership', hue='loan_status',\n    data=loandata, ax=ax2\n)\n\n\n\n\n\n\n\n\nInsights:  From the Annual Income column, there is not enough insights based on the plot. But for the Home Ownership plot shows that, if the house is owned, it’s less likely to be charged off.\n\n\nIssue Date and Verification Status\n\nloandata['issue_d'] = pd.to_datetime(\n    loandata['issue_d'], format='%b-%y'\n)\nloandata = loandata.sort_values('issue_d')\nloan_status_trend = loandata.groupby(\n    ['issue_d', 'loan_status']).size().unstack()\n\nfig = plt.figure(figsize=(8, 10))\nax1 = fig.add_subplot(211)\nloan_status_trend.plot(\n    kind='line', marker='o', ax=ax1\n)\nax1.set_title('Loan Status Over Time by Issue Date')\nax1.set_xlabel('Issue Date(mm-yyyy)')\nax1.set_ylabel('Number of Loans')\nax1.legend(title='Loan Status')\n\nax2 = fig.add_subplot(212)\nsns.countplot(\n    x='verification_status', hue='loan_status',\n    data=loandata, palette='winter', ax=ax2\n)\n\n\n\n\n\n\n\n\n\nInsights:  From issue date plot we see that most of loans that were marked as charged off happened during the year 2012 to 2016, with the pick in in 2015. For the verification status, we can see the less charged off incidents when the source was not varified.\n\n\n\nPurpose and Debt-to-Income Ratio\n\nloandata['purpose'].value_counts()\nfig = plt.figure(figsize=(7.9, 4))\nax1 = fig.add_subplot(121)\nsns.countplot(\n    y='purpose', hue='loan_status',\n    data=loandata, palette='coolwarm'\n)\nax1.set_title('Loan Purpose')\n\nax2 = fig.add_subplot(122)\ndti_threshold = loandata['dti'].quantile(0.95)\nfiltereddata = loandata[loandata['dti'] &lt;= dti_threshold]\n\nsns.boxplot(\n    x='loan_status', y='dti',\n    hue='loan_status', data=filtereddata,\n    palette='coolwarm', ax=ax2\n)\nax2.set_title('Debt-to-Income Ratio on Loan Status')\n\nText(0.5, 1.0, 'Debt-to-Income Ratio on Loan Status')\n\n\n\n\n\n\n\n\n\n\nInsights: From the purpose column, we see that most of the loans that were charged off were used to make debt consolidation. Therefore, debt consolidation may have been a significant factor when a loan is charged off. Another insight we obtain from the debt-to-income ratio is that the charged off loans have higher dti ratio\n\n\n\nNumber of Credit Accounts and Number of Public Records\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nfiltered_open_account_threshold = loandata['open_acc'].quantile(0.98)\nfiltered_open_account = loandata[loandata['open_acc']\n                                 &lt;= filtered_open_account_threshold]\nfiltered_open_account[filtered_open_account['loan_status'] == 'Fully Paid']['open_acc'].hist(\n    alpha=0.5, color='green', bins=30,label='Fully Paid' ,ax=ax1\n)\nfiltered_open_account[filtered_open_account['loan_status'] == 'Charged Off']['open_acc'].hist(\n    alpha=0.5, color='red', bins=30,label='Charged Off' ,ax=ax1\n)\nax1.set_xlabel('Number of opened credit acc.')\nax1.legend()\nax1.set_title('Number of open credit accounts and Loan Status')\n\nloandata['pub_rec_group'] = pd.cut(\n    loandata['pub_rec'], bins=[-1, 0, 1, 3, loandata['pub_rec'].max()],\n    labels=['0', '1', '2-3', '4+']\n)\nloan_status_by_pub_rec = loandata.groupby(\n    ['pub_rec_group', 'loan_status'], observed=False\n).size().unstack()\n\nax2 = fig.add_subplot(122)\nloan_status_by_pub_rec.plot(\n    kind='bar', stacked=False, edgecolor='black',\n    color=['#1f77b4', '#ff7f0e'], ax=ax2\n)\nax2.set_title('Public Records and Loan Status')\nax2.set_xlabel('Public Records')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n\n\n\n\n\n\n\n\n\nsns.boxplot(\n    y='open_acc', x='loan_status',\n    hue='loan_status',\n    data=filtered_open_account\n)\n\n\n\n\n\n\n\n\n\nInsights: Number of credit account seems normally distributed among both groups except for some outliers. However, the mean number of credit accounts are slightly higher for the charged off category than the fully paid category. So, higher credit account has some sort of relation with loan being charged off. Also, people who doesn’t have any public record seems to have higher chance of loan status being charged off\n\n\n\nRevolving Balance and Utilization\n\nfig = plt.figure(figsize=(9,4))\nax1 = fig.add_subplot(121)\nrevol_bal_threshold = loandata['revol_bal'].quantile(0.95)\nfiltered_revol_bal = loandata[loandata['revol_bal']&lt;=revol_bal_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_bal',\n    data=filtered_revol_bal, hue='loan_status',\n    palette='Set2',ax=ax1\n)\nax1.set_xlabel('Loan Status')\nax1.set_ylabel('Revolving Balance')\nax1.set_title('Revolving Balance vs Loan Status')\n\nax2 = fig.add_subplot(122)\nrevol_util_threshold = loandata['revol_util'].quantile(0.95)\nfiltered_revol_util = loandata[loandata['revol_util']&lt;=revol_util_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_util',\n    hue='loan_status', data=filtered_revol_util,\n    palette='Set1',ax=ax2\n)\nax2.set_xlabel('Loan Status')\nax2.set_ylabel('Revolving Utilization')\nax2.set_title('Revolving Utilization vs Loan Status')\n\nText(0.5, 1.0, 'Revolving Utilization vs Loan Status')\n\n\n\n\n\n\n\n\n\n\nInsights: From the above plots, we don’t see much difference between fully paid or charged off status regarding the revolving balance, however, for revolving utilization, it’s higher for the charged off category.\n\n\n\nTotal Account and Initial List Status\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nfiltered_total_account_threshold = loandata['total_acc'].quantile(0.95)\nfiltered_total_account = loandata[loandata['total_acc']\n                                 &lt;= filtered_total_account_threshold]\nfiltered_total_account[filtered_total_account['loan_status'] == 'Fully Paid']['total_acc'].hist(\n    alpha=0.5, color='green', bins=30,label='Fully Paid' ,ax=ax1\n)\nfiltered_total_account[filtered_total_account['loan_status'] == 'Charged Off']['total_acc'].hist(\n    alpha=0.5, color='red', bins=30,label='Charged Off' ,ax=ax1\n)\nax1.set_xlabel('Number of Total Credit Acc.')\nax1.legend()\nax1.set_title('Number of Total credit accounts and Loan Status')\n\n\nax2 = fig.add_subplot(122)\nsns.countplot(\n    x='initial_list_status',hue='loan_status',\n    data=loandata, palette='winter'\n)\nax2.set_title('Initial Status and Loan Status')\nax2.set_xlabel('Initial Status (Funded or Withdrawn)')\n\nText(0.5, 0, 'Initial Status (Funded or Withdrawn)')\n\n\n\n\n\n\n\n\n\nnext,\n\nsns.boxplot(\n    y='total_acc', x='loan_status',\n    hue='loan_status',\n    data=filtered_total_account\n)\n\n\n\n\n\n\n\n\n\nInsights: The number of total credit account seems to have no impact on loan status based on the plottings above. Similarly, Initial Loan Status also doesn’t give much information.\n\n\n\nNumber of Mortgage Account and Number of Public Record of Bankruptcies\n\nfig = plt.figure(figsize=(9,4))\n\nax1 = fig.add_subplot(121)\n\nloandata['mort_acc_group'] = pd.cut(\n    loandata['mort_acc'], bins=[-1, 0, 2, 5, 10, loandata['mort_acc'].max()],\n    labels=['0', '1-2', '3-5', '6-10', '10+']\n)\n\nmort_acc_counts = loandata.groupby(\n    ['mort_acc_group', 'loan_status'],observed=False\n).size().unstack()\n\nmort_acc_counts.plot(\n    kind='bar', stacked=False, \n    colormap='viridis', ax=ax1\n)\nax1.set_title('Loan Status by the # of Mort. Acc')\nax1.set_xlabel('Number of Mortgage Accounts (Grouped)')\nax1.set_ylabel('Number of Loans')\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)\nax1.legend(title='Loan Status')\n\nax2 = fig.add_subplot(122)\n\nloandata['pub_rec_bankruptcies_group'] = pd.cut(\n    loandata['pub_rec_bankruptcies'], bins=[-1, 0, loandata['pub_rec_bankruptcies'].max()],\n    labels=['0', '1 or More']\n)\n\n# Plot the grouped bar chart\npub_rec_bankruptcies_counts = loandata.groupby(\n    ['pub_rec_bankruptcies_group', 'loan_status'],observed=False\n).size().unstack()\n\npub_rec_bankruptcies_counts.plot(\n    kind='bar', stacked=False,\n    colormap='coolwarm', ax=ax2\n)\nax2.set_title('Loan Status by the # of Pub Rec of Bankruptcies')\nax2.set_xlabel('Public Record of Bankruptcies (Grouped)')\nax2.set_ylabel('Number of Loans')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\nax2.legend(title='Loan Status')\nplt.show()\n\n\n\n\n\n\n\n\nInsigths: Again, loans where the applicants has less number of mortgage acc or public record of bankruptcies are more likely to be tagged as charged off.\n\n\nApplication Type\n\nsns.countplot(\n    x='loan_status', hue= 'application_type', \n    data=loandata, palette='winter'\n)\n\n\n\n\n\n\n\n\nWe see only the individual type applications have impact on loans being charged off.\nNow that we have analyzed all the features, how about the correlations among the features?\n\nnumeric_loandata = loandata.select_dtypes(include=['float64','int64'])\nplt.figure(figsize=(10,8))\nsns.heatmap(numeric_loandata.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nConclusion"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#project-overview",
    "href": "portfolio/dsp/lendingclub/index.html#project-overview",
    "title": "Lendingclub’s loan default prediction",
    "section": "Project Overview",
    "text": "Project Overview\n\nLendingClub is a U.S.-based financial services company that initially began as a peer-to-peer (P2P) lending platform, allowing individual investors to lend directly to individual borrowers through a marketplace. Founded in 2006, it grew quickly to become one of the largest and most popular P2P lending platforms, helping connect borrowers in need of personal loans with investors looking for alternative investment opportunities.  LendingClub has been known for its transparent data-sharing practices, making anonymized loan data available to researchers, data scientists, and investors. This data is widely used in financial research, especially for predictive modeling of loan risk and borrower behavior.   The aim of this data science project is to build a machine learning model to predict the likelihood of a loan default.\n\n\nMore information about LendingClub\n\nP2P Lending Model (Early Focus):\n\nBorrowers could apply for loans, typically unsecured personal loans, through LendingClub’s platform.\nLoans were then funded by individual investors who could review borrowers’ profiles, risk grades, and other financial information before committing funds.\nThe model offered borrowers a way to access loans outside traditional banks, often at lower interest rates, and allowed investors to diversify by spreading investments across multiple loans.\n\nRisk and Return:\n\nLendingClub assigned credit grades (A–G) to each loan based on creditworthiness, which affected interest rates. Higher risk meant potentially higher returns for investors but also higher chances of default.\nInvestors bore the risk if a borrower defaulted, which was a notable risk factor compared to FDIC-insured deposits.\n\nShift to a Bank Model:\n\nOver time, LendingClub transitioned away from P2P lending and restructured as a more traditional bank, obtaining a bank charter in 2021.\nIt now offers banking products, such as high-yield savings accounts, and operates more like a digital bank while still focusing on lending products.\n\nBorrower and Loan Profiles:\n\nLendingClub primarily focuses on personal loans for debt consolidation, credit card refinancing, home improvement, and other purposes.\nBorrowers’ profiles typically include information on income, credit score, debt-to-income ratio, and loan purpose, which is used for assessing risk."
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#key-performance-indicators-kpis-of-lendingclub",
    "href": "portfolio/dsp/lendingclub/index.html#key-performance-indicators-kpis-of-lendingclub",
    "title": "Lendingclub’s loan default prediction",
    "section": "Key Performance Indicators (KPIs) of LendingClub",
    "text": "Key Performance Indicators (KPIs) of LendingClub"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#key-aspects-of-lendingclub",
    "href": "portfolio/dsp/lendingclub/index.html#key-aspects-of-lendingclub",
    "title": "Lendingclub’s loan default prediction",
    "section": "Key Aspects of LendingClub",
    "text": "Key Aspects of LendingClub\n\nP2P Lending Model (Early Focus):\n\nBorrowers could apply for loans, typically unsecured personal loans, through LendingClub’s platform.\nLoans were then funded by individual investors who could review borrowers’ profiles, risk grades, and other financial information before committing funds.\nThe model offered borrowers a way to access loans outside traditional banks, often at lower interest rates, and allowed investors to diversify by spreading investments across multiple loans.\n\nRisk and Return:\n\nLendingClub assigned credit grades (A–G) to each loan based on creditworthiness, which affected interest rates. Higher risk meant potentially higher returns for investors but also higher chances of default.\nInvestors bore the risk if a borrower defaulted, which was a notable risk factor compared to FDIC-insured deposits.\n\nShift to a Bank Model:\n\nOver time, LendingClub transitioned away from P2P lending and restructured as a more traditional bank, obtaining a bank charter in 2021.\nIt now offers banking products, such as high-yield savings accounts, and operates more like a digital bank while still focusing on lending products.\n\nBorrower and Loan Profiles:\n\nLendingClub primarily focuses on personal loans for debt consolidation, credit card refinancing, home improvement, and other purposes.\nBorrowers’ profiles typically include information on income, credit score, debt-to-income ratio, and loan purpose, which is used for assessing risk."
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#stakeholders",
    "href": "portfolio/dsp/lendingclub/index.html#stakeholders",
    "title": "Lendingclub’s loan default prediction",
    "section": "Stakeholders",
    "text": "Stakeholders"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#modeling",
    "href": "portfolio/dsp/lendingclub/index.html#modeling",
    "title": "Lendingclub’s loan default prediction",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#results-and-outcome",
    "href": "portfolio/dsp/lendingclub/index.html#results-and-outcome",
    "title": "Lendingclub’s loan default prediction",
    "section": "Results and Outcome",
    "text": "Results and Outcome"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#future-directions",
    "href": "portfolio/dsp/lendingclub/index.html#future-directions",
    "title": "Lendingclub’s loan default prediction",
    "section": "Future Directions",
    "text": "Future Directions"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#dataset",
    "href": "portfolio/dsp/lendingclub/index.html#dataset",
    "title": "Lendingclub’s loan default prediction",
    "section": "Dataset",
    "text": "Dataset\n\nThe dataset is a publicly available data from kaggle.com. It originally contains 396030 entries, with 100.4 MB. However, for easy github push, I reduce the dataset slightly in order to have size smaller than 100 MB. So, in the reduced form, it has 395900 entries with the following columns:\n\n\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60.\n\nint_rate: Interest Rate on the loan installment The monthly payment owed by the borrower if the loan originates.\n\ngrade: LC assigned loan grade\n\nsub_grade: LC assigned loan subgrade\n\nemp_title: The job title supplied by the Borrower when applying for the loan.\n\nemp_length: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n\nhome_ownership: The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n\nannual_inc: The self-reported annual income provided by the borrower during registration.\n\nverification_status: Indicates if income was verified by LC, not verified, or if the income source was verified\n\nissue_d: The month which the loan was funded\n\nloan_status: Current status of the loan\n\npurpose: A category provided by the borrower for the loan request.\n\ntitle: The loan title provided by the borrower\n\nzip_code: The first 3 numbers of the zip code provided by the borrower in the loan application.\n\naddr_state: The state provided by the borrower in the loan application.\n\ndti: A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n\nearliest_cr_line: The month the borrower’s earliest reported credit line was opened.\n\nopen_acc: The number of open credit lines in the borrower’s credit file.\n\npub_rec: Number of derogatory public records.\n\nrevol_bal: Total credit revolving balance.\n\nrevol_util: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n\ntotal_acc: The total number of credit lines currently in the borrower’s credit file\ninitial_list_status: The initial listing status of the loan. Possible values are – W, F\napplication_type: Indicates whether the loan is an individual application or a joint application with two co-borrowers.\n\nmort_acc: Number of mortgage accounts.\n\npub_rec_bankruptcies: Number of public record bankruptcies\n\n\nHere loan_status is the predictive or dependent variable and the rest are predicting or independent variables aka features. We want to predict if the loan will be Fully Paid or Charged Off given the feature values."
  },
  {
    "objectID": "codepages/lendingclub/index.html#data",
    "href": "codepages/lendingclub/index.html#data",
    "title": "Lendingclub’s loan default prediction",
    "section": "Data",
    "text": "Data\nkaggle.api.authenticate()\nkaggle.api.dataset_download_files(\n    'jeandedieunyandwi/lending-club-dataset', path='.', unzip=True\n)"
  },
  {
    "objectID": "codepages/lendingclub/index.html#primary-libraries",
    "href": "codepages/lendingclub/index.html#primary-libraries",
    "title": "Lendingclub’s loan default prediction",
    "section": "Primary Libraries",
    "text": "Primary Libraries\n\nfrom mywebstyle import plot_style\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport kaggle\nplot_style('#f4f4f4')\nloandata = pd.read_csv('lending_club_loan_two.csv')\nfirst_13_cols = loandata.iloc[:, :13]\nfirst_13_cols.head()\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nint_rate\ninstallment\ngrade\nsub_grade\nemp_title\nemp_length\nhome_ownership\nannual_inc\nverification_status\nissue_d\nloan_status\n\n\n\n\n0\n10000\n36 months\n11.44\n329.48\nB\nB4\nMarketing\n10+ years\nRENT\n117000.0\nNot Verified\nJan-15\nFully Paid\n\n\n1\n8000\n36 months\n11.99\n265.68\nB\nB5\nCredit analyst\n4 years\nMORTGAGE\n65000.0\nNot Verified\nJan-15\nFully Paid\n\n\n2\n15600\n36 months\n10.49\n506.97\nB\nB3\nStatistician\n&lt; 1 year\nRENT\n43057.0\nSource Verified\nJan-15\nFully Paid\n\n\n3\n7200\n36 months\n6.49\n220.65\nA\nA2\nClient Advocate\n6 years\nRENT\n54000.0\nNot Verified\nNov-14\nFully Paid\n\n\n4\n24375\n60 months\n17.27\n609.33\nC\nC5\nDestiny Management Inc.\n9 years\nMORTGAGE\n55000.0\nVerified\nApr-13\nCharged Off\n\n\n\n\n\n\n\n\nsecond_13_cols = loandata.iloc[:, 13:]\nsecond_13_cols.head()\n\n\n\n\n\n\n\n\npurpose\ntitle\ndti\nearliest_cr_line\nopen_acc\npub_rec\nrevol_bal\nrevol_util\ntotal_acc\ninitial_list_status\napplication_type\nmort_acc\npub_rec_bankruptcies\naddress\n\n\n\n\n0\nvacation\nVacation\n26.24\nJun-90\n16\n0\n36369\n41.8\n25\nw\nINDIVIDUAL\n0.0\n0.0\n0174 Michelle Gateway\\r\\nMendozaberg, OK 22690\n\n\n1\ndebt_consolidation\nDebt consolidation\n22.05\nJul-04\n17\n0\n20131\n53.3\n27\nf\nINDIVIDUAL\n3.0\n0.0\n1076 Carney Fort Apt. 347\\r\\nLoganmouth, SD 05113\n\n\n2\ncredit_card\nCredit card refinancing\n12.79\nAug-07\n13\n0\n11987\n92.2\n26\nf\nINDIVIDUAL\n0.0\n0.0\n87025 Mark Dale Apt. 269\\r\\nNew Sabrina, WV 05113\n\n\n3\ncredit_card\nCredit card refinancing\n2.60\nSep-06\n6\n0\n5472\n21.5\n13\nf\nINDIVIDUAL\n0.0\n0.0\n823 Reid Ford\\r\\nDelacruzside, MA 00813\n\n\n4\ncredit_card\nCredit Card Refinance\n33.95\nMar-99\n13\n0\n24584\n69.8\n43\nf\nINDIVIDUAL\n1.0\n0.0\n679 Luna Roads\\r\\nGreggshire, VA 11650"
  },
  {
    "objectID": "codepages/lendingclub/temp.html",
    "href": "codepages/lendingclub/temp.html",
    "title": "Mohammad Rafiqul Islam",
    "section": "",
    "text": "from mywebstyle import plot_style\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport kaggle\nplot_style('#f4f4f4')\nloandata = pd.read_csv('lending_club_loan_two.csv')\nfirst_13_cols = loandata.iloc[:, :13]\nfirst_13_cols.head()\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/macpc/.kaggle/kaggle.json'\n\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nint_rate\ninstallment\ngrade\nsub_grade\nemp_title\nemp_length\nhome_ownership\nannual_inc\nverification_status\nissue_d\nloan_status\n\n\n\n\n0\n10000\n36 months\n11.44\n329.48\nB\nB4\nMarketing\n10+ years\nRENT\n117000.0\nNot Verified\nJan-15\nFully Paid\n\n\n1\n8000\n36 months\n11.99\n265.68\nB\nB5\nCredit analyst\n4 years\nMORTGAGE\n65000.0\nNot Verified\nJan-15\nFully Paid\n\n\n2\n15600\n36 months\n10.49\n506.97\nB\nB3\nStatistician\n&lt; 1 year\nRENT\n43057.0\nSource Verified\nJan-15\nFully Paid\n\n\n3\n7200\n36 months\n6.49\n220.65\nA\nA2\nClient Advocate\n6 years\nRENT\n54000.0\nNot Verified\nNov-14\nFully Paid\n\n\n4\n24375\n60 months\n17.27\n609.33\nC\nC5\nDestiny Management Inc.\n9 years\nMORTGAGE\n55000.0\nVerified\nApr-13\nCharged Off\n\n\n\n\n\n\n\n\nsecond_13_cols = loandata.iloc[:, 13:]\nsecond_13_cols.head()\n\n\n\n\n\n\n\n\npurpose\ntitle\ndti\nearliest_cr_line\nopen_acc\npub_rec\nrevol_bal\nrevol_util\ntotal_acc\ninitial_list_status\napplication_type\nmort_acc\npub_rec_bankruptcies\naddress\n\n\n\n\n0\nvacation\nVacation\n26.24\nJun-90\n16\n0\n36369\n41.8\n25\nw\nINDIVIDUAL\n0.0\n0.0\n0174 Michelle Gateway\\r\\nMendozaberg, OK 22690\n\n\n1\ndebt_consolidation\nDebt consolidation\n22.05\nJul-04\n17\n0\n20131\n53.3\n27\nf\nINDIVIDUAL\n3.0\n0.0\n1076 Carney Fort Apt. 347\\r\\nLoganmouth, SD 05113\n\n\n2\ncredit_card\nCredit card refinancing\n12.79\nAug-07\n13\n0\n11987\n92.2\n26\nf\nINDIVIDUAL\n0.0\n0.0\n87025 Mark Dale Apt. 269\\r\\nNew Sabrina, WV 05113\n\n\n3\ncredit_card\nCredit card refinancing\n2.60\nSep-06\n6\n0\n5472\n21.5\n13\nf\nINDIVIDUAL\n0.0\n0.0\n823 Reid Ford\\r\\nDelacruzside, MA 00813\n\n\n4\ncredit_card\nCredit Card Refinance\n33.95\nMar-99\n13\n0\n24584\n69.8\n43\nf\nINDIVIDUAL\n1.0\n0.0\n679 Luna Roads\\r\\nGreggshire, VA 11650\n\n\n\n\n\n\n\n\nloandata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 395900 entries, 0 to 395899\nData columns (total 27 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   loan_amnt             395900 non-null  int64  \n 1   term                  395900 non-null  object \n 2   int_rate              395900 non-null  float64\n 3   installment           395900 non-null  float64\n 4   grade                 395900 non-null  object \n 5   sub_grade             395900 non-null  object \n 6   emp_title             372982 non-null  object \n 7   emp_length            377608 non-null  object \n 8   home_ownership        395900 non-null  object \n 9   annual_inc            395900 non-null  float64\n 10  verification_status   395900 non-null  object \n 11  issue_d               395900 non-null  object \n 12  loan_status           395900 non-null  object \n 13  purpose               395900 non-null  object \n 14  title                 394145 non-null  object \n 15  dti                   395900 non-null  float64\n 16  earliest_cr_line      395900 non-null  object \n 17  open_acc              395900 non-null  int64  \n 18  pub_rec               395900 non-null  int64  \n 19  revol_bal             395900 non-null  int64  \n 20  revol_util            395624 non-null  float64\n 21  total_acc             395900 non-null  int64  \n 22  initial_list_status   395900 non-null  object \n 23  application_type      395900 non-null  object \n 24  mort_acc              358117 non-null  float64\n 25  pub_rec_bankruptcies  395365 non-null  float64\n 26  address               395900 non-null  object \ndtypes: float64(7), int64(5), object(15)\nmemory usage: 81.6+ MB\n\n\n\nsns.heatmap(loandata.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n\n\n\n\n\n\n\n\nloandata.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nloan_amnt\n395900.0\n14114.249305\n8357.637338\n500.00\n8000.00\n12000.00\n20000.00\n40000.00\n\n\nint_rate\n395900.0\n13.639385\n4.472112\n5.32\n10.49\n13.33\n16.49\n30.99\n\n\ninstallment\n395900.0\n431.859947\n250.733444\n16.08\n250.33\n375.43\n567.30\n1533.81\n\n\nannual_inc\n395900.0\n74206.819251\n61645.032777\n0.00\n45000.00\n64000.00\n90000.00\n8706582.00\n\n\ndti\n395900.0\n17.379187\n18.021550\n0.00\n11.28\n16.91\n22.98\n9999.00\n\n\nopen_acc\n395900.0\n11.311081\n5.137591\n0.00\n8.00\n10.00\n14.00\n90.00\n\n\npub_rec\n395900.0\n0.178204\n0.530716\n0.00\n0.00\n0.00\n0.00\n86.00\n\n\nrevol_bal\n395900.0\n15844.331435\n20589.846553\n0.00\n6026.00\n11181.00\n19620.00\n1743266.00\n\n\nrevol_util\n395624.0\n53.793449\n24.452575\n0.00\n35.80\n54.80\n72.90\n892.30\n\n\ntotal_acc\n395900.0\n25.414622\n11.887279\n2.00\n17.00\n24.00\n32.00\n151.00\n\n\nmort_acc\n358117.0\n1.814091\n2.148006\n0.00\n0.00\n1.00\n3.00\n34.00\n\n\npub_rec_bankruptcies\n395365.0\n0.121647\n0.356176\n0.00\n0.00\n0.00\n0.00\n8.00\n\n\n\n\n\n\n\n\nsns.countplot(x='loan_status', data=loandata)\nfp = np.round(\n    len(loandata[loandata['loan_status'] == 'Fully Paid'])/len(loandata)*100, 2\n)\n\nco = np.round(\n    len(loandata[loandata['loan_status'] == 'Charged Off']) /\n    len(loandata)*100, 2\n)\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nloandata[loandata['loan_status'] == 'Charged Off']['loan_amnt'].hist(\n    alpha=0.5, color='red', bins=30, ax=ax1,\n    label='Charged Off'\n)\nloandata[loandata['loan_status'] == 'Fully Paid']['loan_amnt'].hist(\n    alpha=0.5, color='blue', bins=30, ax=ax1,\n    label='Fully Paid'\n)\nax1.set_title('Loan Amount Distribution')\nax1.set_xlabel('Loan Amount')\nax1.legend()\n\nax2 = fig.add_subplot(122)\nloandata[loandata['loan_status'] == 'Charged Off']['installment'].hist(\n    alpha=0.5, color='red', bins=30, ax=ax2,\n    label='Charged Off'\n)\nloandata[loandata['loan_status'] == 'Fully Paid']['installment'].hist(\n    alpha=0.5, color='blue', bins=30, ax=ax2,\n    label='Fully Paid'\n)\nax2.set_title('Installment Distribution')\nax2.set_xlabel('Installment')\nax2.legend()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8.8, 4))\nax1 = fig.add_subplot(121)\nsns.boxplot(\n    x='loan_status', y='loan_amnt', hue='loan_status',\n    data=loandata, ax=ax1, palette='winter'\n)\nax1.set_title('Loan Amount Boxplot')\n\nax2 = fig.add_subplot(122)\nsns.boxplot(\n    x='loan_status', y='installment', hue='loan_status',\n    data=loandata, ax=ax2, palette='winter'\n)\nax2.set_title('Installment Boxplot')\n\nText(0.5, 1.0, 'Installment Boxplot')\n\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8.8, 4))\nax1 = fig.add_subplot(121)\nsns.countplot(\n    x='loan_status',\n    hue='term', data=loandata,\n    palette='RdBu_r', ax=ax1\n)\nax2 = fig.add_subplot(122)\nsns.countplot(\n    x='loan_status',\n    hue='grade', data=loandata,\n    palette='winter', ax=ax2\n)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.histplot(x='sub_grade', hue='loan_status', data=loandata, ax=ax)\n\n\n\n\n\n\n\n\n\nloandata['emp_title'] = loandata['emp_title'].str.lower()\nloandata.emp_title.value_counts()[:25]\n\nemp_title\nmanager                     5635\nteacher                     5426\nregistered nurse            2626\nsupervisor                  2589\nsales                       2381\ndriver                      2306\nowner                       2200\nrn                          2072\nproject manager             1776\noffice manager              1638\ngeneral manager             1460\ntruck driver                1288\ndirector                    1192\nengineer                    1187\npolice officer              1041\nvice president               961\nsales manager                961\noperations manager           960\nstore manager                941\npresident                    877\nadministrative assistant     865\naccountant                   845\naccount manager              845\ntechnician                   839\nmechanic                     753\nName: count, dtype: int64\n\n\n\npd.set_option('future.no_silent_downcasting', True)\nloandata['emp_length'] = loandata['emp_length'].replace({\n    '&lt; 1 year': 0,\n    '1 year': 1,\n    '2 years': 2,\n    '3 years': 3,\n    '4 years': 4,\n    '5 years': 5,\n    '6 years': 6,\n    '7 years': 7,\n    '8 years': 8,\n    '9 years': 9,\n    '10+ years': 10\n}\n).infer_objects(copy=False)\n\nloandata['emp_length_group'] = pd.cut(\n    loandata['emp_length'],\n    bins=[-1, 2, 7, 10],  # Bins: &lt;3 years, 3-7 years, &gt; 7 years\n    labels=['Short-term', 'Mid-term', 'Long-term']\n)\n\nsns.countplot(\n    x='emp_length_group',\n    hue='loan_status',\n    data=loandata,\n    palette='winter',\n    stat='count'\n)\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8, 10))\nax1 = fig.add_subplot(211)\nannual_income_threshod = loandata['annual_inc'].quantile(0.95)\nfiltered_income = loandata[loandata['annual_inc'] &lt;= annual_income_threshod]\nsns.boxplot(\n    x='loan_status', y='annual_inc',\n    hue='loan_status', palette='winter',\n    data=filtered_income, ax=ax1\n)\nax1.set_title('Income Distribution')\nax1.set_xlabel('Loan Status')\nax1.set_ylabel('Annual Income')\n\nax2 = fig.add_subplot(212)\nsns.countplot(\n    x='home_ownership', hue='loan_status',\n    data=loandata, ax=ax2\n)\n\n\n\n\n\n\n\n\n\nloandata['issue_d'] = pd.to_datetime(\n    loandata['issue_d'], format='%b-%y'\n)\nloandata = loandata.sort_values('issue_d')\nloan_status_trend = loandata.groupby(\n    ['issue_d', 'loan_status']).size().unstack()\n\nfig = plt.figure(figsize=(8, 10))\nax1 = fig.add_subplot(211)\nloan_status_trend.plot(\n    kind='line', marker='o', ax=ax1\n)\nax1.set_title('Loan Status Over Time by Issue Date')\nax1.set_xlabel('Issue Date(mm-yyyy)')\nax1.set_ylabel('Number of Loans')\nax1.legend(title='Loan Status')\n\nax2 = fig.add_subplot(212)\nsns.countplot(\n    x='verification_status', hue='loan_status',\n    data=loandata, palette='winter', ax=ax2\n)\n\n\n\n\n\n\n\n\n\nloandata['purpose'].value_counts()\n\npurpose\ndebt_consolidation    234420\ncredit_card            82998\nhome_improvement       24024\nother                  21177\nmajor_purchase          8788\nsmall_business          5701\ncar                     4696\nmedical                 4194\nmoving                  2853\nvacation                2452\nhouse                   2201\nwedding                 1811\nrenewable_energy         328\neducational              257\nName: count, dtype: int64\n\n\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nsns.countplot(\n    y='purpose', hue='loan_status',\n    data=loandata, palette='coolwarm'\n)\nax1.set_title('Loan Purpose')\n\nax2 = fig.add_subplot(122)\ndti_threshold = loandata['dti'].quantile(0.95)\nfiltereddata = loandata[loandata['dti'] &lt;= dti_threshold]\n\nsns.boxplot(\n    x='loan_status', y='dti',\n    hue='loan_status', data=filtereddata,\n    palette='coolwarm', ax=ax2\n)\nax2.set_title('Debt-to-Income Ratio on Loan Status')\n\nText(0.5, 1.0, 'Debt-to-Income Ratio on Loan Status')\n\n\n\n\n\n\n\n\n\n\nInsights: From the purpose column, we see that most of the loans that were charged off were used to make debt consolidation. Therefore, debt consolidation may have been a significant factor when a loan is charged off. Another insight we obtain from the debt-to-income ratio is that the charged off loans have higher dti ratio\n\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nfiltered_open_account_threshold = loandata['open_acc'].quantile(0.98)\nfiltered_open_account = loandata[loandata['open_acc']\n                                 &lt;= filtered_open_account_threshold]\nfiltered_open_account[filtered_open_account['loan_status'] == 'Fully Paid']['open_acc'].hist(\n    alpha=0.5, color='green', bins=30, label='Fully Paid', ax=ax1\n)\nfiltered_open_account[filtered_open_account['loan_status'] == 'Charged Off']['open_acc'].hist(\n    alpha=0.5, color='red', bins=30, label='Charged Off', ax=ax1\n)\nax1.set_xlabel('Number of Credit Acc.')\nax1.legend()\nax1.set_title('Number of credit accounts and Loan Status')\n\nloandata['pub_rec_group'] = pd.cut(\n    loandata['pub_rec'], bins=[-1, 0, 1, 3, loandata['pub_rec'].max()],\n    labels=['0', '1', '2-3', '4+']\n)\nloan_status_by_pub_rec = loandata.groupby(\n    ['pub_rec_group', 'loan_status'], observed=False\n).size().unstack()\n\nax2 = fig.add_subplot(122)\nloan_status_by_pub_rec.plot(\n    kind='bar', stacked=False, edgecolor='black',\n    color=['#1f77b4', '#ff7f0e'], ax=ax2\n)\nax2.set_title('Public Records and Loan Status')\nax2.set_xlabel('Public Records')\n\nText(0.5, 0, 'Public Records')\n\n\n\n\n\n\n\n\n\n\nsns.boxplot(\n    y='open_acc', x='loan_status',\n    hue='loan_status',\n    data=filtered_open_account\n)\n\n\n\n\n\n\n\n\n\n\n\nInsights: Number of credit account seems normally distributed among both groups except for some outliers. However, the mean number of credit accounts are slightly higher for the charged off category than the fully paid category. So, higher credit account has some sort of relation with loan being charged off. Also, people who doesn’t have any public record seems to have higher chance of loan status being charged off\n\n\nrevol_bal_threshold = loandata['revol_bal'].quantile(0.95)\nfiltered_revol_bal = loandata[loandata['revol_bal'] &lt;= revol_bal_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_bal',\n    data=filtered_revol_bal, hue='loan_status',\n    palette='Set2'\n)\n\n\n\n\n\n\n\n\n\nrevol_util_threshold = loandata['revol_util'].quantile(0.95)\nfiltered_revol_util = loandata[loandata['revol_util'] &lt;= revol_util_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_util',\n    hue='loan_status', data=filtered_revol_util,\n    palette='Set1'\n)\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nrevol_bal_threshold = loandata['revol_bal'].quantile(0.95)\nfiltered_revol_bal = loandata[loandata['revol_bal'] &lt;= revol_bal_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_bal',\n    data=filtered_revol_bal, hue='loan_status',\n    palette='Set2', ax=ax1\n)\nax1.set_xlabel('Loan Status')\nax1.set_ylabel('Revolving Balance')\nax1.set_title('Revolving Balance vs Loan Status')\n\nax2 = fig.add_subplot(122)\nrevol_util_threshold = loandata['revol_util'].quantile(0.95)\nfiltered_revol_util = loandata[loandata['revol_util'] &lt;= revol_util_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_util',\n    hue='loan_status', data=filtered_revol_util,\n    palette='Set1', ax=ax2\n)\nax2.set_xlabel('Loan Status')\nax2.set_ylabel('Revolving Utilization')\nax2.set_title('Revolving Utilization vs Loan Status')\n\nText(0.5, 1.0, 'Revolving Utilization vs Loan Status')\n\n\n\n\n\n\n\n\n\n\nloandata['total_acc'].value_counts()\n\ntotal_acc\n21     14274\n22     14255\n20     14220\n23     13915\n24     13874\n       ...  \n151        1\n104        1\n135        1\n108        1\n115        1\nName: count, Length: 118, dtype: int64\n\n\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nfiltered_total_account_threshold = loandata['total_acc'].quantile(0.95)\nfiltered_total_account = loandata[loandata['total_acc']\n                                  &lt;= filtered_total_account_threshold]\nfiltered_total_account[filtered_total_account['loan_status'] == 'Fully Paid']['total_acc'].hist(\n    alpha=0.5, color='green', bins=30, label='Fully Paid', ax=ax1\n)\nfiltered_total_account[filtered_total_account['loan_status'] == 'Charged Off']['total_acc'].hist(\n    alpha=0.5, color='red', bins=30, label='Charged Off', ax=ax1\n)\nax1.set_xlabel('Number of Total Credit Acc.')\nax1.legend()\nax1.set_title('Number of Total credit accounts and Loan Status')\n\n\nax2 = fig.add_subplot(122)\nsns.countplot(\n    x='initial_list_status', hue='loan_status',\n    data=loandata, palette='winter'\n)\nax2.set_title('Initial Status and Loan Status')\nax2.set_xlabel('Initial Status (Funded or Withdrawn)')\n\nText(0.5, 0, 'Initial Status (Funded or Withdrawn)')\n\n\n\n\n\n\n\n\n\n\nsns.boxplot(\n    y='total_acc', x='loan_status',\n    hue='loan_status',\n    data=filtered_total_account\n)\n\n\n\n\n\n\n\n\n\nloandata['application_type'].value_counts()\n\napplication_type\nINDIVIDUAL    395189\nJOINT            425\nDIRECT_PAY       286\nName: count, dtype: int64\n\n\n\nloandata['mort_acc'].value_counts()\n\nmort_acc\n0.0     139727\n1.0      60392\n2.0      49931\n3.0      38040\n4.0      27880\n5.0      18188\n6.0      11067\n7.0       6050\n8.0       3121\n9.0       1655\n10.0       865\n11.0       479\n12.0       264\n13.0       146\n14.0       107\n15.0        61\n16.0        37\n17.0        22\n18.0        18\n19.0        15\n20.0        13\n24.0        10\n22.0         7\n25.0         4\n21.0         4\n27.0         3\n23.0         2\n31.0         2\n32.0         2\n26.0         2\n34.0         1\n30.0         1\n28.0         1\nName: count, dtype: int64\n\n\n\nmort_acc_summary = loandata.groupby('loan_status')['mort_acc'].describe()\nprint(mort_acc_summary)\n\n                count      mean       std  min  25%  50%  75%   max\nloan_status                                                        \nCharged Off   72103.0  1.501214  1.974335  0.0  0.0  1.0  2.0  23.0\nFully Paid   286014.0  1.892967  2.182550  0.0  0.0  1.0  3.0  34.0\n\n\n\n# Bin the mort_acc column into categories\nloandata['mort_acc_group'] = pd.cut(\n    loandata['mort_acc'], bins=[-1, 0, 2, 5, 10, loandata['mort_acc'].max()],\n    labels=['0', '1-2', '3-5', '6-10', '10+']\n)\n\n# Plot the grouped bar chart\nmort_acc_counts = loandata.groupby(\n    ['mort_acc_group', 'loan_status'], observed=False\n).size().unstack()\n\nmort_acc_counts.plot(kind='bar', stacked=False,\n                     figsize=(10, 6), colormap='viridis')\nplt.title('Loan Status by Number of Mortgage Accounts')\nplt.xlabel('Number of Mortgage Accounts (Grouped)')\nplt.ylabel('Number of Loans')\nplt.xticks(rotation=0)\nplt.legend(title='Loan Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nloandata['pub_rec_bankruptcies'].value_counts()\n\npub_rec_bankruptcies\n0.0    350265\n1.0     42776\n2.0      1846\n3.0       351\n4.0        82\n5.0        32\n6.0         7\n7.0         4\n8.0         2\nName: count, dtype: int64\n\n\n\nloandata['pub_rec_bankruptcies_group'] = pd.cut(\n    loandata['pub_rec_bankruptcies'], bins=[-1, 0,\n                                            loandata['pub_rec_bankruptcies'].max()],\n    labels=['0', '1 or More']\n)\n\n# Plot the grouped bar chart\npub_rec_bankruptcies_counts = loandata.groupby(\n    ['pub_rec_bankruptcies_group', 'loan_status'], observed=False\n).size().unstack()\n\npub_rec_bankruptcies_counts.plot(\n    kind='bar', stacked=False, figsize=(10, 6), colormap='viridis')\nplt.title('Loan Status by Number of Public Record of Bankruptcies')\nplt.xlabel('Public Record of Bankruptcies (Grouped)')\nplt.ylabel('Number of Loans')\nplt.xticks(rotation=0)\nplt.legend(title='Loan Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(9, 4))\n\nax1 = fig.add_subplot(121)\n\nloandata['mort_acc_group'] = pd.cut(\n    loandata['mort_acc'], bins=[-1, 0, 2, 5, 10, loandata['mort_acc'].max()],\n    labels=['0', '1-2', '3-5', '6-10', '10+']\n)\n\nmort_acc_counts = loandata.groupby(\n    ['mort_acc_group', 'loan_status'], observed=False\n).size().unstack()\n\nmort_acc_counts.plot(\n    kind='bar', stacked=False,\n    colormap='viridis', ax=ax1\n)\nax1.set_title('Loan Status by the # of Mort. Acc')\nax1.set_xlabel('Number of Mortgage Accounts (Grouped)')\nax1.set_ylabel('Number of Loans')\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)\nax1.legend(title='Loan Status')\n\nax2 = fig.add_subplot(122)\n\nloandata['pub_rec_bankruptcies_group'] = pd.cut(\n    loandata['pub_rec_bankruptcies'], bins=[-1, 0,\n                                            loandata['pub_rec_bankruptcies'].max()],\n    labels=['0', '1 or More']\n)\n\n# Plot the grouped bar chart\npub_rec_bankruptcies_counts = loandata.groupby(\n    ['pub_rec_bankruptcies_group', 'loan_status'], observed=False\n).size().unstack()\n\npub_rec_bankruptcies_counts.plot(\n    kind='bar', stacked=False,\n    colormap='coolwarm', ax=ax2\n)\nax2.set_title('Loan Status by the # of Pub Rec of Bankruptcies')\nax2.set_xlabel('Public Record of Bankruptcies (Grouped)')\nax2.set_ylabel('Number of Loans')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\nax2.legend(title='Loan Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nloandata['application_type'].value_counts()\n\napplication_type\nINDIVIDUAL    395189\nJOINT            425\nDIRECT_PAY       286\nName: count, dtype: int64\n\n\n\nsns.countplot(\n    x='loan_status', hue= 'application_type', \n    data=loandata, palette='winter'\n)\n\n\n\n\n\n\n\n\n\nnumeric_loandata = loandata.select_dtypes(include=['float64','int64'])\nplt.figure(figsize=(10,8))\nsns.heatmap(numeric_loandata.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "portfolio/dsp/nlp-sales/index.html",
    "href": "portfolio/dsp/nlp-sales/index.html",
    "title": "Predicting Product Success Using Customer Reviews and Sales Data",
    "section": "",
    "text": "Notebook GitHub (Private until finished)\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2024,\n  author = {Islam, Rafiq},\n  title = {Predicting {Product} {Success} {Using} {Customer} {Reviews}\n    and {Sales} {Data}},\n  date = {2024-10-11},\n  url = {https://mrislambd.github.io/portfolio/dsp/nlp-sales/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2024. “Predicting Product Success Using Customer\nReviews and Sales Data.” October 11, 2024. https://mrislambd.github.io/portfolio/dsp/nlp-sales/."
  },
  {
    "objectID": "dsandml/adaboost/index.html",
    "href": "dsandml/adaboost/index.html",
    "title": "Boosting Algorithm: Adaptive Boosting Method (AdaBoost)",
    "section": "",
    "text": "Boosting is a powerful ensemble learning technique that focuses on improving the performance of weak learners to build a robust predictive model.   Now the question is what the heck is weak learner? Well, roughly speaking, a statistical learning algorithm is called a weak learner if it is slightly better than just random guess. In contrast, a statistical learning algorithm is called a strong learner if it can be made arbitrarily close to the true value.  Unlike bagging (bootstrap aggregating, e.g. random forest), which builds models independently, boosting builds models sequentially, where each new model corrects the errors of its predecessors. This approach ensures that the ensemble concentrates on the difficult-to-predict instances, making boosting highly effective for both classification and regression problems.\n\n\n\n\nSequential Model Building: Boosting builds one model at a time, with each model improving upon the errors of the previous one.\nWeight Assignment: It assigns weights to instances, emphasizing misclassified or poorly predicted ones in subsequent iterations.\nWeak to Strong Learners: The goal of boosting is to combine multiple weak learners (models slightly better than random guessing) into a strong learner.\n\n\n\n\nBefore writing the formal algorithm, let’s do some math by hand. Say, we have a toy dataset:\n\n\n\n\\(x_1\\)\n\\(x_2\\)\ny\n\n\n\n\n1\n2\n1\n\n\n2\n1\n1\n\n\n3\n2\n-1\n\n\n4\n3\n-1\n\n\n\nHere:\n\n\\(x_1\\) and \\(x_2\\) are features.\n\\(y\\) is the target label, with values \\(+1\\) or \\(-1\\).\n\nNow, let’s apply the AdaBoost algorithm step-by-step using this dataset.\n\n\nStep 1: Initialize Weights\nInitially, all data points are assigned equal weights: \\[\nw_i^{(1)} = \\frac{1}{N} = \\frac{1}{4} = 0.25\n\\]\nWeights: \\(w = [0.25, 0.25, 0.25, 0.25]\\).\nStep 2: Train Weak Learner\nSuppose we use a decision stump (a simple decision rule) as the weak learner. The first decision stump might split on \\(x_1\\) as:\n\nPredict \\(+1\\) if \\(x_1 \\leq 1.5\\), otherwise \\(-1\\).\n\n\\[\nh_1(x) =\n\\begin{cases}\n+1 & \\text{if } x_1 \\leq 1.5 \\\\\n-1 & \\text{otherwise}\n\\end{cases}\n\\]\n\nNote, that even though we are deciding based on the feature \\(x_1\\), however, for \\(h_1(x)\\) learner, \\(x\\) is the row from the data set, i.e. \\(x=[x_1, x_2]\\). Therefore, for \\(h_1(x_1)\\) would mean that, we are feeding first row to the learner \\(h\\) at iteration 1.\n\nStep 3: Evaluate Weak Learner\nPredictions for the dataset: \\[\nh_1(x) = [1, -1, -1, -1]\n\\]\nBut our true labels are \\([1,1,-1,-1]\\). So the error\n\\[\n\\epsilon_1 = \\frac{\\sum_{i=1}^{4}w_i^1 \\mathbb{1}(y_i\\ne h_1(x_i))}{\\sum_{i=1}^{4}w_i^1}\n\\]\nwhere, \\(\\mathbb{1}\\) is an indicator function that equals 1 when the prediction is incorrect and 0 otherwise. Therefore, in iteration 1:\n\\[\n\\epsilon_1 = \\frac{0.25(0+1+0+0)}{1}=0.25\n\\]\nStep 4: Calculate \\(\\alpha_1\\)\n\\[\n\\alpha_1 = \\ln\\left(\\frac{1 - \\epsilon_1}{\\epsilon_1}\\right) = 1.0986\n\\]\nStep 5: Update Weights:\nFor each instance:\n\\[\n   w_i^{(1)} = w_i^{(1)} \\cdot \\exp\\left(\\alpha_1 \\cdot y_i \\cdot h_1(x_i)\\right)\n\\]\nNow you may wonder how and from where we came up with this updating rule? We will explain this update process in the next post, but for now let’s just focus on the update.\n\\[\\begin{align*}\nw_1^1 & = w_1^1e^{\\alpha_1\\mathbb{1}(y_1\\ne h_1(x_1))} = 0.25 e^{1.0986\\times 0} = 0.25 \\\\\nw_2^1 & = w_2^1e^{\\alpha_1\\mathbb{1}(y_1\\ne h_1(x_2))} = 0.25 e^{1.0986\\times 1} = 0.75 \\\\\nw_3^1 & = w_3^1e^{\\alpha_1\\mathbb{1}(y_1\\ne h_1(x_3))} = 0.25 e^{1.0986\\times 0} = 0.25 \\\\\nw_4^1 & = w_4^1e^{\\alpha_1\\mathbb{1}(y_1\\ne h_1(x_4))} = 0.25 e^{1.0986\\times 0} = 0.25 \\\\\n\\end{align*}\\]\nUpdated weights (before normalization): \\[\n[0.25, 0.75, 0.25, 0.25]\n\\]\nNormalize to ensure the weights sum to 1:\n\\[\n   w_i^{(1)} = \\frac{w_i^{(1)}}{\\sum w_i^{(1)}}\n\\]\nFinal normalized weights: \\(w = [0.17, 0.5, 0.17, 0.17]\\). Notice that, for the incorrect prediction, the weight increased and for the correct prediction the weights decreased.\n\n\n\nSimilarly, we proceed with second iteration with the following weak learner:\n\\[\nh_2(x) = \\begin{cases}+1 & \\text{if} x_2\\le 1.5 \\\\ -1 & \\text{otherwise}\\end{cases}\n\\]\nFor this learner, the prediction\n\\[\nh_2(x)= [-1, 1, -1, -1]\n\\]\nwhere as the actual labels are \\([1,1,-1,1]\\). So, the error\n\\[\n\\epsilon_2 = \\frac{0.17\\times 1 + 0.5\\times 0+0.17\\times 0+0.17\\times 0}{1} = 0.17\n\\]\nand\n\\[\n\\alpha_2 = \\ln\\left(\\frac{0.756}{0.244}\\right) = 1.586\n\\]\nNext, we update the weights\n\\[\\begin{align*}\nw_1^{2} &= w_1^2 e^{\\alpha_2\\mathbb{1}(y_1 \\ne h_2(x_1))} = 0.17 e^{1.586\\times 1} = 0.83\\\\\nw_2^{2} &= w_2^2 e^{\\alpha_2\\mathbb{1}(y_1 \\ne h_2(x_2))} = 0.5 e^{1.1308\\times 0} = 0.5\\\\\nw_3^{2} &= w_3^2 e^{\\alpha_2\\mathbb{1}(y_1 \\ne h_2(x_1))} = 0.17 e^{1.586\\times 0} = 0.17\\\\\nw_4^{2} &= w_4^2 e^{\\alpha_2\\mathbb{1}(y_1 \\ne h_2(x_1))} = 0.17 e^{1.586\\times 0} = 0.17\\\\\n\\end{align*}\\]\nSo, \\(w=[0.83,0.5,0.17,0.17]\\) and after normalizing \\(w=[0.50,0.3,0.10,0.10]\\). The final ensemble model combines the weak learners using their weights (\\(\\alpha\\)):\n\\[\nF(x) = \\text{sign}\\left(\\alpha_1 \\cdot h_1(x) + \\alpha_2 \\cdot h_2(x)\\right)\n\\]\nFor the toy dataset:\n\n\\(\\alpha_1 = 1.0986\\), \\(h_1(x) = [1, -1, -1, -1]\\)\n\\(\\alpha_2 = 1.586\\), \\(h_2(x) = [-1, 1, -1, -1]\\)\n\nWeighted predictions:\n\\[\\begin{align*}\nF(x) &= \\left(\\alpha_1 \\cdot h_1(x) + \\alpha_2 \\cdot h_2(x)\\right)\\\\\n& = [1.0986-1.586,-1.0986+1.586,-1.0986-1.586,-1.0986-1.586]\\\\\n& = [-1, 1, -1, -1]\n\\end{align*}\\]\nIf we keep iterating this way, we will have\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np \nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\n\n# Data points for visualization\niterations = [1, 2]\nerrors = [0.25, 0.167]  # Errors from the two iterations\nalphas = [1.0968, 1.586]  # Alpha values for the weak learners\n\n# Extend to further iterations\n# Simulating error reduction and alpha calculation for a few more iterations\nfor i in range(3, 6):  # Iterations 3 to 5\n    new_error = errors[-1] * 0.7  # Simulating decreasing errors\n    errors.append(new_error)\n    alphas.append( np.log((1 - new_error) / new_error))\n    iterations.append(i)\n\n# Plot weighted errors over iterations\nplt.figure(figsize=(8, 6))\nplt.plot(iterations, errors, marker='o')\nplt.title(\"Weighted Error Reduction Over Iterations in AdaBoost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Weighted Error\")\nplt.grid()\nplt.show()\n\n# Plot alpha values (importance of weak learners)\nplt.figure(figsize=(8, 6))\nplt.plot(iterations, alphas, marker='o', color='orange')\nplt.title(\"Alpha Values Over Iterations in AdaBoost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Alpha (Learner Weight)\")\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "dsandml/adaboost/index.html#introduction",
    "href": "dsandml/adaboost/index.html#introduction",
    "title": "Boosting Algorithm: Adaptive Boosting Method (AdaBoost)",
    "section": "",
    "text": "Boosting is a powerful ensemble learning technique that focuses on improving the performance of weak learners to build a robust predictive model.   Now the question is what the heck is weak learner? Well, roughly speaking, a statistical learning algorithm is called a weak learner if it is slightly better than just random guess. In contrast, a statistical learning algorithm is called a strong learner if it can be made arbitrarily close to the true value.  Unlike bagging (bootstrap aggregating, e.g. random forest), which builds models independently, boosting builds models sequentially, where each new model corrects the errors of its predecessors. This approach ensures that the ensemble concentrates on the difficult-to-predict instances, making boosting highly effective for both classification and regression problems.\n\n\n\n\nSequential Model Building: Boosting builds one model at a time, with each model improving upon the errors of the previous one.\nWeight Assignment: It assigns weights to instances, emphasizing misclassified or poorly predicted ones in subsequent iterations.\nWeak to Strong Learners: The goal of boosting is to combine multiple weak learners (models slightly better than random guessing) into a strong learner.\n\n\n\n\nBefore writing the formal algorithm, let’s do some math by hand. Say, we have a toy dataset:\n\n\n\n\\(x_1\\)\n\\(x_2\\)\ny\n\n\n\n\n1\n2\n1\n\n\n2\n1\n1\n\n\n3\n2\n-1\n\n\n4\n3\n-1\n\n\n\nHere:\n\n\\(x_1\\) and \\(x_2\\) are features.\n\\(y\\) is the target label, with values \\(+1\\) or \\(-1\\).\n\nNow, let’s apply the AdaBoost algorithm step-by-step using this dataset.\n\n\nStep 1: Initialize Weights\nInitially, all data points are assigned equal weights: \\[\nw_i^{(1)} = \\frac{1}{N} = \\frac{1}{4} = 0.25\n\\]\nWeights: \\(w = [0.25, 0.25, 0.25, 0.25]\\).\nStep 2: Train Weak Learner\nSuppose we use a decision stump (a simple decision rule) as the weak learner. The first decision stump might split on \\(x_1\\) as:\n\nPredict \\(+1\\) if \\(x_1 \\leq 1.5\\), otherwise \\(-1\\).\n\n\\[\nh_1(x) =\n\\begin{cases}\n+1 & \\text{if } x_1 \\leq 1.5 \\\\\n-1 & \\text{otherwise}\n\\end{cases}\n\\]\n\nNote, that even though we are deciding based on the feature \\(x_1\\), however, for \\(h_1(x)\\) learner, \\(x\\) is the row from the data set, i.e. \\(x=[x_1, x_2]\\). Therefore, for \\(h_1(x_1)\\) would mean that, we are feeding first row to the learner \\(h\\) at iteration 1.\n\nStep 3: Evaluate Weak Learner\nPredictions for the dataset: \\[\nh_1(x) = [1, -1, -1, -1]\n\\]\nBut our true labels are \\([1,1,-1,-1]\\). So the error\n\\[\n\\epsilon_1 = \\frac{\\sum_{i=1}^{4}w_i^1 \\mathbb{1}(y_i\\ne h_1(x_i))}{\\sum_{i=1}^{4}w_i^1}\n\\]\nwhere, \\(\\mathbb{1}\\) is an indicator function that equals 1 when the prediction is incorrect and 0 otherwise. Therefore, in iteration 1:\n\\[\n\\epsilon_1 = \\frac{0.25(0+1+0+0)}{1}=0.25\n\\]\nStep 4: Calculate \\(\\alpha_1\\)\n\\[\n\\alpha_1 = \\ln\\left(\\frac{1 - \\epsilon_1}{\\epsilon_1}\\right) = 1.0986\n\\]\nStep 5: Update Weights:\nFor each instance:\n\\[\n   w_i^{(1)} = w_i^{(1)} \\cdot \\exp\\left(\\alpha_1 \\cdot y_i \\cdot h_1(x_i)\\right)\n\\]\nNow you may wonder how and from where we came up with this updating rule? We will explain this update process in the next post, but for now let’s just focus on the update.\n\\[\\begin{align*}\nw_1^1 & = w_1^1e^{\\alpha_1\\mathbb{1}(y_1\\ne h_1(x_1))} = 0.25 e^{1.0986\\times 0} = 0.25 \\\\\nw_2^1 & = w_2^1e^{\\alpha_1\\mathbb{1}(y_1\\ne h_1(x_2))} = 0.25 e^{1.0986\\times 1} = 0.75 \\\\\nw_3^1 & = w_3^1e^{\\alpha_1\\mathbb{1}(y_1\\ne h_1(x_3))} = 0.25 e^{1.0986\\times 0} = 0.25 \\\\\nw_4^1 & = w_4^1e^{\\alpha_1\\mathbb{1}(y_1\\ne h_1(x_4))} = 0.25 e^{1.0986\\times 0} = 0.25 \\\\\n\\end{align*}\\]\nUpdated weights (before normalization): \\[\n[0.25, 0.75, 0.25, 0.25]\n\\]\nNormalize to ensure the weights sum to 1:\n\\[\n   w_i^{(1)} = \\frac{w_i^{(1)}}{\\sum w_i^{(1)}}\n\\]\nFinal normalized weights: \\(w = [0.17, 0.5, 0.17, 0.17]\\). Notice that, for the incorrect prediction, the weight increased and for the correct prediction the weights decreased.\n\n\n\nSimilarly, we proceed with second iteration with the following weak learner:\n\\[\nh_2(x) = \\begin{cases}+1 & \\text{if} x_2\\le 1.5 \\\\ -1 & \\text{otherwise}\\end{cases}\n\\]\nFor this learner, the prediction\n\\[\nh_2(x)= [-1, 1, -1, -1]\n\\]\nwhere as the actual labels are \\([1,1,-1,1]\\). So, the error\n\\[\n\\epsilon_2 = \\frac{0.17\\times 1 + 0.5\\times 0+0.17\\times 0+0.17\\times 0}{1} = 0.17\n\\]\nand\n\\[\n\\alpha_2 = \\ln\\left(\\frac{0.756}{0.244}\\right) = 1.586\n\\]\nNext, we update the weights\n\\[\\begin{align*}\nw_1^{2} &= w_1^2 e^{\\alpha_2\\mathbb{1}(y_1 \\ne h_2(x_1))} = 0.17 e^{1.586\\times 1} = 0.83\\\\\nw_2^{2} &= w_2^2 e^{\\alpha_2\\mathbb{1}(y_1 \\ne h_2(x_2))} = 0.5 e^{1.1308\\times 0} = 0.5\\\\\nw_3^{2} &= w_3^2 e^{\\alpha_2\\mathbb{1}(y_1 \\ne h_2(x_1))} = 0.17 e^{1.586\\times 0} = 0.17\\\\\nw_4^{2} &= w_4^2 e^{\\alpha_2\\mathbb{1}(y_1 \\ne h_2(x_1))} = 0.17 e^{1.586\\times 0} = 0.17\\\\\n\\end{align*}\\]\nSo, \\(w=[0.83,0.5,0.17,0.17]\\) and after normalizing \\(w=[0.50,0.3,0.10,0.10]\\). The final ensemble model combines the weak learners using their weights (\\(\\alpha\\)):\n\\[\nF(x) = \\text{sign}\\left(\\alpha_1 \\cdot h_1(x) + \\alpha_2 \\cdot h_2(x)\\right)\n\\]\nFor the toy dataset:\n\n\\(\\alpha_1 = 1.0986\\), \\(h_1(x) = [1, -1, -1, -1]\\)\n\\(\\alpha_2 = 1.586\\), \\(h_2(x) = [-1, 1, -1, -1]\\)\n\nWeighted predictions:\n\\[\\begin{align*}\nF(x) &= \\left(\\alpha_1 \\cdot h_1(x) + \\alpha_2 \\cdot h_2(x)\\right)\\\\\n& = [1.0986-1.586,-1.0986+1.586,-1.0986-1.586,-1.0986-1.586]\\\\\n& = [-1, 1, -1, -1]\n\\end{align*}\\]\nIf we keep iterating this way, we will have\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np \nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\n\n# Data points for visualization\niterations = [1, 2]\nerrors = [0.25, 0.167]  # Errors from the two iterations\nalphas = [1.0968, 1.586]  # Alpha values for the weak learners\n\n# Extend to further iterations\n# Simulating error reduction and alpha calculation for a few more iterations\nfor i in range(3, 6):  # Iterations 3 to 5\n    new_error = errors[-1] * 0.7  # Simulating decreasing errors\n    errors.append(new_error)\n    alphas.append( np.log((1 - new_error) / new_error))\n    iterations.append(i)\n\n# Plot weighted errors over iterations\nplt.figure(figsize=(8, 6))\nplt.plot(iterations, errors, marker='o')\nplt.title(\"Weighted Error Reduction Over Iterations in AdaBoost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Weighted Error\")\nplt.grid()\nplt.show()\n\n# Plot alpha values (importance of weak learners)\nplt.figure(figsize=(8, 6))\nplt.plot(iterations, alphas, marker='o', color='orange')\nplt.title(\"Alpha Values Over Iterations in AdaBoost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Alpha (Learner Weight)\")\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "dsandml/adaboost/index.html#adaptive-boosting-adaboost",
    "href": "dsandml/adaboost/index.html#adaptive-boosting-adaboost",
    "title": "Boosting Algorithm: Adaptive Boosting Method (AdaBoost)",
    "section": "Adaptive Boosting (AdaBoost)",
    "text": "Adaptive Boosting (AdaBoost)\n\nAdaptive Boosting, or AdaBoost, is one of the earliest and most widely used boosting algorithms. It was introduced by Freund and Schapire in 1996. AdaBoost combines weak learners, typically decision stumps (single-level decision trees), to form a strong learner.\n\n\nHow AdaBoost Works:\n\nInitialization:\n\nAssign an initial weight \\(w_i^{(1)}\\) to each training instance \\(i\\), where \\(w_i^{(1)} = \\frac{1}{N}\\), and \\(N\\) is the total number of training examples.\nAll instances start with equal weights.\n\nIterative Model Training:\n\nAt each iteration \\(t\\), train a weak learner \\(h_t(x)\\) on the weighted dataset.\nCompute the weighted error rate \\(\\epsilon_t\\): \\[\n\\epsilon_t = \\frac{\\sum_{i=1}^N w_i^{(t)} \\cdot \\mathbb{1}(y_i \\neq h_t(x_i))}{\\sum_{i=1}^N w_i^{(t)}}\n\\]\nHere, \\(\\mathbb{1}\\) is an indicator function that equals 1 when the prediction is incorrect and 0 otherwise.\nCalculate the model’s weight \\(\\alpha_t\\): \\[\n\\alpha_t = \\frac{1}{2} \\ln \\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)\n\\] The weight \\(\\alpha_t\\) determines the importance of \\(h_t(x)\\) in the final ensemble. Models with lower error rates receive higher weights.\n\nUpdate Weights:\n\nAdjust the weights of the training instances: \\[\nw_i^{(t+1)} = w_i^{(t)} \\cdot \\exp\\left(-\\alpha_t \\cdot y_i \\cdot h_t(x_i)\\right)\n\\] Instances misclassified by \\(h_t(x)\\) have their weights increased, making them more influential in the next iteration.\nNormalize the weights to ensure they sum to 1: \\[\nw_i^{(t+1)} \\leftarrow \\frac{w_i^{(t+1)}}{\\sum_{j=1}^N w_j^{(t+1)}}\n\\]\n\nFinal Model:\n\nCombine the weak learners into a final strong learner: \\[\nH(x) = \\text{sign}\\left(\\sum_{t=1}^T \\alpha_t \\cdot h_t(x)\\right)\n\\] The sign function determines the final class label based on the weighted sum of weak learners’ predictions.\n\n\n\n\n\nMathematical Derivation and Explanation of AdaBoost\nThe core idea of AdaBoost is to minimize an exponential loss function: \\[\n\\mathcal{L} = \\sum_{i=1}^N \\exp\\left(-y_i \\cdot F(x_i)\\right),\n\\] where \\(F(x_i)\\) is the weighted combination of weak learners: \\[\nF(x_i) = \\sum_{t=1}^T \\alpha_t \\cdot h_t(x_i).\n\\]\n\n1. Weight Update Rule:\n\nThe exponential term \\(\\exp(-y_i \\cdot F(x_i))\\) increases for misclassified instances (\\(y_i \\neq h_t(x_i)\\)), assigning them more weight in subsequent iterations.\n\n\n\n2. Optimal \\(\\alpha_t\\):\n\nThe weight \\(\\alpha_t\\) is derived to minimize the weighted error \\(\\epsilon_t\\). A higher value of \\(\\alpha_t\\) corresponds to a weak learner with better performance.\n\n\n\n3. Boosting as Gradient Descent:\n\nAdaBoost can be interpreted as a stage-wise optimization of the exponential loss function. Each iteration reduces the overall loss by focusing on misclassified examples.\n\n\n\n\n\nAdvantages of AdaBoost:\n\nSimplicity: Easy to implement with weak learners like decision stumps.\nNo Parameter Tuning: AdaBoost has fewer hyperparameters to tune compared to other boosting methods.\nVersatility: Works well with various types of data and weak learners.\n\n\n\nLimitations of AdaBoost:\n\nSensitivity to Noise: Outliers can receive disproportionately high weights, leading to overfitting.\nWeak Learner Dependency: The performance heavily depends on the choice of the weak learner.\n\n\n\n\nClosing Thoughts\nAdaBoost is a foundational method that inspired more advanced boosting algorithms like Gradient Boosting and XGBoost. While AdaBoost excels in simplicity and interpretability, other methods address its limitations and enhance performance on larger datasets.\nIn the next post, I will delve into Gradient Boosting, exploring its mechanics, mathematical foundation, and how it builds on the ideas of AdaBoost to improve predictive modeling. Stay tuned!"
  },
  {
    "objectID": "dsandml/adaboost/index.html#adaptive-boosting-adaboost-algorithm",
    "href": "dsandml/adaboost/index.html#adaptive-boosting-adaboost-algorithm",
    "title": "Boosting Algorithm: Adaptive Boosting Method (AdaBoost)",
    "section": "Adaptive Boosting (AdaBoost) Algorithm",
    "text": "Adaptive Boosting (AdaBoost) Algorithm\n\nNow it’s time to write the formal algorithm for Adaptive Boosting or AdaBoost method. It is one of the earliest and most widely used boosting algorithms. It was introduced by Freund and Schapire in 1996. AdaBoost combines weak learners, typically decision stumps (single-level decision trees), to form a strong learner.\n\n\n\n\n\n\n\nAlgorithm: AdaBoost\n\n\n\n\n1. Initialize the observation weights \\(w_i=\\frac{1}{N}\\) for \\(i=1,2,\\cdots, N\\)  2. For \\(m=1\\) to \\(M\\):    (a) Fit a classifier \\(G_m(x)\\) to the training data using weights \\(w_i\\)    (b) Compute     \\[err_m=\\frac{\\sum_{i=1}^{N}w_i\\mathbb{1}(y_i\\ne G_m(x_i))}{\\sum_{i=1}^{N}w_i}\\]   (c) Compute \\(\\alpha_m = \\log\\left(\\frac{1-err_m}{err_m}\\right)\\)  (d) Set \\(w_i \\rightarrow w_i\\cdot \\exp{\\left[\\alpha_m\\cdot\\mathbb{1}(y_i\\ne G_m(x_i))\\right]},\\hspace{2mm} i=1,2,\\cdots, N\\) 3. Output \\(G(x)=\\text{sign}\\left[\\sum_{m=1}^{M}\\alpha_mG_m(x)\\right]\\)\n\n\n\nIn the next posts, we will continue discussing on this algorithm, specially the loss function, optimization techniques, advantages and limitations of AdaBoost, and many other facts about this algorithm.\nThanks for reading this."
  },
  {
    "objectID": "dsandml/adaboost/index.html#reference",
    "href": "dsandml/adaboost/index.html#reference",
    "title": "Boosting Algorithm: Adaptive Boosting Method (AdaBoost)",
    "section": "Reference",
    "text": "Reference\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. “The elements of statistical learning: data mining, inference, and prediction.” (2017).\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "publication/pub3/index.html",
    "href": "publication/pub3/index.html",
    "title": "Generalized EXTRA stochastic gradient Langevin dynamics",
    "section": "",
    "text": "Langevin algorithms are popular Markov Chain Monte Carlo methods for Bayesian learning, particularly when the aim is to sample from the posterior distribution of a parametric model, given the input data and the prior distribution over the model parameters. Their stochastic versions such as stochastic gradient Langevin dynamics (SGLD) allow iterative learning based on randomly sampled mini-batches of large datasets and are scalable to large datasets. However, when data is decentralized across a network of agents subject to communication and privacy constraints, standard SGLD algorithms cannot be applied. Instead, we employ decentralized SGLD (DE-SGLD) algorithms, where Bayesian learning is performed collaboratively by a network of agents without sharing individual data. Nonetheless, existing DE-SGLD algorithms induce a bias at every agent that can negatively impact performance; this bias persists even when using full batches and is attributable to network effects. Motivated by the EXTRA algorithm and its generalizations for decentralized optimization, we propose the generalized EXTRA stochastic gradient Langevin dynamics, which eliminates this bias in the full-batch setting. Moreover, we show that, in the mini-batch setting, our algorithm provides performance bounds that significantly improve upon those of standard DE-SGLD algorithms in the literature. Our numerical results also demonstrate the efficiency of the proposed approach.\n\nDownload the pre-print from arXiv.org\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@article{gurbuzbalaban2024,\n  author = {Gurbuzbalaban, Mert and Rafiqul Islam, Mohammad and Wang,\n    Xiaoyu and Zhu, Lingjiong},\n  title = {Generalized {EXTRA} Stochastic Gradient {Langevin} Dynamics},\n  journal = {arXiv preprint},\n  date = {2024-12-02},\n  url = {https://mrislambd.github.io/publication/pub3/},\n  doi = {10.48550/arXiv.2412.01993},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGurbuzbalaban, Mert, Mohammad Rafiqul Islam, Xiaoyu Wang, and Lingjiong\nZhu. 2024. “Generalized EXTRA Stochastic Gradient Langevin\nDynamics.” arXiv Preprint, December. https://doi.org/10.48550/arXiv.2412.01993."
  },
  {
    "objectID": "publication/pub2/index.html",
    "href": "publication/pub2/index.html",
    "title": "GJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies",
    "section": "",
    "text": "Cryptocurrencies are currently traded worldwide, with hundreds of different currencies in existence and even more on the way. This study implements some statistical and machine learning approaches for cryptocurrency investments. First, we implement GJR-GARCH over the GARCH model to estimate the volatility of ten popular cryptocurrencies based on market capitalization: Bitcoin, Bitcoin Cash, Bitcoin SV, Chainlink, EOS, Ethereum, Litecoin, TETHER, Tezos, and XRP. Then, we use Monte Carlo simulations to generate the conditional variance of the cryptocurrencies using the GJR-GARCH model, and calculate the value at risk (VaR) of the simulations. We also estimate the tail-risk using VaR backtesting. Finally, we use an artificial neural network (ANN) for predicting the prices of the ten cryptocurrencies. The graphical analysis and mean square errors (MSEs) from the ANN models confirmed that the predicted prices are close to the market prices. For some cryptocurrencies, the ANN models perform better than traditional ARIMA models.\n\n\n    \n        \n    \n    \n        \n    \n\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@article{mostafa2021,\n  author = {Mostafa, Fahad and Saha, Pritam and Rafiqul Islam, Mohammad\n    and Nguyen, Nguyet},\n  title = {GJR-GARCH {Volatility} {Modeling} Under {NIG} and {ANN} for\n    {Predicting} {Top} {Cryptocurrencies}},\n  journal = {Journal of Risk and Financial Management},\n  date = {2021-09-03},\n  url = {https://mrislambd.github.io/publication/pub2/},\n  doi = {10.3390/jrfm14090421},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMostafa, Fahad, Pritam Saha, Mohammad Rafiqul Islam, and Nguyet Nguyen.\n2021. “GJR-GARCH Volatility Modeling Under NIG and ANN for\nPredicting Top Cryptocurrencies.” Journal of Risk and\nFinancial Management, September. https://doi.org/10.3390/jrfm14090421."
  },
  {
    "objectID": "jobandintern/optionprice/index.html",
    "href": "jobandintern/optionprice/index.html",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "",
    "text": "In this blog, we will explore how to price simple equity derivatives using the Black-Scholes-Merton (BSM) model. We will derive the mathematical formula and then provide Python code to implement it.\n\n\nBefore proceeding to the deep of the discussion, we need to know some definition and terminology\n\nBrownian Motion: Brownian motion is a concept with definitions and applications across various disciplines, named after the botanist Robert Brown, is the random, erratic movement of particles suspended in a fluid (liquid or gas) due to their collisions with the fast-moving molecules of the fluid.\n\nBrownian motion is a stochastic process \\((B_t)_{t \\geq 0}\\) defined as a continuous-time process with the following properties:\n\n\\(B_0 = 0\\) almost surely.\n\\(B_t\\) has independent increments.\nFor \\(t &gt; s\\), \\(B_t - B_s \\sim N(0, t-s)\\) (normally distributed with mean 0 and variance \\(t-s\\)).\n\\(B_t\\) has continuous paths almost surely.\n\n\n\nCode\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nn_steps = 100  # Number of steps\nn_paths = 20   # Number of paths\ntime_horizon = 1  # Total time\ndt = time_horizon / n_steps  # Time step\nt = np.linspace(0, time_horizon, n_steps)  # Time array\n\n# Generate Brownian motion\ndef generate_brownian_paths(n_paths, n_steps, dt):\n    # Standard normal increments scaled by sqrt(dt)\n    increments = np.random.normal(0, np.sqrt(dt), (n_paths, n_steps))\n    # Cumulative sum to generate paths\n    return np.cumsum(increments, axis=1)\n\n# Generate one path and multiple paths\nsingle_path = generate_brownian_paths(1, n_steps, dt)[0]\nmultiple_paths = generate_brownian_paths(n_paths, n_steps, dt)\n\n# Plotting\nfig, axes = plt.subplots(1, 2, figsize=(7.9, 3.9))\n\n# Single path\naxes[0].plot(t, single_path, label=\"Single Path\")\naxes[0].set_title(\"Brownian Motion: Single Path\")\naxes[0].set_xlabel(\"Time\")\naxes[0].set_ylabel(\"Position\")\naxes[0].legend()\n\n# Multiple paths\nfor path in multiple_paths:\n    axes[1].plot(t, path, alpha=0.5, linewidth=0.8)\naxes[1].set_title(f\"Brownian Motion: {n_paths} Paths\")\naxes[1].set_xlabel(\"Time\")\naxes[1].set_ylabel(\"Position\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nGeometric Brownian Motion (GBM)\nA stochastic process \\(S_t\\) is said to follow a geometric Brownian motion if it satisfies the following equation:\n\\[\ndS_t = \\mu S_t dt+\\sigma S_t dB_t\n\\]\nWhich can be written as\n\\[\nS_t - S_0 =\\int_0^t \\mu S_u du + \\int_0^t \\sigma S_u dB_u\n\\]\nTo solve the GBM, we apply Ito’s formula to the function \\(Z_t = f(t, S_t)= \\ln(S_t)\\) and then by Taylor’s expansion, we have\n\\[\\begin{align*}\ndf & = \\frac{\\partial f}{\\partial t}dt+ \\frac{\\partial f}{\\partial s}dS_t + \\frac{1}{2} \\frac{\\partial ^2f}{\\partial s^2}(dS_t)^2+\\frac{1}{2}\\frac{\\partial ^2f}{\\partial s^2}(dt)^2+\\frac{\\partial^2 f}{\\partial t\\partial s}dtdS_t\n\\end{align*}\\]\nBy definition we have \\[\\begin{align*}\ndS_t &= \\mu S_t dt+\\sigma S_t dB_t\\\\\n(dS_t)^2 & = \\mu^2 (dt)^2+2\\mu \\sigma dt dB_t + \\sigma^2 (dB_t)^2\n\\end{align*}\\]\nThe term \\((dt)^2\\) is negligible compared to the term \\(dt\\) and it is also assume that the product \\(dtdB_t\\) is negligible. Furthermore, the quadratic variation of \\(B_t\\) i.e., \\((dB_t)^2= dt\\). With these values, we obtain\n\\[\\begin{align*}\ndZ_t & = \\frac{1}{S_t} dS_t + \\frac{1}{2} \\left\\{-\\frac{1}{S_t^2}\\right\\}[dS_t]^2\\\\\n& =  \\frac{1}{S_t} (\\mu S_t dt+\\sigma S_t dB_t) + \\frac{1}{2} \\left\\{-\\frac{1}{S_t^2}\\right\\}\\sigma^2S_t^2dt\\\\\n\\implies dZ_t &= (\\mu dt +\\sigma dB_t) -\\frac{1}{2}\\sigma^2 dt\\\\\n& = \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)dt+\\sigma dB_t\n\\end{align*}\\]\nwith \\(Z_0=\\ln S_0\\). Now we have the following\n\\[\\begin{align*}\n\\int_0^t dZ_s &= \\int_0^t \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)ds + \\int_0^t\\sigma dB_s\\\\\n\\implies Z_t - Z_0 &= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies \\ln S_t - \\ln S_0&= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies \\ln \\left(\\frac{S_t}{S_0}\\right) &= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies S_t &= S_0 \\exp{\\left\\{\\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\right\\}}\n\\end{align*}\\]"
  },
  {
    "objectID": "jobandintern/optionprice/index.html#introduction",
    "href": "jobandintern/optionprice/index.html#introduction",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "",
    "text": "In this blog, we will explore how to price simple equity derivatives using the Black-Scholes-Merton (BSM) model. We will derive the mathematical formula and then provide Python code to implement it.\n\n\nBefore proceeding to the deep of the discussion, we need to know some definition and terminology\n\nBrownian Motion: Brownian motion is a concept with definitions and applications across various disciplines, named after the botanist Robert Brown, is the random, erratic movement of particles suspended in a fluid (liquid or gas) due to their collisions with the fast-moving molecules of the fluid.\n\nBrownian motion is a stochastic process \\((B_t)_{t \\geq 0}\\) defined as a continuous-time process with the following properties:\n\n\\(B_0 = 0\\) almost surely.\n\\(B_t\\) has independent increments.\nFor \\(t &gt; s\\), \\(B_t - B_s \\sim N(0, t-s)\\) (normally distributed with mean 0 and variance \\(t-s\\)).\n\\(B_t\\) has continuous paths almost surely.\n\n\n\nCode\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nn_steps = 100  # Number of steps\nn_paths = 20   # Number of paths\ntime_horizon = 1  # Total time\ndt = time_horizon / n_steps  # Time step\nt = np.linspace(0, time_horizon, n_steps)  # Time array\n\n# Generate Brownian motion\ndef generate_brownian_paths(n_paths, n_steps, dt):\n    # Standard normal increments scaled by sqrt(dt)\n    increments = np.random.normal(0, np.sqrt(dt), (n_paths, n_steps))\n    # Cumulative sum to generate paths\n    return np.cumsum(increments, axis=1)\n\n# Generate one path and multiple paths\nsingle_path = generate_brownian_paths(1, n_steps, dt)[0]\nmultiple_paths = generate_brownian_paths(n_paths, n_steps, dt)\n\n# Plotting\nfig, axes = plt.subplots(1, 2, figsize=(7.9, 3.9))\n\n# Single path\naxes[0].plot(t, single_path, label=\"Single Path\")\naxes[0].set_title(\"Brownian Motion: Single Path\")\naxes[0].set_xlabel(\"Time\")\naxes[0].set_ylabel(\"Position\")\naxes[0].legend()\n\n# Multiple paths\nfor path in multiple_paths:\n    axes[1].plot(t, path, alpha=0.5, linewidth=0.8)\naxes[1].set_title(f\"Brownian Motion: {n_paths} Paths\")\naxes[1].set_xlabel(\"Time\")\naxes[1].set_ylabel(\"Position\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nGeometric Brownian Motion (GBM)\nA stochastic process \\(S_t\\) is said to follow a geometric Brownian motion if it satisfies the following equation:\n\\[\ndS_t = \\mu S_t dt+\\sigma S_t dB_t\n\\]\nWhich can be written as\n\\[\nS_t - S_0 =\\int_0^t \\mu S_u du + \\int_0^t \\sigma S_u dB_u\n\\]\nTo solve the GBM, we apply Ito’s formula to the function \\(Z_t = f(t, S_t)= \\ln(S_t)\\) and then by Taylor’s expansion, we have\n\\[\\begin{align*}\ndf & = \\frac{\\partial f}{\\partial t}dt+ \\frac{\\partial f}{\\partial s}dS_t + \\frac{1}{2} \\frac{\\partial ^2f}{\\partial s^2}(dS_t)^2+\\frac{1}{2}\\frac{\\partial ^2f}{\\partial s^2}(dt)^2+\\frac{\\partial^2 f}{\\partial t\\partial s}dtdS_t\n\\end{align*}\\]\nBy definition we have \\[\\begin{align*}\ndS_t &= \\mu S_t dt+\\sigma S_t dB_t\\\\\n(dS_t)^2 & = \\mu^2 (dt)^2+2\\mu \\sigma dt dB_t + \\sigma^2 (dB_t)^2\n\\end{align*}\\]\nThe term \\((dt)^2\\) is negligible compared to the term \\(dt\\) and it is also assume that the product \\(dtdB_t\\) is negligible. Furthermore, the quadratic variation of \\(B_t\\) i.e., \\((dB_t)^2= dt\\). With these values, we obtain\n\\[\\begin{align*}\ndZ_t & = \\frac{1}{S_t} dS_t + \\frac{1}{2} \\left\\{-\\frac{1}{S_t^2}\\right\\}[dS_t]^2\\\\\n& =  \\frac{1}{S_t} (\\mu S_t dt+\\sigma S_t dB_t) + \\frac{1}{2} \\left\\{-\\frac{1}{S_t^2}\\right\\}\\sigma^2S_t^2dt\\\\\n\\implies dZ_t &= (\\mu dt +\\sigma dB_t) -\\frac{1}{2}\\sigma^2 dt\\\\\n& = \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)dt+\\sigma dB_t\n\\end{align*}\\]\nwith \\(Z_0=\\ln S_0\\). Now we have the following\n\\[\\begin{align*}\n\\int_0^t dZ_s &= \\int_0^t \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)ds + \\int_0^t\\sigma dB_s\\\\\n\\implies Z_t - Z_0 &= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies \\ln S_t - \\ln S_0&= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies \\ln \\left(\\frac{S_t}{S_0}\\right) &= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies S_t &= S_0 \\exp{\\left\\{\\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\right\\}}\n\\end{align*}\\]"
  },
  {
    "objectID": "jobandintern/optionprice/index.html#black-scholes-merton-formula",
    "href": "jobandintern/optionprice/index.html#black-scholes-merton-formula",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "Black-Scholes-Merton Formula",
    "text": "Black-Scholes-Merton Formula\nNow we are ready to derive the BSM PDE. The payoff of an option \\(V(S,T)\\) at maturity is is known. To find the value at an earlier stage, we need to know how V behaves as a function of \\(S\\) and \\(t\\). By Ito’s lemma we have \\[\\begin{align*}\ndV& = \\left(\\mu S\\frac{\\partial V}{\\partial S}+\\frac{\\partial V}{\\partial t}+\\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2 V}{\\partial S^2}\\right)dt+\\sigma S\\frac{\\partial V}{\\partial S}dB.\n\\end{align*}\\]\nNow let’s consider a portfolio consisting of a short one option and long \\(\\frac{\\partial V}{\\partial S}\\) shares at time \\(t\\). The value of this portfolio is\n\\[\n\\Pi = -V+\\frac{\\partial V}{\\partial S}S\n\\]\nover the time \\([t,t+\\Delta t]\\), the total profit or loss from the changes in the values of the portfolio is \\[\n\\Delta \\Pi = -\\Delta V + \\frac{\\partial V}{\\partial S}\\Delta S\n\\]\nNow by the discretization we have, \\[\\begin{align*}\n\\Delta S &= \\mu S \\Delta t +\\sigma S \\Delta B\\\\\n\\Delta V & = \\left(\\mu S\\frac{\\partial V}{\\partial S}+\\frac{\\partial V}{\\partial t}+\\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2 V}{\\partial S^2}\\right)\\Delta t+\\sigma S\\frac{\\partial V}{\\partial S}\\Delta B\\\\\n\\implies \\Delta \\Pi  & = \\left(-\\frac{\\partial V}{\\partial t} -\\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2 V}{\\partial S^2}\\right)\\Delta t\n\\end{align*}\\]\nAt this point, if \\(r\\) is the risk-free interest rate then we will have following relationship \\[\nr\\Pi \\Delta t = \\Delta \\Pi\n\\]\nThe rationale of this relation is that no-aribtrage assumption. Thus, we have\n\\[\\begin{align*}\n\\left(-\\frac{\\partial V}{\\partial t} -\\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2 V}{\\partial S^2}\\right)\\Delta t & = r \\left(- V + \\frac{\\partial V}{\\partial S} S\\right)\\Delta t \\\\\n\\implies \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} + rS \\frac{\\partial V}{\\partial S} -rV &=0\n\\end{align*}\\]\nThis is the famous Black-Scholes-Merton PDF, formally written with the boundary conditions as follows\n\\[\\begin{align*}\n\\frac{\\partial c}{\\partial t} + \\frac{1}{2} \\sigma^2 c^2 \\frac{\\partial^2 c}{\\partial S^2} + rc \\frac{\\partial c}{\\partial S} -rc &=0\\\\\nc(0,t) &= 0\\\\\nc(S_{+\\infty}, t) &= S - Ke^{-r(T-t)}\\\\\nc(S,T) & = max\\{S-K,0\\}\n\\end{align*}\\]\nThis Black-Scholes-Merton PDE can be reduced to the heat equation using the substitutions \\(S = K e^x\\), \\(t = T - \\frac{\\tau}{\\frac{1}{2} \\sigma^2}\\), and \\(c(S, t) = K v(x, \\tau)\\). Let’s derive the solution step by step in full mathematical detail and show how this leads to the normal CDF.\n\nStep 1: Substitutions\nWe aim to reduce the BSM PDE: \\[\n\\frac{\\partial c}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 c}{\\partial S^2} + r S \\frac{\\partial c}{\\partial S} - r c = 0\n\\]\nto the heat equation. Using the substitutions:\n\n\\(S = K e^x\\), where \\(x = \\ln(S / K)\\), and \\(S \\in (0, \\infty)\\) maps \\(x \\in (-\\infty, \\infty)\\),\n\\(t = T - \\frac{\\tau}{\\frac{1}{2} \\sigma^2}\\), so \\(\\tau = \\frac{1}{2} \\sigma^2 (T - t)\\),\n\\(c(S, t) = K v(x, \\tau)\\), where \\(v(x, \\tau)\\) is the transformed function.\n\n\n\nStep 2: Derivative Transformations\nFor \\(c(S, t) = K v(x, \\tau)\\), we compute derivatives.\n\nThe first derivative of \\(c\\) with respect to \\(S\\): \\[\n\\frac{\\partial c}{\\partial S} = \\frac{\\partial}{\\partial S} \\big(K v(x, \\tau)\\big) = K \\frac{\\partial v}{\\partial x} \\frac{\\partial x}{\\partial S},\n\\] where \\(x = \\ln(S / K)\\) implies \\(\\frac{\\partial x}{\\partial S} = \\frac{1}{S}\\). Thus: \\[\n\\frac{\\partial c}{\\partial S} = K \\frac{\\partial v}{\\partial x} \\frac{1}{S}.\n\\]\nThe second derivative of \\(c\\) with respect to \\(S\\): \\[\n\\frac{\\partial^2 c}{\\partial S^2} = \\frac{\\partial}{\\partial S} \\left( K \\frac{\\partial v}{\\partial x} \\frac{1}{S} \\right).\n\\] Using the product rule: \\[\n\\frac{\\partial^2 c}{\\partial S^2} = K \\frac{\\partial^2 v}{\\partial x^2} \\frac{1}{S^2} - K \\frac{\\partial v}{\\partial x} \\frac{1}{S^2}.\n\\]\nThe time derivative: \\[\n\\frac{\\partial c}{\\partial t} = K \\frac{\\partial v}{\\partial \\tau} \\frac{\\partial \\tau}{\\partial t}, \\quad \\text{and } \\frac{\\partial \\tau}{\\partial t} = -\\frac{1}{\\frac{1}{2} \\sigma^2}.\n\\]\n\n\n\nStep 3: Transforming the PDE\nSubstituting the above derivatives into the BSM PDE, we rewrite each term.\n\nFor \\(\\frac{\\partial c}{\\partial t}\\): \\[\n\\frac{\\partial c}{\\partial t} = -\\frac{1}{\\frac{1}{2} \\sigma^2} K \\frac{\\partial v}{\\partial \\tau}.\n\\]\nFor \\(\\frac{\\partial c}{\\partial S}\\): \\[\nS \\frac{\\partial c}{\\partial S} = S \\cdot \\left(K \\frac{\\partial v}{\\partial x} \\frac{1}{S}\\right) = K \\frac{\\partial v}{\\partial x}.\n\\]\nFor \\(\\frac{\\partial^2 c}{\\partial S^2}\\): \\[\n\\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 c}{\\partial S^2} = \\frac{1}{2} \\sigma^2 S^2 \\left(K \\frac{\\partial^2 v}{\\partial x^2} \\frac{1}{S^2} - K \\frac{\\partial v}{\\partial x} \\frac{1}{S^2}\\right) = \\frac{1}{2} \\sigma^2 K \\frac{\\partial^2 v}{\\partial x^2}.\n\\]\n\nSubstituting all these into the BSM PDE: \\[\n-\\frac{1}{\\frac{1}{2} \\sigma^2} K \\frac{\\partial v}{\\partial \\tau} + \\frac{1}{2} \\sigma^2 K \\frac{\\partial^2 v}{\\partial x^2} + r K \\frac{\\partial v}{\\partial x} - r K v = 0.\n\\]\nDivide through by \\(K\\): \\[\n-\\frac{\\partial v}{\\partial \\tau} + \\frac{\\partial^2 v}{\\partial x^2} + \\frac{2r}{\\sigma^2} \\frac{\\partial v}{\\partial x} - \\frac{2r}{\\sigma^2} v = 0.\n\\]\nTo simplify, let \\(v(x, \\tau) = e^{\\alpha x + \\beta \\tau} u(x, \\tau)\\), where \\(\\alpha\\) and \\(\\beta\\) are constants. Substituting and choosing \\(\\alpha = -\\frac{r}{\\sigma^2}\\) and \\(\\beta = -\\frac{r^2}{2 \\sigma^2}\\), the equation reduces to: \\[\n\\frac{\\partial u}{\\partial \\tau} = \\frac{\\partial^2 u}{\\partial x^2}.\n\\]\n\n\nStep 4: Solving the Heat Equation\nThe heat equation \\(\\frac{\\partial u}{\\partial \\tau} = \\frac{\\partial^2 u}{\\partial x^2}\\) has a well-known solution using Fourier methods: \\[\nu(x, \\tau) = \\frac{1}{\\sqrt{2 \\pi \\tau}} \\int_{-\\infty}^\\infty e^{-\\frac{(x-y)^2}{2\\tau}} f(y) \\, dy,\n\\]\nwhere \\(f(y)\\) is the initial condition.\nFor the BSM problem, the initial condition is the payoff: \\[\nf(y) = \\max(e^y - 1, 0).\n\\]\nPerforming the integration leads to the final solution involving the cumulative normal distribution function: \\[\nv(x, \\tau) = N(d_1) - e^{-x} N(d_2),\n\\]\nwhere: \\[\nd_1 = \\frac{x + \\frac{1}{2} \\tau}{\\sqrt{\\tau}}, \\quad d_2 = \\frac{x - \\frac{1}{2} \\tau}{\\sqrt{\\tau}}.\n\\]\nTransforming back to the original variables gives the Black-Scholes formula: \\[\nC(S, t) = S e^{-q(T-t)} N(d_1) - K e^{-r(T-t)} N(d_2),\n\\] where: \\[\nd_1 = \\frac{\\ln(S / K) + (r - q + \\frac{\\sigma^2}{2})(T-t)}{\\sigma \\sqrt{T-t}}, \\quad d_2 = d_1 - \\sigma \\sqrt{T-t}.\n\\]\nSimilarly, we can derive the price of a European put option:\n\\[\nP = K e^{-rT} N(-d_2) - S e^{-qT} N(-d_1)\n\\]\nWhere: \\[\nd_1 = \\frac{\\ln(\\frac{S}{K}) + (r - q + \\frac{\\sigma^2}{2})T}{\\sigma \\sqrt{T}}, \\quad d_2 = d_1 - \\sigma \\sqrt{T}\n\\]\n\n\nAsymptotic Behavior of the BSM formula for call and put options\nWhat if \\(K\\rightarrow 0\\)? In that case,\n\n\\(\\ln(S_0/K)\\rightarrow \\infty\\), causing \\(d_1 \\rightarrow \\infty\\) and \\(d_2 \\rightarrow \\infty\\)\n\nThe cdf \\(N(d_1)\\rightarrow 1\\) and \\(N(d_2)\\rightarrow 1\\)\n\nThe second term \\(Ke^{-rT}N(d_2)\\rightarrow 0\\) as \\(K\\rightarrow 0\\)\n\nIn this case, the price of a call option \\(C\\rightarrow S_0\\) and the price of a put option \\(P \\rightarrow 0\\)"
  },
  {
    "objectID": "jobandintern/optionprice/index.html#greeks-delta-and-gamma",
    "href": "jobandintern/optionprice/index.html#greeks-delta-and-gamma",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "Greeks: Delta and Gamma",
    "text": "Greeks: Delta and Gamma\nDelta (\\(\\Delta\\)) is the sensitivity of the option price to changes in the underlying asset price:\n\\[\n\\Delta = \\frac{\\partial C}{\\partial S}\\approx \\frac{C(S_0 + h) - C(S_0 - h)}{2h}\n\\]\nThis is the central difference approximation, which provides a more accurate estimate of delta compared to the forward or backward difference methods.\n\n\\(C(S_0 + h)\\): Calculate the option price with the spot price increased by \\(h\\).\n\\(C(S_0 - h)\\): Calculate the option price with the spot price decreased by \\(h\\).\n\nGamma (\\(\\Gamma\\)) measures the rate of change of delta with respect to the underlying asset price:\n\\[\n\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}\\approx \\frac{\\Delta(S_0 + h) - \\Delta(S_0 - h)}{2h}\\approx \\frac{C(S_0 + h) - 2C(S_0) + C(S_0 - h)}{h^2}\n\\]\nGamma (\\(\\Gamma\\)) measures the rate of change of delta (\\(\\Delta\\)) with respect to the underlying spot price (\\(S_0\\)).\n\n\\(C(S_0 + h)\\): Option price with the spot price increased by \\(h\\).\n\\(C(S_0)\\): Option price at the current spot price.\n\\(C(S_0 - h)\\): Option price with the spot price decreased by \\(h\\).\n\nRelationship Between Delta and Gamma:\n\nGamma represents how much delta changes for a small change in \\(S_0\\).\nIf gamma is high, delta is more sensitive to changes in \\(S_0\\), which is important for hedging strategies."
  },
  {
    "objectID": "jobandintern/optionprice/index.html#implementation",
    "href": "jobandintern/optionprice/index.html#implementation",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "Implementation",
    "text": "Implementation\n\nNotation\n\n\\(S\\): Spot price of the stock.\n\\(K\\): Strike price of the option.\n\\(T\\): Time to maturity (in years).\n\\(r\\): Risk-free rate (continuously compounded).\n\\(q\\): Dividend yield (continuously compounded).\n\\(\\sigma\\): Volatility of the stock.\n\\(N(\\cdot)\\): Cumulative distribution function of the standard normal distribution.\n\n\nfrom dataclasses import dataclass\nimport numpy as np\nfrom scipy.stats import norm\n\n@dataclass\nclass Equity:\n    spot: float\n    dividend_yield: float\n    volatility: float\n\n@dataclass\nclass EquityOption:\n    strike: float\n    time_to_maturity: float\n    put_call: str\n\n@dataclass\nclass EquityForward:\n    strike: float\n    time_to_maturity: float\n\ndef bsm(underlying: Equity, option: EquityOption, rate: float) -&gt; float:\n    S = underlying.spot\n    K = option.strike\n    T = option.time_to_maturity\n    r = rate\n    q = underlying.dividend_yield\n    sigma = underlying.volatility\n\n    # Handle edge case where strike is effectively zero\n    if K &lt; 1e-8:\n        if option.put_call.lower() == \"call\":\n            return S \n        else:\n            return 0.0\n\n    d1 = (np.log(S / K) + (r - q + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n\n    if option.put_call.lower() == \"call\":\n        price = S * np.exp(-q * T) * norm.cdf(d1) \\\n                - K * np.exp(-r * T) * norm.cdf(d2)\n    elif option.put_call.lower() == \"put\":\n        price = K * np.exp(-r * T) * norm.cdf(-d2) \\\n                - S * np.exp(-q * T) * norm.cdf(-d1)\n    else:\n        raise ValueError(\"Invalid option type. Must be 'call' or 'put'.\")\n\n    return price\n\ndef delta(underlying: Equity, option: EquityOption, rate: float) -&gt; float:\n    bump = 0.01 * underlying.spot\n    bumped_up = Equity(spot=underlying.spot + bump, \n                       dividend_yield=underlying.dividend_yield, \n                       volatility=underlying.volatility)\n    bumped_down = Equity(spot=underlying.spot - bump, \n                         dividend_yield=underlying.dividend_yield, \n                         volatility=underlying.volatility)\n    price_up = bsm(bumped_up, option, rate)\n    price_down = bsm(bumped_down, option, rate)\n    return (price_up - price_down) / (2 * bump)\n\ndef gamma(underlying: Equity, option: EquityOption, rate: float) -&gt; float:\n    bump = 0.01 * underlying.spot\n    bumped_up = Equity(spot=underlying.spot + bump, \n                       dividend_yield=underlying.dividend_yield, \n                       volatility=underlying.volatility)\n    bumped_down = Equity(spot=underlying.spot - bump, \n                         dividend_yield=underlying.dividend_yield, \n                         volatility=underlying.volatility)\n    original_price = bsm(underlying, option, rate)\n    price_up = bsm(bumped_up, option, rate)\n    price_down = bsm(bumped_down, option, rate)\n    return (price_up - 2 * original_price + price_down) / (bump**2)\n\ndef fwd(underlying: Equity, forward: EquityForward, rate: float) -&gt; float:\n    S = underlying.spot\n    K = forward.strike\n    T = forward.time_to_maturity\n    r = rate\n    q = underlying.dividend_yield\n    forward_price = S * np.exp((r - q) * T) - K\n\n    return forward_price\n\ndef check_put_call_parity(\n    underlying: Equity, \n    call_option: EquityOption, \n    put_option: EquityOption, \n    rate: float\n    ) -&gt; bool:\n\n    call_price = bsm(underlying, call_option, rate)\n    put_price = bsm(underlying, put_option, rate)\n    S = underlying.spot\n    K = call_option.strike\n    T = call_option.time_to_maturity\n    r = rate\n    q = underlying.dividend_yield\n\n    parity_lhs = call_price - put_price\n    parity_rhs = S * np.exp(-q * T) - K * np.exp(-r * T)\n\n    return np.isclose(parity_lhs, parity_rhs, atol=1e-4)\n\n\n\nExample Usage\n\nSay, we want to price a call option on an equity with spot price \\(S_0 = 450\\) with dividend yield \\(q=1.4\\%\\), and volatility \\(14\\%\\). The strike price of the call is \\(K=470\\), with time to maturity in years \\(T=0.23\\) and the risk free rate \\(r = 0.05\\).  Next, we want to see the asymptotic behavior of the call option if the strike price \\(K\\rightarrow 0\\) with interest rate 0.  Next, we want to price a put option on the same equity but strike price \\(K=500\\), time to maturity in years \\(T=0.26\\) and interest rate is 0.  Finally, we want to check if the put-call parity relationship is hold.   In each case, we consider \\(h=0.01\\) a bump or small change in the stock price.\n\n\nif __name__ == \"__main__\":\n    eq = Equity(450, 0.014, 0.14)\n    option_call = EquityOption(470, 0.23, \"call\")\n    option_put = EquityOption(500, 0.26, \"put\")\n    \n    print(bsm(eq, option_call, 0.05))  \n    print(bsm(eq, EquityOption(1e-15, 0.26, \"call\"), 0.0))  \n    print(bsm(Equity(450, 0.0, 1e-9), option_put, 0.0))  \n\n    # Check put-call parity\n    eq = Equity(450, 0.015, 0.15)\n    option_call = EquityOption(470, 0.26, \"call\")\n    option_put = EquityOption(470, 0.26, \"put\")\n    print(check_put_call_parity(eq, option_call, option_put, 0.05)) \n\n5.834035584709994\n450\n50.0\nTrue"
  },
  {
    "objectID": "jobandintern/optionprice/index.html#python-code",
    "href": "jobandintern/optionprice/index.html#python-code",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "Python Code",
    "text": "Python Code\n# Full code as provided above"
  },
  {
    "objectID": "jobandintern/optionprice/index.html#example-usage",
    "href": "jobandintern/optionprice/index.html#example-usage",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "Example Usage",
    "text": "Example Usage\n# Pricing a call option\nprint(bsm_pricer(Equity(4450, 0.015, 0.15), EquityOption(4700, 0.25, \"call\"), 0.055))\n# Output: 57.75426\n\n# Pricing a put option with zero volatility\nprint(bsm_pricer(Equity(4450, 0.0, 1e-9), EquityOption(5000, 0.25, \"put\"), 0.0))\n# Output: 550.0\nIn conclusion, the BSM model provides a robust framework for pricing European options and understanding sensitivities through Greeks.\nfrom dataclasses import dataclass import numpy as np from scipy.stats import norm\n@dataclass class Equity: spot: float dividend_yield: float volatility: float\n@dataclass class EquityOption: strike: float time_to_maturity: float put_call: str\n@dataclass class EquityForward: strike: float time_to_maturity: float\ndef bsm_pricer(underlying: Equity, option: EquityOption, rate: float) -&gt; float: S = underlying.spot K = option.strike T = option.time_to_maturity r = rate q = underlying.dividend_yield sigma = underlying.volatility\n# Handle edge case where strike is effectively zero\nif K &lt; 1e-8:\n    if option.put_call.lower() == \"call\":\n        return S * np.exp(-q * T)\n    else:\n        return 0.0\n\nd1 = (np.log(S / K) + (r - q + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\nd2 = d1 - sigma * np.sqrt(T)\n\nif option.put_call.lower() == \"call\":\n    price = S * np.exp(-q * T) * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\nelif option.put_call.lower() == \"put\":\n    price = K * np.exp(-r * T) * norm.cdf(-d2) - S * np.exp(-q * T) * norm.cdf(-d1)\nelse:\n    raise ValueError(\"Invalid option type. Must be 'call' or 'put'.\")\n\nreturn price\ndef bsm_delta(underlying: Equity, option: EquityOption, rate: float) -&gt; float: bump = 0.001 * underlying.spot bumped_up = Equity(spot=underlying.spot + bump, dividend_yield=underlying.dividend_yield, volatility=underlying.volatility) bumped_down = Equity(spot=underlying.spot - bump, dividend_yield=underlying.dividend_yield, volatility=underlying.volatility) price_up = bsm_pricer(bumped_up, option, rate) price_down = bsm_pricer(bumped_down, option, rate) return (price_up - price_down) / (2 * bump)\ndef bsm_gamma(underlying: Equity, option: EquityOption, rate: float) -&gt; float: bump = 0.001 * underlying.spot bumped_up = Equity(spot=underlying.spot + bump, dividend_yield=underlying.dividend_yield, volatility=underlying.volatility) bumped_down = Equity(spot=underlying.spot - bump, dividend_yield=underlying.dividend_yield, volatility=underlying.volatility) original_price = bsm_pricer(underlying, option, rate) price_up = bsm_pricer(bumped_up, option, rate) price_down = bsm_pricer(bumped_down, option, rate) return (price_up - 2 * original_price + price_down) / (bump**2)\ndef fwd_pricer(underlying: Equity, forward: EquityForward, rate: float) -&gt; float: S = underlying.spot K = forward.strike T = forward.time_to_maturity r = rate q = underlying.dividend_yield forward_price = S * np.exp((r - q) * T) - K return forward_price"
  },
  {
    "objectID": "jobandintern/optionprice/index.html#references",
    "href": "jobandintern/optionprice/index.html#references",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "References",
    "text": "References\n\nKaratzas, I., & Shreve, S. E. (1991). Brownian Motion and Stochastic Calculus.\n\nOptions, Futures, and Other Derivatives by John C. Hull\n\nArbitrage Theory in Continuous Time Book by Tomas Björk\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "jobandintern/correlationandregression/index.html",
    "href": "jobandintern/correlationandregression/index.html",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "",
    "text": "Correlation and regression are two fundamental concepts in statistics, often used to study relationships between variables. While correlation measures the strength and direction of a linear relationship between two variables, regression goes further by modeling the relationship to predict or explain one variable based on another. This blog explores the mathematical underpinnings of both concepts, illustrating their significance in data analysis."
  },
  {
    "objectID": "jobandintern/correlationandregression/index.html#introduction",
    "href": "jobandintern/correlationandregression/index.html#introduction",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "",
    "text": "Correlation and regression are two fundamental concepts in statistics, often used to study relationships between variables. While correlation measures the strength and direction of a linear relationship between two variables, regression goes further by modeling the relationship to predict or explain one variable based on another. This blog explores the mathematical underpinnings of both concepts, illustrating their significance in data analysis."
  },
  {
    "objectID": "jobandintern/correlationandregression/index.html#correlation",
    "href": "jobandintern/correlationandregression/index.html#correlation",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Correlation",
    "text": "Correlation\nTo better explain, we will use the following hypothetical stock data of 10 companies with stock price and their corresponding proportion in the portfolio.\n\n\nCode\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Stock': ['Apple', 'Citi', 'MS', 'WF', 'GS', 'Google', 'Amazon', 'Tesla', 'Toyota', 'SPY'],\n    'StockPrice': [2.11, 2.42, 2.52, 3.21, 3.62, 3.86, 4.13, 4.27, 4.51, 5.01], \n    'Portfolio': [2.12, 2.16, 2.51, 2.65, 3.62, 3.15, 4.32, 3.31, 4.18, 4.45]\n})\n\ndf.set_index('Stock', inplace=True)\n\ndf.T\n\n\n\n\n\n\n\n\nStock\nApple\nCiti\nMS\nWF\nGS\nGoogle\nAmazon\nTesla\nToyota\nSPY\n\n\n\n\nStockPrice\n2.11\n2.42\n2.52\n3.21\n3.62\n3.86\n4.13\n4.27\n4.51\n5.01\n\n\nPortfolio\n2.12\n2.16\n2.51\n2.65\n3.62\n3.15\n4.32\n3.31\n4.18\n4.45\n\n\n\n\n\n\n\nThe scatterplot of the data looks like this\n\n\nCode\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nimport matplotlib.pyplot as plt\nplt.scatter(df.StockPrice, df.Portfolio, color='red')\nplt.xlabel('Stock Price')\nplt.ylabel('Portfolio')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see from the graph that there appears to be a linear relationship between the \\(x\\) and \\(y\\) values in this case. To find the relationship mathematically we define the followings\n\\[\\begin{align*}\nS_{xx}& = \\sum (x_i-\\bar{x})^2 = \\sum (x_i^2-2x_i\\bar{x}+\\bar{x}^2)\\\\\n& = \\sum x_i^2 - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2= \\sum x_i^2 - 2\\bar{x} n \\bar{x} + n \\bar{x}^2 = \\sum x_i ^2 - n \\bar{x}^2\n\\end{align*}\\]\nSimilarly, \\[\\begin{align*}\nS_{yy}& = \\sum (y_i-\\bar{y})^2=\\sum y_i ^2 - n \\bar{y}^2\\\\\nS_{xy} & = \\sum (x_i-\\bar{x})^2 \\sum (y_i-\\bar{y})^2 = \\sum x_iy_i -n \\bar{xy}\n\\end{align*}\\]\nThe sample correlation coefficient \\(r\\) is then given as\n\\[\nr = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}} = \\frac{\\sum x_i ^2 - n \\bar{x}^2}{\\sqrt{\\left(\\sum x_i ^2 - n \\bar{x}^2\\right)\\left(\\sum y_i ^2 - n \\bar{y}^2\\right)}}\n\\]\nYou may have seen a different formula to calculate this quantity which often looks a bit different\n\\[\n\\rho = Corr(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{var(X)var(Y)}}\n\\]\nThe sample correlation coefficient, \\(r\\), is an estimator of the population correlation coefficient, \\(\\rho\\), in the same way as \\(\\bar{X}\\) is an estimator of \\(\\mu\\) or \\(S^2\\) is an estimator of \\(\\sigma^2\\) . Now the question is what does this \\(r\\) values mean?\n\n\n\n\n\n\n\nValue\nMeaning\n\n\n\n\n\\(r=1\\)\nThe two variables move together in the same direction in a perfect linear relationship.\n\n\n\\(0 &lt; r &lt; 1\\)\nThe two variables tend to move together in the same direction but there is NOT a direct relationship.\n\n\n\\(r= 0\\)\nThe two variables can move in either direction and show no linear relationship.\n\n\n\\(-1 &lt; r &lt; 0\\)\nThe two variables tend to move together in opposite directions but there is not a direct relationship.\n\n\n\\(r =-1\\)\nThe two variables move together in opposite directions in a perfect linear relationship.\n\n\n\nLet’s calculate the correlation of our stock data.\n\n\nCode\nimport math\nx = df.StockPrice.values\ny = df.Portfolio.values\n\nn = len(x)\n\nx_sum, y_sum =0,0\ns_xx, s_yy, s_xy = 0,0,0\nfor i in range(n):\n    x_sum += x[i]\n    s_xx += x[i]**2\n    y_sum += y[i]\n    s_yy += y[i]**2\n    s_xy += x[i]*y[i]    \n\ns_xx = s_xx - (x_sum)**2/n\ns_yy = s_yy - (y_sum)**2/n\ns_xy = s_xy - (x_sum * y_sum)/n\n\nr = s_xy/math.sqrt(s_xx * s_yy)\n\n# Print with formatted labels\nprint(f\"Sₓₓ: {s_xx:.2f}\")\nprint(f\"Sᵧᵧ: {s_yy:.2f}\")\nprint(f\"Sₓᵧ: {s_xy:.2f}\")\nprint(' ')\nprint(f\"r : {r:.2f}\")\n\n\nSₓₓ: 8.53\nSᵧᵧ: 6.97\nSₓᵧ: 7.13\n \nr : 0.92"
  },
  {
    "objectID": "jobandintern/correlationandregression/index.html#bivariate-analysis",
    "href": "jobandintern/correlationandregression/index.html#bivariate-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Bivariate Analysis",
    "text": "Bivariate Analysis\nThe joint probability density function for \\(X\\) and \\(Y\\) in the bivariate normal distribution is given by:\n\\[\nf_{X,Y}(x, y) = \\frac{1}{2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}}\n\\exp\\left( -\\frac{1}{2(1-\\rho^2)} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - 2\\rho\\frac{(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X \\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] \\right)\n\\]\n\nWhen \\(|\\rho| = 1\\), the denominator \\(\\sqrt{1-\\rho^2}\\) in the PDF becomes zero, which might appear problematic. However, what happens in this case is that the joint distribution degenerates into a one-dimensional structure (a line) rather than being a two-dimensional probability density.\n\nTo see why, consider the quadratic term inside the exponential:\n\\[\nQ = \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - 2\\rho \\frac{(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X \\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2}\n\\]\nWhen \\(|\\rho| = 1\\), this quadratic expression simplifies, as shown next.\nStart with the simplified \\(Q\\) when \\(|\\rho| = 1\\):\n\\[\\begin{align*}\nQ &= \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)^2 - 2\\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\cdot \\frac{y-\\mu_Y}{\\sigma_Y} \\right) + \\left( \\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2\\\\\n&=\\left( \\frac{x-\\mu_X}{\\sigma_X} - \\rho \\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2\n\\end{align*}\\]\nThis is a perfect square because the “cross term” cancels out all independent variability of \\(X\\) and \\(Y\\) when \\(|\\rho| = 1\\).\nFor the quadratic term \\(Q\\) to have any non-zero probability density (since it appears in the exponent of the PDF), it must be equal to zero: \\[\n\\frac{x-\\mu_X}{\\sigma_X} - \\rho \\frac{y-\\mu_Y}{\\sigma_Y} = 0\n\\]\nRearranging this equation: \\[\n\\frac{y-\\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\n\\]\nMultiply through by \\(\\sigma_Y\\): \\[\ny - \\mu_Y = \\rho \\frac{\\sigma_Y}{\\sigma_X} (x - \\mu_X)\n\\]\nThus:\n\\[\\begin{align*}\n\\mathbb{E}(Y| X=x)&= \\mu_Y + \\rho\\frac{\\sigma_Y}{\\sigma_X}(x-\\mu_X)\\\\\n& = \\mu_Y + \\rho \\frac{\\sigma_Y}{\\sigma_X} (x - \\mu_X)\n\\end{align*}\\]\nThis is the equation of a straight line in the \\((X, Y)\\)-plane. The slope of the line is \\(\\rho \\frac{\\sigma_Y}{\\sigma_X}\\), and the line passes through the point \\((\\mu_X, \\mu_Y)\\). When \\(|\\rho| = 1\\), all the joint probability mass collapses onto this line, meaning \\(X\\) and \\(Y\\) are perfectly linearly dependent.\n\n\nCode\nimport numpy as np\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define the bivariate normal PDF\ndef bivariate_normal_pdf(x, y, mu_x, mu_y, sigma_x, sigma_y, rho):\n    z = (\n        ((x - mu_x) ** 2) / sigma_x**2\n        - 2 * rho * (x - mu_x) * (y - mu_y) / (sigma_x * sigma_y)\n        + ((y - mu_y) ** 2) / sigma_y**2\n    )\n    denominator = 2 * np.pi * sigma_x * sigma_y * np.sqrt(1 - rho**2)\n    return np.exp(-z / (2 * (1 - rho**2))) / denominator\n\n# Parameters\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\n\n# Function to plot the bivariate normal distribution and a line for rho = 1 or -1\ndef plot_bivariate_and_line_side_by_side(rho1, rho2):\n    fig = plt.figure(figsize=(8, 4))\n\n    # Plot for the first rho\n    ax1 = fig.add_subplot(121, projection='3d')\n    if abs(rho1) == 1:\n        # Degenerate case: Straight line\n        line_x = np.linspace(-3, 3, 100)\n        line_y = line_x  # Since rho = 1 implies y = x (perfect correlation)\n        ax1.plot(line_x, line_y, np.zeros_like(line_x), label=f'Degenerate Line (ρ = {rho1})', color='red')\n    else:\n        # General bivariate normal distribution\n        Z = bivariate_normal_pdf(X, Y, 0, 0, 1, 1, rho1)\n        ax1.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.8)\n\n    ax1.set_title(f'Bivariate Normal (ρ = {rho1:.2f})')\n    ax1.set_xlabel('X')\n    ax1.set_ylabel('Y')\n    ax1.set_zlabel('PDF')\n\n    # Plot for the second rho\n    ax2 = fig.add_subplot(122, projection='3d')\n    if abs(rho2) == 1:\n        # Degenerate case: Straight line\n        line_x = np.linspace(-3, 3, 100)\n        line_y = line_x  # Since rho = 1 implies y = x (perfect correlation)\n        ax2.plot(line_x, line_y, np.zeros_like(line_x), label=f'Degenerate Line (ρ = {rho2})', color='red')\n    else:\n        # General bivariate normal distribution\n        Z = bivariate_normal_pdf(X, Y, 0, 0, 1, 1, rho2)\n        ax2.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.8)\n\n    ax2.set_title(f'Bivariate Normal (ρ = {rho2:.2f})')\n    ax2.set_xlabel('X')\n    ax2.set_ylabel('Y')\n    ax2.set_zlabel('PDF')\n\n    plt.tight_layout()\n    plt.show()\n\n# Plot examples side by side\nplot_bivariate_and_line_side_by_side(0.5, 1)  # Example with rho = 0.5 and rho = 1\n\n\n\n\n\n\n\n\n\n\n\\(t-\\)Statistic\nUnder the null hypothesis, where \\(H_0: \\rho =0, \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\\) has a \\(t-\\) distribution with \\(\\nu=n-2\\) degree of freedom.\n\n\nFisher’s Transformation of \\(r\\)\nIf \\(W = \\frac{1}{2}\\ln{\\frac{1+r}{1-r}}=\\tanh^{-1}r\\), then \\(W\\) has approximately a normal distribution with mean \\(\\frac{1}{2}\\ln{\\frac{1+\\rho}{1-\\rho}}\\) and standard deviation \\(\\frac{1}{\\sqrt{n-3}}\\).\nFor our stock data:\nNull Hypothesis \\(H_0\\): There is no association between stock prices and the portfolio values, i.e., \\(\\rho =0\\)\nAlternative Hypothesis \\(H_1\\): There is some association between the stock price and portfolio values, i.e., \\(\\rho &gt; 0\\)\nIf \\(H_0\\) is true, then the test statistic \\(\\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}=\\frac{0.92\\sqrt{8}}{\\sqrt{1-0.92^2}}=6.64\\) has a \\(t_8\\) distribution. The observed value \\(6.64\\) is much greater than the critical value of \\(t_8\\) at \\(0.5\\%\\) level which is \\(3.36\\).\nSo, we reject the null hypothesis \\(H_0\\) at the \\(0.5\\%\\) level and conclude that there is a very strong evidence that \\(\\rho&gt;0\\).\nAlternatively, if we want to use the Fisher’s test:\nIf \\(H_0\\) is true, then the test statistic \\(Z_r=\\tanh^{-1}r=\\tanh^{-1}(0.92)\\) has a \\(N\\left(0,\\frac{1}{7}\\right)\\) distribution.\nThe observed value of this statistic is \\(\\frac{1}{2}\\log{\\frac{1+0.92}{1-0.92}}=1.589\\), which corresponds to a value of \\(\\frac{1.589}{\\sqrt{\\frac{1}{7}}}=4.204\\) on the \\(N(0,1)\\) distribution. This is much greater than \\(3.090\\), the upper \\(0.1\\%\\) point of the standard normal distribution.\nSo, we reject \\(H_0\\) at the \\(0.1\\%\\) level and conclude that there is very strong evidence that \\(\\rho &gt; 0\\) ie that there is a positive linear correlation between the stock price and portfolio value."
  },
  {
    "objectID": "jobandintern/correlationandregression/index.html#correlation-analysis",
    "href": "jobandintern/correlationandregression/index.html#correlation-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nTo better explain, we will use the following hypothetical stock data of 10 companies with stock price and their corresponding proportion in the portfolio.\n\n\nCode\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Stock': ['Apple', 'Citi', 'MS', 'WF', 'GS', 'Google', 'Amazon', 'Tesla', 'Toyota', 'SPY'],\n    'StockPrice': [2.11, 2.42, 2.52, 3.21, 3.62, 3.86, 4.13, 4.27, 4.51, 5.01], \n    'Portfolio': [2.12, 2.16, 2.51, 2.65, 3.62, 3.15, 4.32, 3.31, 4.18, 4.45]\n})\n\ndf.set_index('Stock', inplace=True)\n\ndf.T\n\n\n\n\n\n\n\n\nStock\nApple\nCiti\nMS\nWF\nGS\nGoogle\nAmazon\nTesla\nToyota\nSPY\n\n\n\n\nStockPrice\n2.11\n2.42\n2.52\n3.21\n3.62\n3.86\n4.13\n4.27\n4.51\n5.01\n\n\nPortfolio\n2.12\n2.16\n2.51\n2.65\n3.62\n3.15\n4.32\n3.31\n4.18\n4.45\n\n\n\n\n\n\n\nThe scatterplot of the data looks like this\n\n\nCode\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nimport matplotlib.pyplot as plt\nplt.scatter(df.StockPrice, df.Portfolio, color='red')\nplt.xlabel('Stock Price')\nplt.ylabel('Portfolio')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see from the graph that there appears to be a linear relationship between the \\(x\\) and \\(y\\) values in this case. To find the relationship mathematically we define the followings\n\\[\\begin{align*}\nS_{xx}& = \\sum (x_i-\\bar{x})^2 = \\sum (x_i^2-2x_i\\bar{x}+\\bar{x}^2)\\\\\n& = \\sum x_i^2 - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2= \\sum x_i^2 - 2\\bar{x} n \\bar{x} + n \\bar{x}^2 = \\sum x_i ^2 - n \\bar{x}^2\n\\end{align*}\\]\nSimilarly, \\[\\begin{align*}\nS_{yy}& = \\sum (y_i-\\bar{y})^2=\\sum y_i ^2 - n \\bar{y}^2\\\\\nS_{xy} & = \\sum (x_i-\\bar{x})^2 \\sum (y_i-\\bar{y})^2 = \\sum x_iy_i -n \\bar{xy}\n\\end{align*}\\]\nThe sample correlation coefficient \\(r\\) is then given as\n\\[\nr = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}} = \\frac{\\sum x_i ^2 - n \\bar{x}^2}{\\sqrt{\\left(\\sum x_i ^2 - n \\bar{x}^2\\right)\\left(\\sum y_i ^2 - n \\bar{y}^2\\right)}}\n\\]\nYou may have seen a different formula to calculate this quantity which often looks a bit different\n\\[\n\\rho = Corr(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{var(X)var(Y)}}\n\\]\nThe sample correlation coefficient, \\(r\\), is an estimator of the population correlation coefficient, \\(\\rho\\), in the same way as \\(\\bar{X}\\) is an estimator of \\(\\mu\\) or \\(S^2\\) is an estimator of \\(\\sigma^2\\) . Now the question is what does this \\(r\\) values mean?\n\n\n\n\n\n\n\nValue\nMeaning\n\n\n\n\n\\(r=1\\)\nThe two variables move together in the same direction in a perfect linear relationship.\n\n\n\\(0 &lt; r &lt; 1\\)\nThe two variables tend to move together in the same direction but there is NOT a direct relationship.\n\n\n\\(r= 0\\)\nThe two variables can move in either direction and show no linear relationship.\n\n\n\\(-1 &lt; r &lt; 0\\)\nThe two variables tend to move together in opposite directions but there is not a direct relationship.\n\n\n\\(r =-1\\)\nThe two variables move together in opposite directions in a perfect linear relationship.\n\n\n\nLet’s calculate the correlation of our stock data.\n\n\nCode\nimport math\nx = df.StockPrice.values\ny = df.Portfolio.values\n\nn = len(x)\n\nx_sum, y_sum =0,0\ns_xx, s_yy, s_xy = 0,0,0\nfor i in range(n):\n    x_sum += x[i]\n    s_xx += x[i]**2\n    y_sum += y[i]\n    s_yy += y[i]**2\n    s_xy += x[i]*y[i]    \n\ns_xx = s_xx - (x_sum)**2/n\ns_yy = s_yy - (y_sum)**2/n\ns_xy = s_xy - (x_sum * y_sum)/n\n\nr = s_xy/math.sqrt(s_xx * s_yy)\n\n# Print with formatted labels\nprint(f\"Sum x: {x_sum:.2f}\")\nprint(f\"Sum y: {y_sum:.2f}\")\nprint(f\"Sₓₓ: {s_xx:.2f}\")\nprint(f\"Sᵧᵧ: {s_yy:.2f}\")\nprint(f\"Sₓᵧ: {s_xy:.2f}\")\nprint(' ')\nprint(f\"r : {r:.2f}\")\n\n\nSum x: 35.66\nSum y: 32.47\nSₓₓ: 8.53\nSᵧᵧ: 6.97\nSₓᵧ: 7.13\n \nr : 0.92"
  },
  {
    "objectID": "jobandintern/correlationandregression/index.html#regression-analysis",
    "href": "jobandintern/correlationandregression/index.html#regression-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Regression Analysis",
    "text": "Regression Analysis\nGiven a set of points \\((x_i,y_i)_{i=0}^{n}\\) for a simple linear regression of the form\n\\[\nY_i = \\alpha +\\beta x_i + \\epsilon_i; \\hspace{4mm} i=1,2,\\cdots,n\n\\]\nwith \\(\\mathbb{\\epsilon_i}=0\\) and \\(var[\\epsilon_i]=\\sigma^2\\).\n\nModel Fitting\nWe can estimate the parameters from the method of least squares but that’s not the goal in this case. Fitting the model involves finding \\(\\alpha\\) and \\(\\beta\\) and the estimating the variance \\(\\sigma^2\\).\n\\[\n\\hat{y} = \\hat{\\alpha}+\\hat{\\beta}x\n\\]\nwhere, \\(\\hat{\\beta}= \\frac{S_{xy}}{S_{xx}}\\) and \\(\\hat{\\alpha} = \\bar{y}-\\hat{\\beta}\\bar{x}\\)\n\\(\\hat{\\beta}\\) is the observed value of a statistic \\(\\hat{B}\\) whose sampling distribution has the following properties\n\\[\n\\mathbb{E}[\\hat{B}]=\\beta, \\hspace{4mm} var[\\hat{B}]=\\frac{\\sigma^2}{S_{xx}}\n\\]\nAnd the estimate of the error variance\n\\[\\begin{align*}\n\\hat{\\sigma}^2 & =\\frac{1}{n-2}\\sum (y_i -\\hat{y_i})^2\\\\\n& = \\frac{1}{n-2} \\left(S_{yy}-\\frac{S_{xy}^2}{S_{xx}}\\right)\n\\end{align*}\\]\n\n\nGoodness of fit\nTo better understand the goodness of fit of the model for the data at hand, we can study the total variation in the responses, as given by\n\\[\nS_{yy} = \\sum (y_i-\\bar{y})^2\n\\]\nLet’s see how:\n\\[\\begin{align*}\ny_i - \\bar{y} &= (y_i - \\hat{y_i}) + (\\hat{y_i}-\\bar{y}) \\\\\n\\implies (y_i - \\bar{y})^2 & = \\left((y_i - \\hat{y_i}) + (\\hat{y_i}-\\bar{y})\\right)^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2(y_i - \\hat{y_i})(\\hat{y_i}-\\bar{y})+(\\hat{y_i}-\\bar{y})^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2 [y_i -(\\hat{\\alpha}+\\hat{\\beta}x_i)][\\hat{\\alpha}+\\hat{\\beta}x_i-(\\hat{\\alpha}+\\hat{\\beta}\\bar{x})]+(\\hat{y_i}-\\bar{y})^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left(y_i -\\hat{\\alpha}-\\hat{\\beta}x_i\\right)(x_i-\\bar{x})+(\\hat{y_i}-\\bar{y})^2\\\\\n\\implies \\sum (y_i - \\bar{y})^2 & =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\sum\\left(y_i -\\hat{\\alpha}-\\hat{\\beta}x_i\\right)(x_i-\\bar{x})+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left[\\sum x_iy_i-\\bar{x}\\sum y_i -\\hat{\\alpha}\\sum x_i + n\\hat{\\alpha}\\bar{x}-\\hat{\\beta}\\sum x_i^2\\right.\\\\\n&\\left.\\hspace{4mm}+\\hat{\\beta}\\bar{x}\\sum x_i\\right]+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left(\\sum x_iy_i-n\\bar{x}\\bar{y}\\right)-2\\hat{\\beta}^2\\left(\\sum x_i^2 - n\\bar{x}^2\\right)+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& = \\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}S_{xy}-2\\hat{\\beta}^2S_{xx}+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& = \\sum (y_i - \\hat{y_i})^2 + 2 \\frac{S_{xy}}{S_{xx}}S_{xy}-2\\left(\\frac{S_{xy}}{S_{xx}}\\right)^2S_{xx}+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n\\implies \\sum (y_i - \\bar{y})^2 & =\\sum (y_i - \\hat{y_i})^2 +\\sum(\\hat{y_i}-\\bar{y})^2\\\\\nSS_{TOT} & = SS_{RES}+ SS_{REG}\n\\end{align*}\\]\nIn the case that the data are “close” to a line ( \\(|r|\\) high- a strong linear relationship) the model fits well, the fitted responses (the values on the fitted line) are close to the observed responses, and so \\(SS_{REG}\\) is relatively high with \\(SS_{RES}\\) relatively low.\nIn the case that the data are not “close” to a line ( \\(|r|\\) low - a weak linear relationship) the model does not fit so well, the fitted responses are not so close to the observed responses, and so \\(SS_{REG}\\) is relatively low and \\(SS_{RES}\\) relatively high.\nThe proportion of the total variability of the responses “explained” by a model is called the coefficient of determination, denoted \\(R^2\\) .\n\\[\nR^2 = \\frac{SS_{REG}}{SS_{TOT}} =\\frac{S_{xy}^2}{S_{xx}S_{yy}}\n\\]\nwhich takes value between 0 to 1, inclusive. The higher \\(R^2\\), the better fitting.\nFor our data, we have:\n\\[\\begin{align*}\nn & = 10, \\hspace{4mm} \\sum x = 35.66, \\hspace{4mm} \\sum y = 32.47\\\\\nS_{xx} &= 8.53 \\hspace{4mm} S_{yy}=6.97, \\hspace{4mm} S_{xy}=7.13\\\\\n\\implies \\hat{\\beta} &=\\frac{S_{xy}}{S_{xx}}= \\frac{7.13}{8.53} = 0.836\\\\\n\\hat{\\alpha} &= \\frac{\\sum y}{n} - \\hat{\\beta} \\frac{\\sum x}{n} = \\bar{y}-\\hat{\\beta}\\bar{x}\\\\\n& = 3.247 - 0.836 \\times 3.566 = 0.266\n\\end{align*}\\]\nTherefore, the fitted line would be \\(\\hat{y}=0.266 + 0.836x\\). Now we see the other metrics\n\\[\\begin{align*}\nSS_{TOT} &= 6.97 \\\\\nSS_{REG} & = \\frac{S_{yy}^2}{S_{xx}} = \\frac{6.97^2}{8.53}=5.695\\\\\nSS_{RES} & = 6.97 - 5.695 = 1.275 \\\\\n\\implies \\hat{\\sigma}^2 & = \\frac{1.275}{8}=0.1594\\\\\nR^2 & = \\frac{5.695}{6.97}=0.817\n\\end{align*}\\]\n\n\nCode\n# Parameters for the line\nalpha = 0.266  \nbeta = 0.836   \n\n# Line values\nline_x = np.linspace(min(df.StockPrice), max(df.StockPrice), 100)  \nline_y = alpha + beta * line_x             \n\n# Plot\nplt.scatter(df.StockPrice, df.Portfolio, color='blue', label='Data Points')\nplt.plot(line_x, line_y, color='red', label=f'Line: y = {alpha} + {beta}x')\n\n# Labels and title\nplt.xlabel('Stock Price')\nplt.ylabel('Portfolio')\nplt.title('Scatter Plot with Regression Line')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nInference on \\(\\beta\\)\nWe can rewrite \\(\\hat{\\beta}= \\frac{S_{xy}}{S_{xx}}\\), as\n\\[\\begin{align*}\n\\hat{\\beta}&= \\frac{S_{xy}}{S_{xx}}=\\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})y_i-\\bar{y}\\sum (x_i-\\bar{x})}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x}y_i)-\\bar{y}\\left(\\sum x_i -n\\bar{x}\\right)}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})y_i}{S_{xx}}\n\\end{align*}\\]\nNow we recall that \\(\\hat{B}\\) is the random variable that has \\(\\hat{\\beta}\\) as its realization. Therefore, \\(\\hat{B}=\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\). We also recall that \\(\\mathbb{E}(Y_i)=\\alpha +\\beta x\\). Putting these together we obtain,\n\\[\\begin{align*}\n\\mathbb{E}[\\hat{B}] &= \\mathbb{E}\\left[\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\right] = \\frac{\\sum (x_i -\\bar{x})\\mathbb{E}[Y_i]}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})(\\alpha + \\beta x_i)}{S_{xx}}\\\\\n& = \\frac{\\alpha \\sum (x_i-\\bar{x})+\\beta \\sum x_i (x_i-\\bar{x})}{S_{xx}}\\\\\n& = \\frac{\\alpha \\left(\\sum x_i -n\\bar{x}\\right)+\\beta \\left(\\sum x_i^2-\\bar{x}\\sum x_i\\right)}{S_{xx}} \\\\\n& = \\frac{\\alpha (n\\bar{x}-n\\bar{x})+\\beta\\left(\\sum x_i^2-n\\bar{x}^2\\right)}{S_{xx}}\\\\\n& = \\frac{0+\\beta S_{xx}}{S_{xx}} = \\beta\n\\end{align*}\\]\nNow the fact that \\(Y_i'\\)s are uncorrelated. Therefore, \\(var\\left(\\sum (Y_i)\\right)=\\sum var(Y_i)\\) and we have \\(var(Y_i)=\\sigma^2\\). Therefore,\n\\[\\begin{align*}\nvar[\\hat{B}]& = var\\left[\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\right]= \\frac{\\sum (x_i-\\bar{x})^2var[Y_i]}{S_{xx}^2}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})^2\\sigma^2}{S_{xx}^2} = \\frac{\\sigma^2}{S_{xx}^2}\\sum (x_i-\\bar{x})^2 = \\frac{\\sigma^2}{S_{xx}^2}S_{xx}\\\\\n& = \\frac{\\sigma^2}{S_{xx}}\n\\end{align*}\\]\nSince \\(\\mathbb{E}(\\hat{\\beta})=\\beta\\) and \\(var(\\hat{\\beta})=\\frac{\\sigma^2}{S_{xx}}\\) so\n\\[\nM = \\frac{\\hat{\\beta}-\\beta}{\\sqrt{\\frac{\\sigma^2}{S_{xx}}}}\\sim N(0,1)\n\\]\nand the observed variance \\(\\hat{\\sigma}^2\\) has the property\n\\[\nN = \\frac{(n-2)\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi_{n-2}^2\n\\]\nSince \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are independent, it follows that\n\\[\n\\frac{M}{\\sqrt{\\frac{N}{n-2}}} \\sim t_{n-2}\n\\]\nIn other words: \\[\n\\frac{\\hat{\\beta}-\\beta}{se(\\hat{\\beta})} = \\frac{\\hat{\\beta}-\\beta}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\sim t_{n-2}\n\\]\nNow the big question is what’s the use of this mathematical jargon that we have learned so far? Let’s use our regression problem on stock data to explain.\n\\(H_0: \\beta =0\\), there is no linear relationship\nvs\n\\(H_1: \\beta&gt; 0\\), there is a linear relationship\nBased on our data we have \\(\\hat{\\beta} = 0.836\\) and \\(\\hat{\\sigma}^2=0.1594\\), and \\(S_{xx}=8.53\\). Therefore, under \\(H_0\\), the test statistic\n\\[\n\\frac{\\hat{\\beta}-0}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\text{ has a } t_{10-2} \\text{ or } t_8 \\text{ distribution}\n\\]\nBut the observed value of this statistic \\[\n\\frac{0.836-0}{\\sqrt{0.1594/8.53}}=6.1156\n\\]\nwhich is way higher than the critical value at \\(5\\%\\) significance level.\n\n\nCode\nfrom scipy.stats import t\n\n# Parameters\ndf = 8  # Degrees of freedom\nalpha = 0.05  # Upper tail probability\nt_critical = t.ppf(1 - alpha, df)  # Critical t-value at the 95th percentile\n\n# Generate x values for the t-distribution\nx = np.linspace(-4, 4, 500)\ny = t.pdf(x, df)\n\n# Plot the t-distribution\nplt.plot(x, y, label=f't_{df} Distribution', color='blue')\nplt.fill_between(x, y, where=(x &gt;= t_critical), color='red', alpha=0.5, label=f'Upper {alpha*100}% Area')\n\n# Annotate the critical t-value on the x-axis\nplt.axvline(t_critical, ymin=0.02, ymax=0.30,color='red', linestyle='--', label=f'Critical t-value = {t_critical:.2f}')\nplt.text(t_critical, -0.02, f'{t_critical:.2f}', color='red', ha='center', va='top')\n\n# Add a horizontal line at y = 0\nplt.axhline(0, color='black', linestyle='-', linewidth=0.8)\n\n# Labels, title, and legend\nplt.title(f\"t-Distribution with {df} Degrees of Freedom\")\nplt.xlabel(\"t\")\nplt.ylabel(\"Density\")\nplt.legend()\n\n# Adjust plot limits\n\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nSo, we reject the null hypothesis \\(H_0\\) at the \\(5\\%\\) level and conclude that there is a very strong evidence that \\(\\beta&gt;0\\), i.e., the portfolio value is increasing over stock price.\nAlternatively, let’s put our analysis in a different approach. We claim that\n\\(H_0: \\beta=1\\), there is a linear relationship\nvs\n\\(H_1: \\beta \\ne 1\\)\nIn this case,\n\\[\nse(\\hat{\\beta}) = \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} = \\sqrt{\\frac{0.1594}{8.53}} =0.1367\n\\]\nTherefore, the \\(95\\%\\) confidence interval for \\(\\beta\\) is\n\\[\n\\hat{\\beta} \\pm \\left\\{t_{0.025,8}\\times se(\\hat{\\beta})\\right\\}=0.836 \\pm 2.306\\times 0.1367 = (0.5207,1.1512)\n\\]\nThe \\(95\\%\\) two-sided confidence interval contains the value \\(1\\), so the two-sided test conducted at \\(5\\%\\) level results in \\(H_0\\) being accepted.\n\n\nMean Response and Individual Response\n\nMean Response\n\nIf \\(\\mu_0\\) is the expected (mean) response for a value \\(x_0\\) of the predictor variable, that is \\(\\mu_0 = \\mathbb{E}[Y|x_0]=\\alpha +\\beta x_0\\), then \\(\\mu_0\\) is an unbiased estimator given by\n\n\\[\n\\hat{\\mu}_0 = \\hat{\\alpha}+\\hat{\\beta} x_0\n\\]\nand the variance of the estimator is given by\n\\[\nvar(\\hat{\\mu}_0) = \\left(\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2\n\\]\nTherefore,\n\\[\n\\frac{\\hat{\\mu}_0-\\mu_0}{se[\\hat{\\mu}_0] }= \\frac{\\hat{\\mu}_0-\\mu_0}{\\sqrt{\\left(\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2}}\\sim t_{n-2}\n\\]\n\n\nIndividual Response\nThe actual estimate of an individual response \\[\n\\hat{y}_0 = \\hat{\\alpha} +\\hat{\\beta}x_0\n\\]\n\nHowever, the uncertainty associated with this estimator, as indicated by its variance, is higher compared to the mean estimator because it relies on the value of an individual response \\(y_0\\) rather than the more stable mean. To account for the additional variability of an individual response relative to the mean, an extra term, \\(\\sigma^2\\), must be included in the variance expression for the estimator of a mean response.\n\n\\[\nvar[\\hat{y}_0] = \\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2\n\\]\nThus,\n\\[\n\\frac{\\hat{y}-y_0}{se[\\hat{y}_0]}=\\frac{\\hat{y}-y_0}{\\sqrt{\\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2}}\\sim t_{n-2}\n\\]\n\nLet’s put this two idea through our example. If we want to find a \\(95\\%\\) confidence interval or the expected portfolio value on stock price of say, 360. In that case,\n\n\\[\n\\text{Estimate of the expected portfolio value} = 0.266+0.836\\times 3.6 = 3.276\n\\]\nand\n\\[\n\\text{se}[\\text{Estimate}] = \\sqrt{\\left(\\frac{1}{10}+\\frac{(3.6-3.566)^2}{8.53}\\right) 0.1594}=0.1263\n\\]\nSo, the \\(95\\%\\) CI\n\\[\n3.276\\pm (t_{0.025,8}\\times \\text{se}[\\text{Estimate}]) = 3.276 \\pm 2.306\\times 0.1263 = (2.985,3.567)\n\\]\nThat is for a stock price of \\(\\$360\\), the expected portfolio value would be in the range of \\((\\$298.50,\\$356.70)\\)\nSimilarly, the \\(95\\%\\) CI for the predicted actual portfolio value\n\\[\\begin{align*}\n3.276\\pm (t_{0.025,8}\\times \\text{se}[\\text{Estimate}]) &= 3.276 \\pm 2.306\\sqrt{\\left(1+\\frac{1}{10}+\\frac{(3.6-3.566)^2}{8.53}\\right) 0.1594}\\\\\n& = (2.3103,4.2417)\n\\end{align*}\\]\nor \\((\\$231.03,\\$424.17)\\)\n\n\n\nModel Accuracy\nThe residual from the fit at \\(x_i\\) is the estimated error which is defined by \\[\n\\hat{\\epsilon}_i = y_i - \\hat{y}_i\n\\]\n\nScatter plots of residuals versus the explanatory variable (or the fitted response values) are particularly insightful. A lack of random scatter in the residuals, such as the presence of a discernible pattern, indicates potential shortcomings in the model.\n\n\n\nCode\ndf = pd.DataFrame({\n    'Stock': ['Apple', 'Citi', 'MS', 'WF', 'GS', 'Google', 'Amazon', 'Tesla', 'Toyota', 'SPY'],\n    'StockPrice': [2.11, 2.42, 2.52, 3.21, 3.62, 3.86, 4.13, 4.27, 4.51, 5.01], \n    'Portfolio': [2.12, 2.16, 2.51, 2.65, 3.62, 3.15, 4.32, 3.31, 4.18, 4.45]\n})\nx = df.StockPrice.values\ny = df.Portfolio.values \n\ny_hat = [0.266+0.836*i for i in x]\nplt.scatter(x, y-y_hat)\nplt.axhline(0)\nplt.ylabel('Residuals')\nplt.xlabel('Stock Price')\nplt.title('Scatter plot of the residuals from the fitted line')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn this plot, we can see that the residuals tend to increase as \\(x\\) increases, indicates that the error variance is not bounded, but increasing with \\(x\\). So, the model is not the best one. A transformation of the responses may stabilize the error variance.   In certain case, for some growth models, the appropriate model is that the expected response is related to the exploratory variable through an exponential relationship, i.e.,\n\n\\[\\begin{align*}\n\\mathbb{E}[Y_i|X=x_i] &= \\alpha e^{\\beta x_i}\\\\\n\\implies z_i = \\log y_i & = \\eta + \\beta x_i + \\epsilon_i; \\hspace{4mm}\\text{where }\\eta=\\log \\alpha\n\\end{align*}\\]\n\n\nCode\nx = df.StockPrice.values\ny = np.log(df.Portfolio.values)\n\nn = len(x)\n\nx_sum, y_sum =0,0\ns_xx, s_yy, s_xy = 0,0,0\nfor i in range(n):\n    x_sum += x[i]\n    s_xx += x[i]**2\n    y_sum += y[i]\n    s_yy += y[i]**2\n    s_xy += x[i]*y[i]    \n\ns_xx = s_xx - (x_sum)**2/n\ns_yy = s_yy - (y_sum)**2/n\ns_xy = s_xy - (x_sum * y_sum)/n\n\nr = s_xy/math.sqrt(s_xx * s_yy)\n\n# Print with formatted labels\nprint(f\"Sum x: {x_sum:.2f}\")\nprint(f\"Sum y: {y_sum:.2f}\")\nprint(f\"Sₓₓ: {s_xx:.2f}\")\nprint(f\"Sᵧᵧ: {s_yy:.2f}\")\nprint(f\"Sₓᵧ: {s_xy:.2f}\")\nprint(' ')\nprint(f\"r : {r:.2f}\")\n\n\nSum x: 35.66\nSum y: 11.43\nSₓₓ: 8.53\nSᵧᵧ: 0.70\nSₓᵧ: 2.29\n \nr : 0.94\n\n\nNow we have:\n\\[\\begin{align*}\nn & = 10, \\hspace{4mm} \\sum x = 35.66, \\hspace{4mm} \\sum y = 11.43\\\\\nS_{xx} &= 8.53 \\hspace{4mm} S_{yy}=0.70, \\hspace{4mm} S_{xy}=2.29\\\\\n\\implies \\hat{\\beta} &=\\frac{S_{xy}}{S_{xx}}= \\frac{2.29}{8.53} = 0.268\\\\\n\\hat{\\alpha} &= \\frac{\\sum y}{n} - \\hat{\\beta} \\frac{\\sum x}{n} = \\bar{y}-\\hat{\\beta}\\bar{x}\\\\\n& = 1.143 - 0.268 \\times 3.566 = 0.1873\n\\end{align*}\\]\n\n\nCode\nimport numpy as np\nz_hat = [np.log(0.1873)+0.268*i for i in x]\nz = np.log(y)\nplt.scatter(x, z-z_hat)\nplt.axhline(np.mean(z-z_hat))\nplt.ylabel('Residuals')\nplt.xlabel('Stock Price')\nplt.title('Scatter plot of the residuals from the fitted line')\nplt.show()\n\n\n\n\n\n\n\n\n\nNow the residuals look good, that is no special pattern or increasing the error variance.\nThanks for reading."
  },
  {
    "objectID": "jobandintern/correlationandregression/index.html#references",
    "href": "jobandintern/correlationandregression/index.html#references",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "References",
    "text": "References\n\nMontgomery, D. C., & Runger, G. C. (2014). Applied Statistics and Probability for Engineers. Wiley.\n\nCasella, G., & Berger, R. L. (2002). Statistical Inference. Duxbury.\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. Routledge.\n\nSeber, G. A. F., & Lee, A. J. (2003). Linear Regression Analysis. Wiley.\nNeter, J., Kutner, M. H., Nachtsheim, C. J., & Wasserman, W. (1996). Applied Linear Statistical Models. Irwin.\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\n\nWeisberg, S. (2005). Applied Linear Regression. Wiley.\n\nBivariate Normal Distribution Explanation:\n\nRice, J. A. (2006). Mathematical Statistics and Data Analysis. Thomson Brooks/Cole.\n\nA detailed exploration of the bivariate normal distribution and its properties.\n\nFisher’s Transformation of Correlation Coefficients:\n\nFisher, R. A. (1921). On the probable error of a coefficient of correlation. Metron, 1, 3-32.\n\nThe foundational paper describing Fisher’s transformation and its use in hypothesis testing.\n\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "teaching/sp25.html",
    "href": "teaching/sp25.html",
    "title": "Spring 2025: MAC2311 Calculus With Analytic Geometry I",
    "section": "",
    "text": "COURSE MEETING SCHEDULE:\nMondays, Wednesdays, and Fridays\nTime: 10:40 AM - 11:30 AM Location: HTL 215 Thursdays\nTime: 11:35 AM - 12:50 PM Location: HTL 215\nCREDIT HOURS: 4\nTime Zone: This course is a Main Campus FSU course and all times provided for this course (class meetings, due dates, etc) will be in Eastern Standard Time.\nCOURSE INSTRUCTOR\nRAFIQ ISLAM\nrislam@fsu.edu\nOffice Hours: Monday 12:00-1:00, Tuesday 11:30-12:30, Wednesday  12:00 - 1:00  Office: Lov 331\nCOURSE DESCRIPTION\nIn this course, students will develop problem solving skills, critical thinking, computational proficiency, and contextual fluency through the study of limits, derivatives, and definite and indefinite integrals of functions of one variable, including algebraic, exponential, logarithmic, and trigonometric functions, and applications. Topics will include limits, continuity, differentiation and rates of change, optimization, curve sketching, and introduction to integration and area.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nSpring 2025: MAC2311 Calculus With Analytic Geometry I\n\n\n\n\n\n\n\nSpring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\n\nFall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\n\nSpring 2023: MAC1140 PreCalculus Algebra\n\n\n\n\n\n\n\nFall 2022: MAC2311 Calculus and Analytic Geometry I\n\n\n\n\n\n\n\nFall 2021 and Spring 2022: PreCalculus and Algebra\n\n\n\n\n\n\n\nFall 2018 to Spring 2020: College Algebra, Trigonometry\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html",
    "href": "portfolio/dsp/autoloan/index.html",
    "title": "Auto Loan Decision Model",
    "section": "",
    "text": "Report Presentation"
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#objective",
    "href": "portfolio/dsp/autoloan/index.html#objective",
    "title": "Auto Loan Decision Model",
    "section": "Objective",
    "text": "Objective\n\nThe Auto Loan Credit Decisioning Model project aimed to enhance the application decision process for auto loans by leveraging machine learning techniques to classify applicants into approved or rejected categories. The project focused on improving prediction accuracy and ensuring fairness across demographic groups while addressing challenges like class imbalance and missing data."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#data-overview",
    "href": "portfolio/dsp/autoloan/index.html#data-overview",
    "title": "Auto Loan Decision Model",
    "section": "Data Overview",
    "text": "Data Overview\n\nDataset: Auto loan account data with 21,000 training records and 5,400 test records.\nFeatures: 43 columns, including borrower creditworthiness, loan application attributes, and demographics.\nTarget Variable: ‘Bad Flag’ (binary), indicating ‘Poor Credit Quality’ (95.5%) or ‘Good Credit Quality’ (4.5%).\nChallenges: Significant class imbalance and high proportion of missing values in several predictors."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#methodology",
    "href": "portfolio/dsp/autoloan/index.html#methodology",
    "title": "Auto Loan Decision Model",
    "section": "Methodology",
    "text": "Methodology\n\nExploratory Data Analysis (EDA):\n\nInvestigated class distributions and correlations with the target variable.\nAddressed missing values using mode and median imputations based on feature types.\nAnalyzed key predictors such as FICO scores, loan-to-value ratios, and credit utilization rates.\n\nModel Development:\n\nBuilt models using Logistic Regression, Decision Tree, and Random Forest classifiers.\nConducted hyperparameter tuning via GridSearchCV for optimal model settings.\nAddressed class imbalance with resampling techniques like SMOTE.\n\nEvaluation Metrics:\n\nPrioritized ROC-AUC, F1-Score, and classification reports over accuracy to account for imbalanced data."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#results",
    "href": "portfolio/dsp/autoloan/index.html#results",
    "title": "Auto Loan Decision Model",
    "section": "Results",
    "text": "Results\n\nBest Model: Random Forest Classifier with an ROC-AUC score of 0.8078.\nPerformance:\n\nTest data accuracy: 94.08%\nPrecision (Class 0): 96.16%\nRecall (Class 0): 97.70%\n\nFairness Analysis:\n\nGender-neutral approval rates.\nNo significant racial bias in decisions."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#key-insights",
    "href": "portfolio/dsp/autoloan/index.html#key-insights",
    "title": "Auto Loan Decision Model",
    "section": "Key Insights",
    "text": "Key Insights\n\nFICO scores, loan-to-value ratios, and credit utilization rates were strong predictors of credit quality.\nHigher class imbalance and overfitting issues were observed with resampling techniques."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#innovations",
    "href": "portfolio/dsp/autoloan/index.html#innovations",
    "title": "Auto Loan Decision Model",
    "section": "Innovations",
    "text": "Innovations\n\nImplemented Local Interpretable Model-agnostic Explanations (LIME) for model transparency, allowing stakeholders to understand prediction outcomes."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#conclusion",
    "href": "portfolio/dsp/autoloan/index.html#conclusion",
    "title": "Auto Loan Decision Model",
    "section": "Conclusion",
    "text": "Conclusion\nThe Random Forest Classifier demonstrated strong predictive performance and fairness, providing a reliable foundation for auto loan decisioning. Opportunities for further improvements include advanced resampling methods, enhanced feature engineering, and exploring models like XGBoost or LightGBM for better results.\nKeywords: Auto Loan, Predictive Modeling, Random Forest, Class Imbalance, Machine Learning, LIME."
  },
  {
    "objectID": "posts/sgd/index.html",
    "href": "posts/sgd/index.html",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "",
    "text": "GIF Credit: gbhat.com\n Gradient Descent is an optimization algorithm used to minimize the cost function. The cost function \\(f(\\beta)\\) measures how well a model with parameters \\(\\beta\\) fits the data. The goal is to find the values of \\(\\beta\\) that minimize this cost function. In terms of the iterative method, we want to find \\(\\beta_{k+1}\\) and \\(\\beta_k\\) such that \\(f(\\beta_{k+1})&lt;f(\\beta_k)\\).   For a small change in \\(\\beta\\), we can approximate \\(f(\\beta)\\) using Taylor series expansion\n\\[f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}\\]\n\nThe update rule for vanilla gradient descent is given by:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nWhere:\n\n\\(\\beta_k\\) is the current estimate of the parameters at iteration \\(k\\).\n\\(\\eta\\) is the learning rate, a small positive scalar that controls the step size.\n\\(\\nabla f(\\beta_k)\\) is the gradient of the cost function \\(f\\) with respect to \\(\\beta\\) at the current point \\(\\beta_k\\).\n\n\nThe update rule comes from the idea of moving the parameter vector \\(\\beta\\) in the direction that decreases the cost function the most.\n\nGradient: The gradient \\(\\nabla f(\\beta_k)\\) represents the direction and magnitude of the steepest ascent of the function \\(f\\) at the point \\(\\beta_k\\). Since we want to minimize the function, we move in the opposite direction of the gradient.\nStep Size: The term \\(\\eta \\nabla f(\\beta_k)\\) scales the gradient by the learning rate \\(\\eta\\), determining how far we move in that direction. If \\(\\eta\\) is too large, the algorithm may overshoot the minimum; if it’s too small, the convergence will be slow.\nIterative Update: Starting from an initial guess \\(\\beta_0\\), we repeatedly apply the update rule until the algorithm converges, meaning that the changes in \\(\\beta_k\\) become negligible, and \\(\\beta_k\\) is close to the optimal value \\(\\beta^*\\)."
  },
  {
    "objectID": "posts/sgd/index.html#gradient-descent",
    "href": "posts/sgd/index.html#gradient-descent",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "",
    "text": "GIF Credit: gbhat.com\n Gradient Descent is an optimization algorithm used to minimize the cost function. The cost function \\(f(\\beta)\\) measures how well a model with parameters \\(\\beta\\) fits the data. The goal is to find the values of \\(\\beta\\) that minimize this cost function. In terms of the iterative method, we want to find \\(\\beta_{k+1}\\) and \\(\\beta_k\\) such that \\(f(\\beta_{k+1})&lt;f(\\beta_k)\\).   For a small change in \\(\\beta\\), we can approximate \\(f(\\beta)\\) using Taylor series expansion\n\\[f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}\\]\n\nThe update rule for vanilla gradient descent is given by:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nWhere:\n\n\\(\\beta_k\\) is the current estimate of the parameters at iteration \\(k\\).\n\\(\\eta\\) is the learning rate, a small positive scalar that controls the step size.\n\\(\\nabla f(\\beta_k)\\) is the gradient of the cost function \\(f\\) with respect to \\(\\beta\\) at the current point \\(\\beta_k\\).\n\n\nThe update rule comes from the idea of moving the parameter vector \\(\\beta\\) in the direction that decreases the cost function the most.\n\nGradient: The gradient \\(\\nabla f(\\beta_k)\\) represents the direction and magnitude of the steepest ascent of the function \\(f\\) at the point \\(\\beta_k\\). Since we want to minimize the function, we move in the opposite direction of the gradient.\nStep Size: The term \\(\\eta \\nabla f(\\beta_k)\\) scales the gradient by the learning rate \\(\\eta\\), determining how far we move in that direction. If \\(\\eta\\) is too large, the algorithm may overshoot the minimum; if it’s too small, the convergence will be slow.\nIterative Update: Starting from an initial guess \\(\\beta_0\\), we repeatedly apply the update rule until the algorithm converges, meaning that the changes in \\(\\beta_k\\) become negligible, and \\(\\beta_k\\) is close to the optimal value \\(\\beta^*\\)."
  },
  {
    "objectID": "posts/sgd/index.html#stochastic-gradient-descent-sgd",
    "href": "posts/sgd/index.html#stochastic-gradient-descent-sgd",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Stochastic Gradient Descent (SGD)",
    "text": "Stochastic Gradient Descent (SGD)\n\nStochastic Gradient Descent is a variation of the vanilla gradient descent. Instead of computing the gradient using the entire dataset, SGD updates the parameters using only a single data point or a small batch of data points at each iteration. The later one we call it mini batch SGD.\n\nSuppose our cost function is defined as the average over a dataset of size \\(n\\):\n\\[\nf(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\beta)\n\\]\nWhere \\(f_i(\\beta)\\) represents the contribution of the \\(i\\)-th data point to the total cost function. The gradient of the cost function with respect to \\(\\beta\\) is:\n\\[\n\\nabla f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\beta)\n\\]\nVanilla gradient descent would update the parameters as:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nInstead of using the entire dataset to compute the gradient, SGD approximates the gradient by using only a single data point (or a small batch). The update rule for SGD is:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f_{i_k}(\\beta_k)\n\\]\nWhere:\n\n\\(i_k\\) is the index of a randomly selected data point at iteration \\(k\\).\n\\(\\nabla f_{i_k}(\\beta_k)\\) is the gradient of the cost function with respect to the parameter \\(\\beta_k\\), evaluated only at the data point indexed by \\(i_k\\)."
  },
  {
    "objectID": "posts/sgd/index.html#implementation",
    "href": "posts/sgd/index.html#implementation",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Implementation",
    "text": "Implementation\n\nIn 1D\nAssume that we have a function \\(f(x)=x^2-3x+\\frac{13}{4}\\) which we want to minimize. Meaning, we want to find \\(x\\) that minimizes the function.\n\n\n\n\n\n\n\n\n\nNext, let’s implement the GD\n\n# Define the function\ndef f(x):\n    return x**2-3*x+(13/4)\n# Define the gradient function \ndef grad_f(x):\n    return 2*x-3\n\n# Take a random guess from the domain\nlocal_min = np.random.choice(x)\nprint('Initial guess: ',local_min )\n\n# Hyper-parameters \nlearning_rate = 0.01\ntraining_epochs = 100\n\nmodel_params = np.zeros((training_epochs,2))\nfor i in range(training_epochs):\n    grad = grad_f(local_min)\n    local_min =local_min - learning_rate*grad \n    model_params[i,:] = [local_min, grad]\n\nprint('Empirical/estimated local minimum:',local_min)\n\nfig, axs = plt.subplots(1,2, figsize=(8.5,3.8))\nfor i in range(2):\n    axs[i].plot(model_params[:,i],'-')\n    axs[i].set_xlabel('Iteration')\n    axs[i].set_title(f'Final estimated Min: {local_min:.5f}')\naxs[0].set_ylabel('Local Min')\naxs[1].set_ylabel('Derivative')\nplt.show()\n\nInitial guess:  1.9589589589589589\nEmpirical/estimated local minimum: 1.5608669333110554\n\n\n\n\n\n\n\n\n\nAlternatively, if we want to set a tolerance, this is how we set that\n\n# Take a random guess from the domain\nlocal_min = np.random.choice(x)\nprint('Initial guess:', local_min)\n\n# Hyper-parameters \nlearning_rate = 0.01\ntolerance = 1e-2  # Stop if gradient is smaller than this value\n\n# Lists to store optimization progress\nlocal_min_vals = []\ngrad_vals = []\niteration = 0  # Track the number of iterations\n\n# Gradient Descent Loop (Runs until gradient is small enough)\nwhile True:\n    grad = grad_f(local_min)\n    # Stop when gradient is smaller than tolerance\n    if abs(grad) &lt; tolerance:\n        print(f\"Stopping at iteration {iteration} (grad={grad:.5f})\")\n        break\n    # Update local minimum\n    local_min = local_min - learning_rate * grad  \n    # Store values\n    local_min_vals.append(local_min)\n    grad_vals.append(grad)\n    iteration += 1  # Increment iteration count\n\nprint('Empirical/estimated local minimum:', local_min)\n\n# Convert lists to numpy arrays\nlocal_min_vals = np.array(local_min_vals)\ngrad_vals = np.array(grad_vals)\n\n# Plot Results\nfig, axs = plt.subplots(1, 2, figsize=(8.5, 3.8))\n\naxs[0].plot(local_min_vals, '.', label=\"Local Min Estimate\")\naxs[0].set_xlabel('Iteration')\naxs[0].set_ylabel('Local Min')\naxs[0].legend()\n\naxs[1].plot(grad_vals, '.', label=\"Gradient Value\")\naxs[1].set_xlabel('Iteration')\naxs[1].set_ylabel('Derivative')\naxs[1].legend()\n\nplt.suptitle(f'Final estimated Min: {local_min:.5f}')\nplt.show()\n\nInitial guess: -0.4444444444444444\nStopping at iteration 296 (grad=-0.00983)\nEmpirical/estimated local minimum: 1.4950828444818784\n\n\n\n\n\n\n\n\n\n\n\nIn 2D\n\nLet’s imagine a hypothetical scenario, Walmart Inc. wants to explore their business in a new twon. They want to have their store in location so that the total distance of the store from all the houses in the neighborhood is the smallest possible. If they have the data of \\(n\\) houses with corresponding coordinates of the houses, return the optimized location for the store.\n\nThe Euclidean distance between two points \\((x_1,y_1)\\) and \\((x_2,y_2)\\) is given by\n\\[d=\\sqrt{(x_1-x_2)^2+(y_1-y_2)}\\]\nAssume that \\(P=(x,y)\\) is the coordinate of Walmart. So for a total of \\(n\\) such points the total distance \\(D\\) from the point \\(P\\) is a function of two variable \\(x\\) and \\(y\\) of the following form\n\\[D=f(x,y)=\\sum_{i=1}^{n}\\sqrt{(x-x_i)^2+(y-y_i)^2}\\]\n\nimport plotly.offline as iplot\nimport plotly as py\npy.offline.init_notebook_mode(connected=True)\nimport plotly.graph_objects as go\n\n\ndef f(x,y, c, d):\n    return np.sqrt((x-c)**2+(y-d)**2)\n\nx = np.linspace(-10,10, 400)\ny = np.linspace(-10,10, 400)\nx, y = np.meshgrid(x,y)\n\nc, d = 0,1\n\nz = f(x, y, c, d)\n\nfig = go.Figure(data=[go.Surface(z=z, x=x, y=y)])\nfig.update_layout(\n    title='3D plot',\n    scene=dict(\n        xaxis_title = 'x',\n        yaxis_title = 'y',\n        zaxis_title = 'z'\n    )\n)\n\nfig.show()\n\n                                                \n\n\nall we need to do is to minimize the function \\(f(x,y)\\) and to do that we need to calculate the gradient vector which is the partial derivative of \\(f(x,y)\\) with respect to \\(x\\) and \\(y\\). So,\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}& = \\sum_{i=1}^{n} \\frac{x-x_i}{\\sqrt{(x-x_i)^2+(y-y_i)^2}}\\\\\n\\frac{\\partial f}{\\partial y}& = \\sum_{i=1}^{n} \\frac{y-y_i}{\\sqrt{(x-x_i)^2+(y-y_i)^2}}\\\\\n& \\\\\n\\implies \\nabla f(x,y) &= \\begin{bmatrix}\\frac{\\partial f}{\\partial x}\\\\\\frac{\\partial f}{\\partial y}\\end{bmatrix}\n\\end{align*}\\]\nThen the algorithm\n\\[\\begin{align*}\n\\begin{bmatrix}x_{i+1}\\\\y_{i+1}\\end{bmatrix}&= \\begin{bmatrix}x_{i}\\\\y_{i}\\end{bmatrix} - \\eta_i \\begin{bmatrix}\\frac{\\partial f}{\\partial x}|_{x_i}\\\\\\frac{\\partial f}{\\partial y}|_{y_i}\\end{bmatrix}\n\\end{align*}\\]\nwhere, the \\(\\eta\\) is the step size or learning rate that scales the size of the move towards the opposite of the gradient direction.\nNext, how do we control the numerical stability of the algorithm? We need to decrease the step size at each iteration which. This is called the rate of decay. We also need a termination factor or tolerance level that determines if we can stop the iteration. Sometimes, for a deep down convex function, the process oscillates back and forth around a range of values. In this case, applying a damping factor increases the chance for a smooth convergence.\n\n\nGradient Descent (GD)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\nclass GDdistanceMin:\n    def __init__(self, step_size=1, decay_rate=0.99, tolerance=1e-7, damping_rate=0.75, points=[]):\n        self.step_size = step_size\n        self.decay_rate = decay_rate\n        self.tolerance = tolerance\n        self.damping_rate = damping_rate\n        self.points = points\n        self.x = sum(x for x, y in points) / len(points)  # Initialization\n        self.y = sum(y for x, y in points) / len(points)  # Initialization\n        self.x_updates = []\n        self.y_updates = []\n\n    def _partial_derivative_x(self, x, y):\n        grad_x = 0\n        for xi, yi in self.points:\n            if x != xi or y != yi:\n                grad_x += (x - xi) / math.sqrt((x - xi)**2 + (y - yi)**2)\n        return grad_x\n\n    def _partial_derivative_y(self, x, y):\n        grad_y = 0\n        for xi, yi in self.points:\n            if x != xi or y != yi:\n                grad_y += (y - yi) / math.sqrt((x - xi)**2 + (y - yi)**2)\n        return grad_y\n\n    def gradient_descent(self):\n        dx, dy = 0, 0\n        while self.step_size &gt; self.tolerance:\n            dx = self._partial_derivative_x(self.x, self.y) + self.damping_rate * dx \n            dy = self._partial_derivative_y(self.x, self.y) + self.damping_rate * dy \n            self.x -= self.step_size * dx \n            self.x_updates.append(self.x)\n            self.y -= self.step_size * dy \n            self.y_updates.append(self.y)\n            self.step_size *= self.decay_rate\n        return (self.x, self.y)\n\ndef f(x, y, c, d):\n    return np.sqrt((x - c)**2 + (y - d)**2)\n\n# Define points\npoints = [(1, 3), (-2, 4), (3, 4), (-2, 1), (9, 2), (-5, 2)]\ngd_min = GDdistanceMin(points=points)\nmin_pt = gd_min.gradient_descent()\nxs = gd_min.x_updates\nys = gd_min.y_updates\nprint(\"Minimum point:\", min_pt)\n\nc, d = min_pt\n\n# Create a grid for plotting\nx = np.linspace(-10, 10, 400)\ny = np.linspace(-10, 10, 400)\nx_grid, y_grid = np.meshgrid(x, y)\nz = f(x_grid, y_grid, c, d)\n\n# Calculate z values for the updates\nzs = [f(xi, yi, c, d) for xi, yi in zip(xs, ys)]\n\n# Plotting\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(x_grid, y_grid, z, cmap='viridis', alpha=0.6)\nax.scatter(xs, ys, zs, color='red', s=50, label=\"Updates\", marker='o')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\nMinimum point: (0.9999998869194062, 2.999999989764354)\n\n\n\n\n\n\n\n\n\nTesting\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "research/extrasgld/index.html",
    "href": "research/extrasgld/index.html",
    "title": "Generalized EXTRA stochastic gradient Langevin dynamics",
    "section": "",
    "text": "Langevin algorithms are popular Markov Chain Monte Carlo methods for Bayesian learning, particularly when the aim is to sample from the posterior distribution of a parametric model, given the input data and the prior distribution over the model parameters. Their stochastic versions such as stochastic gradient Langevin dynamics (SGLD) allow iterative learning based on randomly sampled mini-batches of large datasets and are scalable to large datasets. However, when data is decentralized across a network of agents subject to communication and privacy constraints, standard SGLD algorithms cannot be applied. Instead, we employ decentralized SGLD (DE-SGLD) algorithms, where Bayesian learning is performed collaboratively by a network of agents without sharing individual data.      Nonetheless, existing DE-SGLD algorithms induce a bias at every agent that can negatively impact performance; this bias persists even when using full batches and is attributable to network effects. Motivated by the EXTRA algorithm and its generalizations for decentralized optimization, we propose the generalized EXTRA stochastic gradient Langevin dynamics, which eliminates this bias in the full-batch setting. Moreover, we show that, in the mini-batch setting, our algorithm provides performance bounds that significantly improve upon those of standard DE-SGLD algorithms in the literature. Our numerical results also demonstrate the efficiency of the proposed approach.     Performance of the EXTRA SGLD for Bayesian linear regression on four different network structures. Out of 20 agents, we report only the first 4 agents and the mean of the nodes \\(\\bar{\\beta}^{(k)}=\\frac{1}{N}\\sum_{i=1}^{N}\\beta_i^{(k)}\\)      Comparative accuracy distribution of the DE-SGLD and EXTRA SGLD method across different network structures on Breast Cancer data set. The plots are from a randomly selected node.\n\n\n    \n        \n    \n\n    \n        \n    \n\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{gurbuzbalaban2024,\n  author = {Gurbuzbalaban, Mert and Rafiqul Islam, Mohammad and Wang,\n    Xiaoyu and Zhu, Lingjiong},\n  title = {Generalized {EXTRA} Stochastic Gradient {Langevin} Dynamics},\n  date = {2024-02-12},\n  url = {https://mrislambd.github.io/research/extrasgld/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGurbuzbalaban, Mert, Mohammad Rafiqul Islam, Xiaoyu Wang, and Lingjiong\nZhu. 2024. “Generalized EXTRA Stochastic Gradient Langevin\nDynamics.” February 12, 2024. https://mrislambd.github.io/research/extrasgld/."
  },
  {
    "objectID": "research/rulmc/index.html",
    "href": "research/rulmc/index.html",
    "title": "Reflected Underdamped Langevin Monte Carlo",
    "section": "",
    "text": "Ongoing research. The details will be available soon.\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "",
    "section": "Welcome",
    "text": "Welcome\n\nHi, thank you and welcome to my website. My name is Mohammad Rafiqul Islam (Rafiq Islam), and I am a Ph.D. candidate in the Mathematics Department at Florida State University. My research area is Theoretical Machine Learning, particularly optimization and sampling techniques in Bayesian learning using Markov-Chain Monte Carlo (MCMC) algorithms. I work on developing scalable algorithms for collaborative learning under regularization and privacy constraints. My research is conducted under the supervision of professor  Lingjiong Zhu.  My academic journey has been both diverse and enriching, spanning multiple institutions and fields of study. Before joining FSU, I obtained a master’s degree in Mathematics from Youngstown State University in Ohio, USA. Prior to that, I completed my undergraduate degree in Mathematics at the University of Dhaka (DU), Bangladesh, followed by a one-year integrated master’s program in Applied Mathematics at the same institution."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "",
    "section": "About",
    "text": "About\n\nHi, thank you and welcome to my website. My name is Mohammad Rafiqul Islam (Rafiq Islam), and I am a Ph.D. candidate in the Mathematics Department at Florida State University. My research focuses on Theoretical Machine Learning, particularly optimization and sampling techniques in Bayesian learning using Markov-Chain Monte Carlo (MCMC) algorithms. I work on developing scalable algorithms for collaborative learning under regularization and privacy constraints. My research is conducted under the supervision of professor  Lingjiong Zhu.  I hold a Bachelor of Science (BS) in Mathematics degree from the  University of Dhaka, Bangladesh, with minors in Statistics, Physics, and Computer Science. I also completed an integrated master’s degree in Applied Mathematics from the same institution. During my master’s studies, I pursued actuarial exams and passed several professional exams from the Institute and Faculty of Actuaries (IFoA), UK  which are equivalent to the exams P, FM, and Life Contingencies of  Society of Actuaries (SOA), USA . With this background, I began my professional career in the life insurance industries in Bangladesh, working for two years at two prominent life insurance companies.  In 2018, I moved to the United States to pursue a second master’s degree in Mathematics, specializing in Statistics and Data Analytics, at Youngstown State University, Ohio. During my studies and professional work, I developed a deeper interest in data science, machine learning, and research, which ultimately led me to pursue a Ph.D. in Mathematics with a focus on Theoretical Machine Learning."
  },
  {
    "objectID": "publication/pub1/index.html",
    "href": "publication/pub1/index.html",
    "title": "Comparison of financial models for stock price prediction",
    "section": "",
    "text": "Time series analysis of daily stock data and building predictive models are complicated. This project presents a comparative study for stock price prediction using three different methods, namely autoregressive integrated moving average, artificial neural network, and stochastic process-geometric Brownian motion. Each of the methods is used to build predictive models using historical stock data collected from Yahoo Finance. Finally, output from each of the models is compared to the actual stock price. Empirical results show that the conventional statistical model and the stochastic model provide better approximation for next-day stock price prediction compared to the neural network model.\n\n\n    \n        \n    \n    \n        \n    \n\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@article{rafiqul_islam2020,\n  author = {Rafiqul Islam, Mohammad and Nguyen, Nguyet},\n  publisher = {MDPI},\n  title = {Comparison of Financial Models for Stock Price Prediction},\n  journal = {Journal of Risk and Financial Management},\n  date = {2020-08-14},\n  url = {https://www.mdpi.com/1911-8074/13/8/181},\n  doi = {10.3390/jrfm13080181},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRafiqul Islam, Mohammad, and Nguyet Nguyen. 2020. “Comparison of\nFinancial Models for Stock Price Prediction.” Journal of Risk\nand Financial Management, August. https://doi.org/10.3390/jrfm13080181."
  },
  {
    "objectID": "posts/sgd/index.html#implementation-of-gd",
    "href": "posts/sgd/index.html#implementation-of-gd",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Implementation of GD",
    "text": "Implementation of GD\n\nIn 1D\nAssume that we have a function \\(f(x)=x^2-3x+\\frac{13}{4}\\) which we want to minimize. Meaning, we want to find \\(x\\) that minimizes the function.\n\n\n\n\n\n\n\n\n\nNext, let’s implement the GD1\n\n# Define the function\ndef f(x):\n    return x**2-3*x+(13/4)\n# Define the gradient function \ndef grad_f(x):\n    return 2*x-3\n\n# Take a random guess from the domain\nlocal_min = np.random.choice(x)\nprint('Initial guess: ',local_min )\n\n# Hyper-parameters \nlearning_rate = 0.01\ntraining_epochs = 200\n\nmodel_params = np.zeros((training_epochs,2))\nfor i in range(training_epochs):\n    grad = grad_f(local_min)\n    local_min =local_min - learning_rate*grad \n    model_params[i,:] = [local_min, grad]\n\nprint('Empirical/estimated local minimum:',local_min)\n\nfig, axs = plt.subplots(1,2, figsize=(8.5,3.8))\nfor i in range(2):\n    axs[i].plot(model_params[:,i],'-')\n    axs[i].set_xlabel('Iteration')\n    axs[i].set_title(f'Final estimated Min: {local_min:.5f}')\naxs[0].set_ylabel('Local Min')\naxs[1].set_ylabel('Derivative')\nplt.show()\n\nInitial guess:  4.145145145145145\nEmpirical/estimated local minimum: 1.546522671577196\n\n\n\n\n\n\n\n\n\nAlternatively, if we want to set a tolerance, this is how we set that\n\n# Take a random guess from the domain\nlocal_min = np.random.choice(x)\nprint('Initial guess:', local_min)\n\n# Hyper-parameters \nlearning_rate = 0.01\ntolerance = 1e-2  # Stop if gradient is smaller than this value\n\n# Lists to store optimization progress\nlocal_min_vals = []\ngrad_vals = []\niteration = 0  # Track the number of iterations\n\n# Gradient Descent Loop (Runs until gradient is small enough)\nwhile True:\n    grad = grad_f(local_min)\n    # Stop when gradient is smaller than tolerance\n    if abs(grad) &lt; tolerance:\n        print(f\"Stopping at iteration {iteration} (grad={grad:.5f})\")\n        break\n    # Update local minimum\n    local_min = local_min - learning_rate * grad  \n    # Store values\n    local_min_vals.append(local_min)\n    grad_vals.append(grad)\n    iteration += 1  # Increment iteration count\n\nprint('Empirical/estimated local minimum:', local_min)\n\n# Convert lists to numpy arrays\nlocal_min_vals = np.array(local_min_vals)\ngrad_vals = np.array(grad_vals)\n\n# Plot Results\nfig, axs = plt.subplots(1, 2, figsize=(8.5, 3.8))\n\naxs[0].plot(local_min_vals, '.', label=\"Local Min Estimate\")\naxs[0].set_xlabel('Iteration')\naxs[0].set_ylabel('Local Min')\naxs[0].legend()\n\naxs[1].plot(grad_vals, '.', label=\"Gradient Value\")\naxs[1].set_xlabel('Iteration')\naxs[1].set_ylabel('Derivative')\naxs[1].legend()\n\nplt.suptitle(f'Final estimated Min: {local_min:.5f}')\nplt.show()\n\nInitial guess: 4.992992992992993\nStopping at iteration 325 (grad=0.00983)\nEmpirical/estimated local minimum: 1.5049166842018873\n\n\n\n\n\n\n\n\n\n\nParametric variation\nLet’s consider a different problem, \\(f(x)=-e^{-\\frac{x^2}{10}}(0.2x\\cos x+\\sin x)\\). We want to find the \\(x\\) values and optimal hyper-parameters that minimizes \\(f(x)\\).\n\nx = np.linspace(-3*np.pi, 3*np.pi, 400)\ny = -np.exp(-(x**2/10))*(0.2*x*np.cos(x)+np.sin(x))\n\ndy = np.exp(-(x**2/10))*((0.04*x**2-1.2)*np.cos(x)+0.4*x*np.sin(x))\nplt.plot(x,y, x, dy)\nplt.legend(['f(x)','df'])\nplt.show()\n\n\n\n\n\n\n\n\nClearly, the global minimum is somewhere in between 0 to 2.5, maybe around 1.25. Now let’s apply GD with varying parameters\n\ndef f(x):\n    f = -np.exp(-(x**2/10))*(0.2*x*np.cos(x)+np.sin(x))\n    return f\ndef df(x):\n    ddx_of_f = np.exp(-(x**2/10))*((0.04*x**2-1.2)*np.cos(x)+0.4*x*np.sin(x))\n    return ddx_of_f\n\nlearning_rate = 0.003\ntraining_epochs = 1000\nlocal_min = np.random.choice(x,1)\nprint('The chosen initial point: ', local_min)\n\nfor i in range(training_epochs):\n    grad = df(local_min)\n    local_min = local_min - learning_rate*grad \n\nplt.plot(x,f(x), x, df(x),'--')\nplt.plot(local_min, df(local_min),'ro')\nplt.plot(local_min, f(local_min),'ro')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend(['f(x)','df','f(x) min'])\nplt.title('Iterated local min at x = %s'%local_min[0])\nplt.show()\n\nThe chosen initial point:  [6.82646825]\n\n\n\n\n\n\n\n\n\nNow let’s see how we can pick the right initial point\n\n# Vary the starting point \nstarting_points = np.linspace(-5,5,50)\nstopping_points = np.zeros(len(starting_points))\n\ntraining_epochs = 1000\nlearning_rate = 0.01\nfor idx, local_min in enumerate(starting_points):\n    for i in range(training_epochs):\n        grad = df(local_min)\n        local_min = local_min - learning_rate * grad \n    stopping_points[idx] = local_min\nplt.plot(starting_points, stopping_points, 's-')\nplt.xlabel('Starting points')\nplt.ylabel('Stopping points')\nplt.show()\n\n\n\n\n\n\n\n\nBased on this, if we start with any point roughly between \\((-1,3.5)\\) we will end up with the global minimum. However, points outside this interval may take the iterative process to other local minima.\n\n# varying learning rates \nlrs = np.linspace(1e-10, 1e-2, 30)\nstopping_points = np.zeros(len(lrs))\n\ntraining_epochs = 1000\n\nfor idx, lr  in enumerate(lrs):\n    # fixed initial point \n    local_min = -0.03\n    for i in range(training_epochs):\n        grad = df(local_min)\n        local_min = local_min - lr*grad \n    stopping_points[idx] = local_min\nplt.plot(lrs, stopping_points, 's-')\nplt.xlabel('learning rates')\nplt.ylabel('Stopping points')\nplt.show()\n\n\n\n\n\n\n\n\nSo this suggests that a learning rate between 0.003 to 0.005 is a good start.\n\nlrs = np.linspace(1e-10, 1e-2, 30)\ntraining_epochs = np.round(np.linspace(10,1000,50))\n\nstopping_points = np.zeros((len(lrs), len(training_epochs)))\n\nfor lr_idx, lr in enumerate(lrs):\n    for tr_idx, tr_epoch in enumerate(training_epochs):\n        local_min = -0.03\n        \n        for i in range(int(tr_epoch)):\n            grad = df(local_min)\n            local_min = local_min - lr * grad \n\n        stopping_points[lr_idx, tr_idx] = local_min\nplt.imshow(stopping_points, extent=[lrs[0], lrs[-1],\\\n     training_epochs[0], training_epochs[-1]], aspect='auto',\\\n        vmin=0, vmax=1.5)\nplt.xlabel('learning rate')\nplt.ylabel('training epochs')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\nWe know that the expected minimum at \\(x \\approx 1.17\\) so the color corresponding to this is light. Therefore, if we choose a very small learning rate and/or a larger training epoch then we end up to the darker area which is not a good approximation.\n\nplt.plot(lrs, stopping_points)\nplt.xlabel('learning rate')\nplt.ylabel('Iterated solution')\nplt.title('Each line is a training epoch N')\nplt.show\n\n\n\n\n\n\n\n\n\n\n\nIn 2D\n\nLet’s imagine a hypothetical scenario, Walmart Inc. wants to explore their business in a new twon. They want to have their store in location so that the total distance of the store from all the houses in the neighborhood is the smallest possible. If they have the data of \\(n\\) houses with corresponding coordinates of the houses, return the optimized location for the store.\n\nThe Euclidean distance between two points \\((x_1,y_1)\\) and \\((x_2,y_2)\\) is given by\n\\[d=\\sqrt{(x_1-x_2)^2+(y_1-y_2)}\\]\nAssume that \\(P=(x,y)\\) is the coordinate of Walmart. So for a total of \\(n\\) such points the total distance \\(D\\) from the point \\(P\\) is a function of two variable \\(x\\) and \\(y\\) of the following form\n\\[D=f(x,y)=\\sum_{i=1}^{n}\\sqrt{(x-x_i)^2+(y-y_i)^2}\\]\n\nimport plotly.offline as iplot\nimport plotly as py\npy.offline.init_notebook_mode(connected=True)\nimport plotly.graph_objects as go\n\n\ndef f(x,y, c, d):\n    return np.sqrt((x-c)**2+(y-d)**2)\n\nx = np.linspace(-10,10, 400)\ny = np.linspace(-10,10, 400)\nx, y = np.meshgrid(x,y)\n\nc, d = 0,0\n\nz = f(x, y, c, d)\n\nfig = go.Figure(data=[go.Surface(z=z, x=x, y=y)])\nfig.update_layout(\n    title='3D plot',\n    scene=dict(\n        xaxis_title = 'x',\n        yaxis_title = 'y',\n        zaxis_title = 'z'\n    )\n)\n\nfig.show()\n\n                                                \n\n\nall we need to do is to minimize the function \\(f(x,y)\\) and to do that we need to calculate the gradient vector which is the partial derivative of \\(f(x,y)\\) with respect to \\(x\\) and \\(y\\). So,\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}& = \\sum_{i=1}^{n} \\frac{x-x_i}{\\sqrt{(x-x_i)^2+(y-y_i)^2}}\\\\\n\\frac{\\partial f}{\\partial y}& = \\sum_{i=1}^{n} \\frac{y-y_i}{\\sqrt{(x-x_i)^2+(y-y_i)^2}}\\\\\n& \\\\\n\\implies \\nabla f(x,y) &= \\begin{bmatrix}\\frac{\\partial f}{\\partial x}\\\\\\frac{\\partial f}{\\partial y}\\end{bmatrix}\n\\end{align*}\\]\nThen the algorithm\n\\[\\begin{align*}\n\\begin{bmatrix}x_{i+1}\\\\y_{i+1}\\end{bmatrix}&= \\begin{bmatrix}x_{i}\\\\y_{i}\\end{bmatrix} - \\eta_i \\begin{bmatrix}\\frac{\\partial f}{\\partial x}|_{x_i}\\\\\\frac{\\partial f}{\\partial y}|_{y_i}\\end{bmatrix}\n\\end{align*}\\]\nwhere, the \\(\\eta\\) is the step size or learning rate that scales the size of the move towards the opposite of the gradient direction.\nNext, how do we control the numerical stability of the algorithm? We need to decrease the step size at each iteration which. This is called the rate of decay. We also need a termination factor or tolerance level that determines if we can stop the iteration. Sometimes, for a deep down convex function, the process oscillates back and forth around a range of values. In this case, applying a damping factor increases the chance for a smooth convergence.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\nclass GDdistanceMin:\n    def __init__(self, step_size=1, decay_rate=0.99, tolerance=1e-7, damping_rate=0.75, points=[]):\n        self.step_size = step_size\n        self.decay_rate = decay_rate\n        self.tolerance = tolerance\n        self.damping_rate = damping_rate\n        self.points = points\n        self.x = sum(x for x, y in points) / len(points)  # Initialization\n        self.y = sum(y for x, y in points) / len(points)  # Initialization\n        self.x_updates = []\n        self.y_updates = []\n\n    def _partial_derivative_x(self, x, y):\n        grad_x = 0\n        for xi, yi in self.points:\n            if x != xi or y != yi:\n                grad_x += (x - xi) / math.sqrt((x - xi)**2 + (y - yi)**2)\n        return grad_x\n\n    def _partial_derivative_y(self, x, y):\n        grad_y = 0\n        for xi, yi in self.points:\n            if x != xi or y != yi:\n                grad_y += (y - yi) / math.sqrt((x - xi)**2 + (y - yi)**2)\n        return grad_y\n\n    def gradient_descent(self):\n        dx, dy = 0, 0\n        while self.step_size &gt; self.tolerance:\n            dx = self._partial_derivative_x(self.x, self.y) + self.damping_rate * dx \n            dy = self._partial_derivative_y(self.x, self.y) + self.damping_rate * dy \n            self.x -= self.step_size * dx \n            self.x_updates.append(self.x)\n            self.y -= self.step_size * dy \n            self.y_updates.append(self.y)\n            self.step_size *= self.decay_rate\n        return (self.x, self.y)\n\ndef f(x, y, c, d):\n    return np.sqrt((x - c)**2 + (y - d)**2)\n\n# Define points\npoints = [(1, 3), (-2, 4), (3, 4), (-2, 1), (9, 2), (-5, 2)]\ngd_min = GDdistanceMin(points=points)\nmin_pt = gd_min.gradient_descent()\nxs = gd_min.x_updates\nys = gd_min.y_updates\nprint(\"Minimum point:\", min_pt)\n\nc, d = min_pt\n\n# Create a grid for plotting\nx = np.linspace(-10, 10, 400)\ny = np.linspace(-10, 10, 400)\nx_grid, y_grid = np.meshgrid(x, y)\nz = f(x_grid, y_grid, c, d)\n\n# Calculate z values for the updates\nzs = [f(xi, yi, c, d) for xi, yi in zip(xs, ys)]\n\n# Plotting\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(x_grid, y_grid, z, cmap='viridis', alpha=0.6)\nax.scatter(xs, ys, zs, color='red', s=50, label=\"Updates\", marker='o')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\nMinimum point: (0.9999998869194062, 2.999999989764354)\n\n\n\n\n\n\n\n\n\nDifferent approach2\n\nimport sympy as sym \nsym.init_printing()\nfrom IPython.display import display, Math\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nset_matplotlib_formats('svg') \n\n\ndef distance_function(x,y,c,d):\n    x,y = np.meshgrid(x,y)\n    z = np.sqrt((x-c)**2+(y-d)**2)\n    return z\n\nc,d = 0,0\nx = np.linspace(-5,5, 400)\ny = np.linspace(-5,5, 400)\nz = distance_function(x,y,c,d)\n\nplt.imshow(z, extent=[x[0],x[-1], y[0],y[-1]],vmin=-5, vmax=5, origin='lower')\nplt.show()\n\n\n\n\n\n\n\n\nSo, the minimum is in the center of the deep color area. Let’s compute the derivatives using sympy library\n\nsx, sy = sym.symbols('sx,sy')\nsz = sym.sqrt((sx-c)**2+(sy-d)**2)\n\ndf_sx = sym.lambdify((sx,sy), sym.diff(sz, sx), 'sympy')\ndf_sy = sym.lambdify((sx,sy), sym.diff(sz, sy), 'sympy')\n\ndisplay(Math(f\"\\\\frac{{\\\\partial f}}{{\\\\partial x}}={sym.latex(sym.diff(sz,sx))}\\\n     \\\\text{{ and }}  \\\\frac{{\\\\partial f}}{{\\\\partial x}}\\\\mid_{{(1,1)}} = {df_sx(1,1).evalf()}\"))\ndisplay(Math(f\"\\\\frac{{\\\\partial f}}{{\\\\partial y}}={sym.latex(sym.diff(sz,sy))} \\\n     \\\\text{{ and }}  \\\\frac{{\\\\partial f}}{{\\\\partial y}}\\\\mid_{{(1,1)}} = {df_sy(1,1).evalf()}\"))\n\n\\(\\displaystyle \\frac{\\partial f}{\\partial x}=\\frac{sx}{\\sqrt{sx^{2} + sy^{2}}}     \\text{ and }  \\frac{\\partial f}{\\partial x}\\mid_{(1,1)} = 0.707106781186548\\)\n\n\n\\(\\displaystyle \\frac{\\partial f}{\\partial y}=\\frac{sy}{\\sqrt{sx^{2} + sy^{2}}}      \\text{ and }  \\frac{\\partial f}{\\partial y}\\mid_{(1,1)} = 0.707106781186548\\)\n\n\nNow let’s implement the GD\n\n# Let's pick a random starting point(uniformly distributed between -2 and 2)\nlocal_min = np.random.rand(2)*4-2\nstart_point = local_min[:] # make a copy\n\nlearning_rate = 0.01\ntraining_epochs = 1000\n\ntrajectory = np.zeros((training_epochs,2))\n\nfor i in range(training_epochs):\n    grad = np.array([\n        df_sx(local_min[0],local_min[1]).evalf(),\n        df_sy(local_min[0],local_min[1]).evalf()\n    ])\n    local_min = local_min - learning_rate*grad\n    trajectory[i,:] = local_min\n\nprint('Starting point ',start_point)\nprint('Local min found ',local_min)\n\nplt.imshow(z, extent=[x[0],x[-1], y[0],y[-1]],vmin=-5, vmax=5, origin='lower')\nplt.plot(start_point[0], start_point[1], 'bs')\nplt.plot(local_min[0],local_min[1],'rs')\nplt.plot(trajectory[:,0],trajectory[:,1], 'b', linewidth=3)\nplt.legend(['random start', 'local min'])\nplt.colorbar()\nplt.show()\n\nStarting point  [-0.80612295  1.54497531]\nLocal min found  [-0.00121974120823232 0.00233769557750716]"
  },
  {
    "objectID": "posts/sgd/index.html#implementation-of-sgd",
    "href": "posts/sgd/index.html#implementation-of-sgd",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Implementation of SGD",
    "text": "Implementation of SGD\nPersonally, I think this webpage is one of the best resources for learning how to implement SGD.\n\nShare on\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/sgd/index.html#footnotes",
    "href": "posts/sgd/index.html#footnotes",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe implementation is taken from the the Udemy course A Deep Understanding of Deep Learning↩︎\nThe implementation is taken from the the Udemy course A Deep Understanding of Deep Learning↩︎"
  },
  {
    "objectID": "research/holmc/index.html",
    "href": "research/holmc/index.html",
    "title": "Higher-order Langevin Algorithms",
    "section": "",
    "text": "Langevin algorithms are popular Markov chain Monte Carlo (MCMC) methods for large-scale sampling problems that often arise in data science. We propose Monte Carlo algorithms based on \\(P\\)-th order Langevin dynamics for any \\(P\\geq 3\\). Our design of \\(P\\)-th order Langevin Monte Carlo (LMC) algorithms is by combining splitting and accurate integration methods. We obtain Wasserstein convergence guarantees for sampling from distributions with log-concave and smooth densities. Specifically, the mixing time of the \\(P\\)-th order LMC algorithm scales as \\(O\\left(d^{\\frac{1}{\\mathcal{R}}}/\\epsilon^{\\frac{1}{2\\mathcal{R}}} \\right)\\) for \\(\\mathcal{R}=4\\cdot\\mathbf{1}_{\\{ P=3\\}}+ (2P-1)\\cdot\\mathbf{1}_{\\{ P\\geq 4\\}}\\), which have better dependence on the dimension and the accuracy level as \\(P\\) grows. Numerical experiments illustrate the efficiency of our proposed algorithms.\n\n\n  \n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{l._dang2025,\n  author = {L. Dang, Thanh and Gurbuzbalaban, Mert and Rafiqul Islam,\n    Mohammad and Yao, Nihan and Zhu, Lingjiong},\n  title = {Higher-Order {Langevin} {Algorithms}},\n  date = {2025-07-23},\n  url = {https://mrislambd.github.io/research/holmc/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nL. Dang, Thanh, Mert Gurbuzbalaban, Mohammad Rafiqul Islam, Nihan Yao,\nand Lingjiong Zhu. 2025. “Higher-Order Langevin\nAlgorithms.” July 23, 2025. https://mrislambd.github.io/research/holmc/."
  }
]