[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nData Structure and Algorithms: Basic Programming Hacks\n\n\n5 min\n\n\n\nRafiq Islam\n\n\nWednesday, September 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Key Statistical Concepts for Interview Prep\n\n\n5 min\n\n\n\nRafiq Islam\n\n\nThursday, September 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Implementation of General Machine Learning Algorithms\n\n\n1 min\n\n\n\nRafiq Islam\n\n\nThursday, August 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Decision Tree Classifier: A Mathematical Approach\n\n\n7 min\n\n\n\nRafiq Islam\n\n\nFriday, August 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview probabilities\n\n\n3 min\n\n\n\nRafiq Islam\n\n\nThursday, August 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData collection through Webscraping\n\n\n7 min\n\n\n\nRafiq Islam\n\n\nWednesday, August 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonte-Carlo Methods: PRNGs\n\n\n10 min\n\n\n\nRafiq Islam\n\n\nSunday, August 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Inference in Machine Learning: Part 1\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nSunday, July 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to generate social share buttons\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nWednesday, July 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my blog\n\n\n1 min\n\n\n\nRafiq Islam\n\n\nFriday, July 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix multiplication: Let’s make it less expensive!\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nMonday, July 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Stochastic Gradient Descent Using Simple Linear Regression\n\n\n5 min\n\n\n\nRafiq Islam\n\n\nSaturday, May 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n\n26 min\n\n\n\nRafiq Islam\n\n\nTuesday, November 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling viral disease\n\n\n3 min\n\n\n\nRafiq Islam\n\n\nTuesday, February 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Linear Algebra Proofs\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nSunday, January 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Representation: Change of Basis\n\n\n3 min\n\n\n\nRafiq Islam\n\n\nThursday, January 21, 2021\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Download a PDF"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\n\nPh.D in Mathematics, Florida State University; Florida, USA 2026 (expected)\nM.S. in Mathematics, Youngstown State University; Ohio, USA 2020\nM.S. in Applied Mathematics, University of Dhaka; Dhaka, Bangladesh 2016\nB.S. in Mathematics, University of Dhaka; Dhaka, Bangladesh 2014"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "CV",
    "section": "Work experience",
    "text": "Work experience\n\nGraduate Teaching Assistant (Fall 2021- To Date)\n\nFlorida State University\nDuties includes: Teaching, Proctoring, and Grading\nSupervisor: Penelope Kirby, Ph.D\n\nGraduate Teaching Assistant (Fall 2018 - Spring 2020)\n\nYoungstown State University University\nDuties included: Teaching, Proctoring, and Grading\nSupervisor: G. Jay Kerns, Ph.D\n\nAssistant Vice President (September 2017 - July 2018)\n\nDelta Life Insurance Company Ltd. Dhaka, Bangladesh\nDuties included: Calculated all types of claims (death, surrender, and maturity) using excel spreadsheets.\nProcessed approximately 500 claims each week and submitted corresponding statistical reports to the higher authority.\nWorked in a team to develop a new short-term endowment assurance product which played an important role to increase the company’s new business.\nRefurbished a without risk endowment product which was out of the sale. Priced insurance premiums based on different risk factors for bigger clients which impacted our life fund significantly.\nCalculated reserves for group endowment, term and premium back policies which was a vital part of the final valuation report.\nLiaised directly with the consulting actuary and provided all sorts of technical and documental supports during actuarial valuation\nSupervisor: Md. Salahuddin Soud, VP"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "CV",
    "section": "Skills",
    "text": "Skills\n\nLanguage\n\nBengali: Native\nEnglish: Fluent\n\nComputer Literacy\n\nProgramming Languages: Python, FORTRAN, Julia, R, MATLAB, Mathematica\nSoftware Development Tools: Git, GitHub, PyPi\n\nMusical Instrument: Amateur/Novice Bamboo flute player"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "CV",
    "section": "Publications",
    "text": "Publications\n\nGJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies \nMostafa, F; Saha, P; Islam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management.\nComparison of Financial Models for Stock Price Prediction \nIslam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management."
  },
  {
    "objectID": "cv.html#talks-and-presentations",
    "href": "cv.html#talks-and-presentations",
    "title": "CV",
    "section": "Talks and Presentations",
    "text": "Talks and Presentations\n\n The Heavy-Tail Phenomenon in Decentralized Stochastic Gradient Descent\nNovember 20, 2023\nPresentation at James J Love Building, Florida State University, Tallahassee, Florida\n\n Decentralized Stochastic Gradient Langevin Dynamics and Hamiltonian Monte Carlo\nOctober 05, 2023\nPresentation at James J Love Building, Florida State University, Tallahassee, Florida\n\n Sensitivity analysis for Monte Carlo and Quasi Monte Carlo option pricing\nApril 28, 2020\nPresentation at Cafaro Hall, Youngstown State University, Youngstown, Ohio"
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "CV",
    "section": "Teaching",
    "text": "Teaching\n\n Spring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n Fall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n Spring 2023: MAC1140 PreCalculus Algebra\n\n Fall 2022: MAC2311 Calculus and Analytic Geometry I\n\n Fall 2021 and Spring 2022: PreCalculus and Algebra\n\n Fall 2018 to Spring 2020: College Algebra, Trigonometry"
  },
  {
    "objectID": "cv.html#awards-and-affiliations",
    "href": "cv.html#awards-and-affiliations",
    "title": "CV",
    "section": "Awards and Affiliations",
    "text": "Awards and Affiliations\n\nAwards\n\nBettye Anne Busbee Case Graduate Fellowship & Doctoral Mentorship Recognition 2024\n\nOutstanding Graduate Student in Statistics Award for the 2019-2020 academic year, Youngstown State University.\n\nGraduate College Premiere Scholarship, Youngstown State University.\n\nMetLife Bangladesh Actuarial Study Program 2015\n\n\n\nAffiliations\n\nBangladesh Mathematical Society: Life Member\nSociety of Actuaries, SOA: Student Member\nAmerican Mathematical Society, AMS\nSociety for Industrial and Applied Mathematics"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/notebook.html",
    "href": "portfolio/dsp/medicalcost/notebook.html",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/notebook.html#library-and-packages",
    "href": "portfolio/dsp/medicalcost/notebook.html#library-and-packages",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "portfolio/dsp/dp-nlp/index.html",
    "href": "portfolio/dsp/dp-nlp/index.html",
    "title": "Disease diagnosis using classification and NLP",
    "section": "",
    "text": "Team Members\nRebecca Ceppas de Castro, Fulya Tastan, Philip Barron, Mohammad Rafiqul Islam, Nina Adhikari, Viraj Meruliya\nAutomatic Symptom Detection (ASD) and Automatic Diagnosis (AD) have seen several advances in recent years. Patients and medical professionals would benefit from tools that can aid in diagnosing diseases based on antecedents and presenting symptoms. The lack of quality healthcare in many parts of the world makes solving this problem a matter of utmost urgency. The aim of this project is to build a tool that can diagnose a disease based on a list of symptoms and contribute to our understanding of automatic diagnosis.\nProject Details\nSlides\nExecutive Summary\nGitHub Repo\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{ceppas_de_castro,_fulya_tastan,_philip_barron,_mohammad_rafiqul_islam,_nina_adhikari,_viraj_meruliya_2024,\n  author = {Ceppas de Castro, Fulya Tastan, Philip Barron, Mohammad\n    Rafiqul Islam, Nina Adhikari, Viraj Meruliya , Rebecca},\n  title = {Disease Diagnosis Using Classification and {NLP}},\n  date = {2024-06-18},\n  url = {https://mrislambd.github.io/portfolio/dsp/dp-nlp/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCeppas de Castro, Fulya Tastan, Philip Barron, Mohammad Rafiqul Islam,\nNina Adhikari, Viraj Meruliya, Rebecca. 2024. “Disease Diagnosis\nUsing Classification and NLP.” June 18, 2024. https://mrislambd.github.io/portfolio/dsp/dp-nlp/."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "portfolio.html#data-science-and-machine-learning-projects",
    "href": "portfolio.html#data-science-and-machine-learning-projects",
    "title": "",
    "section": "Data Science and Machine Learning Projects",
    "text": "Data Science and Machine Learning Projects\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nInsurance Cost Forecast by using Linear Regression\n\n\n\nFriday, August 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisease diagnosis using classification and NLP\n\n\n\nTuesday, June 18, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio.html#software-package-and-development",
    "href": "portfolio.html#software-package-and-development",
    "title": "",
    "section": "Software, Package, and Development",
    "text": "Software, Package, and Development\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nPython Application Library: desgld packaging\n\n\n\nFriday, May 3, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "codepages/medicalcost/index.html",
    "href": "codepages/medicalcost/index.html",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "import pandas as pd\n\ninsurance = pd.read_csv('insurance.csv')\n\ninsurance.sample(5, random_state=111)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n1000\n30\nmale\n22.99\n2\nyes\nnorthwest\n17361.7661\n\n\n53\n36\nmale\n34.43\n0\nyes\nsoutheast\n37742.5757\n\n\n432\n42\nmale\n26.90\n0\nno\nsouthwest\n5969.7230\n\n\n162\n54\nmale\n39.60\n1\nno\nsouthwest\n10450.5520\n\n\n1020\n51\nmale\n37.00\n0\nno\nsouthwest\n8798.5930\n\n\n\n\n\n\n\n\n\n\n\n\n\ninsurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\nNo missing data. Total 1338 observations.\n\n\n\nStatistical properties of the non-categorical variables\n\nprint(insurance.describe().T)\n\n           count          mean           std        min         25%       50%  \\\nage       1338.0     39.207025     14.049960    18.0000    27.00000    39.000   \nbmi       1338.0     30.663397      6.098187    15.9600    26.29625    30.400   \nchildren  1338.0      1.094918      1.205493     0.0000     0.00000     1.000   \ncharges   1338.0  13270.422265  12110.011237  1121.8739  4740.28715  9382.033   \n\n                   75%          max  \nage          51.000000     64.00000  \nbmi          34.693750     53.13000  \nchildren      2.000000      5.00000  \ncharges   16639.912515  63770.42801  \n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(3,1, figsize = (6,18))\n\nsns.histplot(insurance['age'], color='red', kde=True, ax= axes[0])\nsns.histplot(insurance['bmi'], color='green', kde=True, ax= axes[1])\nsns.histplot(insurance['charges'], color='blue', kde=True, ax= axes[2])\n\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\n\n\n\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8, 12))\ngrid = fig.add_gridspec(3, 2)\n\nax1 = fig.add_subplot(grid[0, 0])\nsns.countplot(x=insurance['sex'], hue=insurance['sex'], palette='Set1', legend=False, ax=ax1)\nax1.set_title('Gender Distribution')\nax1.set_facecolor('#f4f4f4')\n\nax2 = fig.add_subplot(grid[0, 1])\nsns.countplot(x=insurance['smoker'], hue=insurance['smoker'], palette='Set2', ax=ax2)\nax2.set_title('Smoker Distribution')\nax2.set_facecolor('#f4f4f4')\n\nax3 = fig.add_subplot(grid[1, :])\nsns.countplot(x='smoker', data=insurance, hue='sex', palette='Set3', ax=ax3)\nax3.set_title('Smoker Distribution by Gender')\nax3.set_facecolor('#f4f4f4')\n\nax4 = fig.add_subplot(grid[2, :])\nsns.countplot(x=insurance['region'], hue=insurance['region'], palette='Set1', ax=ax4)\nax4.set_title('Region Distribution')\nax4.set_facecolor('#f4f4f4')\n\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ncorr_matrix = insurance[['age','bmi','children','charges']].corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Matrix')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(5,1, figsize=(8,25))\nsns.scatterplot(x='age', y='charges', data=insurance, hue='sex' ,ax=axes[0])\naxes[0].set_title('Age vs Charges')\nsns.scatterplot(x='bmi', y='charges', data=insurance, hue='sex' ,ax=axes[1])\naxes[1].set_title('BMI vs Charges')\nsns.boxplot(x='children', y='charges', data=insurance, ax=axes[2])\naxes[2].set_title('Children vs Charges')\nsns.boxplot(x='sex', y='charges', data=insurance, ax=axes[3])\naxes[3].set_title('Gender vs Charges')\nsns.boxplot(x='smoker', y='charges', data=insurance, ax=axes[4])\naxes[4].set_title('Smoking vs Charges')\n\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\n\n\n\n\n\n\n\n\n\n\n\n\nsns.boxplot(y='charges', data=insurance)\nplt.title('Boxplot of Charges')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Binary Encoding for the variables with two categories\ninsurance['sex'] = insurance['sex'].map({'male':1, 'female':0})\ninsurance['smoker'] = insurance['smoker'].map({'yes':1, 'no':0})\n\n# One-Hot Encoding for the multiclas variable: region\ninsurance = pd.get_dummies(\n    insurance, columns=['region'],\n    drop_first=True,\n    dtype=int\n    )\n\n\n\n\n\n# Round the continuous charge variable to 2 decimal places\ninsurance['charges'] = insurance['charges'].round(2)\n\n# Mofe the predicting variable at the end of the dataframe\ninsurance_charges = insurance.pop('charges')\ninsurance.insert(loc = len(insurance.columns), column='charges', value=insurance_charges)\n\n# Quick look of the dataframe\ninsurance.head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion_northwest\nregion_southeast\nregion_southwest\ncharges\n\n\n\n\n0\n19\n0\n27.900\n0\n1\n0\n0\n1\n16884.92\n\n\n1\n18\n1\n33.770\n1\n0\n0\n1\n0\n1725.55\n\n\n2\n28\n1\n33.000\n3\n0\n0\n1\n0\n4449.46\n\n\n3\n33\n1\n22.705\n0\n0\n1\n0\n0\n21984.47\n\n\n4\n32\n1\n28.880\n0\n0\n1\n0\n0\n3866.86\n\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = insurance.drop('charges', axis=1)\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values,i) for i in range(len(X.columns))]\nprint(vif_data)\n\n            feature        VIF\n0               age   7.686965\n1               sex   2.003185\n2               bmi  11.358443\n3          children   1.809930\n4            smoker   1.261233\n5  region_northwest   1.890281\n6  region_southeast   2.265564\n7  region_southwest   1.960745\n\n\nSince BMI and Age have higher values for the multicolinearity, therefore we adopt the following methods\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(insurance.age.values.reshape(-1,1), insurance.bmi.values)\ninsurance['bmi_adusted_for_age'] = insurance.bmi.values - reg.predict(insurance.age.values.reshape(-1,1))\n\n\n\n\n\ninsurance['bmi_age_interact'] = insurance['bmi']*insurance['age']\n\n\n\n\n\nbins = [18,30,40,50,60,70]\nlabels = ['18-30','31-40','41-50','51-60','61-70']\ninsurance['age_group'] = pd.cut(insurance['age'], bins= bins, labels=labels)\ninsurance['bmi_zscore'] = insurance.groupby('age_group', observed=False)['bmi'].transform(lambda x: (x-x.mean())/x.std())\ninsurance.sample(5, random_state=111)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion_northwest\nregion_southeast\nregion_southwest\ncharges\nbmi_adusted_for_age\nbmi_age_interact\nage_group\nbmi_zscore\n\n\n\n\n1000\n30\n1\n22.99\n2\n1\n1\n0\n0\n17361.77\n-7.236727\n689.70\n18-30\n-1.091779\n\n\n53\n36\n1\n34.43\n0\n1\n0\n1\n0\n37742.58\n3.918706\n1239.48\n31-40\n0.652655\n\n\n432\n42\n1\n26.90\n0\n0\n0\n0\n1\n5969.72\n-3.895862\n1129.80\n41-50\n-0.678956\n\n\n162\n54\n1\n39.60\n1\n0\n0\n0\n1\n10450.55\n8.235003\n2138.40\n51-60\n1.340308\n\n\n1020\n51\n1\n37.00\n0\n0\n0\n0\n1\n8798.59\n5.777287\n1887.00\n51-60\n0.912758\n\n\n\n\n\n\n\nTo check any missing values\n\ninsurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 13 columns):\n #   Column               Non-Null Count  Dtype   \n---  ------               --------------  -----   \n 0   age                  1338 non-null   int64   \n 1   sex                  1338 non-null   int64   \n 2   bmi                  1338 non-null   float64 \n 3   children             1338 non-null   int64   \n 4   smoker               1338 non-null   int64   \n 5   region_northwest     1338 non-null   int64   \n 6   region_southeast     1338 non-null   int64   \n 7   region_southwest     1338 non-null   int64   \n 8   charges              1338 non-null   float64 \n 9   bmi_adusted_for_age  1338 non-null   float64 \n 10  bmi_age_interact     1338 non-null   float64 \n 11  age_group            1269 non-null   category\n 12  bmi_zscore           1269 non-null   float64 \ndtypes: category(1), float64(5), int64(7)\nmemory usage: 127.1 KB\n\n\nTherefore, there are some missing data.\n\ninsurance = insurance.dropna()\ninsurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1269 entries, 0 to 1337\nData columns (total 13 columns):\n #   Column               Non-Null Count  Dtype   \n---  ------               --------------  -----   \n 0   age                  1269 non-null   int64   \n 1   sex                  1269 non-null   int64   \n 2   bmi                  1269 non-null   float64 \n 3   children             1269 non-null   int64   \n 4   smoker               1269 non-null   int64   \n 5   region_northwest     1269 non-null   int64   \n 6   region_southeast     1269 non-null   int64   \n 7   region_southwest     1269 non-null   int64   \n 8   charges              1269 non-null   float64 \n 9   bmi_adusted_for_age  1269 non-null   float64 \n 10  bmi_age_interact     1269 non-null   float64 \n 11  age_group            1269 non-null   category\n 12  bmi_zscore           1269 non-null   float64 \ndtypes: category(1), float64(5), int64(7)\nmemory usage: 130.3 KB"
  },
  {
    "objectID": "codepages/medicalcost/index.html#library-and-packages",
    "href": "codepages/medicalcost/index.html#library-and-packages",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "research.html#research-interest",
    "href": "research.html#research-interest",
    "title": "",
    "section": "Research Interest",
    "text": "Research Interest\n\nMachine Learning: Centralized and Decentralized Stochastic Gradient Descent (SGD);Algorithmic Stability in SGD; Differential Privacy in machine learning algorithms\nApplied Data Science\nFinancial Mathematics"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "",
    "section": "Publications",
    "text": "Publications\n\n\n\n Google Scholar\n ResearchGate\n Orcid\n\n\n\n\nGJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies \nMostafa, F; Saha, P; Islam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management.\nComparison of Financial Models for Stock Price Prediction \nIslam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management."
  },
  {
    "objectID": "research.html#course-projects",
    "href": "research.html#course-projects",
    "title": "",
    "section": "Course Projects",
    "text": "Course Projects\n\nOption pricing techniques: A performance-based comparative study of the randomized quasi-Monte Carlo method and Fourier cosine method\nAdvisor: Prof. Giray Ökten\n\nPricing financial derivatives such as options with desired accuracy can be hard due to the nature of the functions and complicated integrals required by the pricing techniques. In this paper we investigate the pricing methodology of the European style options using two advanced numerical methods, namely, Quasi-Monte Carlo and Fourier Cosine (COS). For the RQMC method, we use the random-start Halton sequence. We use the Black-Scholes-Merton model to measure the pricing quality of both of the methods. For the numerical results we compute the option price of the call option and we found a few reasons to prefer the RQMC method over the COS method to approximate the European style options.\n\nThe Relationship Between Forced Sexual Activities And Suicidal Attempts Of The Victims\nAdvisor: Dr. Andy Chang\n\nIn project, we apply data-analytic methods to further explore the relationship between forced sexual activities and suicidal behavior among adolescents in the United States. Our findings build on existing literature that explores this relationship. The sample of the study was taken from the Youth Risk Behavior Surveillance System survey 2017. We used a chi-squared test to find the association of forced sexual activities and suicidal behavior, and we found a strong association. Then we used bi-variate logistic regression analysis to ascertain the association of race, age, sex, and education with suicidal attempts after experiencing forced sexual activity (sexual assault). The results of the following paper provide greater insight into the relationship between forced sexual activities and suicide attempts by the adolescents.\n\nStudy of Runge-Kutta Method of Higher orders and its Applications\nAdvisor: Dr. Md. Abdus Samad \n\nThis project is concerned with the study on Runge-Kutta method to apply on different order of differential equation and solve different types of problem such as initial value problem and boundary value problem in ordinary differential equation. At first we discuss about the definition and generation of differential equation specially based on partial differential equation and then definition of Runge-kutta method and the derivation of midpoint method and the formula of Runge-Kutta metod of fourth order and sixth order. We also write FORTRAN 90/95 program for different order of Runge-Kutta methods. We have solved some examples of fourth order R-K method and sixth order R-K method to get the application of R-K method. We also compared the solution of R-K method with exact solution for different step sizes. Then we have given simultaneous first order differential equation and second order differential equation and then solved them by fourth order Runge-Kutta method. At last we have discussed the boundary value problem which we have solved by fourth and sixth order R-K method. After that we have written the algorithm of shooting method and showed computer results with the difference between two answer along with percentages of error."
  },
  {
    "objectID": "research.html#talks-and-presentations",
    "href": "research.html#talks-and-presentations",
    "title": "",
    "section": "Talks and Presentations",
    "text": "Talks and Presentations"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "teaching/sp24.html",
    "href": "teaching/sp24.html",
    "title": "Spring 2024: MAP4170 Introduction to Actuarial Mathematics",
    "section": "",
    "text": "One of the course objectives is for each student to develop a mastery of financial mathematics used by actuaries, based on the mathematics of interest theory. Other course objectives are for each student to understand the long-term individual study commitment necessary to achieve a designation within one of the actuarial societies and for each student to increase their knowledge of the actuarial profession\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nSpring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\n\nFall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\n\nSpring 2023: MAC1140 PreCalculus Algebra\n\n\n\n\n\n\n\nFall 2022: MAC2311 Calculus and Analytic Geometry I\n\n\n\n\n\n\n\nFall 2021 and Spring 2022: PreCalculus and Algebra\n\n\n\n\n\n\n\nFall 2018 to Spring 2020: College Algebra, Trigonometry\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "portfolio.html#teaching-experience",
    "href": "portfolio.html#teaching-experience",
    "title": "",
    "section": "Teaching Experience",
    "text": "Teaching Experience\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\nSpring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\nFall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\nSpring 2023: MAC1140 PreCalculus Algebra\n\n\n\n\nFall 2022: MAC2311 Calculus and Analytic Geometry I\n\n\n\n\nFall 2021 and Spring 2022: PreCalculus and Algebra\n\n\n\n\nFall 2018 to Spring 2020: College Algebra, Trigonometry\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/sgdlinreg/index.html",
    "href": "posts/sgdlinreg/index.html",
    "title": "Understanding Stochastic Gradient Descent Using Simple Linear Regression",
    "section": "",
    "text": "Linear regression is a fundamental algorithm in machine learning used for predicting a continuous dependent variable based on one or more independent variables. The objective is to find the best-fit line that minimizes the difference between the predicted and actual values.\nA simple linear regression in multiple predictors/input variables/features/independent variables/ explanatory variables/regressors/ covariates (many names) often takes the form\n\\[y=f(\\mathbf{x})+\\epsilon =\\mathbf{\\beta}\\mathbf{x}+\\epsilon\\]\nwhere \\(\\mathbf{\\beta} \\in \\mathbb{R}^d\\) are regression parameters or constant values that we aim to estimate and \\(\\epsilon \\sim \\mathcal{N}(0,1)\\) is a normally distributed error term independent of \\(x\\) or also called the white noise.\nFor simplicity let’s start with this toy example. Say, we have the data from a class of 10 students and their heights and weights are as follows:\nand this data looks like this:\nIn this case, the model:\n\\[y=f(x)+\\epsilon=\\beta_0+\\beta_1 x+\\epsilon\\]\nTherefore, in our model we need to estimate the parameters \\(\\beta_0,\\beta_1\\). The true relationship between the explanatory variables and the dependent variable is \\(y=f(x)\\). But our model is \\(y=f(x)+\\epsilon\\). Here, this \\(f(x)\\) is the working model with the data. In other words, \\(\\hat{y}=f(x)=\\hat{\\beta}_0+\\hat{\\beta}_1 x\\). Therefore, there should be some error in the model prediction which we are calling \\(\\epsilon=\\|y-\\hat{y}\\|\\) where \\(y\\) is the true value and \\(\\hat{y}\\) is the predicted value. This error term is normally distributed with mean 0 and variance 1. To get the best estimate of the parameters \\(\\beta_0,\\beta_1\\) we can minimize the error term as much as possible. So, we define the residual sum of squares (RSS) as:\n\\[\\begin{align}\nRSS &=\\epsilon_1^2+\\epsilon_2^2+\\cdots+\\epsilon_{10}^2\\\\\n&= \\sum_{i=1}^{10}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)^2\\\\\n\\hat{\\mathcal{l}}(\\bar{\\beta})&=\\sum_{i=1}^{10}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)^2\\\\\n\\end{align}\\]\nUsing multivariate calculus we see\n\\[\\begin{align}\n    \\frac{\\partial l}{\\partial \\beta_0}&=\\sum_{i=1}^{10} 2(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(-1)\\\\\n    \\frac{\\partial l}{\\partial \\beta_1}&= \\sum_{i=1}^{10} 2(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(-x_i)\n\\end{align}\\]\nSetting the partial derivatives to zero we solve for \\(\\hat{\\beta_0},\\hat{\\beta_1}\\) as follows\n\\[\\begin{align*}\n    \\frac{\\partial l}{\\partial \\beta_0}&=0\\\\\n    \\implies \\sum_{i=1}^{10} y_i-10 \\hat{\\beta_0}-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i\\right)&=0\\\\\n    \\implies \\hat{\\beta_0}&=\\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\end{align*}\\]\nand,\n\\[\\begin{align*}\n    \\frac{\\partial l}{\\partial \\beta_1}&=0\\\\\n    \\implies \\sum_{i=1}^{10} 2(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(-x_i)&=0\\\\\n    \\implies \\sum_{i=1}^{10} (y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(x_i)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\hat{\\beta_0}\\left(\\sum_{i=1}^{10} x_i\\right)-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\left(\\bar{y}-\\hat{\\beta_1}\\bar{x}\\right)\\left(\\sum_{i=1}^{10} x_i\\right)-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right)+\\hat{\\beta_1}\\bar{x}\\left(\\sum_{i=1}^{10} x_i\\right)-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right) -\\hat{\\beta_1}\\left(\\sum_{i=1}^{10}x_i^2-\\bar{x}\\sum_{i=1}^{10}x_i\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right) -\\hat{\\beta_1}\\left(\\sum_{i=1}^{10}x_i^2-10\\bar{x}^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right) -\\hat{\\beta_1}\\left(\\sum_{i=1}^{10}x_i^2-2\\times 10\\times \\bar{x}^2+10\\bar{x}^2\\right)&=0\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10} x_iy_i-10\\bar{x}\\bar{y}}{\\sum_{i=1}^{10}x_i^2-2\\times 10\\times \\bar{x}^2+10\\bar{x}^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10} x_iy_i -10\\bar{x}\\bar{y}-10\\bar{x}\\bar{y}+10\\bar{x}\\bar{y}}{\\sum_{i=1}^{10}x_i^2-2\\bar{x}\\times 10\\times\\frac{1}{10}\\sum_{i=1}^{10}x_i +10\\bar{x}^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right)-\\bar{x}\\left(\\sum_{i=1}^{10} y_i\\right)+10\\bar{x}\\bar{y}}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10}\\left(x_iy_i-x_i\\bar{y}-\\bar{x}y_i+\\bar{x}\\bar{y}\\right)}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\\\\\n\\end{align*}\\]\nTherefore, we have the following\n\\[\\begin{align*}\n     \\hat{\\beta_0}&=\\bar{y}-\\hat{\\beta_1}\\bar{x}\\\\\n     \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\n\\end{align*}\\]\nTo be continued in the next post…"
  },
  {
    "objectID": "posts/sgdlinreg/index.html#references",
    "href": "posts/sgdlinreg/index.html#references",
    "title": "Understanding Stochastic Gradient Descent Using Simple Linear Regression",
    "section": "References",
    "text": "References\n\n[1] James, Gareth, et al. An introduction to statistical learning: With applications in python. Springer Nature, 2023.\n\n\n\n\n\n\n\n\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/bayesianinference/index.html",
    "href": "posts/bayesianinference/index.html",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields, including machine learning, to make predictions and decisions under uncertainty.\n\nBayes’s theorem is a fundamental result in probability theory that relates the conditional and marginal probabilities of random events. Mathematically,\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\hspace{4mm} \\implies \\mathbb{P}(A|B) \\propto \\mathbb{P}(B|A)\\mathbb{P}(A)\\]\nwhere, \\(A\\) and \\(B\\) are events and \\(\\mathbb{P}(B)\\ne 0\\).\n\n\\(\\mathbb{P}(A|B)\\) is a conditional probability which states the probability of occuring the event \\(A\\) when the event \\(B\\) is given or true. The other name of this quantity is called posterior probability of \\(A\\) given the event \\(B\\) or simply, posterior distribution.\n\n\\(\\mathbb{P}(B|A)\\) is a conditional probability which states the probability of occuring the event \\(B\\) when the event \\(A\\) is given or true. In other terms, \\(\\mathbb{P}(B|A)\\) is the likelihood: the probability of evidence \\(B\\) given that \\(A\\) is true.\n\n\\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B)\\) are the probabilities of occuring \\(A\\) and \\(B\\) respectively, without any dependence on each other. \\(\\mathbb{P}(A)\\) is called the prior probability or prior distribution and \\(\\mathbb{P}(B)\\) is called the marginal likelihood or marginal probabilities.\n\nExample 1\nConsider a medical example where we want to diagnose a disease based on a test result. Let:\n\n\\(D\\) be the event that a patient has the disease.\n\n\\(T\\) be the event that the test result is positive.\n\nWe are interested in finding the probability that a patient has the disease given a positive test result, \\(\\mathbb{P}(D|T)\\).\nGiven:\n\n\\(\\mathbb{P}(T|D) = 0.99\\) (the probability of a positive test result given the patient has the disease).\n\n\\(\\mathbb{P}(D) = 0.01\\) (the prior probability of the disease).\n\n\\(\\mathbb{P}(T|D') = 0.05\\) (the probability of a positive test result given the patient does not have the disease).\n\nFirst, we need to calculate the marginal likelihood \\(P(T)\\): \\[\\begin{align*}\n    \\mathbb{P}(T) &= \\mathbb{P}(T|D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(T|D') \\cdot \\mathbb{P}(D') \\\\\n    \\mathbb{P}(T) &= 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99\\\\\n    \\mathbb{P}(T) &= 0.0099 + 0.0495 \\\\\n    \\mathbb{P}(T) &= 0.0594\n\\end{align*}\\]\nNow, we can apply Bayes’s theorem:\n\\[\\begin{align*}\n    \\mathbb{P}(D|T) &= \\frac{\\mathbb{P}(T|D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(T)}\\\\\n    \\mathbb{P}(D|T) &= \\frac{0.99 \\cdot 0.01}{0.0594}\\\\\n    \\mathbb{P}(D|T) &\\approx 0.1667\n\\end{align*}\\]\nSo, the probability that the patient has the disease given a positive test result is approximately \\(16.67\\%\\).\nExample 2\n\n Assume that you are in a restuarant and you ordered a plate of 3 pancakes. The chef made three pancakes with one in perfect condition, that is not burnt in any side, one with one side burnt, and the last one burnt in both sides. The waiter wanted to stack the pancakes so that the burnt side does not show up when served. However, the chef recommended not to hide the burnt side and asked her to stack the pancakes randomly. What is the likelyhood that the fully burnt pancake will be on the top?\n\nTo solve this problem, we can use Bayesian approach. We denote the event \\(X\\) as the pancake without any burnt, \\(Y\\) with one side burnt, and \\(Z\\) both side burnt. Then we have the following conditional probabilities\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt} | X)&=0\\\\\n    \\mathbb{P}(\\text{top-burnt} | Y)&=\\frac{1}{2}\\\\\n    \\mathbb{P}(\\text{top-burnt} | Z)&=1\\\\\n\\end{align*}\\]\nThe probability of picking a pancake irrespective of their burnt condition is \\(\\frac{1}{3}\\). So,\n\\[\\begin{equation}\n    \\mathbb{P}(X)=\\mathbb{P}(Y)=\\mathbb{P}(Z)=\\frac{1}{3}\n\\end{equation}\\]\nThe marginal probability of having burnt side in the top position\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt})&=\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)\\\\\n    &=0\\cdot \\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}+1\\cdot\\frac{1}{3}\\\\\n    &=\\frac{1}{2}\n\\end{align*}\\]\nNow, we can only have a burnt side on top if either \\(Z\\) is placed in the top or the burnt side of \\(Y\\) is placed in the top. \\[\\begin{align*}\n    \\mathbb{P}(Y|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{2}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}\\\\\n    \\mathbb{P}(Z|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}\n\\end{align*}\\]\nSo the probability of having the fully burnt pancake on the top is \\(\\frac{2}{3}\\)."
  },
  {
    "objectID": "posts/bayesianinference/index.html#introduction",
    "href": "posts/bayesianinference/index.html#introduction",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields, including machine learning, to make predictions and decisions under uncertainty.\n\nBayes’s theorem is a fundamental result in probability theory that relates the conditional and marginal probabilities of random events. Mathematically,\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\hspace{4mm} \\implies \\mathbb{P}(A|B) \\propto \\mathbb{P}(B|A)\\mathbb{P}(A)\\]\nwhere, \\(A\\) and \\(B\\) are events and \\(\\mathbb{P}(B)\\ne 0\\).\n\n\\(\\mathbb{P}(A|B)\\) is a conditional probability which states the probability of occuring the event \\(A\\) when the event \\(B\\) is given or true. The other name of this quantity is called posterior probability of \\(A\\) given the event \\(B\\) or simply, posterior distribution.\n\n\\(\\mathbb{P}(B|A)\\) is a conditional probability which states the probability of occuring the event \\(B\\) when the event \\(A\\) is given or true. In other terms, \\(\\mathbb{P}(B|A)\\) is the likelihood: the probability of evidence \\(B\\) given that \\(A\\) is true.\n\n\\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B)\\) are the probabilities of occuring \\(A\\) and \\(B\\) respectively, without any dependence on each other. \\(\\mathbb{P}(A)\\) is called the prior probability or prior distribution and \\(\\mathbb{P}(B)\\) is called the marginal likelihood or marginal probabilities.\n\nExample 1\nConsider a medical example where we want to diagnose a disease based on a test result. Let:\n\n\\(D\\) be the event that a patient has the disease.\n\n\\(T\\) be the event that the test result is positive.\n\nWe are interested in finding the probability that a patient has the disease given a positive test result, \\(\\mathbb{P}(D|T)\\).\nGiven:\n\n\\(\\mathbb{P}(T|D) = 0.99\\) (the probability of a positive test result given the patient has the disease).\n\n\\(\\mathbb{P}(D) = 0.01\\) (the prior probability of the disease).\n\n\\(\\mathbb{P}(T|D') = 0.05\\) (the probability of a positive test result given the patient does not have the disease).\n\nFirst, we need to calculate the marginal likelihood \\(P(T)\\): \\[\\begin{align*}\n    \\mathbb{P}(T) &= \\mathbb{P}(T|D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(T|D') \\cdot \\mathbb{P}(D') \\\\\n    \\mathbb{P}(T) &= 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99\\\\\n    \\mathbb{P}(T) &= 0.0099 + 0.0495 \\\\\n    \\mathbb{P}(T) &= 0.0594\n\\end{align*}\\]\nNow, we can apply Bayes’s theorem:\n\\[\\begin{align*}\n    \\mathbb{P}(D|T) &= \\frac{\\mathbb{P}(T|D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(T)}\\\\\n    \\mathbb{P}(D|T) &= \\frac{0.99 \\cdot 0.01}{0.0594}\\\\\n    \\mathbb{P}(D|T) &\\approx 0.1667\n\\end{align*}\\]\nSo, the probability that the patient has the disease given a positive test result is approximately \\(16.67\\%\\).\nExample 2\n\n Assume that you are in a restuarant and you ordered a plate of 3 pancakes. The chef made three pancakes with one in perfect condition, that is not burnt in any side, one with one side burnt, and the last one burnt in both sides. The waiter wanted to stack the pancakes so that the burnt side does not show up when served. However, the chef recommended not to hide the burnt side and asked her to stack the pancakes randomly. What is the likelyhood that the fully burnt pancake will be on the top?\n\nTo solve this problem, we can use Bayesian approach. We denote the event \\(X\\) as the pancake without any burnt, \\(Y\\) with one side burnt, and \\(Z\\) both side burnt. Then we have the following conditional probabilities\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt} | X)&=0\\\\\n    \\mathbb{P}(\\text{top-burnt} | Y)&=\\frac{1}{2}\\\\\n    \\mathbb{P}(\\text{top-burnt} | Z)&=1\\\\\n\\end{align*}\\]\nThe probability of picking a pancake irrespective of their burnt condition is \\(\\frac{1}{3}\\). So,\n\\[\\begin{equation}\n    \\mathbb{P}(X)=\\mathbb{P}(Y)=\\mathbb{P}(Z)=\\frac{1}{3}\n\\end{equation}\\]\nThe marginal probability of having burnt side in the top position\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt})&=\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)\\\\\n    &=0\\cdot \\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}+1\\cdot\\frac{1}{3}\\\\\n    &=\\frac{1}{2}\n\\end{align*}\\]\nNow, we can only have a burnt side on top if either \\(Z\\) is placed in the top or the burnt side of \\(Y\\) is placed in the top. \\[\\begin{align*}\n    \\mathbb{P}(Y|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{2}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}\\\\\n    \\mathbb{P}(Z|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}\n\\end{align*}\\]\nSo the probability of having the fully burnt pancake on the top is \\(\\frac{2}{3}\\)."
  },
  {
    "objectID": "posts/bayesianinference/index.html#why-bayesian-inference-in-machine-learning",
    "href": "posts/bayesianinference/index.html#why-bayesian-inference-in-machine-learning",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "Why Bayesian Inference in Machine Learning?",
    "text": "Why Bayesian Inference in Machine Learning?\nBayesian inference plays a crucial role in machine learning, particularly in areas involving uncertainty and probabilistic reasoning. It allows us to incorporate prior knowledge and update beliefs based on new data, which is especially useful in the following applications:\n\nBayesian Networks\nBayesian networks are graphical models that represent the probabilistic relationships among a set of variables. Each node in the network represents a random variable, and the edges represent conditional dependencies. Bayesian networks are used for various tasks such as classification, prediction, and anomaly detection.\n\n\nBayesian Regression\nBayesian regression extends linear regression by incorporating prior distributions on the model parameters. This approach provides a probabilistic framework for regression analysis, allowing for uncertainty in the parameter estimates. The posterior distribution of the parameters is computed using Bayes’s theorem, and predictions are made by averaging over this distribution.\n\n\nSampling Methods\nIn Bayesian inference, exact computation of the posterior distribution is often intractable. Therefore, sampling methods such as Markov Chain Monte Carlo (MCMC) and Variational Inference are used to approximate the posterior distribution. These methods generate samples from the posterior distribution, allowing us to estimate various statistical properties and make inferences.\nMarkov Chain Monte Carlo (MCMC)\nMCMC methods generate a sequence of samples from the posterior distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution. Common MCMC algorithms include the Underdamped and Overdamped Langevin dynamics, Metropolis-Hastings algorithm and the Gibbs sampler.\nExample: Metropolis-Hastings Algorithm\nConsider a posterior distribution \\(P(\\theta|D)\\) where \\(\\theta\\) represents the model parameters and \\(D\\) represents the data. The Metropolis-Hastings algorithm proceeds as follows:\n\nInitialize the parameters \\(\\theta_0\\).\nFor \\(t = 1\\) to \\(T\\):\n\nPropose a new state \\(\\theta'\\) from a proposal distribution \\(Q(\\theta'|\\theta_t)\\).\nCompute the acceptance ratio \\(\\alpha = \\frac{P(\\theta'|D) \\cdot Q(\\theta_t|\\theta')}{P(\\theta_t|D) \\cdot Q(\\theta'|\\theta_t)}\\).\nAccept the new state with probability \\(\\min(1, \\alpha)\\). If accepted, set \\(\\theta_{t+1} = \\theta'\\); otherwise, set \\(\\theta_{t+1} = \\theta_t\\).\n\n\nThe samples \\(\\theta_1, \\theta_2, \\ldots, \\theta_T\\) form a Markov chain whose stationary distribution is the posterior distribution \\(P(\\theta|D)\\).\n\n\nBayesian Inference in Neural Networks\nBayesian methods are also applied to neural networks, resulting in Bayesian Neural Networks (BNNs). BNNs incorporate uncertainty in the network weights by placing a prior distribution over them and using Bayes’s theorem to update this distribution based on the observed data. This allows BNNs to provide not only point estimates but also uncertainty estimates for their predictions.\nIn the next parts, we will talk about different applications of the Bayesian inferences, specifically, sampling problem using Langevin dynamics.\n\n\nReference\n\nPancake problems on mathstackexchance\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "",
    "section": "Welcome",
    "text": "Welcome\nPh.D. Candidate\n\n I am currently a Ph.D. candidate in the Mathematics Department at Florida State University, where I also serve as a Graduate Teaching Assistant. My academic journey has been both diverse and enriching, spanning multiple institutions and fields of study.   Before joining FSU, I earned a second master’s degree in Mathematics from Youngstown State University in Ohio, USA. Prior to that, I completed my undergraduate degree in Mathematics at the University of Dhaka (DU), Bangladesh, followed by a one-year integrated master’s program in Applied Mathematics at the same institution.  After graduating from DU, I gained valuable industry experience by working for two years in the life insurance sector in Dhaka, Bangladesh. In pursuit of higher education and advanced research opportunities, I relocated to the United States in August 2018.  As a lifelong student and researcher of mathematics, my research interests are centered around Theoretical Machine Learning, Computational Finance, and various other areas of applied mathematics. I am passionate about advancing knowledge in these fields and contributing to their practical applications."
  },
  {
    "objectID": "posts/machinelearning/index.html",
    "href": "posts/machinelearning/index.html",
    "title": "Python Implementation of General Machine Learning Algorithms",
    "section": "",
    "text": "This blog post is my personal repository of most common and useful machine learning algorithms using Python. In summer 2024, I participated in a Data Science Boot Camp by the Erdos Institute. Therefore, the topics listed here mostly align with their bootcamp syllabus. Note that, I did not copy their materials directly that affects the copyright issues. I wrote evrything in my own words and used new dataset."
  },
  {
    "objectID": "posts/machinelearning/index.html#regressions",
    "href": "posts/machinelearning/index.html#regressions",
    "title": "Python Implementation of General Machine Learning Algorithms",
    "section": "Regressions",
    "text": "Regressions\n\nParametric Regressions\n\nSimple Linear Regression\nMultiple Linear Regression\n\n\n\nNon-Parametric Regressions\n\nK Nearest Neighbors: Regression and Classification"
  },
  {
    "objectID": "posts/machinelearning/index.html#classifications",
    "href": "posts/machinelearning/index.html#classifications",
    "title": "Python Implementation of General Machine Learning Algorithms",
    "section": "Classifications",
    "text": "Classifications"
  },
  {
    "objectID": "posts/machinelearning/index.html#time-series",
    "href": "posts/machinelearning/index.html#time-series",
    "title": "Python Implementation of General Machine Learning Algorithms",
    "section": "Time Series",
    "text": "Time Series"
  },
  {
    "objectID": "posts/machinelearning/index.html#ensembles",
    "href": "posts/machinelearning/index.html#ensembles",
    "title": "Python Implementation of General Machine Learning Algorithms",
    "section": "Ensembles",
    "text": "Ensembles"
  },
  {
    "objectID": "posts/machinelearning/index.html#neural-networks",
    "href": "posts/machinelearning/index.html#neural-networks",
    "title": "Python Implementation of General Machine Learning Algorithms",
    "section": "Neural Networks",
    "text": "Neural Networks\nYou may also like"
  },
  {
    "objectID": "index.html#rafiq-islam",
    "href": "index.html#rafiq-islam",
    "title": "",
    "section": "Rafiq Islam",
    "text": "Rafiq Islam\n\n Ph.D. Candidate in Mathematics\nFlorida State University  Contact\nEmail:  mislam@math.fsu.edu\nOffice:  James J. Love Building: Room 331A\nOffice Hours: On appointments\n\n\n\n\nI am currently a Ph.D. candidate and a Graduate Teaching Assistant (GTA) in the Mathematics Department at Florida State University. I am actively involved in research in the area of Data Science, Machine Learning, and Financial Mathematics under the supervision of professor  Lingjiong Zhu.  My academic journey has been both diverse and enriching, spanning multiple institutions and fields of study. Before joining FSU, I obtained a master’s degree in Mathematics from Youngstown State University in Ohio, USA. Prior to that, I completed my undergraduate degree in Mathematics at the University of Dhaka (DU), Bangladesh, followed by a one-year integrated master’s program in Applied Mathematics at the same institution."
  },
  {
    "objectID": "publication/pub2/index.html",
    "href": "publication/pub2/index.html",
    "title": "GJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies",
    "section": "",
    "text": "Cryptocurrencies are currently traded worldwide, with hundreds of different currencies in existence and even more on the way. This study implements some statistical and machine learning approaches for cryptocurrency investments. First, we implement GJR-GARCH over the GARCH model to estimate the volatility of ten popular cryptocurrencies based on market capitalization: Bitcoin, Bitcoin Cash, Bitcoin SV, Chainlink, EOS, Ethereum, Litecoin, TETHER, Tezos, and XRP. Then, we use Monte Carlo simulations to generate the conditional variance of the cryptocurrencies using the GJR-GARCH model, and calculate the value at risk (VaR) of the simulations. We also estimate the tail-risk using VaR backtesting. Finally, we use an artificial neural network (ANN) for predicting the prices of the ten cryptocurrencies. The graphical analysis and mean square errors (MSEs) from the ANN models confirmed that the predicted prices are close to the market prices. For some cryptocurrencies, the ANN models perform better than traditional ARIMA models.\n\nDownload the paper from here\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@article{mostafa2021,\n  author = {Mostafa, Fahad and Saha, Pritam and Rafiqul Islam, Mohammad\n    and Nguyen, Nguyet},\n  title = {GJR-GARCH {Volatility} {Modeling} Under {NIG} and {ANN} for\n    {Predicting} {Top} {Cryptocurrencies}},\n  journal = {Journal of Risk and Financial Management},\n  date = {2021-09-03},\n  url = {https://mrislambd.github.io/publication/pub2/},\n  doi = {10.3390/jrfm14090421},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMostafa, Fahad, Pritam Saha, Mohammad Rafiqul Islam, and Nguyet Nguyen.\n2021. “GJR-GARCH Volatility Modeling Under NIG and ANN for\nPredicting Top Cryptocurrencies.” Journal of Risk and\nFinancial Management, September. https://doi.org/10.3390/jrfm14090421."
  },
  {
    "objectID": "portfolio.html#teaching",
    "href": "portfolio.html#teaching",
    "title": "",
    "section": "Teaching",
    "text": "Teaching\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\nSpring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\nFall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\nSpring 2023: MAC1140 PreCalculus Algebra\n\n\n\n\nFall 2022: MAC2311 Calculus and Analytic Geometry I\n\n\n\n\nFall 2021 and Spring 2022: PreCalculus and Algebra\n\n\n\n\nFall 2018 to Spring 2020: College Algebra, Trigonometry\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "machinelearning/simplelinreg/index.html",
    "href": "machinelearning/simplelinreg/index.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Simple Linear Regression slr is applicable for a single feature data set with contineous response variable.\n\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.linear_model import LinearRegression\n\n\n\nTo implement the algorithm, we need some synthetic data. To generate the synthetic data we use the linear equation \\(y(x)=2x+\\frac{1}{2}+\\xi\\) where \\(\\xi\\sim \\mathbf{N}(0,1)\\)\n\nX=np.random.random(100)\ny=2*X+0.5+np.random.randn(100)\n\nNote that we used two random number generators, np.random.random(n) and np.random.randn(n). The first one generates \\(n\\) random numbers of values from the range (0,1) and the second one generates values from the standard normal distribution with mean 0 and variance or standard deviation 1.\n\nplt.figure(figsize=(9,6))\nplt.scatter(X,y)\nplt.xlabel('$X$')\nplt.ylabel('y')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe want to fit a simple linear regression to the above data.\n\nslr=LinearRegression()\n\nNow to fit our data \\(X\\) and \\(y\\) we need to reshape the input variable. Because if we look at \\(X\\),\n\nX\n\narray([0.47262747, 0.45832511, 0.15185121, 0.07804947, 0.20774999,\n       0.58089888, 0.36426913, 0.28810579, 0.51060088, 0.14737801,\n       0.01947535, 0.11659318, 0.15239961, 0.77137911, 0.65356733,\n       0.43554869, 0.14879946, 0.67806331, 0.30159727, 0.03453039,\n       0.94883979, 0.40186773, 0.07211558, 0.24995041, 0.33692126,\n       0.81543869, 0.53823708, 0.75592182, 0.83204058, 0.03529891,\n       0.23741696, 0.98328148, 0.48062316, 0.25531266, 0.87618272,\n       0.8615627 , 0.06305102, 0.54133681, 0.62650366, 0.96493873,\n       0.67682656, 0.84753771, 0.06571497, 0.83416241, 0.81127227,\n       0.38707397, 0.56183168, 0.109602  , 0.95894655, 0.66784786,\n       0.83809955, 0.31853434, 0.11356387, 0.65628002, 0.94522769,\n       0.15214867, 0.08987173, 0.48295674, 0.10232515, 0.23560979,\n       0.88936428, 0.07104284, 0.33686534, 0.92154236, 0.81412464,\n       0.11095786, 0.99671945, 0.72733826, 0.56818335, 0.96003649,\n       0.82642878, 0.52345794, 0.68296939, 0.29668986, 0.07986357,\n       0.41478236, 0.97602409, 0.07230149, 0.94633914, 0.4115829 ,\n       0.83709765, 0.62815622, 0.39283743, 0.29972847, 0.89327642,\n       0.91434758, 0.77692405, 0.20062007, 0.01173936, 0.36129123,\n       0.47523429, 0.30111291, 0.39288387, 0.60324617, 0.25539593,\n       0.88741911, 0.39390942, 0.18417327, 0.62663703, 0.1763948 ])\n\n\nIt is a one-dimensional array/vector but the slr object accepts input variable as matrix or two-dimensional format.\n\nX=X.reshape(-1,1)\nX[:10]\n\narray([[0.47262747],\n       [0.45832511],\n       [0.15185121],\n       [0.07804947],\n       [0.20774999],\n       [0.58089888],\n       [0.36426913],\n       [0.28810579],\n       [0.51060088],\n       [0.14737801]])\n\n\nNow we fit the data to our model\n\nslr.fit(X,y)\nslr.predict([[2],[3]])\n\narray([4.72058616, 6.90955088])\n\n\nWe have our \\(X=2,3\\) and the corresponding \\(y\\) values are from the above cell output, which are pretty close to the model \\(y=2x+\\frac{1}{2}\\).\n\nintercept = round(slr.intercept_,4)\nslope = slr.coef_\n\nNow our model parameters are: intercept \\(\\beta_0=\\) np.float64(0.3427) and slope \\(\\beta_1=\\) array([2.18896472]).\n\nplt.figure(figsize=(9,6))\nplt.scatter(X,y, alpha=0.7,label=\"Sample Data\")\nplt.plot(np.linspace(0,1,100),\n    slr.predict(np.linspace(0,1,100).reshape(-1,1)),\n    'k',\n    label='Model $\\hat{f}$'\n)\nplt.plot(np.linspace(0,1,100),\n    2*np.linspace(0,1,100)+0.5,\n    'r--',\n    label='$f$'\n)\nplt.xlabel('$X$')\nplt.ylabel('y')\nplt.legend(fontsize=10)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nSo the model fits the data almost perfectly.\nUp next multiple linear regression.\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "machinelearning/simplelinreg/index.html#synthetic-data",
    "href": "machinelearning/simplelinreg/index.html#synthetic-data",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "To implement the algorithm, we need some synthetic data. To generate the synthetic data we use the linear equation \\(y(x)=2x+\\frac{1}{2}+\\xi\\) where \\(\\xi\\sim \\mathbf{N}(0,1)\\)\n\nX=np.random.random(100)\ny=2*X+0.5+np.random.randn(100)\n\nNote that we used two random number generators, np.random.random(n) and np.random.randn(n). The first one generates \\(n\\) random numbers of values from the range (0,1) and the second one generates values from the standard normal distribution with mean 0 and variance or standard deviation 1.\n\nplt.figure(figsize=(9,6))\nplt.scatter(X,y)\nplt.xlabel('$X$')\nplt.ylabel('y')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()"
  },
  {
    "objectID": "machinelearning/simplelinreg/index.html#model",
    "href": "machinelearning/simplelinreg/index.html#model",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "We want to fit a simple linear regression to the above data.\n\nslr=LinearRegression()\n\nNow to fit our data \\(X\\) and \\(y\\) we need to reshape the input variable. Because if we look at \\(X\\),\n\nX\n\narray([0.47262747, 0.45832511, 0.15185121, 0.07804947, 0.20774999,\n       0.58089888, 0.36426913, 0.28810579, 0.51060088, 0.14737801,\n       0.01947535, 0.11659318, 0.15239961, 0.77137911, 0.65356733,\n       0.43554869, 0.14879946, 0.67806331, 0.30159727, 0.03453039,\n       0.94883979, 0.40186773, 0.07211558, 0.24995041, 0.33692126,\n       0.81543869, 0.53823708, 0.75592182, 0.83204058, 0.03529891,\n       0.23741696, 0.98328148, 0.48062316, 0.25531266, 0.87618272,\n       0.8615627 , 0.06305102, 0.54133681, 0.62650366, 0.96493873,\n       0.67682656, 0.84753771, 0.06571497, 0.83416241, 0.81127227,\n       0.38707397, 0.56183168, 0.109602  , 0.95894655, 0.66784786,\n       0.83809955, 0.31853434, 0.11356387, 0.65628002, 0.94522769,\n       0.15214867, 0.08987173, 0.48295674, 0.10232515, 0.23560979,\n       0.88936428, 0.07104284, 0.33686534, 0.92154236, 0.81412464,\n       0.11095786, 0.99671945, 0.72733826, 0.56818335, 0.96003649,\n       0.82642878, 0.52345794, 0.68296939, 0.29668986, 0.07986357,\n       0.41478236, 0.97602409, 0.07230149, 0.94633914, 0.4115829 ,\n       0.83709765, 0.62815622, 0.39283743, 0.29972847, 0.89327642,\n       0.91434758, 0.77692405, 0.20062007, 0.01173936, 0.36129123,\n       0.47523429, 0.30111291, 0.39288387, 0.60324617, 0.25539593,\n       0.88741911, 0.39390942, 0.18417327, 0.62663703, 0.1763948 ])\n\n\nIt is a one-dimensional array/vector but the slr object accepts input variable as matrix or two-dimensional format.\n\nX=X.reshape(-1,1)\nX[:10]\n\narray([[0.47262747],\n       [0.45832511],\n       [0.15185121],\n       [0.07804947],\n       [0.20774999],\n       [0.58089888],\n       [0.36426913],\n       [0.28810579],\n       [0.51060088],\n       [0.14737801]])\n\n\nNow we fit the data to our model\n\nslr.fit(X,y)\nslr.predict([[2],[3]])\n\narray([4.72058616, 6.90955088])\n\n\nWe have our \\(X=2,3\\) and the corresponding \\(y\\) values are from the above cell output, which are pretty close to the model \\(y=2x+\\frac{1}{2}\\).\n\nintercept = round(slr.intercept_,4)\nslope = slr.coef_\n\nNow our model parameters are: intercept \\(\\beta_0=\\) np.float64(0.3427) and slope \\(\\beta_1=\\) array([2.18896472]).\n\nplt.figure(figsize=(9,6))\nplt.scatter(X,y, alpha=0.7,label=\"Sample Data\")\nplt.plot(np.linspace(0,1,100),\n    slr.predict(np.linspace(0,1,100).reshape(-1,1)),\n    'k',\n    label='Model $\\hat{f}$'\n)\nplt.plot(np.linspace(0,1,100),\n    2*np.linspace(0,1,100)+0.5,\n    'r--',\n    label='$f$'\n)\nplt.xlabel('$X$')\nplt.ylabel('y')\nplt.legend(fontsize=10)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nSo the model fits the data almost perfectly.\nUp next multiple linear regression.\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "teaching/fall22.html",
    "href": "teaching/fall22.html",
    "title": "Fall 2022: MAC2311 Calculus and Analytic Geometry I",
    "section": "",
    "text": "Students who have substantial knowledge of precalculus and algebra may require to take this course as a mathematics requirement depending on their majors. The topic of this course includes but is not limited to Foundation for calculus: Functions and Limits, Derivative, The Definite Integral, and Constructing Antiderivatives.  As a recitation instructor for this course, I ran two poster presentation sessions of 30 students in each group where they presented mathematical problems and their solutions step by step to their peer classmates followed by a group activity where they solved another set of problems. I also graded their exam scripts and weekly posters.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "machinelearning/multiplelinreg/index.html",
    "href": "machinelearning/multiplelinreg/index.html",
    "title": "Multiple Liear Regression",
    "section": "",
    "text": "The multiple linear regression takes the form\n\\[y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\cdots +\\beta_d x_d+\\xi=\\vec{x}\\cdot \\vec{\\beta}+\\xi\\]\nwith \\(\\{\\beta_i\\}_{i=0}^{d}\\in \\mathbb{R}\\) constants or parameters of the model. In vector notation, \\(\\vec{\\beta}\\in \\mathbb{R}^{d+1}\\),\n\\[\n\\vec{\\beta}=\\begin{pmatrix}\\beta_0\\\\ \\beta_1\\\\ \\vdots \\\\ \\beta_d \\end{pmatrix};\\hspace{4mm}\\vec{x}=\\begin{pmatrix}1\\\\ x_1\\\\ x_2\\\\ \\vdots\\\\ x_d\\end{pmatrix}\n\\]\nFor \\(n\\) data points, in matrix algebra notation, we can write \\(y=X\\vec{\\beta}+\\xi\\) where \\(X\\in \\mathcal{M}_{n\\times (d+1)}\\) and \\(y\\in \\mathbb{R}^{d+1}\\) with\n\\[X=\\begin{pmatrix}1&x_{11}&x_{12}&\\cdots&x_{1d}\\\\1&x_{21}&x_{22}&\\cdots&x_{2d}\\\\ \\vdots& \\vdots &\\vdots&\\ddots &\\vdots\\\\1&x_{n1}&x_{n2}&\\cdots&x_{nd} \\end{pmatrix};\\hspace{4mm} y=\\begin{pmatrix}y_1\\\\y_2\\\\ \\vdots\\\\ y_n\\end{pmatrix};\\hspace{4mm} \\xi=\\begin{pmatrix}\\xi_1\\\\ \\xi_2\\\\ \\vdots\\\\ \\xi_n\\end{pmatrix}\\]\nWe fit the \\(n\\) data points with the objective to minimize the loss function, mean squared error\n\\[MSE(\\vec{\\beta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-f_{\\vec{\\beta}}(\\vec{x}_i)\\right)^2=\\frac{1}{n}\\left|\\vec{y}-X\\vec{\\beta}\\right|^2\\]"
  },
  {
    "objectID": "machinelearning/multiplelinreg/index.html#multiple-linear-regression",
    "href": "machinelearning/multiplelinreg/index.html#multiple-linear-regression",
    "title": "Multiple Liear Regression",
    "section": "",
    "text": "The multiple linear regression takes the form\n\\[y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\cdots +\\beta_d x_d+\\xi=\\vec{x}\\cdot \\vec{\\beta}+\\xi\\]\nwith \\(\\{\\beta_i\\}_{i=0}^{d}\\in \\mathbb{R}\\) constants or parameters of the model. In vector notation, \\(\\vec{\\beta}\\in \\mathbb{R}^{d+1}\\),\n\\[\n\\vec{\\beta}=\\begin{pmatrix}\\beta_0\\\\ \\beta_1\\\\ \\vdots \\\\ \\beta_d \\end{pmatrix};\\hspace{4mm}\\vec{x}=\\begin{pmatrix}1\\\\ x_1\\\\ x_2\\\\ \\vdots\\\\ x_d\\end{pmatrix}\n\\]\nFor \\(n\\) data points, in matrix algebra notation, we can write \\(y=X\\vec{\\beta}+\\xi\\) where \\(X\\in \\mathcal{M}_{n\\times (d+1)}\\) and \\(y\\in \\mathbb{R}^{d+1}\\) with\n\\[X=\\begin{pmatrix}1&x_{11}&x_{12}&\\cdots&x_{1d}\\\\1&x_{21}&x_{22}&\\cdots&x_{2d}\\\\ \\vdots& \\vdots &\\vdots&\\ddots &\\vdots\\\\1&x_{n1}&x_{n2}&\\cdots&x_{nd} \\end{pmatrix};\\hspace{4mm} y=\\begin{pmatrix}y_1\\\\y_2\\\\ \\vdots\\\\ y_n\\end{pmatrix};\\hspace{4mm} \\xi=\\begin{pmatrix}\\xi_1\\\\ \\xi_2\\\\ \\vdots\\\\ \\xi_n\\end{pmatrix}\\]\nWe fit the \\(n\\) data points with the objective to minimize the loss function, mean squared error\n\\[MSE(\\vec{\\beta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-f_{\\vec{\\beta}}(\\vec{x}_i)\\right)^2=\\frac{1}{n}\\left|\\vec{y}-X\\vec{\\beta}\\right|^2\\]"
  },
  {
    "objectID": "machinelearning/knn/index.html",
    "href": "machinelearning/knn/index.html",
    "title": "K Nearest Neighbors Regression",
    "section": "",
    "text": "Non-parametric model is a statistical model that does not make any assumptions about the underlying data distributions, meaning it does not require specifying functional form for the relationships between variables, instead learning directly from the data points without pre-defined parameters."
  },
  {
    "objectID": "machinelearning/knn/index.html#knn-regression",
    "href": "machinelearning/knn/index.html#knn-regression",
    "title": "K Nearest Neighbors Regression",
    "section": "\\(k\\)NN Regression",
    "text": "\\(k\\)NN Regression\nWe are given a set of data points \\((\\bar{x}_i,y_i)\\) with \\(\\bar{x}_i\\in \\mathbb{R}^d\\) and \\(y_i\\in \\mathbb{R}\\)\n1. Calculate the distances of the given point \\(x\\) from all the data points\n2. Short the distances in increasing order and select the optimal \\(k\\), the hyperparameter that is used select the nearest distances for the problem at hand.\nThis \\(k\\) value controls the fitting of the model. The best value for \\(k\\) is determined from the cross validation and learning curves. Here is the summary regarding the hyperparameter \\(k\\):\n\nsmaller \\(k\\) usually gets low bias but higher variances, which results over fitting.\nlarger \\(k\\) usually gets high bias but lower variances, which resutls under fitting.\n\nFor classification problems, the class is determined by the vote of its neighbors. For a regression problem the response \\(y\\) is calculated by the weighted average of the sorted \\(k\\)-th distances. For example, if \\(k=4\\) and the shorted distances are \\(d_1&lt;d_3&lt;d_2&lt;d_4\\) then \\(y=(d_1+d_3+d_2+d_4)/2\\)"
  },
  {
    "objectID": "machinelearning/knn/index.html#knn-implementation-regression",
    "href": "machinelearning/knn/index.html#knn-implementation-regression",
    "title": "K Nearest Neighbors Regression",
    "section": "\\(k\\)NN Implementation: Regression",
    "text": "\\(k\\)NN Implementation: Regression\n\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nnp.random.seed(123)\nX = 2*np.random.normal(size=(100,1))\ny = (np.cos(X)+0.3*np.random.normal(size=X.shape)).reshape(-1)\n# First 5 entries in X\nX[:5]\n# First 5 entries in y\ny[:5]\n\n# Plot of the data\nplt.scatter(X,y)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4') \nplt.show()\n\n\n\n\n\n\n\n\nNow we fit two models for two different \\(k\\) values to see how it affects the interpolation.\n\nfrom sklearn.neighbors import KNeighborsRegressor\nknn_3 = KNeighborsRegressor(3)\nknn_9 = KNeighborsRegressor(9)\n\nknn_3.fit(X,y)\nknn_9.fit(X,y)\n\nfig, ax = plt.subplots(1,2, figsize=(9,5), sharex=True, sharey=True)\nax[0].scatter(X,y, alpha=0.5, label=\"Sample Data\")\nax[1].scatter(X,y, alpha=0.5, label=\"Sample Data\")\nax[0].plot(\n    np.linspace(np.min(X)-1,np.max(X)+1,200).reshape(-1,1),\n    knn_3.predict(np.linspace(np.min(X)-1,np.max(X)+1,200).reshape(-1,1)),\n    'k',\n    label='KNR'\n)\nax[0].set_title('$k=3$')\nax[0].set_facecolor('#f4f4f4')\nax[0].patch.set_facecolor('#f4f4f4')\n\nax[1].plot(\n    np.linspace(np.min(X)-1,np.max(X)+1,200).reshape(-1,1),\n    knn_9.predict(np.linspace(np.min(X)-1,np.max(X)+1,200).reshape(-1,1)),\n    'k',\n    label='KNR'\n)\nax[1].set_title('$k=9$')\nax[1].set_facecolor('#f4f4f4')\nax[1].patch.set_facecolor('#f4f4f4')\n\nplt.show()"
  },
  {
    "objectID": "machinelearning/knn/index.html#knn-implementation-classification",
    "href": "machinelearning/knn/index.html#knn-implementation-classification",
    "title": "K Nearest Neighbors Regression",
    "section": "\\(k\\)NN Implementation: Classification",
    "text": "\\(k\\)NN Implementation: Classification\nFor this classification problem, we choose that famous iris data from the scikit-learn library.\n\nfrom sklearn import datasets\n\n\n# Load the data\niris = datasets.load_iris()\nX,y = iris.data, iris.target\n\n# First 5 entries of the features  \nX[:5,:5]\n\narray([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2]])\n\n\n\n# First 5 entries of the target values\n\ny[:5]\n\narray([0, 0, 0, 0, 0])\n\n\nThere are total 150 observations of \\(X\\) and \\(y\\) and the scatter plot of the data\n\n# Scatter plot \n_, ax =  plt.subplots()\nscatter = ax.scatter(iris.data[:,0],\n            iris.data[:,1],\n            c=iris.target\n        )\nax.set(xlabel=iris.feature_names[0],\n       ylabel=iris.feature_names[1]\n)\n_ = ax.legend(\n    scatter.legend_elements()[0],\n    iris.target_names, \n    loc=\"lower right\",\n    title=\"Classes\"\n)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4') \n\n\n\n\n\n\n\n\nWe can use the KNeighborsClassifier class from the sklearn library or make a custom classifier.\n\nKNeighborsClassifier: sklearn Libarary\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42) \n\nsklern_clf = KNeighborsClassifier(n_neighbors=5) # 5 is the default number\nsklern_clf.fit(X_train,y_train)\npred = sklern_clf.predict(X_test)\nacc = accuracy_score(y_test,pred)\nprint('Classification Accuracy: ',acc)\n\nClassification Accuracy:  0.98\n\n\n\n\nKNeighborsClassifier: custom Libarary\nThis is a custom made KNN classifier (code credit goes to AssemblyAI). Let’s use this to classify the iris dataset.\nimport numpy as np\nfrom collections import Counter\n\n\ndef distance(x1, x2):\n    return np.sqrt(np.sum((x1-x2)**2))\n\n\nclass KNNClassifier:\n    def __init__(self, k=3):\n        self.k = k\n\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n\n    def predict(self, X):\n        predictions = [self._prediction(x) for x in X]\n        return predictions\n\n    def _prediction(self, x):\n        distances = [distance(x, x_train) for x_train in self.Xtrain]\n        knn_indices = np.argsort(distances)[:self.k]\n        knn_labels = [self.y_train[i] for i in knn_indices]\n\n        # majority vote\n        most_common = Counter(knn_labels).most_common()\n        return most_common[0][0]\nSave the above file as knn.py in the same directory. Then\n\nfrom knn import KNNClassifier\n\ncustomknn_clf = KNNClassifier(k=5)\ncustomknn_clf.fit(X_train,y_train)\npred = customknn_clf.predict(X_test)\nacc = accuracy_score(y_test,pred)\nprint('Classification Accuracy: ',acc)\n\nClassification Accuracy:  0.98\n\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/dsa/index.html",
    "href": "posts/dsa/index.html",
    "title": "Data Structure and Algorithms: Basic Programming Hacks",
    "section": "",
    "text": "Code\nimport time\n\ndef time_required(func):\n    def wrapper(*args, **kwargs):\n        starting = time.perf_counter()\n        output = func(*args, **kwargs)\n        ending = time.perf_counter()\n        elapsed = ending - starting\n        print(f'Time required: {elapsed:.6f} seconds')\n        return output\n    return wrapper"
  },
  {
    "objectID": "posts/dsa/index.html#binary-search",
    "href": "posts/dsa/index.html#binary-search",
    "title": "Data Structure and Algorithms: Basic Programming Hacks",
    "section": "Binary Search",
    "text": "Binary Search\n\n\nCode\ndef binary_search(arr, k):\n    \"\"\"\n    arr:: sorted array\n    k:: search item; float or integer\n    \"\"\"\n    low, high=0, len(arr)-1"
  },
  {
    "objectID": "research.html#current-research",
    "href": "research.html#current-research",
    "title": "",
    "section": "Current Research",
    "text": "Current Research\n\nHigher order Langevin dynamics: General \\(n\\)th order Langevin dynamics\n\nExact First-Order Algorightm (EXTRA) for decentralized stochastic gradient Langevin dynamics"
  },
  {
    "objectID": "posts/machinelearning/index.html#ensemble-learning",
    "href": "posts/machinelearning/index.html#ensemble-learning",
    "title": "Python Implementation of General Machine Learning Algorithms",
    "section": "Ensemble Learning",
    "text": "Ensemble Learning"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html",
    "href": "portfolio/dsp/medicalcost/index.html",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "Notebook\nThis is a general template of how a data science project should be done. It should contain the following attributes."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#project-overview",
    "href": "portfolio/dsp/medicalcost/index.html#project-overview",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Project Overview",
    "text": "Project Overview\n\nThis predictive modeling project involves personal medical data to predict the medical insurance cost by using a linear regression model."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#dataset",
    "href": "portfolio/dsp/medicalcost/index.html#dataset",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Dataset",
    "text": "Dataset\nThe dataset used in this project is collected from Kaggle"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#stakeholders",
    "href": "portfolio/dsp/medicalcost/index.html#stakeholders",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Stakeholders",
    "text": "Stakeholders"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#key-performance-indicators-kpis",
    "href": "portfolio/dsp/medicalcost/index.html#key-performance-indicators-kpis",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Key Performance Indicators (KPIs)",
    "text": "Key Performance Indicators (KPIs)"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#modeling",
    "href": "portfolio/dsp/medicalcost/index.html#modeling",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#results-and-outcomes",
    "href": "portfolio/dsp/medicalcost/index.html#results-and-outcomes",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Results and Outcomes",
    "text": "Results and Outcomes"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#future-directions",
    "href": "portfolio/dsp/medicalcost/index.html#future-directions",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Future Directions",
    "text": "Future Directions"
  },
  {
    "objectID": "machinelearning/multiplelinreg/index.html#black-box-ordinary-least-square-method",
    "href": "machinelearning/multiplelinreg/index.html#black-box-ordinary-least-square-method",
    "title": "Multiple Liear Regression",
    "section": "Black-box: Ordinary Least Square Method",
    "text": "Black-box: Ordinary Least Square Method\nNow we may investigate a little on how the above codes work. The scikit-learn library uses Ordinary Least Squares (OLS) method to find the parameters. This method is good for a simple and relatively smaller dataset. Here is a short note on this method.\nThe goal of OLS is to find the parameter vector \\(\\hat{\\beta}\\) that minimizes the sum of squared errors (SSE) between the observed target values \\(y\\) and the predicted values \\(\\hat{y}\\):\n[ = _{i=1}^{n} (y_i - i)^2 = {i=1}^{n} (y_i - X_i)^2 ]\nThis can be expressed in matrix form as:\n[ = (y - X)^T(y - X) ]\n\n3. Expanding the SSE Expression\nTo minimize the SSE, let’s first expand the expression:\n[ = (y - X)^T(y - X) ]\nExpanding this product, we get:\n[ = y^T y - y^T X- ^T X^T y + ^T X^T X ]\nSince \\(\\beta^T X^T y\\) is a scalar (a 1x1 matrix), it is equal to its transpose:\n[ = y^T y - 2^T X^T y + ^T X^T X ]\n\n\n4. Taking the Derivative with Respect to \\(\\beta\\)\nTo find the minimum of the SSE, we take the derivative with respect to \\(\\beta\\) and set it to zero:\n[ = -2X^T y + 2X^T X = 0 ]\n\n\n5. Solving for \\(\\hat{\\beta}\\)\nNow, solve for \\(\\beta\\):\n[ X^T X = X^T y ]\nTo isolate \\(\\beta\\), we multiply both sides by \\((X^T X)^{-1}\\) (assuming \\(X^T X\\) is invertible):\n[ = (X^T X)^{-1} X^T y ]\n\n\n6. Interpretation\nThe vector \\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\) gives the estimated coefficients that minimize the sum of squared errors between the observed target values \\(y\\) and the predicted values \\(\\hat{y} = X\\hat{\\beta}\\).\n\n\nSummary\n\n\\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\) is the analytical solution for the linear regression coefficients.\nThis solution minimizes the sum of squared errors (SSE) in the OLS framework.\nThe derivation relies on basic principles of calculus (taking derivatives) and linear algebra (matrix inversion).\n\nThis method is exact and works well when \\(X^T X\\) is invertible and the dataset size is manageable.\nUp next knn regression\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "machinelearning/multiplelinreg/index.html#ordinary-least-square-method",
    "href": "machinelearning/multiplelinreg/index.html#ordinary-least-square-method",
    "title": "Multiple Liear Regression",
    "section": "Ordinary Least Square Method",
    "text": "Ordinary Least Square Method\n\nThe scikit-learn library uses Ordinary Least Squares (OLS) method to find the parameters. This method is good for a simple and relatively smaller dataset. Here is a short note on this method. However, when the dimension is very high and the dataset is bigger, scikit-learn uses another method called Stochastic Gradient Descent for optimization which is discussed in the next section.\n\nThe goal of OLS is to find the parameter vector \\(\\hat{\\beta}\\) that minimizes the sum of squared errors (SSE) between the observed target values \\(y\\) and the predicted values \\(\\hat{y}\\):\n\\[\n\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - X_i\\beta)^2\n\\]\nThis can be expressed in matrix form as:\n\\[\n\\text{SSE} = (y - X\\beta)^T(y - X\\beta)\n\\]\nTo minimize the SSE, let’s first expand the expression:\n\\[\\begin{align}\n\\text{SSE} &= (y - X\\beta)^T(y - X\\beta)\\\\\n&=(y^T-X^T\\beta^T)(y-X\\beta)\\\\\n& = y^T y - y^T X\\beta - \\beta^T X^T y + \\beta^T X^T X \\beta\n\\end{align}\\]\nSince \\(\\beta^T X^T y\\) is a scalar (a 1x1 matrix), it is equal to its transpose:\n\\[\n\\text{SSE} = y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta\n\\]\nTo find the minimum of the SSE, we take the derivative with respect to \\(\\beta\\) and set it to zero:\n\\[\n\\frac{\\partial \\text{SSE}}{\\partial \\beta} = -2X^T y + 2X^T X \\beta = 0\n\\]\nNow, solve for \\(\\beta\\):\n\\[\nX^T X \\beta = X^T y\n\\]\nTo isolate \\(\\beta\\), we multiply both sides by \\((X^T X)^{-1}\\) (assuming \\(X^T X\\) is invertible):\n\\[\n\\beta = (X^T X)^{-1} X^T y\n\\]\n\nThe vector \\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\) gives the estimated coefficients that minimize the sum of squared errors between the observed target values \\(y\\) and the predicted values \\(\\hat{y} = X\\hat{\\beta}\\). This method is exact and works well when \\(X^T X\\) is invertible and the dataset size is manageable.   This method is very efficient for small to medium-sized datasets but can become computationally expensive for very large datasets due to the inversion of the matrix \\(X^TX\\)."
  },
  {
    "objectID": "machinelearning/multiplelinreg/index.html#stochastic-gradient-descent-method",
    "href": "machinelearning/multiplelinreg/index.html#stochastic-gradient-descent-method",
    "title": "Multiple Liear Regression",
    "section": "Stochastic Gradient Descent Method",
    "text": "Stochastic Gradient Descent Method\n\nGradient Descent\n\n  GIF Credit: gbhat.com\n Gradient Descent is an optimization algorithm used to minimize the cost function. The cost function \\(f(\\beta)\\) measures how well a model with parameters \\(\\beta\\) fits the data. The goal is to find the values of \\(\\beta\\) that minimize this cost function. In terms of the iterative method, we want to find \\(\\beta_{k+1}\\) and \\(\\beta_k\\) such that \\(f(\\beta_{k+1})&lt;f(\\beta_k)\\).   For a small change in \\(\\beta\\), we can approximate \\(f(\\beta)\\) using Taylor series expansion\n\\[f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}\\]\n\nThe update rule for vanilla gradient descent is given by:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nWhere:\n\n\\(\\beta_k\\) is the current estimate of the parameters at iteration \\(k\\).\n\\(\\eta\\) is the learning rate, a small positive scalar that controls the step size.\n\\(\\nabla f(\\beta_k)\\) is the gradient of the cost function \\(f\\) with respect to \\(\\beta\\) at the current point \\(\\beta_k\\).\n\n\nThe update rule comes from the idea of moving the parameter vector \\(\\beta\\) in the direction that decreases the cost function the most.\n\nGradient:\n\nThe gradient \\(\\nabla f(\\beta_k)\\) represents the direction and magnitude of the steepest ascent of the function \\(f\\) at the point \\(\\beta_k\\). Since we want to minimize the function, we move in the opposite direction of the gradient.\n\nStep Size:\n\nThe term \\(\\eta \\nabla f(\\beta_k)\\) scales the gradient by the learning rate \\(\\eta\\), determining how far we move in that direction. If \\(\\eta\\) is too large, the algorithm may overshoot the minimum; if it’s too small, the convergence will be slow.\n\nIterative Update:\n\nStarting from an initial guess \\(\\beta_0\\), we repeatedly apply the update rule until the algorithm converges, meaning that the changes in \\(\\beta_k\\) become negligible, and \\(\\beta_k\\) is close to the optimal value \\(\\beta^*\\).\n\n\n\n\nStochastic Gradient Descent (SGD)\nStochastic Gradient Descent is a variation of the vanilla gradient descent. Instead of computing the gradient using the entire dataset, SGD updates the parameters using only a single data point or a small batch of data points at each iteration. The later one we call it mini batch SGD.\nSuppose our cost function is defined as the average over a dataset of size \\(n\\):\n\\[\nf(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\beta)\n\\]\nWhere \\(f_i(\\beta)\\) represents the contribution of the \\(i\\)-th data point to the total cost function. The gradient of the cost function with respect to \\(\\beta\\) is:\n\\[\n\\nabla f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\beta)\n\\]\nVanilla gradient descent would update the parameters as:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nInstead of using the entire dataset to compute the gradient, SGD approximates the gradient by using only a single data point (or a small batch). The update rule for SGD is:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f_{i_k}(\\beta_k)\n\\]\nWhere:\n\n\\(i_k\\) is the index of a randomly selected data point at iteration \\(k\\).\n\\(\\nabla f_{i_k}(\\beta_k)\\) is the gradient of the cost function with respect to the parameter \\(\\beta_k\\), evaluated only at the data point indexed by \\(i_k\\).\n\n\nWhy Use SGD?\n\nEfficiency: For large datasets, computing the full gradient \\(\\nabla f(\\beta)\\) can be computationally expensive. SGD reduces this burden by updating the parameters more frequently using only a single (or a few) data points.\nConvergence: While the path taken by SGD is noisier (because it is based on a single data point), it can often lead to faster convergence, especially in high-dimensional spaces.\n\n\n\nConvergence of SGD\n\nConvergence: The sequence \\(\\{\\beta_k\\}\\) generated by SGD converges to the true minimum \\(\\beta^*\\) under certain conditions, such as a sufficiently small learning rate \\(\\eta\\) and diminishing over time.\nTrade-off: There’s a trade-off between the convergence speed and the accuracy of the solution. Smaller learning rates lead to more accurate convergence but slower progress, while larger rates can speed up convergence but might result in oscillations or overshooting the minimum.\n\n\n\n\nSummary:\n\nVanilla Gradient Descent updates parameters based on the full gradient computed from all data points, which can be computationally intensive for large datasets.\nStochastic Gradient Descent (SGD) approximates the gradient using a single data point (or a mini-batch), allowing for faster, albeit noisier, updates.\n\nBoth methods are widely used in machine learning, but SGD is particularly favored for large-scale learning problems due to its computational efficiency.\n\n  GIF Credit: gbhat.com"
  },
  {
    "objectID": "machinelearning/multiplelinreg/index.html#python-execution-of-ols-and-sgd",
    "href": "machinelearning/multiplelinreg/index.html#python-execution-of-ols-and-sgd",
    "title": "Multiple Liear Regression",
    "section": "Python Execution of OLS and SGD",
    "text": "Python Execution of OLS and SGD\n\nSynthetic Data\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nX=np.random.randn(1000,2)\ny=3*X[:,0]+2*X[:,1]+1+np.random.randn(1000)\n\nSo for this project, our known relationship is \\(y=1+3x_1+2x_2+\\xi\\).\n\n\nFit the data: Using scikit-learn Library\n\nmlr=LinearRegression()\nmlr.fit(X,y)\ncoefficients=mlr.coef_.tolist()\nslope=mlr.intercept_.tolist()\n\nSo the model parameters: slope \\(\\beta_0=\\) 1.0293 and coefficients \\(\\beta_1=\\) 2.9878, and \\(\\beta_2=\\) 1.9746\n\n\nFit the data: Using Custom Library\nFirst we create our custom NewLinearRegression using the OLS formula above and save this python class as mlreg.py\nimport numpy as np\n\n\nclass NewLinearRegression:\n    def __init__(self) -&gt; None:\n        self.beta = None\n\n    def fit(self, X, y):\n        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n        X_transpose_X = np.dot(X.transpose(), X)\n        X_transpose_X_inverse = np.linalg.inv(X_transpose_X)\n        X_transpose_y = np.dot(X.transpose(), y)\n        self.beta = np.dot(X_transpose_X_inverse, X_transpose_y)\n\n    def predict(self, X):\n        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n        return np.dot(X, self.beta)\n\n    def coef_(self):\n        return self.beta[1:].tolist()\n\n    def intercept_(self):\n        return self.beta[0].tolist()\nNow it’s time to use the new class\n\nfrom mlreg import NewLinearRegression\nmlr1 = NewLinearRegression()\nmlr1.fit(X,y)\ncoefficients=mlr1.coef_()\nslope=mlr1.intercept_()\n\nSo the model parameters: slope \\(\\beta_0=\\) 1.0293 and coefficients \\(\\beta_1=\\) 2.9878, and \\(\\beta_2=\\) 1.9746\nUp next knn regression\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "machinelearning/multiplelinreg/index.html#python-execution",
    "href": "machinelearning/multiplelinreg/index.html#python-execution",
    "title": "Multiple Liear Regression",
    "section": "Python Execution",
    "text": "Python Execution\n\nSynthetic Data\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nX=np.random.randn(1000,2)\ny=3*X[:,0]+2*X[:,1]+1+np.random.randn(1000)\n\nSo for this project, our known relationship is \\(y=1+3x_1+2x_2+\\xi\\).\n\n\nFit the data: Using scikit-learn Library\n\nmlr=LinearRegression()\nmlr.fit(X,y)\ncoefficients=mlr.coef_.tolist()\nslope=mlr.intercept_.tolist()\n\nSo the model parameters: slope \\(\\beta_0=\\) 0.9733 and coefficients \\(\\beta_1=\\) 2.9936, and \\(\\beta_2=\\) 2.0177\n\n\nFit the data: Using Custom Library OLS\nFirst we create our custom NewLinearRegression using the OLS formula above and save this python class as mlreg.py\nimport numpy as np\n\n\nclass NewLinearRegression:\n    def __init__(self) -&gt; None:\n        self.beta = None\n\n    def fit(self, X, y):\n        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n        X_transpose_X = np.dot(X.transpose(), X)\n        X_transpose_X_inverse = np.linalg.inv(X_transpose_X)\n        X_transpose_y = np.dot(X.transpose(), y)\n        self.beta = np.dot(X_transpose_X_inverse, X_transpose_y)\n\n    def predict(self, X):\n        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n        return np.dot(X, self.beta)\n\n    def coeff_(self):\n        return self.beta[1:].tolist()\n\n    def interceptt_(self):\n        return self.beta[0].tolist()\nNow it’s time to use the new class\n\nfrom mlreg import NewLinearRegression\nmlr1 = NewLinearRegression()\nmlr1.fit(X,y)\ncoefficients1=mlr1.coeff_()\nslope1=mlr1.interceptt_()\n\nSo the model parameters: slope \\(\\beta_0=\\) 0.9733 and coefficients \\(\\beta_1=\\) 2.9936, and \\(\\beta_2=\\) 2.0177\n\n\nFit the data: Using Gradient Descent\nWe create the class\nclass GDLinearRegression:\n    def __init__(self, learning_rate=0.01, number_of_iteration=1000) -&gt; None:\n        self.learning_rate = learning_rate\n        self.number_of_iteration = number_of_iteration\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        num_of_samples, num_of_features = X.shape\n        self.weights = np.zeros(num_of_features)\n        self.bias = 0\n\n        for _ in range(self.number_of_iteration):\n            y_predicted = np.dot(X, self.weights) + self.bias\n\n            d_weights = (1 / num_of_samples) * np.dot(X.T, (y_predicted - y))\n            d_bias = (1 / num_of_samples) * np.sum(y_predicted - y)\n\n            self.weights -= self.learning_rate * d_weights\n            self.bias -= self.learning_rate * d_bias\n\n    def predict(self, X):\n        y_predicted = np.dot(X, self.weights) + self.bias\n        return y_predicted\n\n    def coefff_(self):\n        return self.weights.tolist()\n\n    def intercepttt_(self):\n        return self.bias\nNow we use this similarly as before,\n\nfrom mlreg import GDLinearRegression\nmlr2= GDLinearRegression(learning_rate=0.008)\nmlr2.fit(X,y)\ncoefficients2=mlr2.coefff_()\nslope2=mlr2.intercepttt_()\n\nSo the model parameters: slope \\(\\beta_0=\\) 0.9729 and coefficients \\(\\beta_1=\\) 2.9932, and \\(\\beta_2=\\) 2.0174\n\n\nFit the data: Using Stochastic Gradient Descent\nFirst we define the class\nclass SGDLinearRegression:\n    def __init__(self, learning_rate=0.01, num_iterations=1000, batch_size=1) -&gt; None:\n        self.learning_rate = learning_rate\n        self.num_iterations = num_iterations\n        self.batch_size = batch_size\n        self.theta = None\n        self.mse_list = None  # Initialize mse_list as an instance attribute\n\n    def _loss_function(self, X, y, beta):\n        num_samples = len(y)\n        y_predicted = X.dot(beta)\n        mse = (1/num_samples) * np.sum(np.square(y_predicted - y))\n        return mse\n\n    def _gradient_function(self, X, y, beta):\n        num_samples = len(y)\n        y_predicted = X.dot(beta)\n        grad = (1/num_samples) * X.T.dot(y_predicted - y)\n        return grad\n\n    def fit(self, X, y):\n        # Adding the intercept term (bias) as a column of ones\n        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n        num_features = X.shape[1]\n        self.theta = np.zeros((num_features, 1))\n\n        self.mse_list = np.zeros(self.num_iterations)  # Initialize mse_list\n\n        for i in range(self.num_iterations):\n            # Randomly select a batch of data points\n            indices = np.random.choice(\n                len(y), size=self.batch_size, replace=False)\n            X_i = X[indices]\n            y_i = y[indices].reshape(-1, 1)\n\n            # Compute the gradient and update the weights\n            gradient = self._gradient_function(X_i, y_i, self.theta)\n            self.theta = self.theta - self.learning_rate * gradient\n\n            # Calculate loss for the entire dataset (optional)\n            self.mse_list[i] = self._loss_function(X, y, self.theta)\n\n        return self.theta, self.mse_list\n\n    def predict(self, X):\n        # Adding the intercept term (bias) as a column of ones\n        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n        return X.dot(self.theta)\n\n    def coef_(self):\n        # Return the coefficients (excluding the intercept term)\n        return self.theta[1:].flatten().tolist()\n\n    def intercept_(self):\n        # Return the intercept term\n        return self.theta[0].item()\n\n    def mse_losses(self):\n        # Return the mse_list\n        return self.mse_list.tolist()\nNow\n\nimport matplotlib.pyplot as plt\nfrom mlreg import SGDLinearRegression\nmlr3=SGDLinearRegression(learning_rate=0.01, num_iterations=1000, batch_size=10)\ntheta, _ = mlr3.fit(X, y)\n\nSo the model parameters: slope \\(\\beta_0=\\) array([0.97712198]) and coefficients \\(\\beta_1=\\) array([2.98512209]), and \\(\\beta_2=\\) array([2.01030634])\nUp next knn regression\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "machinelearning/multiplelinreg/index.html#iterative-method",
    "href": "machinelearning/multiplelinreg/index.html#iterative-method",
    "title": "Multiple Liear Regression",
    "section": "Iterative Method",
    "text": "Iterative Method\n\nGradient Descent\n\n  GIF Credit: gbhat.com\n Gradient Descent is an optimization algorithm used to minimize the cost function. The cost function \\(f(\\beta)\\) measures how well a model with parameters \\(\\beta\\) fits the data. The goal is to find the values of \\(\\beta\\) that minimize this cost function. In terms of the iterative method, we want to find \\(\\beta_{k+1}\\) and \\(\\beta_k\\) such that \\(f(\\beta_{k+1})&lt;f(\\beta_k)\\).   For a small change in \\(\\beta\\), we can approximate \\(f(\\beta)\\) using Taylor series expansion\n\\[f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}\\]\n\nThe update rule for vanilla gradient descent is given by:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nWhere:\n\n\\(\\beta_k\\) is the current estimate of the parameters at iteration \\(k\\).\n\\(\\eta\\) is the learning rate, a small positive scalar that controls the step size.\n\\(\\nabla f(\\beta_k)\\) is the gradient of the cost function \\(f\\) with respect to \\(\\beta\\) at the current point \\(\\beta_k\\).\n\n\nThe update rule comes from the idea of moving the parameter vector \\(\\beta\\) in the direction that decreases the cost function the most.\n\nGradient: The gradient \\(\\nabla f(\\beta_k)\\) represents the direction and magnitude of the steepest ascent of the function \\(f\\) at the point \\(\\beta_k\\). Since we want to minimize the function, we move in the opposite direction of the gradient.\nStep Size: The term \\(\\eta \\nabla f(\\beta_k)\\) scales the gradient by the learning rate \\(\\eta\\), determining how far we move in that direction. If \\(\\eta\\) is too large, the algorithm may overshoot the minimum; if it’s too small, the convergence will be slow.\nIterative Update: Starting from an initial guess \\(\\beta_0\\), we repeatedly apply the update rule until the algorithm converges, meaning that the changes in \\(\\beta_k\\) become negligible, and \\(\\beta_k\\) is close to the optimal value \\(\\beta^*\\).\n\n\n\nStochastic Gradient Descent (SGD)\n\nStochastic Gradient Descent is a variation of the vanilla gradient descent. Instead of computing the gradient using the entire dataset, SGD updates the parameters using only a single data point or a small batch of data points at each iteration. The later one we call it mini batch SGD.\n\nSuppose our cost function is defined as the average over a dataset of size \\(n\\):\n\\[\nf(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\beta)\n\\]\nWhere \\(f_i(\\beta)\\) represents the contribution of the \\(i\\)-th data point to the total cost function. The gradient of the cost function with respect to \\(\\beta\\) is:\n\\[\n\\nabla f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\beta)\n\\]\nVanilla gradient descent would update the parameters as:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nInstead of using the entire dataset to compute the gradient, SGD approximates the gradient by using only a single data point (or a small batch). The update rule for SGD is:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f_{i_k}(\\beta_k)\n\\]\nWhere:\n\n\\(i_k\\) is the index of a randomly selected data point at iteration \\(k\\).\n\\(\\nabla f_{i_k}(\\beta_k)\\) is the gradient of the cost function with respect to the parameter \\(\\beta_k\\), evaluated only at the data point indexed by \\(i_k\\)."
  },
  {
    "objectID": "posts/statisticaltalk/index.html",
    "href": "posts/statisticaltalk/index.html",
    "title": "Some Key Statistical Concepts for Interview Prep",
    "section": "",
    "text": "In the world of data analysis and machine learning, statistics plays a vital role in making sense of the data. Whether you’re estimating parameters, testing hypotheses, or understanding relationships between variables, statistical concepts guide how we interpret data. In this post, I want to summarise and collect some fundamental statistical ideas that are quite common and asked in many data science, machine learning, and quant interviews"
  },
  {
    "objectID": "posts/statisticaltalk/index.html#introduction",
    "href": "posts/statisticaltalk/index.html#introduction",
    "title": "Some Statistical Discussion",
    "section": "",
    "text": "The random variable \\(X\\)"
  },
  {
    "objectID": "posts/statisticaltalk/index.html#basic-statistical-terminologies",
    "href": "posts/statisticaltalk/index.html#basic-statistical-terminologies",
    "title": "Some Key Statistical Concepts for Interview Prep",
    "section": "Basic Statistical Terminologies",
    "text": "Basic Statistical Terminologies\n\nThe mean\nThe mean is one of the most basic statistical concepts and represents the average value of a dataset. It’s calculated by summing all the values in a dataset and then dividing by the number of observations.\nMathematically, for a set of discrete observations \\(x_1, x_2, ..., x_n\\), the mean \\(\\mu\\) or Expected Value is defined as:\n\\[\\begin{align*}\n\\mu &=  \\frac{1}{n} \\sum_{i=1}^{n} x_i\\\\\n\\implies \\mathbb{E}[X] &= \\sum_{i=1}^{n} x_i\\mathbb{P}(X=x_i)\n\\end{align*}\\]\nFor a continuous random variable \\(X\\), the mean\n\\[\n\\mu = \\mathbb{E}[X]=\\int_{-\\infty}^{\\infty}xf_X(x)dx\n\\]\n\nwhere, \\(\\mathbb{P}(X=x)\\) is the probability mass function (pmf) and \\(f_X(x)\\) is the probability density function (pdf) of the random variable \\(X\\), depending on whether it is discrete or contineous type. The mean helps describe the central tendency of data, but it can be sensitive to outliers.\n\n\n\nVariance\n\nVariance measures the spread or dispersion of a dataset relative to its mean. It tells us how far the individual data points are from the mean. A small variance indicates that data points are clustered closely around the mean, while a large variance means they are spread out.\n\nThe formula for variance \\(\\sigma^2\\) is:\n\\[\\begin{align*}\n    \\sigma^2=Var(X)&=\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2\\\\\n    &=\\mathbb{E}\\left[(X-\\mathbb{E}[X])^2\\right]\\\\\n    &=\\mathbb{E}\\left[(X^2-2X\\mathbb{E}[X]+(\\mathbb{E}[X])^2)\\right]\\\\\n    &=\\mathbb{E}[X^2]-2\\mathbb{E}[X]\\mathbb{E}[X]+(\\mathbb{E}[X])^2\\\\\n    &=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2\\\\\n\\end{align*}\\]\nHowever, the population and sample variance formula are slightly different. For discrete observations, the sample variance is given as\n\\[ s= \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]\nInstead of dividing by \\(n\\) we devide by \\(n-1\\) to have the sample variance unbiased and bigger than the population variance so that it contains the true population variance.\nExamples\n\nNormal Distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) has the pdf \\(f_{X}(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]}\\)\n\nStandard Normal Distribution with mean \\(0\\) and variance \\(1\\) has the pdf \\(f_{X}(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{x^2}{2}\\right]}\\)\n\nNow if \\(\\log X\\sim \\mathbfcal{N}(0,1)\\) then what is the distribution of \\(X\\)?\n\n\n\nCovariance\n\nCovariance measures how two variables move together. If the covariance is positive, the two variables tend to increase or decrease together. If negative, one variable tends to increase when the other decreases.\n\nThe formula for covariance between two variables \\(X\\) and \\(Y\\) is:\n\\[\n\\text{Cov}(X, Y) = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\mu_X)(Y_i - \\mu_Y)\n\\]\nHowever, covariance doesn’t indicate the strength of the relationship, which brings us to correlation.\n\n\nCorrelation\n\nCorrelation is a standardized measure of the relationship between two variables. It ranges from \\(-1\\) to \\(1\\), where \\(1\\) indicates a perfect positive relationship, \\(-1\\) a perfect negative relationship, and \\(0\\) no relationship.\n\nThe most common correlation metric is Pearson correlation, defined as:\n\\[\n\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\\]\nUnlike covariance, correlation gives a clearer picture of the strength and direction of a linear relationship between variables.\n\n\nP-Values and Hypothesis Testing\n\nP-values and hypothesis testing form the backbone of inferential statistics. Hypothesis testing is used to determine if a given assumption (the null hypothesis \\(H_0\\)) about a population parameter is true or not.\n\n\nThe null hypothesis \\(H_0\\) typically suggests no effect or no difference.\nThe alternative hypothesis \\(H_1\\) is the claim you want to test.\n\nThe p-value is the probability of observing a result as extreme as, or more extreme than, the one obtained, assuming the null hypothesis is true. A small p-value (usually less than 0.05) indicates that the null hypothesis is unlikely, and we may reject it in favor of the alternative hypothesis.\n\n\nMaximum Likelihood Estimation (MLE)\n\nMaximum Likelihood Estimation (MLE) is a method for estimating the parameters of a statistical model. The idea behind MLE is to find the parameter values that maximize the likelihood function, which represents the probability of observing the given data under a particular model.\n\nGiven a parameter \\(\\theta\\) and observed data \\(X\\), the likelihood function is:\n\\[\nL(\\theta | X) = P(X | \\theta)\n\\]\nMLE finds the parameter \\(\\hat{\\theta}\\) that maximizes this likelihood:\n\\[\n\\hat{\\theta} = \\arg\\max_{\\theta} L(\\theta | X)\n\\]\nMLE is widely used in statistical modeling, from simple linear regression to complex machine learning algorithms.\n\n7. Maximum A Posteriori (MAP)\nWhile MLE focuses on maximizing the likelihood, Maximum A Posteriori (MAP) estimation incorporates prior information about the parameters. MAP is rooted in Bayesian statistics, where the goal is to find the parameter that maximizes the posterior distribution.\nThe posterior is given by Bayes’ Theorem:\n\\[\nP(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{P(X)}\n\\]\nMAP finds the parameter \\(\\hat{\\theta}_{\\text{MAP}}\\) that maximizes the posterior probability:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} P(\\theta | X)\n\\]\nUnlike MLE, MAP estimation incorporates the prior distribution \\(P(\\theta)\\), making it more robust when prior knowledge is available."
  },
  {
    "objectID": "posts/decisiontree/index.html",
    "href": "posts/decisiontree/index.html",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "",
    "text": "The Decision Tree Classifier is a powerful, interpretable, and widely-used algorithm in machine learning for binary or multi-class classification problems. Its simplicity and visual appeal make it a go-to choice for classification tasks. However, behind this simplicity lies a series of mathematical decisions that guide how the tree is constructed."
  },
  {
    "objectID": "posts/decisiontree/index.html#decision-tree",
    "href": "posts/decisiontree/index.html#decision-tree",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "",
    "text": "The Decision Tree Classifier is a powerful, interpretable, and widely-used algorithm in machine learning for binary or multi-class classification problems. Its simplicity and visual appeal make it a go-to choice for classification tasks. However, behind this simplicity lies a series of mathematical decisions that guide how the tree is constructed."
  },
  {
    "objectID": "posts/decisiontree/index.html#the-core-idea-behind-decision-trees",
    "href": "posts/decisiontree/index.html#the-core-idea-behind-decision-trees",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "The Core Idea Behind Decision Trees",
    "text": "The Core Idea Behind Decision Trees\n\nDecision Tree contains two main type of nodes, decision nodes and leaf nodes. A decision node is a node where a condition is applied to split the data and a leaf node contains the class of a data point. At its heart, a decision tree works by recursively splitting the dataset based on feature values. The goal of each split is to increase the homogeneity of the resulting subgroups, ideally separating the different classes as much as possible. The splitting process relies on a measure of impurity or disorder. The two most common metrics used for this purpose are Gini Impurity and Entropy (used in Information Gain).\n\nGini Impurity\nThe Gini Impurity measures the likelihood of misclassifying a randomly chosen element from the dataset if it were labeled according to the distribution of classes in that subset. Mathematically, the Gini Impurity for a node \\(t\\) is calculated as:\n\\[\\begin{align*}\nG(t) &= 1 - \\sum_{i=1}^{n} p_i^2\n\\end{align*}\\]\nwhere \\(p_i\\) is the proportion of samples belonging to class \\(i\\) at node \\(t\\).\nEntropy and Information Gain\nEntropy, borrowed from information theory, measures the disorder or uncertainty in the dataset. It is defined as:\n\\[H(t) = -\\sum_{i=1}^{n} p_i \\log_2(p_i)\\]\n\n\nCode\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt \n\nx=np.arange(0.01,0.99,0.0001)\ny=[-p*math.log(p,2)-(1-p)*math.log(1-p,2) for p in x]\nplt.plot(x,y)\nplt.xlabel('$p_{\\oplus}$')\nplt.ylabel('$H(t)$')\nplt.title('Entropy')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nInformation Gain is the reduction in entropy after a dataset is split on a feature. It is calculated as:\n\\[IG(D, A) = H(D) - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} H(D_v)\\]\nwhere:\n\n\\(D\\) is the dataset,\n\\(A\\) is the feature on which the split is made,\n\\(D_v\\) is the subset of \\(D\\) for which feature \\(A\\) has value \\(v\\).\n\n\nLet’s explain the math with following example.\nSay, I have the data set like this\n\n\n\n\\(x_0\\)\n\\(x_1\\)\nClass\n\n\n\n\n2\n3\n0\n\n\n3\n4\n0\n\n\n4\n6\n0\n\n\n6\n8\n1\n\n\n7\n10\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\nTotal 20 data points and the scatter plot looks like this\n\n\nCode\ndata = [\n    [2, 3, 0], [3, 4, 0], [4, 6, 0], [6, 8, 1], [7, 10, 1],\n    [8, 12, 1], [5, 7, 1], [2, 5, 0], [9, 15, 1], [1, 2, 0],\n    [11, 3, 0], [4, 13, 1], [8, 14, 1], [1, 5, 0], [6, 2, 1],\n    [9, 3, 1], [15, 13, 0], [7, 5, 0], [5, 9, 0], [8, 3, 1]\n]\n\nx0 = [row[0] for row in data]\nx1 = [row[1] for row in data]\nclasses = [row[2] for row in data]\n\ncolors = ['red' if c == 0 else 'blue' for c in classes]\n\nplt.figure(figsize=(7, 5))\nplt.grid(True)\n\nplt.scatter(x0, x1, color=colors, s=100, edgecolor='black')\n\n# Label points with class values\nfor i in range(len(x0)):\n    plt.text(x0[i] + 0.2, x1[i] + 0.2, str(classes[i]), fontsize=9)\n\n# Set limits for the axes\nplt.xlim(0, 16)\nplt.ylim(0, 16)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\n# Label axes and show plot\nplt.xlabel('$x_0$')\nplt.ylabel('$x_1$')\nplt.title('Figure 1: Scatter Plot of $x_0$ vs $x_1$ ')\nplt.show()\n\n\n\n\n\n\n\n\n\nAt this point, we see that the classes are not linearly separable, meaning, we can not draw any line that separate the two classes. Notice that the minimum and maximum of feature \\(x_0\\) is 1 and 15, respectively. So, let’s pick a few numbers in between these two numbers. Say, our first number is \\(3.5\\). In the first node, that is the root node, we divide the data based on the feature \\(x_0\\le 3.5\\)\n\n\n\nFigure 2: First Split\n\n\nAt the root node, we have equal number of blue and red points so the proportion of the data class is \\(p_1=p_2=0.5\\), so the entropy\n\\[\\begin{align*}\n    H(\\text{root node})&=-(0.5)\\log_2(0.5)-(0.5)\\log_2(0.5)=1\\\\\n\\end{align*}\\]\nBased on the condition \\(x_0\\le 3.5\\), the left and right child recieves 5 and 15 feature points \\(X=(x_0,x_1)\\), respectively. We see that the left node is a pure node, because it contains only the red points. Therefore, the entropies at these child nodes\n\\[\\begin{align*}\n    H(\\text{left child})&=-1\\log_2(1)-0\\log_2(0)=0\\\\\n    H(\\text{right child})&=-\\frac{5}{15}\\log_2\\left(\\frac{5}{15}\\right)-\\frac{10}{15}\\log_2\\left(\\frac{10}{15}\\right)=0.92\\\\\n\\end{align*}\\]\nand the information gain at this split\n\\[IG(split_1)=1-\\left(\\frac{5}{20}\\cdot 0+\\frac{15}{20}\\cdot 0.92\\right)=0.31\\]\nNow the burning question is how did we select the condition \\(x_0\\le 3.5\\)? It could have been any other number, say we set \\(x_0\\le 6.5\\). Then\n\n\n\nFigure 3: Alternative Split\n\n\nBased on the condition \\(x_0\\le 6.5\\), the left and right child recieves 11 and 9 feature points \\(X=(x_0,x_1)\\), respectively. But in this case we don’t see any pure nodes and the entropies at these child nodes\n\\[\\begin{align*}\n    H(\\text{left child})&=-\\frac{7}{11}\\log_2\\left(\\frac{7}{11}\\right)-\\frac{4}{11}\\log_2\\left(\\frac{4}{11}\\right)=0.95\\\\\n    H(\\text{right child})&=-\\frac{3}{9}\\log_2\\left(\\frac{3}{9}\\right)-\\frac{6}{9}\\log_2\\left(\\frac{6}{9}\\right)=0.92\\\\\n\\end{align*}\\]\nand the information gain at this split\n\\[IG(split_1)=1-\\left(\\frac{11}{20}\\cdot 0.95+\\frac{9}{20}\\cdot 0.92\\right)=0.06\\]\nNote that the information gain is much lower than the first option. Therefore, the first split is better than this alternative split. Because the goal is to have minimum entropy value and/or the maximum information gain. This is where the machine learning gets in the game. The algorithm finds the optimal split based on each feature values.\n\n\n\nFigure 4: Second Split\n\n\nNow say we have a new set of feature values \\((x_0,x_1,Class)=(10,7,1)\\). Based on our tree above, since \\(x_0\\) is NOT less than or equal to \\(3.5\\) so it goes to the right first child. Then it satisfies \\(x_0\\le 10\\). So it moves to the left grand child gradually traverse through the tree and ended up to the very bottom layer left leaf node."
  },
  {
    "objectID": "posts/decisiontree/index.html#building-a-decision-tree",
    "href": "posts/decisiontree/index.html#building-a-decision-tree",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Building a Decision Tree",
    "text": "Building a Decision Tree\n\nChoose the best feature to split on: Calculate Gini impurity or Information Gain for each feature and select the feature that results in the highest Information Gain or lowest Gini impurity.\nSplit the dataset: Partition the data based on the chosen feature and repeat the process for each partition.\nStop conditions: The tree stops growing when all samples in a node belong to the same class, the maximum depth is reached, or further splitting doesn’t add value."
  },
  {
    "objectID": "posts/decisiontree/index.html#implementation-of-decision-tree-scikit-learn",
    "href": "posts/decisiontree/index.html#implementation-of-decision-tree-scikit-learn",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Implementation of Decision Tree: Scikit-learn",
    "text": "Implementation of Decision Tree: Scikit-learn\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier,plot_tree\nfrom sklearn.metrics import accuracy_score\n\nX=pd.DataFrame({'Feature 1':x0, 'Feature 2':x1})\ny=classes\n\n\nclf= DecisionTreeClassifier(criterion=\"entropy\")\nclf.fit(X,y)\n\n\nX_test=pd.DataFrame({'Feature 1':[10,9,11],'Feature 2':[7,9,5]})\ny_test=pd.DataFrame({'Class':[1,0,1]})\n\ntest_data=pd.concat([X_test,y_test], axis=1)\nprint('Test Data \\n')\nprint(test_data)\n\n\ny_prediction=clf.predict(X_test)\nprediction=pd.DataFrame({'Predicted_Class':y_prediction})\nprediction=pd.concat([test_data,prediction],axis=1)\nprint('\\n')\nprint('Result \\n')\nprint(prediction)\nprint('\\n')\nprint('Accuracy score:',round(accuracy_score(y_prediction,y_test),2))\n\nplt.figure(figsize=(11,7))\nplot_tree(clf, filled=True, \n          feature_names=['$x_0$','$x_1$'], \n          class_names=['R', 'B'], impurity=True,\n          )\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\nTest Data \n\n   Feature 1  Feature 2  Class\n0         10          7      1\n1          9          9      0\n2         11          5      1\n\n\nResult \n\n   Feature 1  Feature 2  Class  Predicted_Class\n0         10          7      1                1\n1          9          9      0                0\n2         11          5      1                0\n\n\nAccuracy score: 0.67"
  },
  {
    "objectID": "posts/decisiontree/index.html#discussion-on-decision-tree",
    "href": "posts/decisiontree/index.html#discussion-on-decision-tree",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Discussion on Decision Tree",
    "text": "Discussion on Decision Tree\n\nBeing a simple algorithm, it has both pros and cons. It is robust to training data and the training data can contain missing values. However, it is a greedy algorithm, a problem-solving technique that chooses the best option in the current situation, without considering the overall outcome. It also face the overfitting issue."
  },
  {
    "objectID": "posts/decisiontree/index.html#reference",
    "href": "posts/decisiontree/index.html#reference",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Reference",
    "text": "Reference\nDecision Tree Classification Clearly Explained by Normalized Nerd\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "codepages/medicalcost/index.html#data",
    "href": "codepages/medicalcost/index.html#data",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Data",
    "text": "Data\n\nmedical = pd.read_csv('insurance.csv')\nmedical.head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520"
  },
  {
    "objectID": "codepages/medicalcost/index.html#data-gathering-defining-stakeholders-and-kpis",
    "href": "codepages/medicalcost/index.html#data-gathering-defining-stakeholders-and-kpis",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "import pandas as pd\n\ninsurance = pd.read_csv('insurance.csv')\n\ninsurance.sample(5, random_state=111)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n1000\n30\nmale\n22.99\n2\nyes\nnorthwest\n17361.7661\n\n\n53\n36\nmale\n34.43\n0\nyes\nsoutheast\n37742.5757\n\n\n432\n42\nmale\n26.90\n0\nno\nsouthwest\n5969.7230\n\n\n162\n54\nmale\n39.60\n1\nno\nsouthwest\n10450.5520\n\n\n1020\n51\nmale\n37.00\n0\nno\nsouthwest\n8798.5930\n\n\n\n\n\n\n\n\n\n\n\n\n\ninsurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\nNo missing data. Total 1338 observations.\n\n\n\nStatistical properties of the non-categorical variables\n\nprint(insurance.describe().T)\n\n           count          mean           std        min         25%       50%  \\\nage       1338.0     39.207025     14.049960    18.0000    27.00000    39.000   \nbmi       1338.0     30.663397      6.098187    15.9600    26.29625    30.400   \nchildren  1338.0      1.094918      1.205493     0.0000     0.00000     1.000   \ncharges   1338.0  13270.422265  12110.011237  1121.8739  4740.28715  9382.033   \n\n                   75%          max  \nage          51.000000     64.00000  \nbmi          34.693750     53.13000  \nchildren      2.000000      5.00000  \ncharges   16639.912515  63770.42801  \n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(3,1, figsize = (6,18))\n\nsns.histplot(insurance['age'], color='red', kde=True, ax= axes[0])\nsns.histplot(insurance['bmi'], color='green', kde=True, ax= axes[1])\nsns.histplot(insurance['charges'], color='blue', kde=True, ax= axes[2])\n\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\n\n\n\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8, 12))\ngrid = fig.add_gridspec(3, 2)\n\nax1 = fig.add_subplot(grid[0, 0])\nsns.countplot(x=insurance['sex'], hue=insurance['sex'], palette='Set1', legend=False, ax=ax1)\nax1.set_title('Gender Distribution')\nax1.set_facecolor('#f4f4f4')\n\nax2 = fig.add_subplot(grid[0, 1])\nsns.countplot(x=insurance['smoker'], hue=insurance['smoker'], palette='Set2', ax=ax2)\nax2.set_title('Smoker Distribution')\nax2.set_facecolor('#f4f4f4')\n\nax3 = fig.add_subplot(grid[1, :])\nsns.countplot(x='smoker', data=insurance, hue='sex', palette='Set3', ax=ax3)\nax3.set_title('Smoker Distribution by Gender')\nax3.set_facecolor('#f4f4f4')\n\nax4 = fig.add_subplot(grid[2, :])\nsns.countplot(x=insurance['region'], hue=insurance['region'], palette='Set1', ax=ax4)\nax4.set_title('Region Distribution')\nax4.set_facecolor('#f4f4f4')\n\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\ncorr_matrix = insurance[['age','bmi','children','charges']].corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Matrix')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nsns.scatterplot(x='age', y='charges', data=insurance)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Binary Encoding for the variables with two categories\ninsurance['sex'] = insurance['sex'].map({'male':1, 'female':0})\ninsurance['smoker'] = insurance['smoker'].map({'yes':1, 'no':0})\n\n# One-Hot Encoding for the multiclas variable: region\ninsurance = pd.get_dummies(\n    insurance, columns=['region'],drop_first=True,\n    dtype=int\n    )\n\n\n\n\n\n# Round the continuous charge variable to 2 decimal places\ninsurance['charges'] = insurance['charges'].round(2)\n\n# Mofe the predicting variable at the end of the dataframe\ninsurance_charges = insurance.pop('charges')\ninsurance.insert(loc = len(insurance.columns), column='charges', value=insurance_charges)\n\n# Quick look of the dataframe\ninsurance.head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion_northwest\nregion_southeast\nregion_southwest\ncharges\n\n\n\n\n0\n19\n0\n27.900\n0\n1\n0\n0\n1\n16884.92\n\n\n1\n18\n1\n33.770\n1\n0\n0\n1\n0\n1725.55\n\n\n2\n28\n1\n33.000\n3\n0\n0\n1\n0\n4449.46\n\n\n3\n33\n1\n22.705\n0\n0\n1\n0\n0\n21984.47\n\n\n4\n32\n1\n28.880\n0\n0\n1\n0\n0\n3866.86"
  },
  {
    "objectID": "codepages/medicalcost/index.html#data-cleaning-and-preprocessing",
    "href": "codepages/medicalcost/index.html#data-cleaning-and-preprocessing",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Data Cleaning and Preprocessing",
    "text": "Data Cleaning and Preprocessing\n\nData Information\n\ninsurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\n\n\nData Processing for Categorical Variables\n\n# Binary Encoding for the variables with two categories\ninsurance['sex'] = insurance['sex'].map({'male':1, 'female':0})\ninsurance['smoker'] = insurance['smoker'].map({'yes':1, 'no':0})\n\n# One-Hot Encoding for the multiclas variable: region\ninsurance = pd.get_dummies(\n    insurance, columns=['region'],drop_first=True,\n    dtype=int\n    )\n\n# Round the continuous charge variable to 2 decimal places\ninsurance['charges'] = insurance['charges'].round(2)\n\n# Mofe the predicting variable at the end of the dataframe\ninsurance_charges = insurance.pop('charges')\ninsurance.insert(loc = len(insurance.columns), column='charges', value=insurance_charges)\n\n# Quick look of the dataframe\ninsurance.head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion_northwest\nregion_southeast\nregion_southwest\ncharges\n\n\n\n\n0\n19\n0\n27.900\n0\n1\n0\n0\n1\n16884.92\n\n\n1\n18\n1\n33.770\n1\n0\n0\n1\n0\n1725.55\n\n\n2\n28\n1\n33.000\n3\n0\n0\n1\n0\n4449.46\n\n\n3\n33\n1\n22.705\n0\n0\n1\n0\n0\n21984.47\n\n\n4\n32\n1\n28.880\n0\n0\n1\n0\n0\n3866.86"
  },
  {
    "objectID": "codepages/medicalcost/index.html#exploratory-data-analysis-eda",
    "href": "codepages/medicalcost/index.html#exploratory-data-analysis-eda",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nData Information\n\ninsurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\nNo missing data. Total 1338 observations.\n\n\nData Description\nStatistical properties of the non-categorical variables\n\nprint(insurance.describe().T)\n\n           count          mean           std        min         25%       50%  \\\nage       1338.0     39.207025     14.049960    18.0000    27.00000    39.000   \nbmi       1338.0     30.663397      6.098187    15.9600    26.29625    30.400   \nchildren  1338.0      1.094918      1.205493     0.0000     0.00000     1.000   \ncharges   1338.0  13270.422265  12110.011237  1121.8739  4740.28715  9382.033   \n\n                   75%          max  \nage          51.000000     64.00000  \nbmi          34.693750     53.13000  \nchildren      2.000000      5.00000  \ncharges   16639.912515  63770.42801  \n\n\n\n\nAnalyis of the continuous variables\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(3,1, figsize = (6,18))\n\nsns.histplot(insurance['age'], color='red', kde=True, ax= axes[0])\nsns.histplot(insurance['bmi'], color='green', kde=True, ax= axes[1])\nsns.histplot(insurance['charges'], color='blue', kde=True, ax= axes[2])\n\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\n\n\n\n\n\n\n\n\n\n\nAnalysis of the categorical variables\n\nfig = plt.figure(figsize=(8, 12))\ngrid = fig.add_gridspec(3, 2)\n\nax1 = fig.add_subplot(grid[0, 0])\nsns.countplot(x=insurance['sex'], hue=insurance['sex'], palette='Set1', legend=False, ax=ax1)\nax1.set_title('Gender Distribution')\nax1.set_facecolor('#f4f4f4')\n\nax2 = fig.add_subplot(grid[0, 1])\nsns.countplot(x=insurance['smoker'], hue=insurance['smoker'], palette='Set2', ax=ax2)\nax2.set_title('Smoker Distribution')\nax2.set_facecolor('#f4f4f4')\n\nax3 = fig.add_subplot(grid[1, :])\nsns.countplot(x='smoker', data=insurance, hue='sex', palette='Set3', ax=ax3)\nax3.set_title('Smoker Distribution by Gender')\nax3.set_facecolor('#f4f4f4')\n\nax4 = fig.add_subplot(grid[2, :])\nsns.countplot(x=insurance['region'], hue=insurance['region'], palette='Set1', ax=ax4)\nax4.set_title('Region Distribution')\nax4.set_facecolor('#f4f4f4')\n\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAnalysis of the correlation of continuous variables\n\ncorr_matrix = insurance[['age','bmi','children','charges']].corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Matrix')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRelationsship Between Features and Target\n\nfig, axes = plt.subplots(5,1, figsize=(8,25))\nsns.scatterplot(x='age', y='charges', data=insurance, hue='sex' ,ax=axes[0])\naxes[0].set_title('Age vs Charges')\nsns.scatterplot(x='bmi', y='charges', data=insurance, hue='sex' ,ax=axes[1])\naxes[1].set_title('BMI vs Charges')\nsns.boxplot(x='children', y='charges', data=insurance, ax=axes[2])\naxes[2].set_title('Children vs Charges')\nsns.boxplot(x='sex', y='charges', data=insurance, ax=axes[3])\naxes[3].set_title('Gender vs Charges')\nsns.boxplot(x='smoker', y='charges', data=insurance, ax=axes[4])\naxes[4].set_title('Smoking vs Charges')\n\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\n\n\n\n\n\n\n\n\n\n\nCheck for Outliers\n\nsns.boxplot(y='charges', data=insurance)\nplt.title('Boxplot of Charges')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()"
  },
  {
    "objectID": "codepages/medicalcost/index.html#data-loading",
    "href": "codepages/medicalcost/index.html#data-loading",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Data Loading",
    "text": "Data Loading\n\nimport pandas as pd\n\ninsurance = pd.read_csv('insurance.csv')\n\ninsurance.sample(5, random_state=111)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n1000\n30\nmale\n22.99\n2\nyes\nnorthwest\n17361.7661\n\n\n53\n36\nmale\n34.43\n0\nyes\nsoutheast\n37742.5757\n\n\n432\n42\nmale\n26.90\n0\nno\nsouthwest\n5969.7230\n\n\n162\n54\nmale\n39.60\n1\nno\nsouthwest\n10450.5520\n\n\n1020\n51\nmale\n37.00\n0\nno\nsouthwest\n8798.5930"
  },
  {
    "objectID": "codepages/medicalcost/index.html#data-pre-processing-for-modeling",
    "href": "codepages/medicalcost/index.html#data-pre-processing-for-modeling",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Data Pre-processing for Modeling",
    "text": "Data Pre-processing for Modeling\n\nCategorical Variables\n\n# Binary Encoding for the variables with two categories\ninsurance['sex'] = insurance['sex'].map({'male':1, 'female':0})\ninsurance['smoker'] = insurance['smoker'].map({'yes':1, 'no':0})\n\n# One-Hot Encoding for the multiclas variable: region\ninsurance = pd.get_dummies(\n    insurance, columns=['region'],\n    drop_first=True,\n    dtype=int\n    )\n\n\n\nContinuous Variable\n\n# Round the continuous charge variable to 2 decimal places\ninsurance['charges'] = insurance['charges'].round(2)\n\n# Mofe the predicting variable at the end of the dataframe\ninsurance_charges = insurance.pop('charges')\ninsurance.insert(loc = len(insurance.columns), column='charges', value=insurance_charges)\n\n# Quick look of the dataframe\ninsurance.head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion_northwest\nregion_southeast\nregion_southwest\ncharges\n\n\n\n\n0\n19\n0\n27.900\n0\n1\n0\n0\n1\n16884.92\n\n\n1\n18\n1\n33.770\n1\n0\n0\n1\n0\n1725.55\n\n\n2\n28\n1\n33.000\n3\n0\n0\n1\n0\n4449.46\n\n\n3\n33\n1\n22.705\n0\n0\n1\n0\n0\n21984.47\n\n\n4\n32\n1\n28.880\n0\n0\n1\n0\n0\n3866.86\n\n\n\n\n\n\n\n\n\nCheck for Multicollinearity\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = insurance.drop('charges', axis=1)\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values,i) for i in range(len(X.columns))]\nprint(vif_data)\n\n            feature        VIF\n0               age   7.686965\n1               sex   2.003185\n2               bmi  11.358443\n3          children   1.809930\n4            smoker   1.261233\n5  region_northwest   1.890281\n6  region_southeast   2.265564\n7  region_southwest   1.960745\n\n\nSince BMI and Age have higher values for the multicolinearity, therefore we adopt the following methods\n\n\nResidual Adjustment\n\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(insurance.age.values.reshape(-1,1), insurance.bmi.values)\ninsurance['bmi_adusted_for_age'] = insurance.bmi.values - reg.predict(insurance.age.values.reshape(-1,1))\n\n\n\nInteraction Term Adjustment\n\ninsurance['bmi_age_interact'] = insurance['bmi']*insurance['age']\n\n\n\nZ-Score Normalization\n\nbins = [18,30,40,50,60,70]\nlabels = ['18-30','31-40','41-50','51-60','61-70']\ninsurance['age_group'] = pd.cut(insurance['age'], bins= bins, labels=labels)\ninsurance['bmi_zscore'] = insurance.groupby('age_group', observed=False)['bmi'].transform(lambda x: (x-x.mean())/x.std())\ninsurance.sample(5, random_state=111)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion_northwest\nregion_southeast\nregion_southwest\ncharges\nbmi_adusted_for_age\nbmi_age_interact\nage_group\nbmi_zscore\n\n\n\n\n1000\n30\n1\n22.99\n2\n1\n1\n0\n0\n17361.77\n-7.236727\n689.70\n18-30\n-1.091779\n\n\n53\n36\n1\n34.43\n0\n1\n0\n1\n0\n37742.58\n3.918706\n1239.48\n31-40\n0.652655\n\n\n432\n42\n1\n26.90\n0\n0\n0\n0\n1\n5969.72\n-3.895862\n1129.80\n41-50\n-0.678956\n\n\n162\n54\n1\n39.60\n1\n0\n0\n0\n1\n10450.55\n8.235003\n2138.40\n51-60\n1.340308\n\n\n1020\n51\n1\n37.00\n0\n0\n0\n0\n1\n8798.59\n5.777287\n1887.00\n51-60\n0.912758\n\n\n\n\n\n\n\nTo check any missing values\n\ninsurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 13 columns):\n #   Column               Non-Null Count  Dtype   \n---  ------               --------------  -----   \n 0   age                  1338 non-null   int64   \n 1   sex                  1338 non-null   int64   \n 2   bmi                  1338 non-null   float64 \n 3   children             1338 non-null   int64   \n 4   smoker               1338 non-null   int64   \n 5   region_northwest     1338 non-null   int64   \n 6   region_southeast     1338 non-null   int64   \n 7   region_southwest     1338 non-null   int64   \n 8   charges              1338 non-null   float64 \n 9   bmi_adusted_for_age  1338 non-null   float64 \n 10  bmi_age_interact     1338 non-null   float64 \n 11  age_group            1269 non-null   category\n 12  bmi_zscore           1269 non-null   float64 \ndtypes: category(1), float64(5), int64(7)\nmemory usage: 127.1 KB\n\n\nTherefore, there are some missing data.\n\ninsurance = insurance.dropna()\ninsurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1269 entries, 0 to 1337\nData columns (total 13 columns):\n #   Column               Non-Null Count  Dtype   \n---  ------               --------------  -----   \n 0   age                  1269 non-null   int64   \n 1   sex                  1269 non-null   int64   \n 2   bmi                  1269 non-null   float64 \n 3   children             1269 non-null   int64   \n 4   smoker               1269 non-null   int64   \n 5   region_northwest     1269 non-null   int64   \n 6   region_southeast     1269 non-null   int64   \n 7   region_southwest     1269 non-null   int64   \n 8   charges              1269 non-null   float64 \n 9   bmi_adusted_for_age  1269 non-null   float64 \n 10  bmi_age_interact     1269 non-null   float64 \n 11  age_group            1269 non-null   category\n 12  bmi_zscore           1269 non-null   float64 \ndtypes: category(1), float64(5), int64(7)\nmemory usage: 130.3 KB"
  },
  {
    "objectID": "codepages/medicalcost/index.html#modelling-approaches",
    "href": "codepages/medicalcost/index.html#modelling-approaches",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Modelling Approaches",
    "text": "Modelling Approaches\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\ninsurance_train, insurance_test = train_test_split(\n    insurance, test_size=0.2,\n    random_state=111\n)\n\n\nkfold = KFold(9, shuffle=True, random_state=111)\nmses = np.zeros((9,9))\nk = 12\nfor i, (train_index, test_index) in enumerate(kfold.split(insurance_train)):\n    # Training Set\n    insurance_train_train = insurance_train.iloc[train_index]\n    # Holdout Set \n    insurance_train_test = insurance_train.iloc[test_index]\n\n    prediction0 = insurance_train_train.charges.mean()*np.ones(len(test_index))\n\n    model1 = LinearRegression()\n    model2 = LinearRegression()\n    model3 = LinearRegression()\n    model4 = LinearRegression()\n    model5 = LinearRegression()\n    model6 = LinearRegression()\n    model7 = LinearRegression()\n    model8 = KNeighborsRegressor(k)\n\n    # Fit the models \n\n    model1.fit(\n        insurance_train_train.bmi_adusted_for_age.values.reshape(-1,1), insurance_train_train.charges.values\n    )\n    model2.fit(\n        insurance_train_train.bmi_age_interact.values.reshape(-1,1), insurance_train_train.charges.values\n    )\n    model3.fit(\n        insurance_train_train.bmi_zscore.values.reshape(-1,1), insurance_train_train.charges.values\n    )\n    model4.fit(\n        insurance_train_train[\n            ['age','bmi_adusted_for_age','sex','children','region_northwest','region_southeast','region_southwest']\n        ], insurance_train_train.charges.values\n    )\n    model5.fit(\n        insurance_train_train[\n            ['age','bmi','bmi_age_interact','sex','children','region_northwest','region_southeast','region_southwest']\n        ], insurance_train_train.charges.values\n    )\n    model6.fit(\n        insurance_train_train[\n            ['age','bmi_zscore','sex','children','region_northwest','region_southeast','region_southwest']\n        ], insurance_train_train.charges.values\n    )\n    model7.fit(\n        insurance_train_train[\n            ['bmi_adusted_for_age','bmi_age_interact','bmi_zscore','sex','children','region_northwest','region_southeast','region_southwest']\n        ], insurance_train_train.charges.values\n    )\n    model8.fit(\n        insurance_train_train[\n            ['age','bmi','sex','children','region_northwest','region_southeast','region_southwest']\n        ], insurance_train_train.charges.values\n    )\n    prediction1 = model1.predict(\n        insurance_train_test.bmi_adusted_for_age.values.reshape(-1,1)\n    )\n    prediction2 = model2.predict(\n        insurance_train_test.bmi_age_interact.values.reshape(-1,1)\n    )\n    prediction3 = model3.predict(\n        insurance_train_test.bmi_zscore.values.reshape(-1,1)\n    )\n    prediction4 = model4.predict(\n        insurance_train_test[['age','bmi_adusted_for_age','sex','children','region_northwest','region_southeast','region_southwest']]\n    )\n    prediction5 = model5.predict(\n        insurance_train_test[\n            ['age','bmi','bmi_age_interact','sex','children','region_northwest','region_southeast','region_southwest']\n        ]\n    )\n    prediction6 = model6.predict(\n        insurance_train_test[\n            ['age','bmi_zscore','sex','children','region_northwest','region_southeast','region_southwest']\n        ]\n    )\n    prediction7 = model7.predict(\n        insurance_train_test[\n            ['bmi_adusted_for_age','bmi_age_interact','bmi_zscore','sex','children','region_northwest','region_southeast','region_southwest']\n        ]\n    )\n    prediction8 = model8.predict(\n        insurance_train_test[\n            ['age','bmi','sex','children','region_northwest','region_southeast','region_southwest']\n        ]\n    )\n    \n    mses[0,i] = mean_squared_error(insurance_train_test.charges.values, prediction0)\n    mses[1,i] = mean_squared_error(insurance_train_test.charges.values, prediction1)\n    mses[2,i] = mean_squared_error(insurance_train_test.charges.values, prediction2)\n    mses[3,i] = mean_squared_error(insurance_train_test.charges.values, prediction3)\n    mses[4,i] = mean_squared_error(insurance_train_test.charges.values, prediction4)\n    mses[5,i] = mean_squared_error(insurance_train_test.charges.values, prediction5)\n    mses[6,i] = mean_squared_error(insurance_train_test.charges.values, prediction6)\n    mses[7,i] = mean_squared_error(insurance_train_test.charges.values, prediction7)\n    mses[8,i] = mean_squared_error(insurance_train_test.charges.values, prediction8)\n\n\nplt.figure(figsize=(9.5,5))\n\nplt.scatter(np.zeros(9), mses[0,:], s=60, c='white', edgecolors='black', label= 'Single Split')\nplt.scatter(np.ones(9), mses[1,:], s=60, c='white', edgecolors='black')\nplt.scatter(2*np.ones(9), mses[2,:], s=60, c='white', edgecolors='black')\nplt.scatter(3*np.ones(9), mses[3,:], s=60, c='white', edgecolors='black')\nplt.scatter(4*np.ones(9), mses[4,:], s=60, c='white', edgecolors='black')\nplt.scatter(5*np.ones(9), mses[5,:], s=60, c='white', edgecolors='black')\nplt.scatter(6*np.ones(9), mses[6,:], s=60, c='white', edgecolors='black')\nplt.scatter(7*np.ones(9), mses[7,:], s=60, c='white', edgecolors='black')\nplt.scatter(8*np.ones(9), mses[8,:], s=60, c='white', edgecolors='black')\nplt.scatter([0,1,2,3,4,5,6,7,8], np.mean(mses, axis=1), s=60, c='r', marker='X', label='Mean')\nplt.legend(loc='upper right',fontsize=12)\nplt.xticks([0,1,2,3,4,5,6,7,8],['Baseline','Model 1','Model 2','Model 3','Model 4','Model 5','Model 6','Model 7', ' Model 8'])\nplt.yticks(fontsize=10)\nplt.ylabel('MSE', fontsize=12)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(np.mean(np.sqrt(mses), axis=1))\nprint('\\n')\n# Minimum and the position\nprint('Minimum RMSE={} \\n Model {}'.format(min(np.mean(np.sqrt(mses), axis=1)),np.argmin(np.mean(np.sqrt(mses), axis=1))))\n\n[12162.86176484 11912.55503323 11529.9323124  11938.2448626\n 11436.50914175 11436.2984215  11444.6399563  11451.88013765\n 11852.49612033]\n\n\nMinimum RMSE=11436.298421497275 \n Model 5"
  },
  {
    "objectID": "codepages/medicalcost/index.html#final-model",
    "href": "codepages/medicalcost/index.html#final-model",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Final Model",
    "text": "Final Model\n\nmodel = LinearRegression()\nmodel.fit(insurance_train[['age','bmi']],insurance_train.charges.values)\nprint(\"RMSE on the training set: \",\n    np.round(np.sqrt(mean_squared_error(insurance_train.charges.values, model.predict(insurance_train[['age','bmi']]))),2)\n)\n\nprint(\"RMSE on the test set: \",\n    np.round(np.sqrt(mean_squared_error(insurance_test.charges.values, model.predict(insurance_test[['age','bmi']]))),2)\n)\n\nRMSE on the training set:  11444.53\nRMSE on the test set:  11392.1"
  },
  {
    "objectID": "posts/dsa/index.html#arrays-and-lists",
    "href": "posts/dsa/index.html#arrays-and-lists",
    "title": "Data Structure and Algorithms: Basic Programming Hacks",
    "section": "Arrays and Lists",
    "text": "Arrays and Lists\n\n1. Intersection of two arrays\nSay you have two arrays. Write a function to get the intersection of the two. For example, if \\(A=[2,3,5,6,8]\\) and \\(B=[4,6,8]\\), then the function should return \\([6,8]\\)\nBrute Force\n\n\nOne way to solve this problem is using brute force solution, that is using two nested loops. But this method takes the time complexity of \\(O(n\\times m)\\) given that the lenght of set A is \\(n\\) and set B is \\(m\\). And here is how it is:\n\n\n@time_required\ndef intersection_of_two_sets(A,B):\n    set_A = set(A)\n    set_B = set(B)\n    intersection = []\n    for a in set_A:\n        for b in set_B:\n            if a==b:\n                intersection.append(a)\n    return intersection\nA = [2,3,5,6,8]\nB = [4,6,8]\nprint(intersection_of_two_sets(A,B))\n\nTime required: 0.000003 seconds\n[6, 8]\n\n\nHash Map Approach: In hash map approach, we can solve the same problem but in this case the time and space complexity is \\(O(n+m)\\)\n\n@time_required\ndef intersection_of_two_sets(A,B):\n    set_A = set(A)\n    set_B = set(B)\n    if len(set_A) &lt; len(set_B):\n        return [a for a in set_A if a in set_B]\n    return [b for b in set_B if b in set_A]\n\nA = [2,3,5,6,8]\nB = [4,6,8]\nprint(intersection_of_two_sets(A,B))\n\nTime required: 0.000003 seconds\n[8, 6]\n\n\nThe reason we’re getting \\([8,6]\\) instead of \\([6,8]\\) is because sets in Python are unordered collections, meaning that when you convert the lists \\(A\\) and \\(B\\) to sets, the order of elements is not preserved. So, when we iterate over set_A or set_A, the order can change.\nBetter Approach: If we want to maintain the order of the elements in the original list \\(A\\) or \\(B\\), we can iterate over the original list directly rather than converting it to a set. Here’s how:\n\n@time_required\ndef intersection_of_two_sets(A, B):\n    set_B = set(B)  \n    return [a for a in A if a in set_B]\n\nA = [2, 3, 5, 6, 8]\nB = [4, 6, 8]\nprint(intersection_of_two_sets(A, B))\n\nTime required: 0.000002 seconds\n[6, 8]\n\n\n\n\n2. Approximation of \\(\\pi\\) using Monte-Carlo Method\nA circle with radius \\(r=1\\) has the area \\(A=\\pi r^2= \\pi\\) and a square with length \\(l=1\\) has the area \\(B=1\\). Now if we consider the following situation, where a quarter of a unit circle is inscribed inside a unit square like this\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np \n\nnum_points = 100\npts = np.random.rand(num_points,2)\n\nfig, axes = plt.subplots()\ntheta = np.linspace(0, np.pi/2,100)\nx = np.cos(theta)\ny = np.sin(theta)\n\naxes.plot(x, y, 'b')\naxes.plot([0,1],[0,0],'k')\naxes.plot([1,1],[0,1],'k')\naxes.plot([1,0],[1,1],'k')\naxes.plot([0,0],[1,0],'k')\n\nfor p in pts:\n    if p[0]**2+p[1]**2 &lt;=1:\n        axes.plot(p[0], p[1], 'go')\n    else:\n        axes.plot(p[0], p[1], 'ro')\naxes.set_aspect('equal')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nwe get\n\\[\n\\frac{\\text{Area of the quarter of a unit circle: C}}{\\text{Area of a unit square: S}}= \\frac{\\frac{\\pi}{4}}{1}=\\frac{\\pi}{4}\\hspace{4mm}\\implies \\pi = \\frac{4C}{S}\n\\]\n\nThe above relation tells us some interesting fact. If we uniformly create as many points as possible inside the square then the number of points inside the circle will be approximately 4 times the number of the points outside the circular region.\n\n\n@time_required \ndef monte_carlo_pi(n):\n    inside_circle = 0 \n    for i in range(n):\n        x = np.random.rand()\n        y = np.random.rand()\n        if x**2+y**2 &lt;=1:\n            inside_circle = inside_circle+1\n    pi = (inside_circle/n)*4\n    return pi \nn=1000\n\nprint(f\"Monte carlo estimated value of π from {n} points = {monte_carlo_pi(n)}\")\n\nTime required: 0.000450 seconds\nMonte carlo estimated value of π from 1000 points = 3.2\n\n\n\n\n3. Max product of \\(k\\) elements from an array of \\(n\\) elements\n\nSay we have an array of size \\(n\\). We want to find the maximum of the products of \\(k\\) elements from the array where \\(k &lt; n\\). For example, if we have \\(A=[1,2,3,4,5,6]\\) then the answer is 120, if we have \\(B=[-3,-4,3,5]\\) then the answer is 60."
  },
  {
    "objectID": "posts/dsa/index.html#linked-list",
    "href": "posts/dsa/index.html#linked-list",
    "title": "Data Structure and Algorithms: Basic Programming Hacks",
    "section": "Linked List",
    "text": "Linked List\n\n\nCode\nclass Node:\n    def __init__(self, value, next=None) -&gt; None:\n        self.value = value\n        self.next = next\n\ndef linklist(arr):\n    if not arr:\n        return None \n    head = Node(arr[0])\n    current = head \n    for value in arr[1:]:\n        current.next = Node(value)\n        current = current.next \n    return head \n\ndef print_linklist(head):\n    current = head\n    print(\"[\", end=\"\")\n    while current:\n        print(current.value, end=\", \" if current.next else \"]\")\n        current = current.next\n    print()\n\n\n\n1. Reverse a linked list: Type I\n\ndef reverse(head):\n    prev = None \n    curr = head \n    while curr:\n        next = curr.next \n        curr.next = prev \n        prev = curr \n        curr = next \n    return prev \n\nh = linklist([1,2,3,4,5])\nprint('Original List:')\nprint_linklist(h)\n\nh_reversed = reverse(h)\nprint('Reversed List')\nprint_linklist(h_reversed)\n\nOriginal List:\n[1, 2, 3, 4, 5]\nReversed List\n[5, 4, 3, 2, 1]\n\n\n\n\n2. Reverse a linked list: Type II\n\ndef reverse_in_between(head, left, right):\n    dummy = Node(0, head)\n    leftPrev = dummy\n    curr = head \n\n    for _ in range(left-1):\n        leftPrev = curr \n        curr = curr.next \n    \n    prev = None \n    tail = curr \n\n    for _ in range(right - left + 1):\n        next = curr.next \n        curr.next  = prev \n        prev = curr \n        curr = next \n    \n    leftPrev.next = prev \n    tail.next = curr \n\n    return dummy.next if left != 1 else prev\n\nh = linklist([1,2,3,4,5])\nprint('Original List:')\nprint_linklist(h)  \n\nh_reversed = reverse_in_between(h,2,4)\nprint('Reversed List between 2 and 4')\nprint_linklist(h_reversed)\n\nOriginal List:\n[1, 2, 3, 4, 5]\nReversed List between 2 and 4\n[1, 4, 3, 2, 5]"
  },
  {
    "objectID": "posts/dsa/index.html#arrays-lists-and-strings",
    "href": "posts/dsa/index.html#arrays-lists-and-strings",
    "title": "Data Structure and Algorithms: Basic Programming Hacks",
    "section": "Arrays, Lists, and Strings",
    "text": "Arrays, Lists, and Strings\n\n1. Intersection of two arrays\nSay you have two arrays. Write a function to get the intersection of the two. For example, if \\(A=[2,3,5,6,8]\\) and \\(B=[4,6,8]\\), then the function should return \\([6,8]\\)\nBrute Force\n\n\nOne way to solve this problem is using brute force solution, that is using two nested loops. But this method takes the time complexity of \\(O(n\\times m)\\) given that the lenght of set A is \\(n\\) and set B is \\(m\\). And here is how it is:\n\n\n@time_required\ndef intersection_of_two_sets(A,B):\n    set_A = set(A)\n    set_B = set(B)\n    intersection = []\n    for a in set_A:\n        for b in set_B:\n            if a==b:\n                intersection.append(a)\n    return intersection\nA = [2,3,5,6,8]\nB = [4,6,8]\nprint(intersection_of_two_sets(A,B))\n\nTime required: 0.000003 seconds\n[6, 8]\n\n\nHash Map Approach: In hash map approach, we can solve the same problem but in this case the time and space complexity is \\(O(n+m)\\)\n\n@time_required\ndef intersection_of_two_sets(A,B):\n    set_A = set(A)\n    set_B = set(B)\n    if len(set_A) &lt; len(set_B):\n        return [a for a in set_A if a in set_B]\n    return [b for b in set_B if b in set_A]\n\nA = [2,3,5,6,8]\nB = [4,6,8]\nprint(intersection_of_two_sets(A,B))\n\nTime required: 0.000002 seconds\n[8, 6]\n\n\nThe reason we’re getting \\([8,6]\\) instead of \\([6,8]\\) is because sets in Python are unordered collections, meaning that when you convert the lists \\(A\\) and \\(B\\) to sets, the order of elements is not preserved. So, when we iterate over set_A or set_A, the order can change.\nBetter Approach: If we want to maintain the order of the elements in the original list \\(A\\) or \\(B\\), we can iterate over the original list directly rather than converting it to a set. Here’s how:\n\n@time_required\ndef intersection_of_two_sets(A, B):\n    set_B = set(B)  \n    return [a for a in A if a in set_B]\n\nA = [2, 3, 5, 6, 8]\nB = [4, 6, 8]\nprint(intersection_of_two_sets(A, B))\n\nTime required: 0.000001 seconds\n[6, 8]\n\n\n\n\n2. Max product of \\(k\\) elements from an array of \\(n\\) elements\n\nSay we have an array of size \\(n\\). We want to find the maximum of the products of \\(k\\) elements from the array where \\(k &lt; n\\). For example, if we have \\(A=[1,2,3,4,5,6]\\) then the answer is 120, if we have \\(B=[-3,-4,3,5]\\) then the answer is 60."
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Review probabilities",
    "section": "",
    "text": "In today’s world, getting placement in data science world is highly challenging and competitive. It requires a lot of things including but not limited to basic knowledge in statistics, probability, machine learning, deep learning, and computer science. Even sometimes we face some basic problems from statistics and probability that we probably have solve long ago but forgot due to lack of practice or it’s taking longer due to rusty memory. Because, in master’s and Ph.D’s we focus on a very narrow topic and get our experties on those topics. So, it’s not a shame or humiliation if we can’t do a very simple problem even though we are capable of solving thousand time harder problems than that. It’s normal."
  },
  {
    "objectID": "posts/probability/index.html#introduction",
    "href": "posts/probability/index.html#introduction",
    "title": "Review probabilities",
    "section": "Introduction",
    "text": "Introduction\n\nIn today’s world, getting placement in data science world is highly challenging and competitive. It requires a lot of things including but not limited to basic knowledge in statistics, probability, machine learning, deep learning, and computer science. Even sometimes we face some basic problems from statistics and probability that we probably have solve long ago but forgot due to lack of practice or it’s taking longer due to rusty memory. Because, in master’s and Ph.D’s we focus on a very narrow topic and get our experties on those topics. So, it’s not a shame or humiliation if we can’t do a very simple problem even though we are capable of solving thousand time harder problems than that. It’s normal."
  },
  {
    "objectID": "posts/probability/index.html#conditional-probabilities-bayess-theorem",
    "href": "posts/probability/index.html#conditional-probabilities-bayess-theorem",
    "title": "Review probabilities",
    "section": "Conditional Probabilities: Bayes’s Theorem",
    "text": "Conditional Probabilities: Bayes’s Theorem\n\nAssume two coins, one fair (i.e. equal chance of getting head and tail if tossed) and the other one is unfair and always gets head if tossed. If a coin is chosen at random and tossed six times and you get heads in all six tosses, what is the probability that you are tossing the unfair one?\nSolution:\nLet,\n\n\\(F\\) be the event the coin is fair, \\(F'\\) being the event of unfair coin and\n\n\\(H\\) be the event showing up head.\n\nWe neeed to find \\(\\mathbb{P}(F'|6H)\\), the probability that we are tossing the unfair \\(F'\\) coins given that we got 6 heads.\n\\[\\begin{align}\n     \\mathbb{P}(F'|6H)&=\\frac{\\mathbb{P}(6H|F')\\mathbb{P}(F')}{\\mathbb{P}(6H)}\n\\end{align}\\]\nHere,\n\n\\(\\mathbb{P}(F)=\\mathbb{P}(F')=\\frac{1}{2}\\), the probability of chosing a fair or unfair coin\n\\(\\mathbb{P}(6H|F)=\\left(\\frac{1}{2}\\right)^6\\), by the principle that flipping a fair coin 6 times are indpendent events, and thus the probability got multiplied\n\\(\\mathbb{P}(6H|F')=1\\), sure event, since unfair coin.\n\nSo, the total probability, \\[\\mathbb{P}(6H)=\\mathbb{P}(6H|F)\\mathbb{P}(F)+\\mathbb{P}(6H|F')\\mathbb{P}(F')=\\left(\\frac{1}{2}\\right)^6\\frac{1}{2}+1\\cdot \\frac{1}{2}\\]\nTherefore,\n\\[\\mathbb{P}(F'|6H)=\\frac{\\mathbb{P}(6H|F')\\mathbb{P}(F')}{\\mathbb{P}(6H)}=\\frac{1\\cdot\\frac{1}{2}}{\\left(\\frac{1}{2}\\right)^6\\frac{1}{2}+1\\cdot \\frac{1}{2}}\\]\nOne in thousand people\n\nShare on\n\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/montecarlo1/index.html",
    "href": "posts/montecarlo1/index.html",
    "title": "Monte-Carlo Methods: PRNGs",
    "section": "",
    "text": "The Monte Carlo method is a widely used statistical technique that leverages the power of randomness to solve complex mathematical problems and simulate the behavior of various systems. It’s a method that has found applications across diverse fields, including physics, finance, engineering, and biology. In this blog post, we’ll dive deeper into the Monte Carlo method and explore the mathematics behind it, along with a discussion of random number generators like Linear Congruential Generators (LCGs) and the infamous RANDU.\n\n\n\n\nThe Monte Carlo method is based on the idea of using randomness to approximate solutions to problems that may be deterministic in nature but are too complex for analytical methods. The name “Monte Carlo” is a nod to the randomness associated with the famous casino in Monaco.  The basic principle behind the Monte Carlo method is to simulate the behavior of a system by generating random samples and using them to estimate the desired quantities. Let’s consider a mathematical problem where we need to compute an integral that does not have a straightforward analytical solution:\n\n\\[\\begin{align*}\nI &= \\int_{a}^{b} f(x) \\, dx\n\\end{align*}\\]\n\nThe Monte Carlo method approximates this integral by sampling random points \\(x_i\\) uniformly from the interval \\([a, b]\\) and evaluating the function \\(f(x)\\) at these points. The integral can then be approximated as:\n\n\\[\\begin{align*}\nI \\approx \\frac{b - a}{N} \\sum_{i=1}^{N} f(x_i)\n\\end{align*}\\]\n\nwhere \\(N\\) is the number of random samples. As \\(N\\) increases, the approximation becomes more accurate, thanks to the Law of Large Numbers.  This approach is particularly useful for high-dimensional integrals, where traditional numerical integration methods become computationally expensive or infeasible.\n\n\n\n\n\nAt the heart of the Monte Carlo method lies the generation of random numbers. In practice, most simulations do not use true random numbers but rather pseudorandom numbers generated by deterministic algorithms. These pseudorandom number generators (PRNGs) produce sequences that mimic the properties of true randomness.\n\n\n\nOne of the most commonly used PRNGs is the Linear Congruential Generator (LCG). The LCG generates a sequence of numbers \\(X_1, X_2, X_3, \\ldots\\) using the recursive relation:\n\\[\\begin{align*}\nX_{n+1} &= (aX_n + c) \\mod m\n\\end{align*}\\]\nwhere:\n\n\\(X_n\\) is the \\(n\\)-th number in the sequence.\n\\(a\\) is the multiplier.\n\\(c\\) is the increment.\n\\(m\\) is the modulus.\n\n\nThe sequence starts with an initial value \\(X_0\\), known as the seed, and the parameters \\(a\\), \\(c\\), and \\(m\\) are carefully chosen to maximize the period and quality of the generated sequence.  The quality of the LCG depends on the choice of these parameters. For instance, to achieve a full period (i.e., the sequence cycles through all possible values before repeating), the following conditions must be met:\n\n\n\\(c\\) and \\(m\\) must be relatively prime.\n\\(a - 1\\) must be divisible by all prime factors of \\(m\\).\nIf \\(m\\) is divisible by 4, then \\(a - 1\\) must also be divisible by 4.\n\nA well-known example of an LCG is the minstd_rand generator used in the C++ Standard Library, which uses \\(a = 16807\\), \\(c = 0\\), and \\(m = 2^{31} - 1\\).\n\n\n\nRANDU is an example of a poorly designed LCG that became notorious for its flaws. It is defined by the recurrence relation:\n\\[\\begin{align*}\nX_{n+1} &= (65539X_n) \\mod 2^{31}\n\\end{align*}\\]\n\nAlthough RANDU was widely used in the 1960s and 1970s due to its simplicity, it was later discovered to produce sequences with significant correlations. For example, points generated using RANDU tend to lie on a small number of planes in three-dimensional space, which can severely impact the accuracy of Monte Carlo simulations.  The generator’s flaws arise from poor parameter selection. In RANDU, the modulus \\(m = 2^{31}\\) and the multiplier \\(a = 65539\\) result in a sequence with poor distribution properties. As a consequence, RANDU’s generated numbers do not pass modern statistical tests for randomness, rendering it unsuitable for serious applications.  Let’s solve some math problems and visualize randomness.\n\n\n\n\nGiven an LCG with parameters \\(a,c,m\\), prove that\n\nwhich shows that the \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\nWe know from D. H. Lehmer’s linear congruential generator that\n\\[\\begin{equation}\nx_n \\equiv ax_{n-1}+c \\mod m\n\\end{equation}\\]\nwhere \\(a\\) is called the multiplier, \\(c\\) is called the shift or increment, and \\(m\\) is called the modulus of the generator. The given equation is also an LCG. We can prove this by induction method. Since \\(k\\ge 0\\) so, let \\(k=0\\). Then the given relation can be written as\n\nIf \\(k=1\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+1}& \\equiv ax_n+\\frac{a-1}{a-1}c \\mod m\\\\\n&\\equiv ax_n+c \\mod m\n\\end{align*}\\]\nIf \\(k=2\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+2}& \\equiv a^2x_n+\\frac{a^2-1}{a-1}c \\mod m\\\\\n&\\equiv a^2x_n+(a+1)c \\mod m\\\\\n&\\equiv a^2x_n+ac+c \\mod m \\\\\n&\\equiv a(ax_n+c)+c \\mod m\\\\\n&\\equiv ax_{n+1}+c \\mod m\n\\end{align*}\\]\nNow for any \\(k=p\\) where \\(p\\in \\mathbb{N}\\), \\[\\begin{align*}\nx_{n+p}& \\equiv a^px_n+\\frac{a^p-1}{a-1}c \\mod m \\\\\n\\end{align*}\\]\nNow by the method of induction, the given equation would be a lcg if it holds for any \\(k=p\\in \\mathbb{N}\\) then it must hold for \\(k=p+1\\) where \\(p\\in \\mathbb{N}\\). Now from equation (1) \\[\\begin{align*}\nx_{n+p+1} &\\equiv ax_{(n+p+1)-1}+c \\mod m\\\\\n& \\equiv ax_{n+p}+c \\mod m \\\\\n& \\equiv a(a^px_n+\\frac{a^p-1}{a-1}c) +c \\mod m\\\\\n& \\equiv a^{p+1}x_n+(a\\frac{a^p-1}{a-1}+1)c \\mod m\\\\\n& \\equiv a^{p+1}x_n+\\frac{a^{p+1}-1}{a-1}c \\mod m\\\\\n\\end{align*}\\]\nWhich proves that \\(x_{n+k}=a^kx_n+\\frac{(a^k-1)}{a-1}c (\\mod m)\\); \\((a\\ge 2, k\\ge0)\\) is an lcg such that \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\n(a)\nIf \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\) show that \\(U+V (\\mod 1)\\) is also \\(U(0,1)\\).\nSolution\nLet \\(Z=U+V\\) where \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\). So the minimum value that \\(Z\\) can have is \\(0\\) and the maximum value could be \\(2\\). If \\(f_U(u)\\) is the PDF of \\(U\\) and \\(f_V(v)\\) is the PDF of \\(V\\) then the PDF of \\(Z\\) can be found from the convolution of two distribution as follows \\[\\begin{align*}\n  f_Z(z)=\\int_{-\\infty}^{+\\infty}f_U(u)f_V(z-u)du=\\begin{cases}\n          z & \\text{for} \\hspace{2mm} 0 &lt; z &lt; 1\\\\\n          2-z & \\text{for} \\hspace{2mm} 1 \\le z &lt;2\\\\\n          0 & \\text{otherwise}\n         \\end{cases}\n\\end{align*}\\] Now for any \\(x\\in (0,1)\\) \\[\\begin{align*}\n  \\mathbb{P}(U+V (\\mod 1) \\le x) &= \\mathbb{P}(Z \\le x)+ \\mathbb{P}(1\\le Z \\le x+1)\\\\\n                                 &= \\int_{0}^{x} z dz +\\int_{1}^{1+x}(2-z)dz\\\\\n                                 &=x\n\\end{align*}\\]\nwhich is the CDF of a random variable distributed \\(U(0,1)\\)\n(b)\nA random number generator is designed by\n\nwhere \\(X_0=0, Y_0=1, X_{n+1}=(9X_n+3) \\mod 8\\) and \\(Y_{n+1}=3Y_n \\mod 7\\) for \\(n=0,1,2,\\cdots\\). Calculate \\(R_0,R_1,R_2, \\cdots , R_5.\\). What is the period of the generator \\(\\{R_n\\}\\)?\nSolution\n\nrand.gen&lt;-function(n){\n  RN&lt;-vector(length = n)\n  x&lt;-rep(n)\n  y&lt;-rep(n)\n  x[1]&lt;-0;\n  y[1]&lt;-1;\n  RN[1]&lt;-(x[1]/8+y[1]/7)%% 1\n  for (i in 1:n) {\n    x[i+1]&lt;-(9*x[i]+3)%% 8\n    y[i+1]&lt;-(3*y[i]) %% 7\n    RN[i+1]&lt;-(x[i+1]/8+y[i+1]/7)%% 1\n  }\n  return(data.frame(X_values=x,Y_values=y,R_values=RN))\n}\nrand.gen(4)  \n\n  X_values Y_values   R_values\n1        0        1 0.14285714\n2        3        3 0.80357143\n3        6        2 0.03571429\n4        1        6 0.98214286\n5        4        4 0.07142857\n\n\nSo the unique values are\n\n\n     R_values\n1  0.14285714\n2  0.80357143\n3  0.03571429\n4  0.98214286\n5  0.07142857\n6  0.58928571\n7  0.39285714\n8  0.05357143\n9  0.28571429\n10 0.23214286\n11 0.32142857\n12 0.83928571\n13 0.64285714\n14 0.30357143\n15 0.53571429\n16 0.48214286\n17 0.57142857\n18 0.08928571\n19 0.89285714\n20 0.55357143\n21 0.78571429\n22 0.73214286\n23 0.82142857\n24 0.33928571\n\n\nSo from the above data we can see that the period is \\(24\\).\n\n\n\nWrite a code that would implement RANDU. For debugging purpose print \\(x_{1000}\\) when the seed is \\(x_0=1\\)\n(a)\nUsing RANDU generate \\(u_1,u_2,\\cdots, u_{20,002}\\) where \\(u=\\frac{x_n}{M}\\). For all triplets in your sequence, \\(u_i, u_{i+1}, u_{i+2}\\), in which \\(0.5\\le u_{i+1} \\le 0.51\\) plot \\(u_i\\) versus \\(u_{i+2}\\). Comment on the pattern of your scatterplot.\n\n\n\n\n\n\n\n\n\n(b)\nGenerate a sequence of lenght 1002. Use a program that plots points in 3 dimensions and rotates the axes to rotate the points until you can see the 15 planes.\n\n\nCode\nlibrary(\"rgl\")\nlibrary(\"rglwidget\")\n\nN = 1002\nA = matrix(0, ncol=3, nrow=N)\nseed &lt;- as.double(1)\n\nRANDU &lt;- function() {\n  seed &lt;&lt;- ((2^16 + 3) * seed) %% (2^31)\n  round(seed/(2^31), 6)\n}\n\nfor (i in 1:N) {\n  A[i, ] &lt;- c(RANDU(), RANDU(), RANDU())\n}\nB = as.data.frame(A)\n\nbg3d(color = \"#f4f4f4\")\nplot3d(B$V1, B$V2, B$V3, type=\"s\", size=1, lit=TRUE, col = rainbow(1000))\nspin &lt;- spin3d(axis= c(0, 0, 1), rpm = 5)\nplay3d(spin, duration = 10)\n\n# Render the 3D plot as a WebGL widget\nrglwidget()\n\n\n\n\n\n\nCode\nrgl.close()\n\n\n\n\n\nA circle with radius \\(r=1\\) has the area \\(A=\\pi r^2= \\pi\\) and a square with length \\(l=1\\) has the area \\(B=1\\). Now if we consider the following situation, where a quarter of a unit circle is inscribed inside a unit square like this\n# I used python to generate this plot. Else everywhere the codes are in R\nimport matplotlib.pyplot as plt\nimport numpy as np \n\nnum_points = 100\npts = np.random.rand(num_points,2)\n\nfig, axes = plt.subplots()\ntheta = np.linspace(0, np.pi/2,100)\nx = np.cos(theta)\ny = np.sin(theta)\n\naxes.plot(x, y, 'b')\naxes.plot([0,1],[0,0],'k')\naxes.plot([1,1],[0,1],'k')\naxes.plot([1,0],[1,1],'k')\naxes.plot([0,0],[1,0],'k')\n\nfor p in pts:\n    if p[0]**2+p[1]**2 &lt;=1:\n        axes.plot(p[0], p[1], 'go')\n    else:\n        axes.plot(p[0], p[1], 'ro')\naxes.set_aspect('equal')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\nwe get,\n\\[\\begin{align*}\n\\frac{\\text{Area of the quarter of a unit circle: C}}{\\text{Area of a unit square: S}}&= \\frac{\\frac{\\pi}{4}}{1}=\\frac{\\pi}{4}\\hspace{4mm}\\implies \\pi = \\frac{4C}{S}\n\\end{align*}\\]\n\nThe above relation tells us some interesting fact. If we uniformly create as many points as possible inside the square then the number of points inside the circle will be approximately 4 times the number of the points outside the circular region.\n\n\nmonte_carlo_pi &lt;- function(n) {\n  inside_circle &lt;- 0\n  for (i in 1:n) {\n    x &lt;- runif(1)\n    y &lt;- runif(1)\n    if (x^2 + y^2 &lt;= 1) {\n      inside_circle &lt;- inside_circle + 1\n    }\n  }\n  pi_estimate &lt;- (inside_circle / n) * 4\n  return(pi_estimate)\n}\n\nn &lt;- 10000\ncat(sprintf(\"Monte Carlo estimated value of π from %d points = %f\\n\", n, monte_carlo_pi(n)))\n\nMonte Carlo estimated value of π from 10000 points = 3.152400\n\n\nThat’s all for this post."
  },
  {
    "objectID": "posts/montecarlo1/index.html#introduction",
    "href": "posts/montecarlo1/index.html#introduction",
    "title": "Monte-Carlo Methods: PRNGs",
    "section": "",
    "text": "The Monte Carlo method is a widely used statistical technique that leverages the power of randomness to solve complex mathematical problems and simulate the behavior of various systems. It’s a method that has found applications across diverse fields, including physics, finance, engineering, and biology. In this blog post, we’ll dive deeper into the Monte Carlo method and explore the mathematics behind it, along with a discussion of random number generators like Linear Congruential Generators (LCGs) and the infamous RANDU.\n\n\n\n\nThe Monte Carlo method is based on the idea of using randomness to approximate solutions to problems that may be deterministic in nature but are too complex for analytical methods. The name “Monte Carlo” is a nod to the randomness associated with the famous casino in Monaco.  The basic principle behind the Monte Carlo method is to simulate the behavior of a system by generating random samples and using them to estimate the desired quantities. Let’s consider a mathematical problem where we need to compute an integral that does not have a straightforward analytical solution:\n\n\\[\\begin{align*}\nI &= \\int_{a}^{b} f(x) \\, dx\n\\end{align*}\\]\n\nThe Monte Carlo method approximates this integral by sampling random points \\(x_i\\) uniformly from the interval \\([a, b]\\) and evaluating the function \\(f(x)\\) at these points. The integral can then be approximated as:\n\n\\[\\begin{align*}\nI \\approx \\frac{b - a}{N} \\sum_{i=1}^{N} f(x_i)\n\\end{align*}\\]\n\nwhere \\(N\\) is the number of random samples. As \\(N\\) increases, the approximation becomes more accurate, thanks to the Law of Large Numbers.  This approach is particularly useful for high-dimensional integrals, where traditional numerical integration methods become computationally expensive or infeasible.\n\n\n\n\n\nAt the heart of the Monte Carlo method lies the generation of random numbers. In practice, most simulations do not use true random numbers but rather pseudorandom numbers generated by deterministic algorithms. These pseudorandom number generators (PRNGs) produce sequences that mimic the properties of true randomness.\n\n\n\nOne of the most commonly used PRNGs is the Linear Congruential Generator (LCG). The LCG generates a sequence of numbers \\(X_1, X_2, X_3, \\ldots\\) using the recursive relation:\n\\[\\begin{align*}\nX_{n+1} &= (aX_n + c) \\mod m\n\\end{align*}\\]\nwhere:\n\n\\(X_n\\) is the \\(n\\)-th number in the sequence.\n\\(a\\) is the multiplier.\n\\(c\\) is the increment.\n\\(m\\) is the modulus.\n\n\nThe sequence starts with an initial value \\(X_0\\), known as the seed, and the parameters \\(a\\), \\(c\\), and \\(m\\) are carefully chosen to maximize the period and quality of the generated sequence.  The quality of the LCG depends on the choice of these parameters. For instance, to achieve a full period (i.e., the sequence cycles through all possible values before repeating), the following conditions must be met:\n\n\n\\(c\\) and \\(m\\) must be relatively prime.\n\\(a - 1\\) must be divisible by all prime factors of \\(m\\).\nIf \\(m\\) is divisible by 4, then \\(a - 1\\) must also be divisible by 4.\n\nA well-known example of an LCG is the minstd_rand generator used in the C++ Standard Library, which uses \\(a = 16807\\), \\(c = 0\\), and \\(m = 2^{31} - 1\\).\n\n\n\nRANDU is an example of a poorly designed LCG that became notorious for its flaws. It is defined by the recurrence relation:\n\\[\\begin{align*}\nX_{n+1} &= (65539X_n) \\mod 2^{31}\n\\end{align*}\\]\n\nAlthough RANDU was widely used in the 1960s and 1970s due to its simplicity, it was later discovered to produce sequences with significant correlations. For example, points generated using RANDU tend to lie on a small number of planes in three-dimensional space, which can severely impact the accuracy of Monte Carlo simulations.  The generator’s flaws arise from poor parameter selection. In RANDU, the modulus \\(m = 2^{31}\\) and the multiplier \\(a = 65539\\) result in a sequence with poor distribution properties. As a consequence, RANDU’s generated numbers do not pass modern statistical tests for randomness, rendering it unsuitable for serious applications.  Let’s solve some math problems and visualize randomness.\n\n\n\n\nGiven an LCG with parameters \\(a,c,m\\), prove that\n\nwhich shows that the \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\nWe know from D. H. Lehmer’s linear congruential generator that\n\\[\\begin{equation}\nx_n \\equiv ax_{n-1}+c \\mod m\n\\end{equation}\\]\nwhere \\(a\\) is called the multiplier, \\(c\\) is called the shift or increment, and \\(m\\) is called the modulus of the generator. The given equation is also an LCG. We can prove this by induction method. Since \\(k\\ge 0\\) so, let \\(k=0\\). Then the given relation can be written as\n\nIf \\(k=1\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+1}& \\equiv ax_n+\\frac{a-1}{a-1}c \\mod m\\\\\n&\\equiv ax_n+c \\mod m\n\\end{align*}\\]\nIf \\(k=2\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+2}& \\equiv a^2x_n+\\frac{a^2-1}{a-1}c \\mod m\\\\\n&\\equiv a^2x_n+(a+1)c \\mod m\\\\\n&\\equiv a^2x_n+ac+c \\mod m \\\\\n&\\equiv a(ax_n+c)+c \\mod m\\\\\n&\\equiv ax_{n+1}+c \\mod m\n\\end{align*}\\]\nNow for any \\(k=p\\) where \\(p\\in \\mathbb{N}\\), \\[\\begin{align*}\nx_{n+p}& \\equiv a^px_n+\\frac{a^p-1}{a-1}c \\mod m \\\\\n\\end{align*}\\]\nNow by the method of induction, the given equation would be a lcg if it holds for any \\(k=p\\in \\mathbb{N}\\) then it must hold for \\(k=p+1\\) where \\(p\\in \\mathbb{N}\\). Now from equation (1) \\[\\begin{align*}\nx_{n+p+1} &\\equiv ax_{(n+p+1)-1}+c \\mod m\\\\\n& \\equiv ax_{n+p}+c \\mod m \\\\\n& \\equiv a(a^px_n+\\frac{a^p-1}{a-1}c) +c \\mod m\\\\\n& \\equiv a^{p+1}x_n+(a\\frac{a^p-1}{a-1}+1)c \\mod m\\\\\n& \\equiv a^{p+1}x_n+\\frac{a^{p+1}-1}{a-1}c \\mod m\\\\\n\\end{align*}\\]\nWhich proves that \\(x_{n+k}=a^kx_n+\\frac{(a^k-1)}{a-1}c (\\mod m)\\); \\((a\\ge 2, k\\ge0)\\) is an lcg such that \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\n(a)\nIf \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\) show that \\(U+V (\\mod 1)\\) is also \\(U(0,1)\\).\nSolution\nLet \\(Z=U+V\\) where \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\). So the minimum value that \\(Z\\) can have is \\(0\\) and the maximum value could be \\(2\\). If \\(f_U(u)\\) is the PDF of \\(U\\) and \\(f_V(v)\\) is the PDF of \\(V\\) then the PDF of \\(Z\\) can be found from the convolution of two distribution as follows \\[\\begin{align*}\n  f_Z(z)=\\int_{-\\infty}^{+\\infty}f_U(u)f_V(z-u)du=\\begin{cases}\n          z & \\text{for} \\hspace{2mm} 0 &lt; z &lt; 1\\\\\n          2-z & \\text{for} \\hspace{2mm} 1 \\le z &lt;2\\\\\n          0 & \\text{otherwise}\n         \\end{cases}\n\\end{align*}\\] Now for any \\(x\\in (0,1)\\) \\[\\begin{align*}\n  \\mathbb{P}(U+V (\\mod 1) \\le x) &= \\mathbb{P}(Z \\le x)+ \\mathbb{P}(1\\le Z \\le x+1)\\\\\n                                 &= \\int_{0}^{x} z dz +\\int_{1}^{1+x}(2-z)dz\\\\\n                                 &=x\n\\end{align*}\\]\nwhich is the CDF of a random variable distributed \\(U(0,1)\\)\n(b)\nA random number generator is designed by\n\nwhere \\(X_0=0, Y_0=1, X_{n+1}=(9X_n+3) \\mod 8\\) and \\(Y_{n+1}=3Y_n \\mod 7\\) for \\(n=0,1,2,\\cdots\\). Calculate \\(R_0,R_1,R_2, \\cdots , R_5.\\). What is the period of the generator \\(\\{R_n\\}\\)?\nSolution\n\nrand.gen&lt;-function(n){\n  RN&lt;-vector(length = n)\n  x&lt;-rep(n)\n  y&lt;-rep(n)\n  x[1]&lt;-0;\n  y[1]&lt;-1;\n  RN[1]&lt;-(x[1]/8+y[1]/7)%% 1\n  for (i in 1:n) {\n    x[i+1]&lt;-(9*x[i]+3)%% 8\n    y[i+1]&lt;-(3*y[i]) %% 7\n    RN[i+1]&lt;-(x[i+1]/8+y[i+1]/7)%% 1\n  }\n  return(data.frame(X_values=x,Y_values=y,R_values=RN))\n}\nrand.gen(4)  \n\n  X_values Y_values   R_values\n1        0        1 0.14285714\n2        3        3 0.80357143\n3        6        2 0.03571429\n4        1        6 0.98214286\n5        4        4 0.07142857\n\n\nSo the unique values are\n\n\n     R_values\n1  0.14285714\n2  0.80357143\n3  0.03571429\n4  0.98214286\n5  0.07142857\n6  0.58928571\n7  0.39285714\n8  0.05357143\n9  0.28571429\n10 0.23214286\n11 0.32142857\n12 0.83928571\n13 0.64285714\n14 0.30357143\n15 0.53571429\n16 0.48214286\n17 0.57142857\n18 0.08928571\n19 0.89285714\n20 0.55357143\n21 0.78571429\n22 0.73214286\n23 0.82142857\n24 0.33928571\n\n\nSo from the above data we can see that the period is \\(24\\).\n\n\n\nWrite a code that would implement RANDU. For debugging purpose print \\(x_{1000}\\) when the seed is \\(x_0=1\\)\n(a)\nUsing RANDU generate \\(u_1,u_2,\\cdots, u_{20,002}\\) where \\(u=\\frac{x_n}{M}\\). For all triplets in your sequence, \\(u_i, u_{i+1}, u_{i+2}\\), in which \\(0.5\\le u_{i+1} \\le 0.51\\) plot \\(u_i\\) versus \\(u_{i+2}\\). Comment on the pattern of your scatterplot.\n\n\n\n\n\n\n\n\n\n(b)\nGenerate a sequence of lenght 1002. Use a program that plots points in 3 dimensions and rotates the axes to rotate the points until you can see the 15 planes.\n\n\nCode\nlibrary(\"rgl\")\nlibrary(\"rglwidget\")\n\nN = 1002\nA = matrix(0, ncol=3, nrow=N)\nseed &lt;- as.double(1)\n\nRANDU &lt;- function() {\n  seed &lt;&lt;- ((2^16 + 3) * seed) %% (2^31)\n  round(seed/(2^31), 6)\n}\n\nfor (i in 1:N) {\n  A[i, ] &lt;- c(RANDU(), RANDU(), RANDU())\n}\nB = as.data.frame(A)\n\nbg3d(color = \"#f4f4f4\")\nplot3d(B$V1, B$V2, B$V3, type=\"s\", size=1, lit=TRUE, col = rainbow(1000))\nspin &lt;- spin3d(axis= c(0, 0, 1), rpm = 5)\nplay3d(spin, duration = 10)\n\n# Render the 3D plot as a WebGL widget\nrglwidget()\n\n\n\n\n\n\nCode\nrgl.close()\n\n\n\n\n\nA circle with radius \\(r=1\\) has the area \\(A=\\pi r^2= \\pi\\) and a square with length \\(l=1\\) has the area \\(B=1\\). Now if we consider the following situation, where a quarter of a unit circle is inscribed inside a unit square like this\n# I used python to generate this plot. Else everywhere the codes are in R\nimport matplotlib.pyplot as plt\nimport numpy as np \n\nnum_points = 100\npts = np.random.rand(num_points,2)\n\nfig, axes = plt.subplots()\ntheta = np.linspace(0, np.pi/2,100)\nx = np.cos(theta)\ny = np.sin(theta)\n\naxes.plot(x, y, 'b')\naxes.plot([0,1],[0,0],'k')\naxes.plot([1,1],[0,1],'k')\naxes.plot([1,0],[1,1],'k')\naxes.plot([0,0],[1,0],'k')\n\nfor p in pts:\n    if p[0]**2+p[1]**2 &lt;=1:\n        axes.plot(p[0], p[1], 'go')\n    else:\n        axes.plot(p[0], p[1], 'ro')\naxes.set_aspect('equal')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\nwe get,\n\\[\\begin{align*}\n\\frac{\\text{Area of the quarter of a unit circle: C}}{\\text{Area of a unit square: S}}&= \\frac{\\frac{\\pi}{4}}{1}=\\frac{\\pi}{4}\\hspace{4mm}\\implies \\pi = \\frac{4C}{S}\n\\end{align*}\\]\n\nThe above relation tells us some interesting fact. If we uniformly create as many points as possible inside the square then the number of points inside the circle will be approximately 4 times the number of the points outside the circular region.\n\n\nmonte_carlo_pi &lt;- function(n) {\n  inside_circle &lt;- 0\n  for (i in 1:n) {\n    x &lt;- runif(1)\n    y &lt;- runif(1)\n    if (x^2 + y^2 &lt;= 1) {\n      inside_circle &lt;- inside_circle + 1\n    }\n  }\n  pi_estimate &lt;- (inside_circle / n) * 4\n  return(pi_estimate)\n}\n\nn &lt;- 10000\ncat(sprintf(\"Monte Carlo estimated value of π from %d points = %f\\n\", n, monte_carlo_pi(n)))\n\nMonte Carlo estimated value of π from 10000 points = 3.152400\n\n\nThat’s all for this post."
  },
  {
    "objectID": "posts/montecarlo1/index.html#reference",
    "href": "posts/montecarlo1/index.html#reference",
    "title": "Monte-Carlo Methods: PRNGs",
    "section": "Reference",
    "text": "Reference\n\nOkten, G. (1999). Contributions to the theory of Monte Carlo and quasi-Monte Carlo methods. Universal-Publishers.\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  }
]