[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nUnderstanding Decision Tree Classifier: A Mathematical Approach\n\n\n7 min\n\n\n\nRafiq Islam\n\n\nFriday, August 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData collection through Webscraping\n\n\n7 min\n\n\n\nRafiq Islam\n\n\nWednesday, August 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonte-Carlo Methods: PRNGs\n\n\n9 min\n\n\n\nRafiq Islam\n\n\nSunday, August 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Inference in Machine Learning: Part 1\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nSunday, July 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to generate social share buttons\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nWednesday, July 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my blog\n\n\n1 min\n\n\n\nRafiq Islam\n\n\nFriday, July 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix multiplication: Let’s make it less expensive!\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nMonday, July 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Stochastic Gradient Descent Using Simple Linear Regression\n\n\n5 min\n\n\n\nRafiq Islam\n\n\nSaturday, May 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n\n26 min\n\n\n\nRafiq Islam\n\n\nTuesday, November 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling viral disease\n\n\n3 min\n\n\n\nRafiq Islam\n\n\nTuesday, February 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Linear Algebra Proofs\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nSunday, January 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Representation: Change of Basis\n\n\n3 min\n\n\n\nRafiq Islam\n\n\nThursday, January 21, 2021\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Download a PDF"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\n\nPh.D in Mathematics, Florida State University; Florida, USA 2026 (expected)\nM.S. in Mathematics, Youngstown State University; Ohio, USA 2020\nM.S. in Applied Mathematics, University of Dhaka; Dhaka, Bangladesh 2016\nB.S. in Mathematics, University of Dhaka; Dhaka, Bangladesh 2014"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "CV",
    "section": "Work experience",
    "text": "Work experience\n\nGraduate Teaching Assistant (Fall 2021- To Date)\n\nFlorida State University\nDuties includes: Teaching, Proctoring, and Grading\nSupervisor: Penelope Kirby, Ph.D\n\nGraduate Teaching Assistant (Fall 2018 - Spring 2020)\n\nYoungstown State University University\nDuties included: Teaching, Proctoring, and Grading\nSupervisor: G. Jay Kerns, Ph.D\n\nAssistant Vice President (September 2017 - July 2018)\n\nDelta Life Insurance Company Ltd. Dhaka, Bangladesh\nDuties included: Calculated all types of claims (death, surrender, and maturity) using excel spreadsheets.\nProcessed approximately 500 claims each week and submitted corresponding statistical reports to the higher authority.\nWorked in a team to develop a new short-term endowment assurance product which played an important role to increase the company’s new business.\nRefurbished a without risk endowment product which was out of the sale. Priced insurance premiums based on different risk factors for bigger clients which impacted our life fund significantly.\nCalculated reserves for group endowment, term and premium back policies which was a vital part of the final valuation report.\nLiaised directly with the consulting actuary and provided all sorts of technical and documental supports during actuarial valuation\nSupervisor: Md. Salahuddin Soud, VP"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "CV",
    "section": "Skills",
    "text": "Skills\n\nLanguage\n\nBengali: Native\nEnglish: Fluent\n\nComputer Literacy\n\nProgramming Languages: Python, FORTRAN, Julia, R, MATLAB, Mathematica\nSoftware Development Tools: Git, GitHub, PyPi\n\nMusical Instrument: Amateur/Novice Bamboo flute player"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "CV",
    "section": "Publications",
    "text": "Publications\n\nGJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies \nMostafa, F; Saha, P; Islam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management.\nComparison of Financial Models for Stock Price Prediction \nIslam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management."
  },
  {
    "objectID": "cv.html#talks-and-presentations",
    "href": "cv.html#talks-and-presentations",
    "title": "CV",
    "section": "Talks and Presentations",
    "text": "Talks and Presentations\n\n The Heavy-Tail Phenomenon in Decentralized Stochastic Gradient Descent\nNovember 20, 2023\nPresentation at James J Love Building, Florida State University, Tallahassee, Florida\n\n Decentralized Stochastic Gradient Langevin Dynamics and Hamiltonian Monte Carlo\nOctober 05, 2023\nPresentation at James J Love Building, Florida State University, Tallahassee, Florida\n\n Sensitivity analysis for Monte Carlo and Quasi Monte Carlo option pricing\nApril 28, 2020\nPresentation at Cafaro Hall, Youngstown State University, Youngstown, Ohio"
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "CV",
    "section": "Teaching",
    "text": "Teaching\n\n Spring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n Fall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n Spring 2023: MAC1140 PreCalculus Algebra\n\n Fall 2022: MAC2311 Calculus and Analytic Geometry I\n\n Fall 2021 and Spring 2022: PreCalculus and Algebra\n\n Fall 2018 to Spring 2020: College Algebra, Trigonometry"
  },
  {
    "objectID": "cv.html#awards-and-affiliations",
    "href": "cv.html#awards-and-affiliations",
    "title": "CV",
    "section": "Awards and Affiliations",
    "text": "Awards and Affiliations\n\nAwards\n\nBettye Anne Busbee Case Graduate Fellowship & Doctoral Mentorship Recognition 2024\n\nOutstanding Graduate Student in Statistics Award for the 2019-2020 academic year, Youngstown State University.\n\nGraduate College Premiere Scholarship, Youngstown State University.\n\nMetLife Bangladesh Actuarial Study Program 2015\n\n\n\nAffiliations\n\nBangladesh Mathematical Society: Life Member\nSociety of Actuaries, SOA: Student Member\nAmerican Mathematical Society, AMS\nSociety for Industrial and Applied Mathematics"
  },
  {
    "objectID": "portfolio/desgld/index.html",
    "href": "portfolio/desgld/index.html",
    "title": "Python Application Library: desgld packaging",
    "section": "",
    "text": "This package is related to my ongoing project “EXTRA decentralized stochastic gradient Langevin dynamics”. The detail of the package can be found here\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2024,\n  author = {Islam, Rafiq},\n  title = {Python {Application} {Library:} Desgld Packaging},\n  date = {2024-05-03},\n  url = {https://mrislambd.github.io/portfolio/desgld/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2024. “Python Application Library: Desgld\nPackaging.” May 3, 2024. https://mrislambd.github.io/portfolio/desgld/."
  },
  {
    "objectID": "teaching/fall21.html",
    "href": "teaching/fall21.html",
    "title": "Fall 2021 and Spring 2022: PreCalculus and Algebra",
    "section": "",
    "text": "As a lecture TA, my job was to facilitate the instructor during the class. This included helping students in class activities such as answering short questions that counted as class attendance, checking students’ eligibility forms for taking this course, and others as needed by the instructor.  As a lab TA I worked in a computer lab where students take their weekly quizzes and midterm tests.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/fall22.html",
    "href": "teaching/fall22.html",
    "title": "Fall 2022: MAC2311 Calculus and Analytic Geometry I",
    "section": "",
    "text": "Students who have substantial knowledge of precalculus and algebra may require to take this course as a mathematics requirement depending on their majors. The topic of this course includes but is not limited to Foundation for calculus: Functions and Limits, Derivative, The Definite Integral, and Constructing Antiderivatives.  As a recitation instructor for this course, I ran two poster presentation sessions of 30 students in each group where they presented mathematical problems and their solutions step by step to their peer classmates followed by a group activity where they solved another set of problems. I also graded their exam scripts and weekly posters.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/sp24.html",
    "href": "teaching/sp24.html",
    "title": "Spring 2024: MAP4170 Introduction to Actuarial Mathematics",
    "section": "",
    "text": "One of the course objectives is for each student to develop a mastery of financial mathematics used by actuaries, based on the mathematics of interest theory. Other course objectives are for each student to understand the long-term individual study commitment necessary to achieve a designation within one of the actuarial societies and for each student to increase their knowledge of the actuarial profession\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\nWelcome to my website! I am currently a Ph.D. candidate in the Mathematics Department at Florida State University, where I also serve as a Graduate Teaching Assistant. My academic journey has been both diverse and enriching, spanning multiple institutions and fields of study.   Before joining FSU, I earned a second master’s degree in Mathematics from Youngstown State University in Ohio, USA. Prior to that, I completed my undergraduate degree in Mathematics at the University of Dhaka (DU), Bangladesh, followed by a one-year integrated master’s program in Applied Mathematics at the same institution.  After graduating from DU, I gained valuable industry experience by working for two years in the life insurance sector in Dhaka, Bangladesh. In pursuit of higher education and advanced research opportunities, I relocated to the United States in August 2018.  As a lifelong student and researcher of mathematics, my research interests are centered around Theoretical Machine Learning, Computational Finance, and various other areas of applied mathematics. I am passionate about advancing knowledge in these fields and contributing to their practical applications.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "talks/2023-11-20-candi.html",
    "href": "talks/2023-11-20-candi.html",
    "title": "The Heavy-Tail Phenomenon in Decentralized Stochastic Gradient Descent",
    "section": "",
    "text": "Stochastic Gradient Descent (SGD) method is one of the most popular optimization techniques in machine learning, particularly in Deep Neural Network (DNN). The gradient noise in this method is often modeled by Gaussian or assumed to have finite variance. However, empirical evidence suggests that the gradient noises can be highly non-Gaussian and often exhibit heavy tails in nature. This heaviness has a direct relationship to the generalization performance of the algorithm.\n\n\nIn this candidacy paper we discuss materials from three papers where we first present the tail-index analysis in SGD that shows empirically that gradient noise can have heavy tails, and through metastability analysis, the heavy-tailed SGD validates the wide minima phenomenon. We then present the paper about the heavy-tail phenomenon in SGD and investigates the origins of the heavy tails. We show that the heaviness of the tail is related to the choice of stepsize, batch-size and other hyperparameters of the algorithm. Finally, we discuss the heavy-tail phenomenon in decentralized SGD. We conclude the candidacy paper by proposing a few future research directions.\n\nA copy of the presentation can be found here\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "publication/pub2/index.html",
    "href": "publication/pub2/index.html",
    "title": "GJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies",
    "section": "",
    "text": "Cryptocurrencies are currently traded worldwide, with hundreds of different currencies in existence and even more on the way. This study implements some statistical and machine learning approaches for cryptocurrency investments. First, we implement GJR-GARCH over the GARCH model to estimate the volatility of ten popular cryptocurrencies based on market capitalization: Bitcoin, Bitcoin Cash, Bitcoin SV, Chainlink, EOS, Ethereum, Litecoin, TETHER, Tezos, and XRP. Then, we use Monte Carlo simulations to generate the conditional variance of the cryptocurrencies using the GJR-GARCH model, and calculate the value at risk (VaR) of the simulations. We also estimate the tail-risk using VaR backtesting. Finally, we use an artificial neural network (ANN) for predicting the prices of the ten cryptocurrencies. The graphical analysis and mean square errors (MSEs) from the ANN models confirmed that the predicted prices are close to the market prices. For some cryptocurrencies, the ANN models perform better than traditional ARIMA models.\n\nDownload the paper from here\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@article{mostafa2021,\n  author = {Mostafa, Fahad and Saha, Pritam and Rafiqul Islam, Mohammad\n    and Nguyen, Nguyet},\n  title = {GJR-GARCH {Volatility} {Modeling} Under {NIG} and {ANN} for\n    {Predicting} {Top} {Cryptocurrencies}},\n  journal = {Journal of Risk and Financial Management},\n  date = {2021-09-03},\n  url = {https://mrislambd.github.io/publication/pub2/},\n  doi = {10.3390/jrfm14090421},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMostafa, Fahad, Pritam Saha, Mohammad Rafiqul Islam, and Nguyet Nguyen.\n2021. “GJR-GARCH Volatility Modeling Under NIG and ANN for\nPredicting Top Cryptocurrencies.” Journal of Risk and\nFinancial Management, September. https://doi.org/10.3390/jrfm14090421."
  },
  {
    "objectID": "posts/sgdlinreg/index.html",
    "href": "posts/sgdlinreg/index.html",
    "title": "Understanding Stochastic Gradient Descent Using Simple Linear Regression",
    "section": "",
    "text": "Linear regression is a fundamental algorithm in machine learning used for predicting a continuous dependent variable based on one or more independent variables. The objective is to find the best-fit line that minimizes the difference between the predicted and actual values.\nA simple linear regression in multiple predictors/input variables/features/independent variables/ explanatory variables/regressors/ covariates (many names) often takes the form\n\\[y=f(\\mathbf{x})+\\epsilon =\\mathbf{\\beta}\\mathbf{x}+\\epsilon\\]\nwhere \\(\\mathbf{\\beta} \\in \\mathbb{R}^d\\) are regression parameters or constant values that we aim to estimate and \\(\\epsilon \\sim \\mathcal{N}(0,1)\\) is a normally distributed error term independent of \\(x\\) or also called the white noise.\nFor simplicity let’s start with this toy example. Say, we have the data from a class of 10 students and their heights and weights are as follows:\nand this data looks like this:\nIn this case, the model:\n\\[y=f(x)+\\epsilon=\\beta_0+\\beta_1 x+\\epsilon\\]\nTherefore, in our model we need to estimate the parameters \\(\\beta_0,\\beta_1\\). The true relationship between the explanatory variables and the dependent variable is \\(y=f(x)\\). But our model is \\(y=f(x)+\\epsilon\\). Here, this \\(f(x)\\) is the working model with the data. In other words, \\(\\hat{y}=f(x)=\\hat{\\beta}_0+\\hat{\\beta}_1 x\\). Therefore, there should be some error in the model prediction which we are calling \\(\\epsilon=\\|y-\\hat{y}\\|\\) where \\(y\\) is the true value and \\(\\hat{y}\\) is the predicted value. This error term is normally distributed with mean 0 and variance 1. To get the best estimate of the parameters \\(\\beta_0,\\beta_1\\) we can minimize the error term as much as possible. So, we define the residual sum of squares (RSS) as:\n\\[\\begin{align}\nRSS &=\\epsilon_1^2+\\epsilon_2^2+\\cdots+\\epsilon_{10}^2\\\\\n&= \\sum_{i=1}^{10}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)^2\\\\\n\\hat{\\mathcal{l}}(\\bar{\\beta})&=\\sum_{i=1}^{10}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)^2\\\\\n\\end{align}\\]\nUsing multivariate calculus we see\n\\[\\begin{align}\n    \\frac{\\partial l}{\\partial \\beta_0}&=\\sum_{i=1}^{10} 2(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(-1)\\\\\n    \\frac{\\partial l}{\\partial \\beta_1}&= \\sum_{i=1}^{10} 2(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(-x_i)\n\\end{align}\\]\nSetting the partial derivatives to zero we solve for \\(\\hat{\\beta_0},\\hat{\\beta_1}\\) as follows\n\\[\\begin{align*}\n    \\frac{\\partial l}{\\partial \\beta_0}&=0\\\\\n    \\implies \\sum_{i=1}^{10} y_i-10 \\hat{\\beta_0}-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i\\right)&=0\\\\\n    \\implies \\hat{\\beta_0}&=\\bar{y}-\\hat{\\beta_1}\\bar{x}\n\\end{align*}\\]\nand,\n\\[\\begin{align*}\n    \\frac{\\partial l}{\\partial \\beta_1}&=0\\\\\n    \\implies \\sum_{i=1}^{10} 2(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(-x_i)&=0\\\\\n    \\implies \\sum_{i=1}^{10} (y_i-\\hat{\\beta}_0-\\hat{\\beta}_1 x_i)(x_i)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\hat{\\beta_0}\\left(\\sum_{i=1}^{10} x_i\\right)-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\left(\\bar{y}-\\hat{\\beta_1}\\bar{x}\\right)\\left(\\sum_{i=1}^{10} x_i\\right)-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right)+\\hat{\\beta_1}\\bar{x}\\left(\\sum_{i=1}^{10} x_i\\right)-\\hat{\\beta_1}\\left(\\sum_{i=1}^{10} x_i^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right) -\\hat{\\beta_1}\\left(\\sum_{i=1}^{10}x_i^2-\\bar{x}\\sum_{i=1}^{10}x_i\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right) -\\hat{\\beta_1}\\left(\\sum_{i=1}^{10}x_i^2-10\\bar{x}^2\\right)&=0\\\\\n    \\implies \\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right) -\\hat{\\beta_1}\\left(\\sum_{i=1}^{10}x_i^2-2\\times 10\\times \\bar{x}^2+10\\bar{x}^2\\right)&=0\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10} x_iy_i-10\\bar{x}\\bar{y}}{\\sum_{i=1}^{10}x_i^2-2\\times 10\\times \\bar{x}^2+10\\bar{x}^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10} x_iy_i -10\\bar{x}\\bar{y}-10\\bar{x}\\bar{y}+10\\bar{x}\\bar{y}}{\\sum_{i=1}^{10}x_i^2-2\\bar{x}\\times 10\\times\\frac{1}{10}\\sum_{i=1}^{10}x_i +10\\bar{x}^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10} x_iy_i-\\bar{y}\\left(\\sum_{i=1}^{10} x_i\\right)-\\bar{x}\\left(\\sum_{i=1}^{10} y_i\\right)+10\\bar{x}\\bar{y}}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10}\\left(x_iy_i-x_i\\bar{y}-\\bar{x}y_i+\\bar{x}\\bar{y}\\right)}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\\\\\n    \\implies \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\\\\\n\\end{align*}\\]\nTherefore, we have the following\n\\[\\begin{align*}\n     \\hat{\\beta_0}&=\\bar{y}-\\hat{\\beta_1}\\bar{x}\\\\\n     \\hat{\\beta_1}&=\\frac{\\sum_{i=1}^{10}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{10}(x_i-\\bar{x})^2}\n\\end{align*}\\]\nTo be continued in the next post…"
  },
  {
    "objectID": "posts/sgdlinreg/index.html#references",
    "href": "posts/sgdlinreg/index.html#references",
    "title": "Understanding Stochastic Gradient Descent Using Simple Linear Regression",
    "section": "References",
    "text": "References\n\n[1] James, Gareth, et al. An introduction to statistical learning: With applications in python. Springer Nature, 2023.\n\n\n\n\n\n\n\n\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/lu/index.html",
    "href": "posts/lu/index.html",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "",
    "text": "! This program factors a full rank matrix A into lower triangular (or trapezoidal) L and upper\n! triangular matrix U\nprogram LU_decompostion\n    implicit none\n    !############# #################List of main program variable##################################\n    integer::m,n\n    ! m is the # of rows of the matrix that we are working with\n    ! n is the # of columns that we are working with\n    doubleprecision, allocatable,dimension(:,:)::A,A1\n    ! A is the working matrix, A1 is the original matrix preserved to check correctness of factoring\n    integer,allocatable,dimension(:):: P,Q\n    ! P, Q are the row and column permutation vectors for partial and complete pivoting\n    character::method\n    !################################################################################################\n\n    ! ############################ Open an Input Data File###########################################\n    open(unit=1,file=&quot;data.txt&quot;)\n    ! ###############################################################################################\n\n    ! ################################# Read m, n of the matrix A ###################################\n    write(*,*)&quot;Input the number of rows of the matrix A, m&quot;\n    read(*,*) m\n    write(*,*)&quot;Input the number of columns of the matrix A, n&quot;\n    read(*,*)n\n    ! ##############################################################################################\n\n    ! ########################### Allocate Space ###################################################\n    allocate(A(m,n),A1(m,n),P(m),Q(n))\n    ! ##############################################################################################\n\n    ! Create the matrix A\n    print*,\n    call matrixA(m,n,A,A1)\n    print*,\n\n    !##################################### Choose the method #######################################\n    print*,&quot;What method you want to apply?&quot;\n    print*,&quot;For No Pivot input: N&quot;\n    print*, &quot;For Partial Pivot input: P&quot;\n    print*, &quot;For Complete Pivot input: C&quot;\n    read(*,*) method\n    !###############################################################################################\n\n    ! ############################### Execution of the methods #####################################\n    IF(method.eq.&quot;C&quot;.or.method.eq.&quot;c&quot;) THEN\n        print*, &quot;Complete Pivoting method has been selected&quot;\n        print*,\n        call completePivot(m,n,A,A1,P,Q)\n    ELSE IF(method.eq.&quot;P&quot;.or.method.eq.&quot;p&quot;) then\n        print*,&quot;Partial Pivoting method has been selected&quot;\n        print*,\n        call partialPivot(m,n,A,A1,P)\n    ELSE IF (method.eq.&quot;N&quot;.or.method.eq.&quot;n&quot;) then\n        print*, &quot;No Pivoting method has been selected&quot;\n        print*,\n        call noPivot(m,n,A,A1)\n    END IF\n\nend program\n\nsubroutine matrixA(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer::i\n    print*,\n    print*, &quot;This is the provided working matrix&quot;\n    print*\n    do i=1,m\n        read(1,*)A(i,:)\n        A1(i,:)=A(i,:)\n        print*,A(i,:)\n    end do\n    do i=1,n\n        IF(A(i,i)==0) then\n            print*,&quot;A 0 entry was found in the main diagonal.&quot;\n            print*, &quot;Therefore, pivoting is a must required&quot;\n        else if(A(i,i).lt.0.0001) then\n            print*, &quot;WARNING!! Diagonal Element is too small.&quot;\n        END IF\n    end do\nend subroutine\n\nsubroutine completePivot(m,n,A,A1,P,Q)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer,dimension(m),intent(out)::P\n    integer,dimension(n),intent(out)::Q\n    integer::i,j,k,row,col\n    doubleprecision::temp\n\n    do k=1,n\n       call max_val(A,m,n,k,row,col)\n       do i=k,n\n            temp=A(i,col)\n            A(i,col)=A(i,k)\n            A(i,k)=temp\n        end do\n        Q(k)=col\n        do j=k,n\n            temp=A(k,j)\n            A(k,j)=A(row,j)\n            A(row,j)=temp\n        end do\n        P(k)=row\n\n        A(k+1:n,k)=A(k+1:n,k)/A(k,k)\n        do j=k+1,n\n            do i=k+1,n\n                A(i,j)=A(i,j)-A(i,k)*A(k,j)\n            end do\n        end do\n    end do\n    print*,\n    print*,&quot;------------------------------------------------------&quot;\n    print*,&quot;         Complete Pivot A=LU factorized array&quot;\n    print*,&quot;-------------------------------------------------------&quot;\n    print*,\n    do i=1,m\n        print*,A(i,:)\n    end do\n    print*,\n    print*, &quot;Permutation vector P=(&quot;,(P(i),i=1,m-1),&quot;)&quot;\n    print*,\n    print*, &quot;Permutation vector Q=(&quot;,(Q(i),i=1,n-1),&quot;)&quot;\n    print*,\n    print*,&quot;*******************************************************&quot;\n    print*,&quot;       Checking Correctness of the factorization&quot;\n    print*,&quot;*******************************************************&quot;\n    print*,\n    call CheckingCompletePivot(m,n,A,A1)\n\nend subroutine\n\n\nsubroutine CheckingCompletePivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    doubleprecision,dimension(n,n):: U\n    doubleprecision,dimension(m,n)::L,A2\n    integer::i,j\n\n    do i=1,m\n        do j=1,n\n            if(i.le.j)then\n                L(i,j)=0\n            else if (i.gt.j) then\n                L(i,j)=A(i,j)\n            end if\n            L(i,i)=1\n        end do\n    end do\n    do i=1,n\n        do j=1,n\n            if(i.le.j)then\n                U(i,j)=A(i,j)\n            else\n                U(i,j)=0.0\n            end if\n        end do\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Complete Pivot Upper triangular matrix U&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,n\n        print*,U(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Complete Pivot Lower triangular matrix L&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,L(i,:)\n    end do\n\n    A2=matmul(L,U)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;              Product of L U=&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,A2(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;  Factoring Accuracy with the Frobenius-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,Frobenius(m,n,A1-A2)/Frobenius(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, Frobenius(m,n,matmul(abs(L),abs(U)))/Frobenius(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the 1-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,norm1(m,n,A1-A2)/norm1(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, norm1(m,n,matmul(abs(L),abs(U)))/norm1(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the Infinity-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,infinityNorm(m,n,A1-A2)/infinityNorm(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, infinityNorm(m,n,matmul(abs(L),abs(U)))/infinityNorm(m,n,A1)\nend subroutine\n\nsubroutine partialPivot(m,n,A,A1,P)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer,dimension(m),intent(out)::P\n    integer::i,j,k,row\n    doubleprecision::temp\n\n    do k=1,n\n       call max_valP(A,m,n,k,row)\n        do j=k,n\n            temp=A(k,j)\n            A(k,j)=A(row,j)\n            A(row,j)=temp\n        end do\n        P(k)=row\n\n        A(k+1:n,k)=A(k+1:n,k)/A(k,k)\n        do j=k+1,n\n            do i=k+1,n\n                A(i,j)=A(i,j)-A(i,k)*A(k,j)\n            end do\n        end do\n    end do\n    print*,\n    print*,&quot;------------------------------------------------------&quot;\n    print*,&quot;          Partial Pivot A=LU factorized array&quot;\n    print*,&quot;-------------------------------------------------------&quot;\n    print*,\n    do i=1,m\n        print*,A(i,:)\n    end do\n    print*,\n    print*, &quot;Permutation vector P=(&quot;,(P(i),i=1,m-1),&quot;)&quot;\n    print*,\n    print*,&quot;*******************************************************&quot;\n    print*,&quot;       Checking Correctness of the factorization&quot;\n    print*,&quot;*******************************************************&quot;\n    print*,\n    call CheckingPartialPivot(m,n,A,A1)\nend subroutine\n\nsubroutine CheckingPartialPivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    doubleprecision,dimension(n,n):: U\n    doubleprecision,dimension(m,n)::L,A2\n    integer::i,j\n\n    do i=1,m\n        do j=1,n\n            if(i.le.j)then\n                L(i,j)=0\n            else if (i.gt.j) then\n                L(i,j)=A(i,j)\n            end if\n            L(i,i)=1\n        end do\n    end do\n    do i=1,n\n        do j=1,n\n            if(i.le.j)then\n                U(i,j)=A(i,j)\n            else\n                U(i,j)=0.0\n            end if\n        end do\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Partial Pivot Upper triangular matrix U&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,n\n        print*,U(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Partial Pivot Lower triangular matrix L&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,L(i,:)\n    end do\n\n    A2=matmul(L,U)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;       Partial Pivot Product of L U=&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,A2(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot; Factoring Accuracy with the Frobenius-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,Frobenius(m,n,A1-A2)/Frobenius(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, Frobenius(m,n,matmul(abs(L),abs(U)))/Frobenius(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the 1-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,norm1(m,n,A1-A2)/norm1(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, norm1(m,n,matmul(abs(L),abs(U)))/norm1(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the Infinity-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,infinityNorm(m,n,A1-A2)/infinityNorm(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, infinityNorm(m,n,matmul(abs(L),abs(U)))/infinityNorm(m,n,A1)\nend subroutine\n\nsubroutine noPivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer::i,j,k\n\n    do k=1,n\n        A(k+1:n,k)=A(k+1:n,k)/A(k,k)\n        do j=k+1,n\n            do i=k+1,n\n                A(i,j)=A(i,j)-A(i,k)*A(k,j)\n            end do\n        end do\n    end do\n    print*,\n    print*,&quot;------------------------------------------------------&quot;\n    print*,&quot;          No Pivot A=LU factorized array&quot;\n    print*,&quot;-------------------------------------------------------&quot;\n    print*,\n    do i=1,m\n        print*,A(i,:)\n    end do\n    print*,\n    print*,&quot;*******************************************************&quot;\n    print*,&quot;       Checking Correctness of the factorization&quot;\n    print*,&quot;*******************************************************&quot;\n    print*,\n    call CheckingNoPivot(m,n,A,A1)\nend subroutine\n\nsubroutine CheckingNoPivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    doubleprecision,dimension(n,n):: U\n    doubleprecision,dimension(m,n)::L,A2\n    integer::i,j\n\n    do i=1,m\n        do j=1,n\n            if(i.le.j)then\n                L(i,j)=0\n            else if (i.gt.j) then\n                L(i,j)=A(i,j)\n            end if\n            L(i,i)=1\n        end do\n    end do\n    do i=1,n\n        do j=1,n\n            if(i.le.j)then\n                U(i,j)=A(i,j)\n            else\n                U(i,j)=0.0\n            end if\n        end do\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  No Pivot Upper triangular matrix U&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,n\n        print*,U(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  No Pivot Lower triangular matrix L&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,L(i,:)\n    end do\n\n    A2=matmul(L,U)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;       No Pivot Product of L U=&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,A2(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;  Factoring Accuracy with the Frobenius Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,Frobenius(m,n,A1-A2)/Frobenius(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, Frobenius(m,n,matmul(abs(L),abs(U)))/Frobenius(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the 1-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,norm1(m,n,A1-A2)/norm1(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, norm1(m,n,matmul(abs(L),abs(U)))/norm1(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the Infinity-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,infinityNorm(m,n,A1-A2)/infinityNorm(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, infinityNorm(m,n,matmul(abs(L),abs(U)))/infinityNorm(m,n,A1)\nend subroutine\n\nsubroutine max_val(A,m,n,k,row,col)\n    implicit none\n    integer,intent(in)::m,n,k\n    integer,intent(out)::row,col\n    doubleprecision,dimension(m,n),intent(inout)::A\n    doubleprecision::maximum\n    integer::i,j\n\n    maximum=maxval(A(k:m,k:n))\n\n    do i=k,m\n        do j=k,n\n            if(A(i,j)==maximum) then\n                row=i\n                col=j\n                goto 100\n            end if\n        end do\n    end do\n100 end subroutine\n\nsubroutine max_valP(A,m,n,k,row)\n    implicit none\n    integer,intent(in)::m,n,k\n    integer,intent(out)::row\n    doubleprecision,dimension(m,n),intent(inout)::A\n    doubleprecision::maximum\n    integer::i,j\n\n    maximum=maxval(A(k:m,k))\n\n    do i=k,m\n        if(A(i,k)==maximum) then\n            row=i\n            goto 101\n        end if\n    end do\n101 end subroutine\n\n\nfunction norm1(m,n,A)\n    integer::m,n,i,j\n    doubleprecision,dimension(m,n)::A\n    doubleprecision,dimension(n):: normvector\n    doubleprecision::normval\n\n    normval=0\n    do j=1,n\n        do i=1,m\n            normval=normval+abs(A(i,j))\n        end do\n        normvector(j)=normval\n    end do\n    norm1=maxval(normvector(1:n))\n    return\nend function\n\nfunction infinityNorm(m,n,A)\n    integer::m,n,i,j\n    doubleprecision,dimension(m,n)::A\n    doubleprecision,dimension(n):: normvector\n    doubleprecision:: normval\n\n    normval=0\n    do j=1,n\n        do i=1,m\n            normval=normval+abs(A(i,j))\n            normvector(j)=normval\n        end do\n    end do\n    infinityNorm=maxval(normvector(1:m))\n    return\nend function\n\nfunction Frobenius(m,n,A)\n    integer::m,n,i,j\n    doubleprecision,dimension(m,n)::A\n    doubleprecision:: normval\n\n    normval=0\n    do i=1,m\n        do j=1,n\n            normval=normval+(abs(A(i,j)))**2\n        end do\n    end do\n    Frobenius=sqrt(normval)\n    return\nend function"
  },
  {
    "objectID": "posts/lu/index.html#no-pivot",
    "href": "posts/lu/index.html#no-pivot",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "No Pivot",
    "text": "No Pivot\nInput Matrix (Stored in a data file in the same directory where the program file .f90 located)\n8 2 9\n4 9 4\n6 7 9\n\nComand prompt: \n Input the number of rows of the matrix A, m\n3\n Input the number of columns of the matrix A, n\n3\n\n\n This is the provided working matrix\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n   4.0000000000000000        9.0000000000000000        4.0000000000000000\n   6.0000000000000000        7.0000000000000000        9.0000000000000000\n\n What method you want to apply?\n For No Pivot input: N\n For Partial Pivot input: P\n For Complete Pivot input: C\nn\n No Pivoting method has been selected\n\n\n ------------------------------------------------------\n           No Pivot A=LU factorized array\n -------------------------------------------------------\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n  0.50000000000000000        8.0000000000000000      -0.50000000000000000\n  0.75000000000000000       0.68750000000000000        2.5937500000000000\n\n *******************************************************\n        Checking Correctness of the factorization\n *******************************************************\n\n\n ----------------------------------------------\n   No Pivot Upper triangular matrix U\n ----------------------------------------------\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n   0.0000000000000000        8.0000000000000000      -0.50000000000000000\n   0.0000000000000000        0.0000000000000000        2.5937500000000000\n\n ----------------------------------------------\n   No Pivot Lower triangular matrix L\n ----------------------------------------------\n\n   1.0000000000000000        0.0000000000000000        0.0000000000000000\n  0.50000000000000000        1.0000000000000000        0.0000000000000000\n  0.75000000000000000       0.68750000000000000        1.0000000000000000\n\n ----------------------------------------------\n        No Pivot Product of L U=\n ----------------------------------------------\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n   4.0000000000000000        9.0000000000000000        4.0000000000000000\n   6.0000000000000000        7.0000000000000000        9.0000000000000000\n\n ----------------------------------------------\n   Factoring Accuracy with the Frobenius Norm\n ----------------------------------------------\n\n Relative Error=   0.00000000\n\n Growth Factor=   1.02520537\n\n ----------------------------------------------\n      Factoring Accuracy with the 1-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\n\n ----------------------------------------------\n      Factoring Accuracy with the Infinity-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1"
  },
  {
    "objectID": "posts/lu/index.html#partial-pivot",
    "href": "posts/lu/index.html#partial-pivot",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "Partial Pivot",
    "text": "Partial Pivot\nInput Matrix (Stored in a data file in the same directory where the program file .f90 located)\n1 2 4\n2 1 3\n3 2 4\n\nComand Prompt: \n Input the number of rows of the matrix A, m\n3\n Input the number of columns of the matrix A, n\n3\n\n\n This is the provided working matrix\n\n   1.0000000000000000        2.0000000000000000        4.0000000000000000\n   2.0000000000000000        1.0000000000000000        3.0000000000000000\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n\n What method you want to apply?\n For No Pivot input: N\n For Partial Pivot input: P\n For Complete Pivot input: C\np\n Partial Pivoting method has been selected\n\n\n ------------------------------------------------------\n           Partial Pivot A=LU factorized array\n -------------------------------------------------------\n\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n  0.66666666666666663        1.3333333333333335        2.6666666666666670\n  0.33333333333333331      -0.24999999999999992        1.0000000000000000\n\n Permutation vector P=(           3           3 )\n\n *******************************************************\n        Checking Correctness of the factorization\n *******************************************************\n\n\n ----------------------------------------------\n   Partial Pivot Upper triangular matrix U\n ----------------------------------------------\n\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n   0.0000000000000000        1.3333333333333335        2.6666666666666670\n   0.0000000000000000        0.0000000000000000        1.0000000000000000\n\n ----------------------------------------------\n   Partial Pivot Lower triangular matrix L\n ----------------------------------------------\n\n   1.0000000000000000        0.0000000000000000        0.0000000000000000\n  0.66666666666666663        1.0000000000000000        0.0000000000000000\n  0.33333333333333331      -0.24999999999999992        1.0000000000000000\n\n ----------------------------------------------\n        Partial Pivot Product of L U=\n ----------------------------------------------\n\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n   2.0000000000000000        2.6666666666666670        5.3333333333333339\n   1.0000000000000000       0.33333333333333337        1.6666666666666667\n\n ----------------------------------------------\n  Factoring Accuracy with the Frobenius-Norm\n ----------------------------------------------\n\n Relative Error=  0.618016541\n\n Growth Factor=   1.11492395\n\n ----------------------------------------------\n      Factoring Accuracy with the 1-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\n\n ----------------------------------------------\n      Factoring Accuracy with the Infinity-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1"
  },
  {
    "objectID": "posts/lu/index.html#complete-pivot",
    "href": "posts/lu/index.html#complete-pivot",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "Complete Pivot",
    "text": "Complete Pivot\nInput Matrix (Stored in a data file in the same directory where the program file .f90 located)\n  \n2 3 4\n4 7 5\n4 9 5\n\nCommand Prompt:\n Input the number of rows of the matrix A, m\n3\n Input the number of columns of the matrix A, n\n3\n\n\n This is the provided working matrix\n\n   2.0000000000000000        3.0000000000000000        4.0000000000000000\n   4.0000000000000000        7.0000000000000000        5.0000000000000000\n   4.0000000000000000        9.0000000000000000        5.0000000000000000\n\n What method you want to apply?\n For No Pivot input: N\n For Partial Pivot input: P\n For Complete Pivot input: C\nc\n Complete Pivoting method has been selected\n\n\n ------------------------------------------------------\n          Complete Pivot A=LU factorized array\n -------------------------------------------------------\n\n   9.0000000000000000        4.0000000000000000        5.0000000000000000\n  0.77777777777777779        2.3333333333333335       0.66666666666666674\n  0.33333333333333331       0.47619047619047616       0.57142857142857140\n\n Permutation vector P=(           3           3 )\n\n Permutation vector Q=(           2           3 )\n\n *******************************************************\n        Checking Correctness of the factorization\n *******************************************************\n\n\n ----------------------------------------------\n   Complete Pivot Upper triangular matrix U\n ----------------------------------------------\n\n   9.0000000000000000        4.0000000000000000        5.0000000000000000\n   0.0000000000000000        2.3333333333333335       0.66666666666666674\n   0.0000000000000000        0.0000000000000000       0.57142857142857140\n\n ----------------------------------------------\n   Complete Pivot Lower triangular matrix L\n ----------------------------------------------\n\n   1.0000000000000000        0.0000000000000000        0.0000000000000000\n  0.77777777777777779        1.0000000000000000        0.0000000000000000\n  0.33333333333333331       0.47619047619047616        1.0000000000000000\n\n ----------------------------------------------\n               Product of L U=\n ----------------------------------------------\n\n   9.0000000000000000        4.0000000000000000        5.0000000000000000\n   7.0000000000000000        5.4444444444444446        4.5555555555555554\n   3.0000000000000000        2.4444444444444442        2.5555555555555554\n\n ----------------------------------------------\n   Factoring Accuracy with the Frobenius-Norm\n ----------------------------------------------\n\n Relative Error=  0.683437467\n\n Growth Factor=   1.00393677\n\n ----------------------------------------------\n      Factoring Accuracy with the 1-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\n\n ----------------------------------------------\n      Factoring Accuracy with the Infinity-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to my blog",
    "section": "",
    "text": "Here is a demo picture\n\n\n\n\n\n\n\n\n\nShare on\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Inference in Machine Learning: Part 1\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nSunday, July 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to generate social share buttons\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nWednesday, July 17, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to topCitationBibTeX citation:@online{islam2024,\n  author = {Islam, Rafiq},\n  title = {Welcome to My Blog},\n  date = {2024-07-12},\n  url = {https://mrislambd.github.io/posts/welcome/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2024. “Welcome to My Blog.” July 12, 2024. https://mrislambd.github.io/posts/welcome/."
  },
  {
    "objectID": "posts/socialshare/index.html",
    "href": "posts/socialshare/index.html",
    "title": "How to generate social share buttons",
    "section": "",
    "text": "If you want to share any blog posts on social media, you can have a share button at the bottom of each post so that the reader can easily share this on their preferred social media such as Facebook, LinkedIn, and X. Here I am showing only three but can be added more.\n\n\n\n# Library you need\nimport urllib.parse\n\n# Define the function to parse facebook sharable link\ndef fblink(link):\n    base_url = \"https://www.facebook.com/sharer/sharer.php\"\n    encoded_url = urllib.parse.quote(link, safe='')\n    full_url= f\"{base_url}?u={encoded_url}&amp;src=sdkpreparse\"\n    return full_url  \n\n# Suppose this is the link you want to share. Replace with your own link  \nlink=\"https://mrislambd.github.io/posts/social-share/\"\n\n# Then you can use this template\nprint('&lt;div id=\"fb-root\"&gt;&lt;/div&gt;')\nprint('&lt;script async defer crossorigin=\"anonymous\"\\n src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"&gt;&lt;/script&gt;')\nprint(' ')\nprint('&lt;div class=\"share-buttons\"&gt;')\nprint('&lt;div class=\"fb-share-button\" data-href=\"{}\"'.format(link))\nprint('data-layout=\"button_count\" data-size=\"small\"&gt;&lt;a target=\"_blank\" \\n href=\"{}\" \\n class=\"fb-xfbml-parse-ignore\"&gt;Share&lt;/a&gt;&lt;/div&gt;'.format(fblink(link)))\nprint('')\nprint('&lt;script src=\"https://platform.linkedin.com/in.js\" type=\"text/javascript\"&gt;lang: en_US&lt;/script&gt;')\nprint('&lt;script type=\"IN/Share\" data-url=\"{}\"&gt;&lt;/script&gt; '.format(link)) \nprint(' ')\nprint('&lt;a href=\"https://twitter.com/share?ref_src=twsrc%5Etfw\" class=\"twitter-share-button\" \\n data-url=\"{}\" data-show-count=\"true\"&gt;Tweet&lt;/a&gt;'.format(link))\nprint('&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;')\nprint('&lt;/div&gt;')\nprint('')\nprint('&lt;div class=\"fb-comments\" data-href=\"{}\"\\n data-width=\"\" data-numposts=\"5\"&gt;&lt;/div&gt;'.format(link)) \n# Then you can directly post the following output at the bottom of your page \n\n&lt;div id=\"fb-root\"&gt;&lt;/div&gt;\n&lt;script async defer crossorigin=\"anonymous\"\n src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"&gt;&lt;/script&gt;\n \n&lt;div class=\"share-buttons\"&gt;\n&lt;div class=\"fb-share-button\" data-href=\"https://mrislambd.github.io/posts/social-share/\"\ndata-layout=\"button_count\" data-size=\"small\"&gt;&lt;a target=\"_blank\" \n href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fmrislambd.github.io%2Fposts%2Fsocial-share%2F&amp;src=sdkpreparse\" \n class=\"fb-xfbml-parse-ignore\"&gt;Share&lt;/a&gt;&lt;/div&gt;\n\n&lt;script src=\"https://platform.linkedin.com/in.js\" type=\"text/javascript\"&gt;lang: en_US&lt;/script&gt;\n&lt;script type=\"IN/Share\" data-url=\"https://mrislambd.github.io/posts/social-share/\"&gt;&lt;/script&gt; \n \n&lt;a href=\"https://twitter.com/share?ref_src=twsrc%5Etfw\" class=\"twitter-share-button\" \n data-url=\"https://mrislambd.github.io/posts/social-share/\" data-show-count=\"true\"&gt;Tweet&lt;/a&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n&lt;/div&gt;\n\n&lt;div class=\"fb-comments\" data-href=\"https://mrislambd.github.io/posts/social-share/\"\n data-width=\"\" data-numposts=\"5\"&gt;&lt;/div&gt;\n\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Inference in Machine Learning: Part 1\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nSunday, July 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData collection and process for data science and machine learning projects\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nWednesday, August 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to topCitationBibTeX citation:@online{islam2024,\n  author = {Islam, Rafiq},\n  title = {How to Generate Social Share Buttons},\n  date = {2024-07-17},\n  url = {https://mrislambd.github.io/posts/socialshare/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2024. “How to Generate Social Share Buttons.”\nJuly 17, 2024. https://mrislambd.github.io/posts/socialshare/."
  },
  {
    "objectID": "posts/montecarlo1/index.html",
    "href": "posts/montecarlo1/index.html",
    "title": "Monte-Carlo Methods: PRNGs",
    "section": "",
    "text": "The Monte Carlo method is a widely used statistical technique that leverages the power of randomness to solve complex mathematical problems and simulate the behavior of various systems. It’s a method that has found applications across diverse fields, including physics, finance, engineering, and biology. In this blog post, we’ll dive deeper into the Monte Carlo method and explore the mathematics behind it, along with a discussion of random number generators like Linear Congruential Generators (LCGs) and the infamous RANDU.\n\n\n\n\nThe Monte Carlo method is based on the idea of using randomness to approximate solutions to problems that may be deterministic in nature but are too complex for analytical methods. The name “Monte Carlo” is a nod to the randomness associated with the famous casino in Monaco.  The basic principle behind the Monte Carlo method is to simulate the behavior of a system by generating random samples and using them to estimate the desired quantities. Let’s consider a mathematical problem where we need to compute an integral that does not have a straightforward analytical solution:\n\n\\[\\begin{align*}\nI &= \\int_{a}^{b} f(x) \\, dx\n\\end{align*}\\]\n\nThe Monte Carlo method approximates this integral by sampling random points \\(x_i\\) uniformly from the interval \\([a, b]\\) and evaluating the function \\(f(x)\\) at these points. The integral can then be approximated as:\n\n\\[\\begin{align*}\nI \\approx \\frac{b - a}{N} \\sum_{i=1}^{N} f(x_i)\n\\end{align*}\\]\n\nwhere \\(N\\) is the number of random samples. As \\(N\\) increases, the approximation becomes more accurate, thanks to the Law of Large Numbers.  This approach is particularly useful for high-dimensional integrals, where traditional numerical integration methods become computationally expensive or infeasible.\n\n\n\n\n\nAt the heart of the Monte Carlo method lies the generation of random numbers. In practice, most simulations do not use true random numbers but rather pseudorandom numbers generated by deterministic algorithms. These pseudorandom number generators (PRNGs) produce sequences that mimic the properties of true randomness.\n\n\n\nOne of the most commonly used PRNGs is the Linear Congruential Generator (LCG). The LCG generates a sequence of numbers \\(X_1, X_2, X_3, \\ldots\\) using the recursive relation:\n\\[\\begin{align*}\nX_{n+1} &= (aX_n + c) \\mod m\n\\end{align*}\\]\nwhere:\n\n\\(X_n\\) is the \\(n\\)-th number in the sequence.\n\\(a\\) is the multiplier.\n\\(c\\) is the increment.\n\\(m\\) is the modulus.\n\n\nThe sequence starts with an initial value \\(X_0\\), known as the seed, and the parameters \\(a\\), \\(c\\), and \\(m\\) are carefully chosen to maximize the period and quality of the generated sequence.  The quality of the LCG depends on the choice of these parameters. For instance, to achieve a full period (i.e., the sequence cycles through all possible values before repeating), the following conditions must be met:\n\n\n\\(c\\) and \\(m\\) must be relatively prime.\n\\(a - 1\\) must be divisible by all prime factors of \\(m\\).\nIf \\(m\\) is divisible by 4, then \\(a - 1\\) must also be divisible by 4.\n\nA well-known example of an LCG is the minstd_rand generator used in the C++ Standard Library, which uses \\(a = 16807\\), \\(c = 0\\), and \\(m = 2^{31} - 1\\).\n\n\n\nRANDU is an example of a poorly designed LCG that became notorious for its flaws. It is defined by the recurrence relation:\n\\[\\begin{align*}\nX_{n+1} &= (65539X_n) \\mod 2^{31}\n\\end{align*}\\]\n\nAlthough RANDU was widely used in the 1960s and 1970s due to its simplicity, it was later discovered to produce sequences with significant correlations. For example, points generated using RANDU tend to lie on a small number of planes in three-dimensional space, which can severely impact the accuracy of Monte Carlo simulations.  The generator’s flaws arise from poor parameter selection. In RANDU, the modulus \\(m = 2^{31}\\) and the multiplier \\(a = 65539\\) result in a sequence with poor distribution properties. As a consequence, RANDU’s generated numbers do not pass modern statistical tests for randomness, rendering it unsuitable for serious applications.  Let’s solve some math problems and visualize randomness.\n\n\n\n\nGiven an LCG with parameters \\(a,c,m\\), prove that\n\nwhich shows that the \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\nWe know from D. H. Lehmer’s linear congruential generator that\n\\[\\begin{equation}\nx_n \\equiv ax_{n-1}+c \\mod m\n\\end{equation}\\]\nwhere \\(a\\) is called the multiplier, \\(c\\) is called the shift or increment, and \\(m\\) is called the modulus of the generator. The given equation is also an LCG. We can prove this by induction method. Since \\(k\\ge 0\\) so, let \\(k=0\\). Then the given relation can be written as\n\nIf \\(k=1\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+1}& \\equiv ax_n+\\frac{a-1}{a-1}c \\mod m\\\\\n&\\equiv ax_n+c \\mod m\n\\end{align*}\\]\nIf \\(k=2\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+2}& \\equiv a^2x_n+\\frac{a^2-1}{a-1}c \\mod m\\\\\n&\\equiv a^2x_n+(a+1)c \\mod m\\\\\n&\\equiv a^2x_n+ac+c \\mod m \\\\\n&\\equiv a(ax_n+c)+c \\mod m\\\\\n&\\equiv ax_{n+1}+c \\mod m\n\\end{align*}\\]\nNow for any \\(k=p\\) where \\(p\\in \\mathbb{N}\\), \\[\\begin{align*}\nx_{n+p}& \\equiv a^px_n+\\frac{a^p-1}{a-1}c \\mod m \\\\\n\\end{align*}\\]\nNow by the method of induction, the given equation would be a lcg if it holds for any \\(k=p\\in \\mathbb{N}\\) then it must hold for \\(k=p+1\\) where \\(p\\in \\mathbb{N}\\). Now from equation (1) \\[\\begin{align*}\nx_{n+p+1} &\\equiv ax_{(n+p+1)-1}+c \\mod m\\\\\n& \\equiv ax_{n+p}+c \\mod m \\\\\n& \\equiv a(a^px_n+\\frac{a^p-1}{a-1}c) +c \\mod m\\\\\n& \\equiv a^{p+1}x_n+(a\\frac{a^p-1}{a-1}+1)c \\mod m\\\\\n& \\equiv a^{p+1}x_n+\\frac{a^{p+1}-1}{a-1}c \\mod m\\\\\n\\end{align*}\\]\nWhich proves that \\(x_{n+k}=a^kx_n+\\frac{(a^k-1)}{a-1}c (\\mod m)\\); \\((a\\ge 2, k\\ge0)\\) is an lcg such that \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\n(a)\nIf \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\) show that \\(U+V (\\mod 1)\\) is also \\(U(0,1)\\).\nSolution\nLet \\(Z=U+V\\) where \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\). So the minimum value that \\(Z\\) can have is \\(0\\) and the maximum value could be \\(2\\). If \\(f_U(u)\\) is the PDF of \\(U\\) and \\(f_V(v)\\) is the PDF of \\(V\\) then the PDF of \\(Z\\) can be found from the convolution of two distribution as follows \\[\\begin{align*}\n  f_Z(z)=\\int_{-\\infty}^{+\\infty}f_U(u)f_V(z-u)du=\\begin{cases}\n          z & \\text{for} \\hspace{2mm} 0 &lt; z &lt; 1\\\\\n          2-z & \\text{for} \\hspace{2mm} 1 \\le z &lt;2\\\\\n          0 & \\text{otherwise}\n         \\end{cases}\n\\end{align*}\\] Now for any \\(x\\in (0,1)\\) \\[\\begin{align*}\n  \\mathbb{P}(U+V (\\mod 1) \\le x) &= \\mathbb{P}(Z \\le x)+ \\mathbb{P}(1\\le Z \\le x+1)\\\\\n                                 &= \\int_{0}^{x} z dz +\\int_{1}^{1+x}(2-z)dz\\\\\n                                 &=x\n\\end{align*}\\]\nwhich is the CDF of a random variable distributed \\(U(0,1)\\)\n(b)\nA random number generator is designed by\n\nwhere \\(X_0=0, Y_0=1, X_{n+1}=(9X_n+3) \\mod 8\\) and \\(Y_{n+1}=3Y_n \\mod 7\\) for \\(n=0,1,2,\\cdots\\). Calculate \\(R_0,R_1,R_2, \\cdots , R_5.\\). What is the period of the generator \\(\\{R_n\\}\\)?\nSolution\n\n\nCode\nrand.gen&lt;-function(n){\n  RN&lt;-vector(length = n)\n  x&lt;-rep(n)\n  y&lt;-rep(n)\n  x[1]&lt;-0;\n  y[1]&lt;-1;\n  RN[1]&lt;-(x[1]/8+y[1]/7)%% 1\n  for (i in 1:n) {\n    x[i+1]&lt;-(9*x[i]+3)%% 8\n    y[i+1]&lt;-(3*y[i]) %% 7\n    RN[i+1]&lt;-(x[i+1]/8+y[i+1]/7)%% 1\n  }\n  return(data.frame(X_values=x,Y_values=y,R_values=RN))\n}\nrand.gen(4)  \n\n\n  X_values Y_values   R_values\n1        0        1 0.14285714\n2        3        3 0.80357143\n3        6        2 0.03571429\n4        1        6 0.98214286\n5        4        4 0.07142857\n\n\nSo the unique values are\n\n\n     R_values\n1  0.14285714\n2  0.80357143\n3  0.03571429\n4  0.98214286\n5  0.07142857\n6  0.58928571\n7  0.39285714\n8  0.05357143\n9  0.28571429\n10 0.23214286\n11 0.32142857\n12 0.83928571\n13 0.64285714\n14 0.30357143\n15 0.53571429\n16 0.48214286\n17 0.57142857\n18 0.08928571\n19 0.89285714\n20 0.55357143\n21 0.78571429\n22 0.73214286\n23 0.82142857\n24 0.33928571\n\n\nSo from the above data we can see that the period is \\(24\\).\n\n\n\nWrite a code that would implement RANDU. For debugging purpose print \\(x_{1000}\\) when the seed is \\(x_0=1\\)\n(a)\nUsing RANDU generate \\(u_1,u_2,\\cdots, u_{20,002}\\) where \\(u=\\frac{x_n}{M}\\). For all triplets in your sequence, \\(u_i, u_{i+1}, u_{i+2}\\), in which \\(0.5\\le u_{i+1} \\le 0.51\\) plot \\(u_i\\) versus \\(u_{i+2}\\). Comment on the pattern of your scatterplot.\n\n\n\n\n\n\n\n\n\n(b)\nGenerate a sequence of lenght 1002. Use a program that plots points in 3 dimensions and rotates the axes to rotate the points until you can see the 15 planes.\n\n\nCode\nlibrary(\"rgl\")\nlibrary(\"rglwidget\")\n\nN = 1002\nA = matrix(0, ncol=3, nrow=N)\nseed &lt;- as.double(1)\n\nRANDU &lt;- function() {\n  seed &lt;&lt;- ((2^16 + 3) * seed) %% (2^31)\n  round(seed/(2^31), 6)\n}\n\nfor (i in 1:N) {\n  A[i, ] &lt;- c(RANDU(), RANDU(), RANDU())\n}\nB = as.data.frame(A)\n\nbg3d(color = \"#f4f4f4\")\nplot3d(B$V1, B$V2, B$V3, type=\"s\", size=1, lit=TRUE, col = rainbow(1000))\nspin &lt;- spin3d(axis= c(0, 0, 1), rpm = 5)\nplay3d(spin, duration = 10)\n\n# Render the 3D plot as a WebGL widget\nrglwidget()\n\n\n\n\n\n\nCode\nrgl.close()\n\n\nThat’s all for this post."
  },
  {
    "objectID": "posts/montecarlo1/index.html#introduction",
    "href": "posts/montecarlo1/index.html#introduction",
    "title": "Monte-Carlo Methods: PRNGs",
    "section": "",
    "text": "The Monte Carlo method is a widely used statistical technique that leverages the power of randomness to solve complex mathematical problems and simulate the behavior of various systems. It’s a method that has found applications across diverse fields, including physics, finance, engineering, and biology. In this blog post, we’ll dive deeper into the Monte Carlo method and explore the mathematics behind it, along with a discussion of random number generators like Linear Congruential Generators (LCGs) and the infamous RANDU.\n\n\n\n\nThe Monte Carlo method is based on the idea of using randomness to approximate solutions to problems that may be deterministic in nature but are too complex for analytical methods. The name “Monte Carlo” is a nod to the randomness associated with the famous casino in Monaco.  The basic principle behind the Monte Carlo method is to simulate the behavior of a system by generating random samples and using them to estimate the desired quantities. Let’s consider a mathematical problem where we need to compute an integral that does not have a straightforward analytical solution:\n\n\\[\\begin{align*}\nI &= \\int_{a}^{b} f(x) \\, dx\n\\end{align*}\\]\n\nThe Monte Carlo method approximates this integral by sampling random points \\(x_i\\) uniformly from the interval \\([a, b]\\) and evaluating the function \\(f(x)\\) at these points. The integral can then be approximated as:\n\n\\[\\begin{align*}\nI \\approx \\frac{b - a}{N} \\sum_{i=1}^{N} f(x_i)\n\\end{align*}\\]\n\nwhere \\(N\\) is the number of random samples. As \\(N\\) increases, the approximation becomes more accurate, thanks to the Law of Large Numbers.  This approach is particularly useful for high-dimensional integrals, where traditional numerical integration methods become computationally expensive or infeasible.\n\n\n\n\n\nAt the heart of the Monte Carlo method lies the generation of random numbers. In practice, most simulations do not use true random numbers but rather pseudorandom numbers generated by deterministic algorithms. These pseudorandom number generators (PRNGs) produce sequences that mimic the properties of true randomness.\n\n\n\nOne of the most commonly used PRNGs is the Linear Congruential Generator (LCG). The LCG generates a sequence of numbers \\(X_1, X_2, X_3, \\ldots\\) using the recursive relation:\n\\[\\begin{align*}\nX_{n+1} &= (aX_n + c) \\mod m\n\\end{align*}\\]\nwhere:\n\n\\(X_n\\) is the \\(n\\)-th number in the sequence.\n\\(a\\) is the multiplier.\n\\(c\\) is the increment.\n\\(m\\) is the modulus.\n\n\nThe sequence starts with an initial value \\(X_0\\), known as the seed, and the parameters \\(a\\), \\(c\\), and \\(m\\) are carefully chosen to maximize the period and quality of the generated sequence.  The quality of the LCG depends on the choice of these parameters. For instance, to achieve a full period (i.e., the sequence cycles through all possible values before repeating), the following conditions must be met:\n\n\n\\(c\\) and \\(m\\) must be relatively prime.\n\\(a - 1\\) must be divisible by all prime factors of \\(m\\).\nIf \\(m\\) is divisible by 4, then \\(a - 1\\) must also be divisible by 4.\n\nA well-known example of an LCG is the minstd_rand generator used in the C++ Standard Library, which uses \\(a = 16807\\), \\(c = 0\\), and \\(m = 2^{31} - 1\\).\n\n\n\nRANDU is an example of a poorly designed LCG that became notorious for its flaws. It is defined by the recurrence relation:\n\\[\\begin{align*}\nX_{n+1} &= (65539X_n) \\mod 2^{31}\n\\end{align*}\\]\n\nAlthough RANDU was widely used in the 1960s and 1970s due to its simplicity, it was later discovered to produce sequences with significant correlations. For example, points generated using RANDU tend to lie on a small number of planes in three-dimensional space, which can severely impact the accuracy of Monte Carlo simulations.  The generator’s flaws arise from poor parameter selection. In RANDU, the modulus \\(m = 2^{31}\\) and the multiplier \\(a = 65539\\) result in a sequence with poor distribution properties. As a consequence, RANDU’s generated numbers do not pass modern statistical tests for randomness, rendering it unsuitable for serious applications.  Let’s solve some math problems and visualize randomness.\n\n\n\n\nGiven an LCG with parameters \\(a,c,m\\), prove that\n\nwhich shows that the \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\nWe know from D. H. Lehmer’s linear congruential generator that\n\\[\\begin{equation}\nx_n \\equiv ax_{n-1}+c \\mod m\n\\end{equation}\\]\nwhere \\(a\\) is called the multiplier, \\(c\\) is called the shift or increment, and \\(m\\) is called the modulus of the generator. The given equation is also an LCG. We can prove this by induction method. Since \\(k\\ge 0\\) so, let \\(k=0\\). Then the given relation can be written as\n\nIf \\(k=1\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+1}& \\equiv ax_n+\\frac{a-1}{a-1}c \\mod m\\\\\n&\\equiv ax_n+c \\mod m\n\\end{align*}\\]\nIf \\(k=2\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+2}& \\equiv a^2x_n+\\frac{a^2-1}{a-1}c \\mod m\\\\\n&\\equiv a^2x_n+(a+1)c \\mod m\\\\\n&\\equiv a^2x_n+ac+c \\mod m \\\\\n&\\equiv a(ax_n+c)+c \\mod m\\\\\n&\\equiv ax_{n+1}+c \\mod m\n\\end{align*}\\]\nNow for any \\(k=p\\) where \\(p\\in \\mathbb{N}\\), \\[\\begin{align*}\nx_{n+p}& \\equiv a^px_n+\\frac{a^p-1}{a-1}c \\mod m \\\\\n\\end{align*}\\]\nNow by the method of induction, the given equation would be a lcg if it holds for any \\(k=p\\in \\mathbb{N}\\) then it must hold for \\(k=p+1\\) where \\(p\\in \\mathbb{N}\\). Now from equation (1) \\[\\begin{align*}\nx_{n+p+1} &\\equiv ax_{(n+p+1)-1}+c \\mod m\\\\\n& \\equiv ax_{n+p}+c \\mod m \\\\\n& \\equiv a(a^px_n+\\frac{a^p-1}{a-1}c) +c \\mod m\\\\\n& \\equiv a^{p+1}x_n+(a\\frac{a^p-1}{a-1}+1)c \\mod m\\\\\n& \\equiv a^{p+1}x_n+\\frac{a^{p+1}-1}{a-1}c \\mod m\\\\\n\\end{align*}\\]\nWhich proves that \\(x_{n+k}=a^kx_n+\\frac{(a^k-1)}{a-1}c (\\mod m)\\); \\((a\\ge 2, k\\ge0)\\) is an lcg such that \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\n(a)\nIf \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\) show that \\(U+V (\\mod 1)\\) is also \\(U(0,1)\\).\nSolution\nLet \\(Z=U+V\\) where \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\). So the minimum value that \\(Z\\) can have is \\(0\\) and the maximum value could be \\(2\\). If \\(f_U(u)\\) is the PDF of \\(U\\) and \\(f_V(v)\\) is the PDF of \\(V\\) then the PDF of \\(Z\\) can be found from the convolution of two distribution as follows \\[\\begin{align*}\n  f_Z(z)=\\int_{-\\infty}^{+\\infty}f_U(u)f_V(z-u)du=\\begin{cases}\n          z & \\text{for} \\hspace{2mm} 0 &lt; z &lt; 1\\\\\n          2-z & \\text{for} \\hspace{2mm} 1 \\le z &lt;2\\\\\n          0 & \\text{otherwise}\n         \\end{cases}\n\\end{align*}\\] Now for any \\(x\\in (0,1)\\) \\[\\begin{align*}\n  \\mathbb{P}(U+V (\\mod 1) \\le x) &= \\mathbb{P}(Z \\le x)+ \\mathbb{P}(1\\le Z \\le x+1)\\\\\n                                 &= \\int_{0}^{x} z dz +\\int_{1}^{1+x}(2-z)dz\\\\\n                                 &=x\n\\end{align*}\\]\nwhich is the CDF of a random variable distributed \\(U(0,1)\\)\n(b)\nA random number generator is designed by\n\nwhere \\(X_0=0, Y_0=1, X_{n+1}=(9X_n+3) \\mod 8\\) and \\(Y_{n+1}=3Y_n \\mod 7\\) for \\(n=0,1,2,\\cdots\\). Calculate \\(R_0,R_1,R_2, \\cdots , R_5.\\). What is the period of the generator \\(\\{R_n\\}\\)?\nSolution\n\n\nCode\nrand.gen&lt;-function(n){\n  RN&lt;-vector(length = n)\n  x&lt;-rep(n)\n  y&lt;-rep(n)\n  x[1]&lt;-0;\n  y[1]&lt;-1;\n  RN[1]&lt;-(x[1]/8+y[1]/7)%% 1\n  for (i in 1:n) {\n    x[i+1]&lt;-(9*x[i]+3)%% 8\n    y[i+1]&lt;-(3*y[i]) %% 7\n    RN[i+1]&lt;-(x[i+1]/8+y[i+1]/7)%% 1\n  }\n  return(data.frame(X_values=x,Y_values=y,R_values=RN))\n}\nrand.gen(4)  \n\n\n  X_values Y_values   R_values\n1        0        1 0.14285714\n2        3        3 0.80357143\n3        6        2 0.03571429\n4        1        6 0.98214286\n5        4        4 0.07142857\n\n\nSo the unique values are\n\n\n     R_values\n1  0.14285714\n2  0.80357143\n3  0.03571429\n4  0.98214286\n5  0.07142857\n6  0.58928571\n7  0.39285714\n8  0.05357143\n9  0.28571429\n10 0.23214286\n11 0.32142857\n12 0.83928571\n13 0.64285714\n14 0.30357143\n15 0.53571429\n16 0.48214286\n17 0.57142857\n18 0.08928571\n19 0.89285714\n20 0.55357143\n21 0.78571429\n22 0.73214286\n23 0.82142857\n24 0.33928571\n\n\nSo from the above data we can see that the period is \\(24\\).\n\n\n\nWrite a code that would implement RANDU. For debugging purpose print \\(x_{1000}\\) when the seed is \\(x_0=1\\)\n(a)\nUsing RANDU generate \\(u_1,u_2,\\cdots, u_{20,002}\\) where \\(u=\\frac{x_n}{M}\\). For all triplets in your sequence, \\(u_i, u_{i+1}, u_{i+2}\\), in which \\(0.5\\le u_{i+1} \\le 0.51\\) plot \\(u_i\\) versus \\(u_{i+2}\\). Comment on the pattern of your scatterplot.\n\n\n\n\n\n\n\n\n\n(b)\nGenerate a sequence of lenght 1002. Use a program that plots points in 3 dimensions and rotates the axes to rotate the points until you can see the 15 planes.\n\n\nCode\nlibrary(\"rgl\")\nlibrary(\"rglwidget\")\n\nN = 1002\nA = matrix(0, ncol=3, nrow=N)\nseed &lt;- as.double(1)\n\nRANDU &lt;- function() {\n  seed &lt;&lt;- ((2^16 + 3) * seed) %% (2^31)\n  round(seed/(2^31), 6)\n}\n\nfor (i in 1:N) {\n  A[i, ] &lt;- c(RANDU(), RANDU(), RANDU())\n}\nB = as.data.frame(A)\n\nbg3d(color = \"#f4f4f4\")\nplot3d(B$V1, B$V2, B$V3, type=\"s\", size=1, lit=TRUE, col = rainbow(1000))\nspin &lt;- spin3d(axis= c(0, 0, 1), rpm = 5)\nplay3d(spin, duration = 10)\n\n# Render the 3D plot as a WebGL widget\nrglwidget()\n\n\n\n\n\n\nCode\nrgl.close()\n\n\nThat’s all for this post."
  },
  {
    "objectID": "posts/montecarlo1/index.html#reference",
    "href": "posts/montecarlo1/index.html#reference",
    "title": "Monte-Carlo Methods: PRNGs",
    "section": "Reference",
    "text": "Reference\n\nOkten, G. (1999). Contributions to the theory of Monte Carlo and quasi-Monte Carlo methods. Universal-Publishers.\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "",
    "section": "",
    "text": "Code\nCurrently I am working under the supervision of professor Lingjiong Zhu. My ongoing research project is on the topic “EXTRA decentralized stochastic gradient Langevin dynamics”."
  },
  {
    "objectID": "research.html#research-interest",
    "href": "research.html#research-interest",
    "title": "",
    "section": "Research Interest",
    "text": "Research Interest\n\nTheoretical Machine Learning: Centralized and Decentralized Stochastic Gradient Descent (SGD);Algorithmic Stability in SGD; Differential Privacy in machine learning algorithms\nApplied Data Science\nFinancial Mathematics"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "",
    "section": "Publications",
    "text": "Publications\n\nYou can also find my articles on my Google Scholar profile.\n\n\nGJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies \nMostafa, F; Saha, P; Islam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management.\nComparison of Financial Models for Stock Price Prediction \nIslam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management."
  },
  {
    "objectID": "research.html#course-projects",
    "href": "research.html#course-projects",
    "title": "",
    "section": "Course Projects",
    "text": "Course Projects\n\nOption pricing techniques: A performance-based comparative study of the randomized quasi-Monte Carlo method and Fourier cosine method\nAdvisor: Prof. Giray Ökten\n\nPricing financial derivatives such as options with desired accuracy can be hard due to the nature of the functions and complicated integrals required by the pricing techniques. In this paper we investigate the pricing methodology of the European style options using two advanced numerical methods, namely, Quasi-Monte Carlo and Fourier Cosine (COS). For the RQMC method, we use the random-start Halton sequence. We use the Black-Scholes-Merton model to measure the pricing quality of both of the methods. For the numerical results we compute the option price of the call option and we found a few reasons to prefer the RQMC method over the COS method to approximate the European style options.\n\nThe Relationship Between Forced Sexual Activities And Suicidal Attempts Of The Victims\nAdvisor: Dr. Andy Chang\n\nIn project, we apply data-analytic methods to further explore the relationship between forced sexual activities and suicidal behavior among adolescents in the United States. Our findings build on existing literature that explores this relationship. The sample of the study was taken from the Youth Risk Behavior Surveillance System survey 2017. We used a chi-squared test to find the association of forced sexual activities and suicidal behavior, and we found a strong association. Then we used bi-variate logistic regression analysis to ascertain the association of race, age, sex, and education with suicidal attempts after experiencing forced sexual activity (sexual assault). The results of the following paper provide greater insight into the relationship between forced sexual activities and suicide attempts by the adolescents.\n\nStudy of Runge-Kutta Method of Higher orders and its Applications\nAdvisor: Dr. Md. Abdus Samad \n\nThis project is concerned with the study on Runge-Kutta method to apply on different order of differential equation and solve different types of problem such as initial value problem and boundary value problem in ordinary differential equation. At first we discuss about the definition and generation of differential equation specially based on partial differential equation and then definition of Runge-kutta method and the derivation of midpoint method and the formula of Runge-Kutta metod of fourth order and sixth order. We also write FORTRAN 90/95 program for different order of Runge-Kutta methods. We have solved some examples of fourth order R-K method and sixth order R-K method to get the application of R-K method. We also compared the solution of R-K method with exact solution for different step sizes. Then we have given simultaneous first order differential equation and second order differential equation and then solved them by fourth order Runge-Kutta method. At last we have discussed the boundary value problem which we have solved by fourth and sixth order R-K method. After that we have written the algorithm of shooting method and showed computer results with the difference between two answer along with percentages of error."
  },
  {
    "objectID": "research.html#talks-and-presentations",
    "href": "research.html#talks-and-presentations",
    "title": "",
    "section": "Talks and Presentations",
    "text": "Talks and Presentations"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nSpring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\n\nFall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\n\nSpring 2023: MAC1140 PreCalculus Algebra\n\n\n\n\n\n\n\nFall 2022: MAC2311 Calculus and Analytic Geometry I\n\n\n\n\n\n\n\nFall 2021 and Spring 2022: PreCalculus and Algebra\n\n\n\n\n\n\n\nFall 2018 to Spring 2020: College Algebra, Trigonometry\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/mathbiology/index.html",
    "href": "posts/mathbiology/index.html",
    "title": "Modeling viral disease",
    "section": "",
    "text": "Consider the spreading of a highly communicable disease on an isolated island with population size \\(N\\). A portion of the population travels abroad and returns to the island infected with the disease. You would like to predict the number of people \\(X\\) who will have been infected by some time \\(t\\). Consider the following model, where \\(k &gt; 0\\) is constant:\n\\[\\begin{equation*}\n  \\frac{dX}{dt}=k\\textcolor{red}{X}(N-X)\n\\end{equation*}\\]\n\nList two major assumptions implicit in the preceding model. How reasonable are your assumptions?\nAnswer: Here are two major assumptions:\n\n\n\nFixed population \\(\\implies\\) all infected. We assume the population size remain unchanged that is no one gets in the island or no one gets out of the island. This will lead everyone affected by the disease eventually.\nNo immediate cure or vaccination. We also assume that there is no immediate hard immunity build up among the population or invention of vaccination.\n\n\nGraph \\(\\frac{dX}{dt}\\) versus \\(X\\)\n\n\n\n\nPhoto\n\n\n\nGraph \\(X\\) versus \\(t\\) if the initial number of infections is \\(X_1 &lt; \\frac{N}{2}\\). Graph \\(X\\) versus \\(t\\) if the initial number of infections is \\(X_2 &gt;\\frac{N}{2}\\).\nAnswer: For equilibrium of the model\n\n\\[\\begin{align*}\n  f(X)&=kX(N-X)=0\\\\\n  \\implies kX&=0 & N-X=0\\\\\n  \\implies X=&0  & X=N\n\\end{align*}\\] For the stability analysis:\n\\[\\begin{align*}\nf(X)&=(kN)X-kX^2 & \\implies f'(X)=kN-2kX\n\\end{align*}\\]\nNow, \\(f'(0)=kN&gt;0\\) therefore, \\(X=0\\) is an unstable equilibrium. And \\(f'(N)=kN-2kN=-kN&lt;0\\) since \\(k, N&gt;0\\). So, \\(X=N\\) is a stable equilibrium.\n\n\n\nequilibrium\n\n\nIf the initial infection \\(X_1&lt;\\frac{N}{2}\\) it might decrease and reach to 0 but that is not a stable equilibrium. So eventually it will hit \\(N\\).\n\nSolve the model given earlier for \\(X\\) as a function of \\(t\\).\nAnswer: Solving the ODE we have\n\n\\[\\begin{align*}\n  \\frac{dX}{dt}&=kX(N-X)\\\\\n  \\text{Since}\\hspace{2mm} X&&gt;0\\\\\n  \\frac{dX}{X(N-X)}&=kdt\\\\\n  \\implies \\int \\frac{dX}{X(N-X)}&=\\int kdt\\\\\n  \\implies \\frac{1}{N}\\int \\left(\\frac{1}{X}+\\frac{1}{N-X}\\right)dX&= \\int kdt\\\\\n  \\implies \\frac{1}{N} \\ln\\left(\\frac{X}{N-X}\\right)&=kt+c\\\\\n  \\implies \\ln\\left(\\frac{X}{N-X}\\right)&=Nkt+Nc\\\\\n  \\implies \\frac{X}{N-X}&=e^{Nkt+Nc}\\\\\n  \\implies X&=Ne^{Nkt+Nc}-Xe^{Nkt+Nc}\\\\\n  \\implies X\\left(1+e^{Nkt+Nc}\\right)&=Ne^{Nkt+Nc}\\\\\n  \\implies X(t)&=\\frac{Ne^{Nkt+Nc}}{1+e^{Nkt+Nc}}\\\\\n  \\implies X(t)&=\\frac{N}{1+e^{-(Nkt+Nc)}}\n\\end{align*}\\]\n\nFrom part (d), find the limit of \\(X\\) as \\(t\\) approaches infinity.\nAnswer:\n\n\\[\\begin{align*}\n\\lim_{t\\longrightarrow \\infty} X(t)&=\\lim_{t\\longrightarrow \\infty} \\frac{N}{1+e^{-(Nkt+Nc)}}=N\n\\end{align*}\\]\n\nConsider an island with a population of \\(5000\\). At various times during the epidemic the number of people infected was recorded as follows:\n\n\n\n\n\\(t\\) (days)\n2\n6\n\n\n\n\n\\(X\\) (People infected)\n\\(1887\\)\n\\(4087\\)\n\n\n\\(\\ln{\\left(\\frac{X}{N-X}\\right)}\\)\n\\(-0.5\\)\n\\(1.5\\)\n\n\n\nDo the collected data support the given model?\nAnswer: If we look at part (d) we have\n\\[\\begin{align*}\n\\ln\\left(\\frac{X}{N-X}\\right)&=Nkt+Nc & \\text(And),\\\\\nX(t)&=\\frac{N}{1+e^{-(Nkt+Nc)}}\n\\end{align*}\\]\nSo we get if \\(2Nk+Nc=-0.5\\) then \\(X(2)=\\frac{5000}{1+e^{0.5}}=1887.703\\), if \\(6Nk+Nc=1.5\\) then \\(X(6)=\\frac{5000}{e^{-1.5}}=4087.87\\), and if \\(10Nk+Nc=3.5\\) then \\(X(10)=\\frac{5000}{e^{-3.5}}=4853.44\\)\nTherefore, the collected data supports the model.\n\nUse the results in part (f) to estimate the constants in the model, and predict the number of people who will be infected by \\(t = 12\\) days.\nAnswer: We have\n\n\\[\\begin{align*}\n2Nk+Nc&=-0.5\\\\\n6Nk+Nc&=1.5\n\\end{align*}\\]\nSolving the above system we have \\(k=\\frac{1}{2N}\\) and \\(c=\\frac{-1.5}{N}\\). If we substitute these values in the solution we got in part (d) we have\n\\[\\begin{align*}\n  X(t)&=\\frac{N}{1+e^{-\\left(\\frac{t}{2}-1.5\\right)}}\n\\end{align*}\\]\nSo, \\(X(12)\\approx 4945\\)\n\n\n\n\n\n\n\n\nShare on\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Inference in Machine Learning: Part 1\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nSunday, July 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to generate social share buttons\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nWednesday, July 17, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Modeling Viral Disease},\n  date = {2021-02-23},\n  url = {https://mrislambd.github.io/posts/mathbiology/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Modeling Viral Disease.” February 23,\n2021. https://mrislambd.github.io/posts/mathbiology/."
  },
  {
    "objectID": "posts/bayesianinference/index.html",
    "href": "posts/bayesianinference/index.html",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields, including machine learning, to make predictions and decisions under uncertainty.\n\nBayes’s theorem is a fundamental result in probability theory that relates the conditional and marginal probabilities of random events. Mathematically,\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\hspace{4mm} \\implies \\mathbb{P}(A|B) \\propto \\mathbb{P}(B|A)\\mathbb{P}(A)\\]\nwhere, \\(A\\) and \\(B\\) are events and \\(\\mathbb{P}(B)\\ne 0\\).\n\n\\(\\mathbb{P}(A|B)\\) is a conditional probability which states the probability of occuring the event \\(A\\) when the event \\(B\\) is given or true. The other name of this quantity is called posterior probability of \\(A\\) given the event \\(B\\) or simply, posterior distribution.\n\n\\(\\mathbb{P}(B|A)\\) is a conditional probability which states the probability of occuring the event \\(B\\) when the event \\(A\\) is given or true. In other terms, \\(\\mathbb{P}(B|A)\\) is the likelihood: the probability of evidence \\(B\\) given that \\(A\\) is true.\n\n\\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B)\\) are the probabilities of occuring \\(A\\) and \\(B\\) respectively, without any dependence on each other. \\(\\mathbb{P}(A)\\) is called the prior probability or prior distribution and \\(\\mathbb{P}(B)\\) is called the marginal likelihood or marginal probabilities.\n\nExample 1\nConsider a medical example where we want to diagnose a disease based on a test result. Let:\n\n\\(D\\) be the event that a patient has the disease.\n\n\\(T\\) be the event that the test result is positive.\n\nWe are interested in finding the probability that a patient has the disease given a positive test result, \\(\\mathbb{P}(D|T)\\).\nGiven:\n\n\\(\\mathbb{P}(T|D) = 0.99\\) (the probability of a positive test result given the patient has the disease).\n\n\\(\\mathbb{P}(D) = 0.01\\) (the prior probability of the disease).\n\n\\(\\mathbb{P}(T|D') = 0.05\\) (the probability of a positive test result given the patient does not have the disease).\n\nFirst, we need to calculate the marginal likelihood \\(P(T)\\): \\[\\begin{align*}\n    \\mathbb{P}(T) &= \\mathbb{P}(T|D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(T|D') \\cdot \\mathbb{P}(D') \\\\\n    \\mathbb{P}(T) &= 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99\\\\\n    \\mathbb{P}(T) &= 0.0099 + 0.0495 \\\\\n    \\mathbb{P}(T) &= 0.0594\n\\end{align*}\\]\nNow, we can apply Bayes’s theorem:\n\\[\\begin{align*}\n    \\mathbb{P}(D|T) &= \\frac{\\mathbb{P}(T|D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(T)}\\\\\n    \\mathbb{P}(D|T) &= \\frac{0.99 \\cdot 0.01}{0.0594}\\\\\n    \\mathbb{P}(D|T) &\\approx 0.1667\n\\end{align*}\\]\nSo, the probability that the patient has the disease given a positive test result is approximately \\(16.67\\%\\).\nExample 2\n\n Assume that you are in a restuarant and you ordered a plate of 3 pancakes. The chef made three pancakes with one in perfect condition, that is not burnt in any side, one with one side burnt, and the last one burnt in both sides. The waiter wanted to stack the pancakes so that the burnt side does not show up when served. However, the chef recommended not to hide the burnt side and asked her to stack the pancakes randomly. What is the likelyhood that the fully burnt pancake will be on the top?\n\nTo solve this problem, we can use Bayesian approach. We denote the event \\(X\\) as the pancake without any burnt, \\(Y\\) with one side burnt, and \\(Z\\) both side burnt. Then we have the following conditional probabilities\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt} | X)&=0\\\\\n    \\mathbb{P}(\\text{top-burnt} | Y)&=\\frac{1}{2}\\\\\n    \\mathbb{P}(\\text{top-burnt} | Z)&=1\\\\\n\\end{align*}\\]\nThe probability of picking a pancake irrespective of their burnt condition is \\(\\frac{1}{3}\\). So,\n\\[\\begin{equation}\n    \\mathbb{P}(X)=\\mathbb{P}(Y)=\\mathbb{P}(Z)=\\frac{1}{3}\n\\end{equation}\\]\nThe marginal probability of having burnt side in the top position\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt})&=\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)\\\\\n    &=0\\cdot \\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}+1\\cdot\\frac{1}{3}\\\\\n    &=\\frac{1}{2}\n\\end{align*}\\]\nNow, we can only have a burnt side on top if either \\(Z\\) is placed in the top or the burnt side of \\(Y\\) is placed in the top. \\[\\begin{align*}\n    \\mathbb{P}(Y|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{2}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}\\\\\n    \\mathbb{P}(Z|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}\n\\end{align*}\\]\nSo the probability of having the fully burnt pancake on the top is \\(\\frac{2}{3}\\)."
  },
  {
    "objectID": "posts/bayesianinference/index.html#introduction",
    "href": "posts/bayesianinference/index.html#introduction",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields, including machine learning, to make predictions and decisions under uncertainty.\n\nBayes’s theorem is a fundamental result in probability theory that relates the conditional and marginal probabilities of random events. Mathematically,\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\hspace{4mm} \\implies \\mathbb{P}(A|B) \\propto \\mathbb{P}(B|A)\\mathbb{P}(A)\\]\nwhere, \\(A\\) and \\(B\\) are events and \\(\\mathbb{P}(B)\\ne 0\\).\n\n\\(\\mathbb{P}(A|B)\\) is a conditional probability which states the probability of occuring the event \\(A\\) when the event \\(B\\) is given or true. The other name of this quantity is called posterior probability of \\(A\\) given the event \\(B\\) or simply, posterior distribution.\n\n\\(\\mathbb{P}(B|A)\\) is a conditional probability which states the probability of occuring the event \\(B\\) when the event \\(A\\) is given or true. In other terms, \\(\\mathbb{P}(B|A)\\) is the likelihood: the probability of evidence \\(B\\) given that \\(A\\) is true.\n\n\\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B)\\) are the probabilities of occuring \\(A\\) and \\(B\\) respectively, without any dependence on each other. \\(\\mathbb{P}(A)\\) is called the prior probability or prior distribution and \\(\\mathbb{P}(B)\\) is called the marginal likelihood or marginal probabilities.\n\nExample 1\nConsider a medical example where we want to diagnose a disease based on a test result. Let:\n\n\\(D\\) be the event that a patient has the disease.\n\n\\(T\\) be the event that the test result is positive.\n\nWe are interested in finding the probability that a patient has the disease given a positive test result, \\(\\mathbb{P}(D|T)\\).\nGiven:\n\n\\(\\mathbb{P}(T|D) = 0.99\\) (the probability of a positive test result given the patient has the disease).\n\n\\(\\mathbb{P}(D) = 0.01\\) (the prior probability of the disease).\n\n\\(\\mathbb{P}(T|D') = 0.05\\) (the probability of a positive test result given the patient does not have the disease).\n\nFirst, we need to calculate the marginal likelihood \\(P(T)\\): \\[\\begin{align*}\n    \\mathbb{P}(T) &= \\mathbb{P}(T|D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(T|D') \\cdot \\mathbb{P}(D') \\\\\n    \\mathbb{P}(T) &= 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99\\\\\n    \\mathbb{P}(T) &= 0.0099 + 0.0495 \\\\\n    \\mathbb{P}(T) &= 0.0594\n\\end{align*}\\]\nNow, we can apply Bayes’s theorem:\n\\[\\begin{align*}\n    \\mathbb{P}(D|T) &= \\frac{\\mathbb{P}(T|D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(T)}\\\\\n    \\mathbb{P}(D|T) &= \\frac{0.99 \\cdot 0.01}{0.0594}\\\\\n    \\mathbb{P}(D|T) &\\approx 0.1667\n\\end{align*}\\]\nSo, the probability that the patient has the disease given a positive test result is approximately \\(16.67\\%\\).\nExample 2\n\n Assume that you are in a restuarant and you ordered a plate of 3 pancakes. The chef made three pancakes with one in perfect condition, that is not burnt in any side, one with one side burnt, and the last one burnt in both sides. The waiter wanted to stack the pancakes so that the burnt side does not show up when served. However, the chef recommended not to hide the burnt side and asked her to stack the pancakes randomly. What is the likelyhood that the fully burnt pancake will be on the top?\n\nTo solve this problem, we can use Bayesian approach. We denote the event \\(X\\) as the pancake without any burnt, \\(Y\\) with one side burnt, and \\(Z\\) both side burnt. Then we have the following conditional probabilities\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt} | X)&=0\\\\\n    \\mathbb{P}(\\text{top-burnt} | Y)&=\\frac{1}{2}\\\\\n    \\mathbb{P}(\\text{top-burnt} | Z)&=1\\\\\n\\end{align*}\\]\nThe probability of picking a pancake irrespective of their burnt condition is \\(\\frac{1}{3}\\). So,\n\\[\\begin{equation}\n    \\mathbb{P}(X)=\\mathbb{P}(Y)=\\mathbb{P}(Z)=\\frac{1}{3}\n\\end{equation}\\]\nThe marginal probability of having burnt side in the top position\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt})&=\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)\\\\\n    &=0\\cdot \\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}+1\\cdot\\frac{1}{3}\\\\\n    &=\\frac{1}{2}\n\\end{align*}\\]\nNow, we can only have a burnt side on top if either \\(Z\\) is placed in the top or the burnt side of \\(Y\\) is placed in the top. \\[\\begin{align*}\n    \\mathbb{P}(Y|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{2}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}\\\\\n    \\mathbb{P}(Z|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}\n\\end{align*}\\]\nSo the probability of having the fully burnt pancake on the top is \\(\\frac{2}{3}\\)."
  },
  {
    "objectID": "posts/bayesianinference/index.html#why-bayesian-inference-in-machine-learning",
    "href": "posts/bayesianinference/index.html#why-bayesian-inference-in-machine-learning",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "Why Bayesian Inference in Machine Learning?",
    "text": "Why Bayesian Inference in Machine Learning?\nBayesian inference plays a crucial role in machine learning, particularly in areas involving uncertainty and probabilistic reasoning. It allows us to incorporate prior knowledge and update beliefs based on new data, which is especially useful in the following applications:\n\nBayesian Networks\nBayesian networks are graphical models that represent the probabilistic relationships among a set of variables. Each node in the network represents a random variable, and the edges represent conditional dependencies. Bayesian networks are used for various tasks such as classification, prediction, and anomaly detection.\n\n\nBayesian Regression\nBayesian regression extends linear regression by incorporating prior distributions on the model parameters. This approach provides a probabilistic framework for regression analysis, allowing for uncertainty in the parameter estimates. The posterior distribution of the parameters is computed using Bayes’s theorem, and predictions are made by averaging over this distribution.\n\n\nSampling Methods\nIn Bayesian inference, exact computation of the posterior distribution is often intractable. Therefore, sampling methods such as Markov Chain Monte Carlo (MCMC) and Variational Inference are used to approximate the posterior distribution. These methods generate samples from the posterior distribution, allowing us to estimate various statistical properties and make inferences.\nMarkov Chain Monte Carlo (MCMC)\nMCMC methods generate a sequence of samples from the posterior distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution. Common MCMC algorithms include the Underdamped and Overdamped Langevin dynamics, Metropolis-Hastings algorithm and the Gibbs sampler.\nExample: Metropolis-Hastings Algorithm\nConsider a posterior distribution \\(P(\\theta|D)\\) where \\(\\theta\\) represents the model parameters and \\(D\\) represents the data. The Metropolis-Hastings algorithm proceeds as follows:\n\nInitialize the parameters \\(\\theta_0\\).\nFor \\(t = 1\\) to \\(T\\):\n\nPropose a new state \\(\\theta'\\) from a proposal distribution \\(Q(\\theta'|\\theta_t)\\).\nCompute the acceptance ratio \\(\\alpha = \\frac{P(\\theta'|D) \\cdot Q(\\theta_t|\\theta')}{P(\\theta_t|D) \\cdot Q(\\theta'|\\theta_t)}\\).\nAccept the new state with probability \\(\\min(1, \\alpha)\\). If accepted, set \\(\\theta_{t+1} = \\theta'\\); otherwise, set \\(\\theta_{t+1} = \\theta_t\\).\n\n\nThe samples \\(\\theta_1, \\theta_2, \\ldots, \\theta_T\\) form a Markov chain whose stationary distribution is the posterior distribution \\(P(\\theta|D)\\).\n\n\nBayesian Inference in Neural Networks\nBayesian methods are also applied to neural networks, resulting in Bayesian Neural Networks (BNNs). BNNs incorporate uncertainty in the network weights by placing a prior distribution over them and using Bayes’s theorem to update this distribution based on the observed data. This allows BNNs to provide not only point estimates but also uncertainty estimates for their predictions.\nIn the next parts, we will talk about different applications of the Bayesian inferences, specifically, sampling problem using Langevin dynamics.\n\n\nReference\n\nPancake problems on mathstackexchance\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/eigen/index.html",
    "href": "posts/eigen/index.html",
    "title": "Generalized eigenvectors and eigenspaces",
    "section": "",
    "text": "Definition: Let \\(\\alpha\\in End(V)\\) and \\(\\lambda\\in spec(\\alpha)\\). A non-zero vector \\(v\\) is called a generalized eigenvector vector of \\(\\alpha\\) associated with \\(\\lambda\\) if \\((\\alpha-\\lambda I)^{k}(v)=0\\) and \\((\\alpha-\\lambda I)^{k-1}(v)\\ne 0\\) for some \\(k\\ge 1\\) where \\(k\\) is called the degree of nilpotence for \\(v\\).\n\n\nLet \\(\\lambda\\in spec(\\alpha)\\). Then, \\(M_{\\lambda}=\\bigcup\\limits_{m=1} ker(\\lambda I-\\alpha)^m\\) is what we call it the generalized eigenspace corresponding to \\(\\lambda\\). Clearly, \\(M_{\\lambda}\\) is the union of the zero vector and the set of all generalized eigenvectors of \\(\\alpha\\) associated with \\(\\lambda\\)\n\n\nFact: \\(M_{\\lambda}\\) is a subspace and \\(\\alpha-\\)invariant and if \\(v\\) is a generalized vector of index \\(k\\) then \\(\\{v,(\\alpha-\\lambda I)v,\\cdots, (\\alpha-\\lambda I)^{k-1}v\\}\\) is linearly independent.\n\n\nProof: Let \\(a\\in \\mathbb{F}\\) and let \\(v,w\\in V\\) be generalized eigenvectors of \\(\\alpha\\) associated with \\(\\lambda\\) of degrees \\(k\\) and \\(h\\) respectively. Then,\n\n\n\\((\\alpha-\\lambda I)^k(v)=0\\) and \\((\\alpha-\\lambda I)^h(w)=0\\)\n\n\n\\(\\implies v\\in ker (\\alpha-\\lambda I)^k\\) and \\(w\\in ker(\\alpha-\\lambda I)^h\\)\n\n\n\\(\\implies v\\in ker (\\alpha-\\lambda I)^{k+h}\\) and \\(w\\in ker(\\alpha-\\lambda I)^{k+h}\\) because \\((\\alpha-\\lambda I)^{k+h}(v)=0\\) and \\((\\alpha-\\lambda I)^{k+h}(w)=0\\)\n\n\n\\(\\implies v+w \\in ker(\\alpha-\\lambda)^{k+h}\\)\n\n\nAnd, \\((\\alpha-\\lambda I)^{k+h}(av)=a.(\\alpha-\\lambda I)^{k+h}(v)=0\\).\n\n\nThis implies that \\(M_{\\lambda}\\) is a subspace of \\(V\\).\n\n\nInvariance: If \\(\\beta\\in End(V)\\) commutes with \\(\\alpha\\) and if \\(v\\) is a generalized eigenvector of \\(\\alpha\\) associated with \\(\\lambda\\) such that \\(v\\in ker(\\alpha-\\lambda I)^k\\) then,\n\n\n\\((\\alpha-\\lambda I)^k\\beta(v)=\\beta(\\alpha-\\lambda I)^k(v)=0_V\\)\n\n\n\\(\\implies \\beta(v)\\) is also a generalized eigenvector of \\(\\alpha\\) associated with \\(\\lambda\\)\n\n\n\n\n\nLinearly Independence: If \\(v\\) is a generalized vector of \\(\\alpha\\) associated with \\(\\lambda\\) then \\((\\alpha-\\lambda I)^k(v)=0\\) and \\((\\alpha-\\lambda I)^{k-1}(v)\\ne 0\\). Now we assume that,\n\n\n\\(c_0v+c_1(\\alpha-\\lambda I)(v)+\\cdots+c_{k-1}(\\alpha-\\lambda I)^{k-1}(v)=0\\).\n\n\nWe need to show that \\(c_i's\\) are zero for \\(0\\le i\\le k-1\\).\n\n\nApplying \\((\\alpha-\\lambda)^{k-1}\\) we get,\n\n\n\\((\\alpha-\\lambda)^{k-1}(c_0v+c_1(\\alpha-\\lambda I)(v)+\\cdots+c_{k-1}(\\alpha-\\lambda I)^{k-1}(v))=0\\)\n\n\n\\(\\implies c_0(\\alpha-\\lambda I)^{k-1}(v)=0\\). Since \\((\\alpha-\\lambda I)^{k-1}(v)\\ne 0\\) so we get \\(c_0=0\\).\n\n\nSimilarly applying \\((\\alpha-\\lambda I)^{k-2},(\\alpha-\\lambda I)^{k-3},\\) and so on, we have\n\n\n\\(c_i=0\\) for \\(1\\le i\\le k-1\\).\n\n\nHence, \\(\\{v,(\\alpha-\\lambda I)v,\\cdots, (\\alpha-\\lambda I)^{k-1}v\\}\\) is linearly independent.\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Inference in Machine Learning: Part 1\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nSunday, July 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to generate social share buttons\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nWednesday, July 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n\n26 min\n\n\n\nRafiq Islam\n\n\nTuesday, November 9, 2021\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Generalized Eigenvectors and Eigenspaces},\n  date = {2021-01-25},\n  url = {https://mrislambd.github.io/posts/eigen/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Generalized Eigenvectors and\nEigenspaces.” January 25, 2021. https://mrislambd.github.io/posts/eigen/."
  },
  {
    "objectID": "posts/someproofs/index.html",
    "href": "posts/someproofs/index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "posts/someproofs/index.html#let-n-be-a-positive-integer.-show-that-every-matrix-a-in-m_n-times-nmathbbr-can-be-written-as-the-sum-of-two-non-singular-matrices.",
    "href": "posts/someproofs/index.html#let-n-be-a-positive-integer.-show-that-every-matrix-a-in-m_n-times-nmathbbr-can-be-written-as-the-sum-of-two-non-singular-matrices.",
    "title": "",
    "section": "1. Let \\(n\\) be a positive integer. Show that every matrix \\(A \\in M_{n \\times n}(\\mathbb{R})\\) can be written as the sum of two non-singular matrices.",
    "text": "1. Let \\(n\\) be a positive integer. Show that every matrix \\(A \\in M_{n \\times n}(\\mathbb{R})\\) can be written as the sum of two non-singular matrices.\nProof: To prove this, we will use two known properties of matrices.\n\n\\(det(A)=\\) Product of the eigenvalues of \\(A\\)\n\nIf \\(\\lambda\\) is an eigenvalue of \\(A\\) then \\(\\lambda+n\\) is an eigenvalue of \\(A+nI\\) matrix.\n\nSince, \\(A\\in M_{n\\times n}(\\mathbb{R}),\\) let \\(\\lambda_i\\) for \\(1\\le i\\le n\\) be the eigenvalues of \\(A\\). The matrix \\(A+(n+1)I\\) has eigenvalues \\(\\lambda_{i+n+1}\\) for \\(1\\le i\\le n\\).\nLet,\n\\[\\begin{align*}\nn&=max\\{|\\lambda_i| : 1\\le i \\le n\\}\\\\\n\\implies& -n\\le \\lambda_i \\le n \\text{ for all }1\\le i \\le n\\\\\n\\implies& -n+n+1\\le \\lambda_i+n+1 \\le n+n+1\\text{ for all }1\\le i \\le n\\\\\n\\implies& 1\\le \\lambda_i+n+1 \\le 2n+1\\text{ for all }1\\le i \\le n\n\\end{align*}\\] Thus, \\(\\lambda_i+n+1\\ge 1\\) that is \\(\\lambda_i+n+1 \\ne 0\\) and \\(0\\) is not an eigenvalue of \\(A\\).\nNow, from property (1), we have,\n\\(det(A)=\\prod_{i=1}^{n}\\lambda_i\\)\nand\n\\[\\begin{align*}\ndet(A+(n+1)I)&=\\prod_{i=1}^{n}(\\lambda_i+n+1)\\ne 0\\\\\n\\end{align*}\\] \\(\\implies A\\text{ or }A+(n+1)I\\) both are non-singular.\n\\(-(n+1)I\\) is of course non-singular.\nThen\n\\(A=(A+(n+1)I)+(-(n+1)I)\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#let-alpha-in-mathcallv-and-dim-vn-infty",
    "href": "posts/someproofs/index.html#let-alpha-in-mathcallv-and-dim-vn-infty",
    "title": "",
    "section": "2. Let \\(\\alpha \\in \\mathcal{L}(V)\\) and \\(\\dim V=n< \\infty\\)",
    "text": "2. Let \\(\\alpha \\in \\mathcal{L}(V)\\) and \\(\\dim V=n&lt; \\infty\\)\n\nSuppose that \\(\\alpha\\) has two distinct eigenvalues \\(\\lambda\\) and \\(\\mu\\). Prove that if \\(\\dim E_{\\lambda}=n-1\\) then \\(\\alpha\\) is diagonalizable.\n\n\nProof: Since \\(\\mu\\) and \\(\\lambda\\) are two distinct eigenvalues associated with \\(\\alpha\\), so \\(V=E_{\\lambda}\\bigoplus E_{\\mu}\\) and \\(\\dim E_{\\lambda}(\\alpha)+\\dim E_{\\mu}(\\alpha)=n\\).\n\n\nHere, \\(\\dim E_{\\mu}(\\alpha)\\ge 1\\) and \\(\\dim E_{\\lambda}(\\alpha)=n-1\\). So,\n\n\n\\(\\dim E_{\\lambda}(\\alpha)+\\dim E_{\\mu}(\\alpha)=n-1+1=n\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#let-alphain-mathcallv-and-0ne-vin-v-where-dim-vn-infty.",
    "href": "posts/someproofs/index.html#let-alphain-mathcallv-and-0ne-vin-v-where-dim-vn-infty.",
    "title": "",
    "section": "3. Let \\(\\alpha\\in \\mathcal{L}(V)\\) and \\(0\\ne v\\in V\\) where \\(\\dim V=n< \\infty\\).",
    "text": "3. Let \\(\\alpha\\in \\mathcal{L}(V)\\) and \\(0\\ne v\\in V\\) where \\(\\dim V=n&lt; \\infty\\).\n\n\nProve that there is a unique monic polynomial \\(p(t)\\) of the smallest degree such that \\(p(\\alpha)(v)=0\\)\n\n\nProof: Since \\(V\\) is finite-dimensional so there exists smallest \\(k\\) such that \\(\\{v,\\alpha(v),\\cdots,\\alpha^{k-1}(v)\\}\\) is linearly independent but \\(\\{v,\\alpha(v),\\cdots,\\alpha^{k-1}(v),\\alpha^k(v)\\}\\) is linearly dependent. So there exists \\(c_0,c_1,\\cdots,c_k\\in \\mathbb{F}\\) not all zero such that\n\n\n\\(c_0v+c_1\\alpha(v)+c_2\\alpha^2(v)+\\cdots+c_{k-1}\\alpha^{k-1}(v)+c_k\\alpha^k(v)=0\\)\n\n\nWithout loss of generality, let’s assume that \\(c_k\\ne 0\\). Then\n\n\n\\(a_0v+a_1\\alpha(v)+a_2\\alpha^2(v)+\\cdots+a_{k-1}\\alpha^{k-1}(v)+\\alpha^k(v)=0\\)\n\n\nwhere, \\(a_i=\\frac{c_i}{c_k}\\) for \\(1\\le i \\le k\\).\n\n\nThus, \\(p(t)=a_0+a_1t+a_2t^2+\\cdots+a_{k-1}t^{k-1}+t^k\\), a unique monic polynomial such that \\(p(\\alpha)(v)=0\\)\n\n\n\n\n\n(ii) Prove that \\(p(t)\\) from (i) divides the minimal polynomial of \\(\\alpha\\)\n\n\nProof: By polynomial division we have,\n\n\n\\(m(t)=p(t)h(t)+r(t)\\) where, \\(m(t)\\) is the minimal polynomial.\n\n\nThen, \\(m(\\alpha)(v)=p(\\alpha)h(\\alpha)(v)+r(\\alpha)(v)\\)\n\n\n\\(\\implies 0=0+r(\\alpha)(v)\\)\n\n\n\\(\\implies r(\\alpha)(v)=0\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#let-abin-m_ntimes-nmathbbf-such-that-there-exists-an-invertible-matrix-sin-m_ntimes-nmathbbf-such-that-sas-1-and-sbs-1-are-upper-triangular-matrices.-show-that-every-eigenvalue-of-ab-ba-is-zero",
    "href": "posts/someproofs/index.html#let-abin-m_ntimes-nmathbbf-such-that-there-exists-an-invertible-matrix-sin-m_ntimes-nmathbbf-such-that-sas-1-and-sbs-1-are-upper-triangular-matrices.-show-that-every-eigenvalue-of-ab-ba-is-zero",
    "title": "",
    "section": "4. Let \\(A,B\\in M_{n\\times n}(\\mathbb{F})\\) such that there exists an invertible matrix \\(S\\in M_{n\\times n}(\\mathbb{F})\\) such that \\(SAS^{-1}\\) and \\(SBS^{-1}\\) are upper triangular matrices. Show that every eigenvalue of \\(AB-BA\\) is zero",
    "text": "4. Let \\(A,B\\in M_{n\\times n}(\\mathbb{F})\\) such that there exists an invertible matrix \\(S\\in M_{n\\times n}(\\mathbb{F})\\) such that \\(SAS^{-1}\\) and \\(SBS^{-1}\\) are upper triangular matrices. Show that every eigenvalue of \\(AB-BA\\) is zero\n\nProof: To prove the above statement, it is enough to show that \\(spec(AB-BA)=\\{0\\}\\)\n\n\nWe know that if \\(C\\) and \\(D\\) are upper triangular matrices then \\(spec(CD-DC)=\\{0\\}\\). Now let’s assume that \\(C=SAS^{-1}\\) and \\(D=SBS^{-1}\\). Then,\n\n\n\\(CD-DC=SAS^{-1}SBS^{-1}-SBS^{-1}SAS^{-1}\\)\n\n\n\\(\\implies CD-DC=SABS^{-1}-SBAS^{-1}\\)\n\n\n\\(\\implies CD-DC=S(AB-BA)S^{-1}\\)\n\n\nHence \\(CD-DC\\) and \\(AB-BA\\) are similar matrices. So they have the same eigenvalues, that is \\(spec(AB-BA)=\\{0\\}\\)."
  },
  {
    "objectID": "posts/someproofs/index.html#caley-hamilton-theorem",
    "href": "posts/someproofs/index.html#caley-hamilton-theorem",
    "title": "",
    "section": "5. Caley-Hamilton Theorem",
    "text": "5. Caley-Hamilton Theorem\n\nTheorem: Let \\(p(t)\\) be the characteristic polynomial of a matrix \\(A\\). Then \\(p(A)=0\\)\n\n\nBefore we start proving the theorem, we need to discuss some basics.\n\n\nFor Linear Operator: If \\(\\alpha\\in \\mathcal{L}(V)\\) and \\(A=\\Phi_{BB}(\\alpha)\\) is a representation matrix of \\(\\alpha\\) with respect to the basis \\(B\\), then \\(p(A)\\) is the representation matrix of \\(p(\\alpha)\\). Thus we also have \\(p(\\alpha)=0\\) if \\(p\\) is the characteristic polynomial of \\(\\alpha\\)\n\n\nAdjoint Matrix Method: If we have a matrix \\(A=[a_{ij}]\\in M_{n\\times n}(\\mathbb{F})\\) then we define the \\(\\textit{adjoint}\\) of \\(A\\) to be the matrix \\(adj(A)=[b_{ij}]\\in M_{n\\times n}(\\mathbb{F})\\), where \\(b_{ij}=(-1)^{i+j}|A_{ji}|\\) for all \\(1 \\le i,j \\le n\\)\n\n\nAnd, \\(A_{ji}\\) is the matrix obtained by deleting the i-th row and j-th column.\n\n\nExample: Let \\(A=\\left(\\begin{array}{ccc}1 &4 &7\\\\2 &5 &8\\\\3 &6 &9\\end{array}\\right)\\)\n\n\n\\(b_{11}=(-1)^{1+1}|A_{11}|=det\\left(\\begin{array}{cc}5 &8\\\\6 &9\\end{array}\\right)=-3\\)\n\n\n\\(b_{12}=(-1)^{1+2}|A_{21}|=-det\\left(\\begin{array}{cc}4 &7\\\\6 &9\\end{array}\\right)=6\\)\n\n\n\\(b_{13}=(-1)^{1+3}|A_{21}|=det\\left(\\begin{array}{cc}4 &7\\\\5 &8\\end{array}\\right)=-3\\)\n\n\n\\(b_{21}=(-1)^{2+1}|A_{12}|=-det\\left(\\begin{array}{cc}2 &8\\\\3 &9\\end{array}\\right)=6\\)\n\n\n\\(b_{22}=(-1)^{2+2}|A_{22}|=det\\left(\\begin{array}{cc}1 &7\\\\3 &9\\end{array}\\right)=-12\\)\n\n\n\\(b_{23}=(-1)^{2+3}|A_{32}|=-det\\left(\\begin{array}{cc}1 &7\\\\2 &8\\end{array}\\right)=6\\)\n\n\n\\(b_{31}=(-1)^{3+1}|A_{13}|=det\\left(\\begin{array}{cc}2 &5\\\\3 &6\\end{array}\\right)=-3\\)\n\n\n\\(b_{32}=(-1)^{3+2}|A_{23}|=-det\\left(\\begin{array}{cc}1 &4\\\\3 &6\\end{array}\\right)=6\\)\n\n\n\\(b_{33}=(-1)^{3+3}|A_{33}|=-det\\left(\\begin{array}{cc}1 &4\\\\2 &5\\end{array}\\right)=-3\\)\n\n\nThus,\n\n\n\\(adj(A)=\\left(\\begin{array}{ccc}-3 &6 &-3\\\\6 &-12 &6\\\\-3 &6 &-3\\end{array}\\right)\\).\n\n\nThe important formula that we are going to use is that,\n\n\n\\(AA^{-1}=I \\implies A\\frac{adj(A)}{det(A)}=I \\implies A.adj(A)=det(A).I\\) (*)\n\n\n\n\n\nProof: Let \\(A\\in M_{n\\times n}(\\mathbb{F})\\) have the minimal polynomial \\(p(t)=t^n+\\sum_{i=0}^{n-1}a_it^i\\).\n\n\nNow let, \\(adj(tI-A)=[g_{ij}(t)]=\\left(\\begin{array}{cccc}g_{11}(t) &g_{12}(t) &\\cdots &g_{1n}(t)\\\\g_{21}(t) &g_{22}(t) &\\cdots &g_{2n}(t)\\\\\\vdots &\\vdots &\\ddots &\\vdots\\\\g_{n1}(t) &g_{n2}(t) &\\cdots &g_{nn}(t)\\end{array}\\right)\\) be the adjoint matrix of \\((tI-A)\\).\n\n\nSince each \\(g_{ij}(t)\\) is a polynomial of degree at most \\(n-1\\), we can write this as, \\(adj(tI-A)=\\sum_{i=1}^{n}B_it^{n-i}\\) where \\(B_i\\in M_{n\\times n}(\\mathbb{F})\\).\n\n\nThen by (*) we have,\n\n\\[\\begin{align*}\np(t)I&=det(tI-A).I=(tI-A)adj(tI-A)=(tI-A)\\sum_{i=1}^{n}B_it^{n-i}\\\\\n\\implies& (a_0+a_1t+a_2t^2+\\cdots+a_{n-1}t^{n-1}+t^n)I=(tI-A)B_1t^{n-1}+\\cdots+(tI-A)B_{n-1}t+(tI-A)B_n\\\\\n\\implies& (a_0+a_1t+a_2t^2+\\cdots+a_{n-1}t^{n-1}+t^n)I=B_1t^n-AB_1t^{n-1}+B_2t^{n-1}-AB_2t^{n-2}+\\cdots+B_{n-1}t^2-AB_n\n\\end{align*}\\]\n\nBy comparing the coefficients, we get\n\n\n\\(B_1=I\\)\n\n\n\\(B_2-AB_1=a_{n-1}I\\)\n\n\n\\(B_3-AB_2=a_{n-2}I\\)\n\n\n\\(\\vdots\\)\n\n\n\\(B_n-AB_{n-1}=a_1I\\)\n\n\n\\(-AB_n=a_0I\\)\n\n\nNow multiply \\(A^{n+1-j}\\) to the \\(j-th\\) equation, and then sum up both sides we get,\n\n\n\\(A^{n+1-1}B_1=IA^{n+1-1}\\hspace{2.3in} \\implies A^nB_1=A^n\\)\n\n\n\\(A^{n+1-2}(B_2-AB_1)=a_{n-1}A^{n+1-2}\\hspace{1in} \\implies A^{n-1}B_2-AB_1=a_{n-1}A^{n-1}\\)\n\n\n\\(A^{n+1-3}(B_3-AB_2)=a_{n-2}A^{n+1-3}\\hspace{1in} \\implies A^{n-2}B_3-A^{n-1}B_2=a_{n-1}A^{n-2}\\)\n\n\n\\(\\vdots\\hspace{5in} \\vdots\\)\n\n\n\\(A^{n+1-n}(B_n-AB_{n-1})=a_1A^{n+1-n}\\hspace{1in} \\implies AB_n-A^2B_{n-1}=a_1A\\)\n\n\n\\(-AB_n=a_0I\\hspace{3.2in}\\implies -AB_n=a_0I\\)\n\n\nIf we add both sides then we obtain, \\(p(A)=0\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#prove-that-the-spectral-radius-of-the-textitmarkov-matrix-is-less-than-or-equal-to-1",
    "href": "posts/someproofs/index.html#prove-that-the-spectral-radius-of-the-textitmarkov-matrix-is-less-than-or-equal-to-1",
    "title": "",
    "section": "6. Prove that the spectral radius of the \\(\\textit{Markov}\\) matrix is less than or equal to 1",
    "text": "6. Prove that the spectral radius of the \\(\\textit{Markov}\\) matrix is less than or equal to 1\n\nWe need to prove that if \\(A\\) is a Markov matrix then \\(\\rho(A)\\le 1\\). Now, what is a Markov matrix?\n\n\nMarkov Matrix: A matrix \\(A=[a_{i,j}]_{n\\times n}\\) is called a Markov matrix if \\(a_{i,j}\\ge 0\\) for all \\(1\\le i,j \\le n\\) and \\(\\sum_{j=1}^{n} a_i=1\\), that is the sum of the elements in any row is equal to 1.\n\n\nExample: If we have a matrix like this, \\(A=\\left(\\begin{array}{ccc}0.2 &0.4 &0.4\\\\0.1 &0.4 &0.5\\\\0.9 &0.1 &0\\end{array}\\right)\\) then \\(A\\) is a Markov matrix.\n\n\n\n\n\nProof: Let \\(\\lambda \\in spec(A)\\) and \\(x=\\left(\\begin{array}{c}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{array}\\right)\\ne 0\\) be a column vector. Then we define, \\(x_h=max\\{|x_i|: 1\\le i \\le n\\}\\) & \\(&gt;0\\). Here we are assuming \\(x_h &gt;0\\) because \\(x\\ne 0\\), as a result at least one of the coordinate of \\(x\\) must be greater than \\(0\\).\n\n\nNow,\n\n\n\\(Ax=\\lambda x=\\left(\\begin{array}{c}\\lambda x_1\\\\ \\lambda x_2 \\\\ \\vdots \\\\ \\lambda x_n\\end{array}\\right)\\)\n\n\n\\(\\implies \\lambda x_h =\\sum_{j=1}^{n} a_{hj}x_j\\)\n\n\n\\(\\implies |\\lambda x_h|=|\\lambda |.|x_h|=|\\sum_{j=1}^{n} a_{hj}x_j|\\le \\sum_{j=1}^{n} |a_{hj}| |x_j|\\)\n\n\n\\(\\implies |\\lambda |.|x_h| \\le (\\sum_{j=1}^{n} |a_{hj}|) |x_h|=1. |x_h|\\)\n\n\n\\(\\implies |\\lambda| \\le 1\\)\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/matmul/index.html",
    "href": "posts/matmul/index.html",
    "title": "Matrix multiplication: Let’s make it less expensive!",
    "section": "",
    "text": "Have you ever wondered why your code takes forever to run? Sometimes a simple code may take significant time because of an inefficient implementation approach. Let’s take a simple example of matrix multiplication, and explore the time and space complexity, specifically focusing on multiplying matrices where one of the matrices is formed as an outer product of a vector with itself.\nMatrix multiplication is a fundamental operation in many areas such as computer graphics, machine learning, and scientific computing. Given two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), the product \\(\\mathbf{AB}\\) or \\(\\mathbf{BA}\\) is a new matrix where each element is computed as the dot product of the corresponding row of \\(\\mathbf{A}\\) and the column of \\(\\mathbf{B}\\) or the other way around.\nConsider the scenario where \\(\\mathbf{A}\\) is an outer product of a column vector \\(\\mathbf{a}\\) with itself, i.e.,\n\\[\\begin{align*}\n\\mathbf{A}=\\mathbf{a} \\mathbf{a}^T&=\\begin{pmatrix}a_1\\\\a_2\\\\\\vdots \\\\a_n\\end{pmatrix}\\begin{pmatrix}a_1&a_2&\\cdots &a_n\\end{pmatrix}\\\\\n&=\\begin{pmatrix}\na_1a_1 & a_1a_2& \\cdots &a_1a_n\\\\\na_2a_1& a_2a_2&\\cdots &a_2a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_na_1 & a_na_2 &\\cdots& a_na_n\\end{pmatrix}\\\\\n\\end{align*}\n\\]\nNow simply, if \\(\\mathbf{B}\\) is another \\(n\\times n\\) matrix, then\n\\[\n\\begin{align*}\n\\mathbf{BA}&=\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1n}\\\\\nb_{21} & b_{22} & \\cdots & b_{2n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{nn}\\\\\n\\end{pmatrix}\\begin{pmatrix}\na_1a_1 & a_1a_2& \\cdots &a_1a_n\\\\\na_2a_1& a_2a_2&\\cdots &a_2a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_na_1 & a_na_2 &\\cdots& a_na_n\\end{pmatrix}\n\\end{align*}\n\\]\nLet’s analyze the complexity of this matrix matrix multiplication.\nWorst Case: The worst case scenario would be performing the multiplication naively without exploiting the rank-1 structure. How? When we compute any element in the resultant matrix \\(\\mathbf{BA}\\) or \\(\\mathbf{AB}\\) we precisely perform \\(n\\) multiplication and there are total \\(n^2\\) elements to compute for a matrix of \\(n\\times n\\). This would result in the standard matrix multiplication time complexity of \\(O(n^3)\\).\n\\[\n\\begin{align*}\n\\mathbf{BA}&=\\begin{pmatrix}\nb_{11}a_1a_1+\\cdots+b_{1n}a_na_1& b_{11}a_1a_2+\\cdots+b_{1n}a_na_2 &\\cdots &\nb_{11}a_1a_n+\\cdots+b_{1n}a_na_n\\\\\nb_{21}a_1a_1+\\cdots+b_{2n}a_na_1&b_{21}a_1a_2+\\cdots+b_{2n}a_na_2 &\\cdots &\nb_{21}a_1a_n+\\cdots+b_{2n}a_na_n\\\\\n\\vdots & \\vdots &\\ddots & \\vdots\\\\\nb_{n1}a_1a_1+\\cdots+b_{nn}a_na_1&b_{n1}a_1a_2+\\cdots+b_{nn}a_na_2 &\\cdots &\nb_{n1}a_1a_n+\\cdots+b_{nn}a_na_n\\end{pmatrix}\n\\end{align*}\n\\]\nBest Case: The best case scenario in terms of time complexity occurs when we exploit the structure of \\(\\mathbf{A}\\). Since \\(\\mathbf{A}\\) is a rank-1 matrix, we can simplify the multiplication: \\[\n\\begin{align*}\n\\mathbf{BA}&=\\mathbf{B}\\begin{pmatrix}\na_1a_1 & a_1a_2& \\cdots &a_1a_n\\\\\na_2a_1& a_2a_2&\\cdots &a_2a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_na_1 & a_na_2 &\\cdots& a_na_n\\end{pmatrix}\\\\\n&\\\\\n&=\\mathbf{B}\\begin{pmatrix}a_1 \\mathbf{a} & a_2 \\mathbf{a} &\\cdots a_n \\mathbf{a}\\end{pmatrix}\\\\\n&=\\begin{pmatrix}a_1 \\mathbf{B}\\mathbf{a} & a_2 \\mathbf{B}\\mathbf{a} &\\cdots a_n \\mathbf{B}\\mathbf{a}\\end{pmatrix}\\\\\n&= (\\mathbf{Ba}) a^T\n\\end{align*}\n\\]\nWe break this algorithm in to two steps.\nStep 1: Since \\(\\mathbf{B}\\) is a matrix of \\(n\\times n\\) and \\(\\mathbf{a}\\) is a matrix of \\(n\\times 1\\), therefore \\(\\mathbf{Ba}\\) is a matrix of size \\(n\\times 1\\) or just a vector of size \\(n\\).\n\\[\n\\begin{align*}\n\\mathbf{Ba}&=\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1n}\\\\\nb_{21} & b_{22} & \\cdots & b_{2n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{nn}\\\\\n\\end{pmatrix}\\begin{pmatrix}a_1\\\\a_2\\\\ \\vdots \\\\a_n \\end{pmatrix}\\\\\n&\\\\\n&=\\begin{pmatrix}\n    b_{11}a_1+b_{12}a_2+\\cdots b_{1n}a_n\\\\\n    b_{21}a_1+b_{22}a_2+\\cdots b_{2n}a_n\\\\\n    \\vdots\\\\\n    b_{n1}a_1+b_{n2}a_2+\\cdots b_{nn}a_n\\\\\n\\end{pmatrix}\n\\end{align*}\n\\] The matrix \\(\\mathbf{Ba}\\) contains \\(n\\) elements where each element takes \\(n\\) multiplications. Thus, computing \\(\\mathbf{Ba}\\) takes \\(O(n^2)\\) time.\nStep 2: Next, we compute \\((\\mathbf{Ba})\\mathbf{a}^T\\).\n\\[\n\\begin{align*}\n(\\mathbf{Ba})\\mathbf{a}^T&=\\begin{pmatrix}ba_1\\\\ ba_2\\\\ \\vdots\\\\ ba_n \\end{pmatrix}\n\\begin{pmatrix}a_{1}& a_{2}& \\cdots a_{n} \\end{pmatrix}\\\\\n&\\\\\n&=\\begin{pmatrix}\n(ba_1)a_1 & (ba_1)a_2 &\\cdots &(ba_1)a_n\\\\\n(ba_2)a_1 & (ba_2)a_2 &\\cdots &(ba_2)a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n(ba_n)a_1 & (ba_n)a_2 &\\cdots &(ba_n)a_n\\\\\n\\end{pmatrix}\n\\end{align*}\n\\] Forming the outer product of \\(\\mathbf{Ba}\\) and \\(\\mathbf{a}^T\\) also takes \\(O(n^2)\\) time. Thus, the best case time complexity is \\(O(n^2)\\).\nWell, how about the other way around? What’s the optimal strategy for \\(\\mathbf{AB}\\)? We can reach similar results in the following way \\[\n\\begin{align*}\n\\mathbf{AB}&=(\\mathbf{a} \\mathbf{a}^T) \\mathbf{B} = \\mathbf{a} (\\mathbf{a}^T \\mathbf{B})\n\\end{align*}\n\\]\nHere, \\(\\mathbf{a}^T \\mathbf{B}\\) is a row vector of size \\(n\\). Computing \\(\\mathbf{a}^T \\mathbf{B}\\) takes \\(O(n^2)\\) time. Then, multiplying the column vector \\(\\mathbf{a}\\) by the resulting row vector forms an \\(n \\times n\\) matrix, also in \\(O(n^2)\\) time. Thus, the best case time complexity is \\(O(n^2)\\). Note, that \\(\\mathbf{AB}\\ne \\mathbf{BA}\\).\nComparison: So, what’s the big difference? There is a significant difference in two algorithms. In the first algorithm the time complexity is \\(O(n^3)\\) where as in the second algorithm the time complexity is \\(O(n^2)+O(n^2)\\) or \\(2O(n^2)\\) or just \\(C\\hspace{1mm} O(n^2)\\). For example, if \\(n=500\\) then the first algorithm requires 125 million multiplications and the second one just takes 500,000 multiplications which is 250 times faster.\nUnderstanding the structure of the matrices involved in multiplication can significantly optimize the performance of our code. By exploiting the rank-1 structure of the outer product matrix \\(\\mathbf{a} = \\mathbf{a} \\mathbf{a}^T\\), we can reduce the time complexity from \\(O(n^3)\\) to \\(O(n^2)\\) in the best case scenario. This optimization can lead to considerable performance improvements, especially for large matrices.\nSpace Complexity: Regardless of the case, the space complexity remains \\(O(n^2)\\) since we need to store the resulting \\(n \\times n\\) matrix \\(\\mathbf{BA}\\).\nPython Code:"
  },
  {
    "objectID": "posts/matmul/index.html#output",
    "href": "posts/matmul/index.html#output",
    "title": "Matrix multiplication: Let’s make it less expensive!",
    "section": "Output",
    "text": "Output\nNaive Multiplication Time: 27.198208 seconds\nOptimized Multiplication Time: 0.001841 seconds\nWhat about when \\(\\mathbf{A}\\) is not given as \\(\\mathbf{A}=\\mathbf{aa}^T\\) (i.e., it’s not a rank-1 matrix)? We simply cannot exploit the same optimization based on the outer product. In this case, we have to use the general matrix multiplication approach, which typically has a time complexity of \\(O(n^3)\\) for naive multiplication. However, there are optimized algorithms that can reduce the time complexity:\n\nStrassen’s Algorithm: Reduces the time complexity to approximately \\(O(n^{2.81})\\)\n\nCoppersmith-Winograd Algorithm: Further reduces the time complexity to approximately \\(O(n^{2.376})\\)\n\nParallel Algorithms: Use parallel computing techniques to perform matrix multiplication more efficiently.\n\nMay be some other day we can talk about these algorithms.\n\n\n\n\n\n\n\n\nShare on\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/matrixrep/index.html",
    "href": "posts/matrixrep/index.html",
    "title": "Matrix Representation: Change of Basis",
    "section": "",
    "text": "Let \\(\\alpha: \\mathcal{P}_2(\\mathbb{R}) \\longrightarrow M_{2\\times 2}(\\mathbb{R})\\) be defined by\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\nFirst, let’s show that \\(\\alpha\\) is a linear transformation. Let \\(f(x),g(x) \\in \\mathcal{P}_2(\\mathbb{R})\\) and \\(a,b\\in \\mathbb{R}\\). Then by definition, we have\n\n\n\\(\\alpha(af(x)+bg(x))=\\left(\\begin{array}{cc}af'(0)+bg'(0)& 2af(1)+2bg(1)\\\\0& af''(3)+bg''(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1.6in}\\)=\\(\\left(\\begin{array}{cc}af'(0)& 2af(1)\\\\0& af''(3)\\end{array}\\right)\\)+\\(\\left(\\begin{array}{cc}bg'(0)& 2bg(1)\\\\0& bg''(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1.6in}\\)=\\(a\\left(\\begin{array}{cc}f'(0)& 2f(1)\\\\0& f''(3)\\end{array}\\right)\\)+\\(b\\left(\\begin{array}{cc}g'(0)& 2g(1)\\\\0& g''(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1.6in}=a\\alpha(f(x))+b\\alpha(g(x))\\)\n\n\nSo that \\(\\alpha\\) is a linear transformation.\n\n\nSecond, we find the kernel space \\(ker(\\alpha)\\), then use the Dimension Theorem (formula) to decide the rank of \\(\\alpha\\)\n\n\nThe kernel of \\(\\alpha\\) is defined as\n\n\n\\(ker(\\alpha)=\\{v\\in V|\\alpha(v)=0_{M_{2\\times2}(\\mathbb{R})}\\}\\)\n\n\n\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)=[0]\\)\n\n\n\\(\\implies f'(0)=0, 2f(1)=0, f''(3)=0\\)\n\n\nIf \\(f(x)=a+bx+cx^2\\) then we have,\n\n\n\\(\\begin{array}{c}f'(0)\\implies b=0\\\\2f(1)=0\\implies 2(a+b+c)=0\\\\f''(3)=0\\implies 2c=0\\end{array}\\)\n\n\n\\(\\implies a=b=c=0 \\implies ker(\\alpha)=\\{0_{\\mathcal{P}_2(\\mathbb{R})}\\}\\)\n\n\nThen \\(nullity(\\alpha)=\\dim ker(\\alpha)=0\\) and if we use the dimension formula then, \\(rank(\\alpha)=\\dim \\mathcal{P}_2(\\mathbb{R})-nullity(\\alpha)=3-0=3\\)\n\n\nThird, we will find the representation matrix \\(\\phi_{BD}(\\alpha)\\), where \\(B=\\{1+x,1-x,x^2\\}\\) is an ordered basis for \\(\\mathcal{P}_2(\\mathbb{R})\\)\n\n\nand\n\n\\(D=\\begin{Bmatrix}\\begin{bmatrix}1 & 0\\\\0 &0\\end{bmatrix},\\begin{bmatrix}0 & 1\\\\0 &0\\end{bmatrix},\\begin{bmatrix}0 & 0\\\\1 &0\\end{bmatrix},\\begin{bmatrix}0 & 0\\\\0 &1\\end{bmatrix}\\end{Bmatrix}\\)\nis an ordered basis for \\(\\mathbf{M}_{2\\times 2}(\\mathbb{R})\\)\n\n\n\nNow if \\(f(x)=1+x\\) then\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{cc}1& 4\\\\0& 0\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{c}1\\\\4\\\\0\\\\0\\end{array}\\right)\\)\n\n\n\n\n\n\nNow if \\(f(x)=1-x\\) then\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{cc}-1& 0\\\\0& 0\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{c}-1\\\\0\\\\0\\\\0\\end{array}\\right)\\)\n\n\n\n\n\n\nNow if \\(f(x)=x^2\\) then\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{cc}0& 2\\\\0& 2\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{c}-2\\\\2\\\\0\\\\2\\end{array}\\right)\\)\n\n\n\n\n\nbecause,\n\n\n\n\n\n\n\\(\\left(\\begin{array}{cc}0& 2\\\\0& 2\\end{array}\\right)\\)\\(=-2\\left(\\begin{array}{cc}1& 0\\\\0& 0\\end{array}\\right)\\)\\(+2\\left(\\begin{array}{cc}0& 1\\\\0& 0\\end{array}\\right)\\)\\(+0\\left(\\begin{array}{cc}0& 0\\\\1& 0\\end{array}\\right)\\)\\(+2\\left(\\begin{array}{cc}1& 0\\\\0& 1\\end{array}\\right)\\)\n\n\n\n\n\nTherefore, \\(\\phi_{BD}(\\alpha)=\\)\\(\\left(\\begin{array}{ccc}1& -1& -2\\\\4& 0& 2\\\\0& 0& 0\\\\0& 0& 2\\end{array}\\right)\\)\n\n\n\n\n\n\n\n\n\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Inference in Machine Learning: Part 1\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nSunday, July 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to generate social share buttons\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nWednesday, July 17, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Matrix {Representation:} {Change} of {Basis}},\n  date = {2021-01-21},\n  url = {https://mrislambd.github.io/posts/matrixrep/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Matrix Representation: Change of\nBasis.” January 21, 2021. https://mrislambd.github.io/posts/matrixrep/."
  },
  {
    "objectID": "publication/pub1/index.html",
    "href": "publication/pub1/index.html",
    "title": "Comparison of financial models for stock price prediction",
    "section": "",
    "text": "Time series analysis of daily stock data and building predictive models are complicated. This project presents a comparative study for stock price prediction using three different methods, namely autoregressive integrated moving average, artificial neural network, and stochastic process-geometric Brownian motion. Each of the methods is used to build predictive models using historical stock data collected from Yahoo Finance. Finally, output from each of the models is compared to the actual stock price. Empirical results show that the conventional statistical model and the stochastic model provide better approximation for next-day stock price prediction compared to the neural network model.\n\nDownload a pdf from here\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@article{rafiqul_islam2020,\n  author = {Rafiqul Islam, Mohammad and Nguyen, Nguyet},\n  publisher = {MDPI},\n  title = {Comparison of Financial Models for Stock Price Prediction},\n  journal = {Journal of Risk and Financial Management},\n  date = {2020-08-14},\n  url = {https://www.mdpi.com/1911-8074/13/8/181},\n  doi = {10.3390/jrfm13080181},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRafiqul Islam, Mohammad, and Nguyet Nguyen. 2020. “Comparison of\nFinancial Models for Stock Price Prediction.” Journal of Risk\nand Financial Management, August. https://doi.org/10.3390/jrfm13080181."
  },
  {
    "objectID": "talks/2020-04-26-project.html",
    "href": "talks/2020-04-26-project.html",
    "title": "Sensitivity analysis for Monte Carlo and Quasi Monte Carlo option pricing",
    "section": "",
    "text": "A copy of the presentation can be found  here \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "talks/2023-10-05-st-op.html",
    "href": "talks/2023-10-05-st-op.html",
    "title": "Decentralized Stochastic Gradient Langevin Dynamics andHamiltonian Monte Carlo",
    "section": "",
    "text": "This presentation is based on the paper Decentralized Stochastic Gradient Langevin Dynamics andHamiltonian Monte Carlo by Dr. Lingjiong Zhu\nA copy of the presentation can be found here\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/sp23.html",
    "href": "teaching/sp23.html",
    "title": "Spring 2023: MAC1140 PreCalculus Algebra",
    "section": "",
    "text": "PreCalculus and Algebra are one of the many important foundation math courses that open doors to many upper-level math and science courses. The topic of this course includes but not is limited to Complex Numbers, Piecewise Functions, Quadratic Functions, Polynomial Functions, Polynomial Division, Zeros of Polynomials, Rational Functions, Polynomial and Rational Inequalities, Inverse Functions, Exponential Functions, Logarithmic Functions, Properties of Logarithms, Exponential and Logarithmic Equations, and so on.  As an Instructor of Record for this course, I taught a class of 27 undergraduate students from different majors. I also proctor their lab classes where they take their quizzes and tests online and other application based lab activities.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/fall23.html",
    "href": "teaching/fall23.html",
    "title": "Fall 2023: MAP4170 Introduction to Actuarial Mathematics",
    "section": "",
    "text": "Worked as a greader for this course.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/f18-f21.html",
    "href": "teaching/f18-f21.html",
    "title": "Fall 2018 to Spring 2020: College Algebra, Trigonometry",
    "section": "",
    "text": "Responsible for the preparation and delivery of all lectures, making question paper for all exams, and the grading of tests and homework assignments for the following courses: - College Algebra, Fall 2018 - Trigonometry, Fall 2019 and Spring 2020\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "portfolio/dp-nlp/index.html",
    "href": "portfolio/dp-nlp/index.html",
    "title": "Disease diagnosis using classification and NLP",
    "section": "",
    "text": "Team Members\nRebecca Ceppas de Castro, Fulya Tastan, Philip Barron, Mohammad Rafiqul Islam, Nina Adhikari, Viraj Meruliya\nAutomatic Symptom Detection (ASD) and Automatic Diagnosis (AD) have seen several advances in recent years. Patients and medical professionals would benefit from tools that can aid in diagnosing diseases based on antecedents and presenting symptoms. The lack of quality healthcare in many parts of the world makes solving this problem a matter of utmost urgency. The aim of this project is to build a tool that can diagnose a disease based on a list of symptoms and contribute to our understanding of automatic diagnosis.\nProject Details\nSlides\nExecutive Summary\nGitHub Repo\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{ceppas_de_castro,_fulya_tastan,_philip_barron,_mohammad_rafiqul_islam,_nina_adhikari,_viraj_meruliya_2024,\n  author = {Ceppas de Castro, Fulya Tastan, Philip Barron, Mohammad\n    Rafiqul Islam, Nina Adhikari, Viraj Meruliya , Rebecca},\n  title = {Disease Diagnosis Using Classification and {NLP}},\n  date = {2024-06-18},\n  url = {https://mrislambd.github.io/portfolio/dp-nlp/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCeppas de Castro, Fulya Tastan, Philip Barron, Mohammad Rafiqul Islam,\nNina Adhikari, Viraj Meruliya, Rebecca. 2024. “Disease Diagnosis\nUsing Classification and NLP.” June 18, 2024. https://mrislambd.github.io/portfolio/dp-nlp/."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nDisease diagnosis using classification and NLP\n\n\n\nTuesday, June 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Application Library: desgld packaging\n\n\n\nFriday, May 3, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/datacollection/index.html",
    "href": "posts/datacollection/index.html",
    "title": "Data collection through Webscraping",
    "section": "",
    "text": "Collecting data and preparing it for a project is one of the most important tasks in any data science or machine learning project. There are many sources from where we can collect data for a project, such as\n\nConnecting to a SQL database server\n\nData Source Websites such as Kaggle, Google Dataset Search, UCI Machine Learning Repo etc\n\nWeb Scraping with Beautiful Soup\nUsing Python API"
  },
  {
    "objectID": "posts/datacollection/index.html#introduction",
    "href": "posts/datacollection/index.html#introduction",
    "title": "Data collection through Webscraping",
    "section": "",
    "text": "Collecting data and preparing it for a project is one of the most important tasks in any data science or machine learning project. There are many sources from where we can collect data for a project, such as\n\nConnecting to a SQL database server\n\nData Source Websites such as Kaggle, Google Dataset Search, UCI Machine Learning Repo etc\n\nWeb Scraping with Beautiful Soup\nUsing Python API"
  },
  {
    "objectID": "posts/datacollection/index.html#section",
    "href": "posts/datacollection/index.html#section",
    "title": "Data collection and process for data science and machine learning projects",
    "section": "",
    "text": "Share on\n\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/datacollection/index.html#data-source-websites",
    "href": "posts/datacollection/index.html#data-source-websites",
    "title": "Data collection through Webscraping",
    "section": "Data Source Websites",
    "text": "Data Source Websites\nData source websites mainly falls into two categories such as data repositories and data science competitions. There are many such websites.\n\nThe UCI Machine Learning Repository\n\nThe Harvard Dataverse\nThe Mendeley Data Repository\nThe 538\nThe New Yourk Times\n\nThe International Data Analysis Olympiad\nKaggle Competition\n\nExample of collecting data from UCI Machine Learning Repository\n\nfrom ucimlrepo import fetch_ucirepo \n  \n# fetch dataset \niris = fetch_ucirepo(id=53) \n  \n# data (as pandas dataframes) \nX = iris.data.features \ny = iris.data.targets \n  \n# metadata \nprint(iris.metadata) \n  \n# variable information \nprint(iris.variables) \n\n{'uci_id': 53, 'name': 'Iris', 'repository_url': 'https://archive.ics.uci.edu/dataset/53/iris', 'data_url': 'https://archive.ics.uci.edu/static/public/53/data.csv', 'abstract': 'A small classic dataset from Fisher, 1936. One of the earliest known datasets used for evaluating classification methods.\\n', 'area': 'Biology', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 150, 'num_features': 4, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1936, 'last_updated': 'Tue Sep 12 2023', 'dataset_doi': '10.24432/C56C76', 'creators': ['R. A. Fisher'], 'intro_paper': {'title': 'The Iris data set: In search of the source of virginica', 'authors': 'A. Unwin, K. Kleinman', 'published_in': 'Significance, 2021', 'year': 2021, 'url': 'https://www.semanticscholar.org/paper/4599862ea877863669a6a8e63a3c707a787d5d7e', 'doi': '1740-9713.01589'}, 'additional_info': {'summary': 'This is one of the earliest datasets used in the literature on classification methods and widely used in statistics and machine learning.  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are not linearly separable from each other.\\n\\nPredicted attribute: class of iris plant.\\n\\nThis is an exceedingly simple domain.\\n\\nThis data differs from the data presented in Fishers article (identified by Steve Chadwick,  spchadwick@espeedaz.net ).  The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features.  ', 'purpose': 'N/A', 'funded_by': None, 'instances_represent': 'Each instance is a plant', 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': None, 'citation': None}}\n           name     role         type demographic  \\\n0  sepal length  Feature   Continuous        None   \n1   sepal width  Feature   Continuous        None   \n2  petal length  Feature   Continuous        None   \n3   petal width  Feature   Continuous        None   \n4         class   Target  Categorical        None   \n\n                                         description units missing_values  \n0                                               None    cm             no  \n1                                               None    cm             no  \n2                                               None    cm             no  \n3                                               None    cm             no  \n4  class of iris plant: Iris Setosa, Iris Versico...  None             no  \n\n\nyou may need to install the UCI Machine Learning Repository as a package using pip.\npip install ucimlrepo\n\nX.head()\n\n\n\n\n\n\n\n\nsepal length\nsepal width\npetal length\npetal width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2"
  },
  {
    "objectID": "posts/datacollection/index.html#references",
    "href": "posts/datacollection/index.html#references",
    "title": "Data collection through Webscraping",
    "section": "References",
    "text": "References\n\nFisher,R. A.. (1988). Iris. UCI Machine Learning Repository.\n\nShare on\n\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/datacollection/index.html#web-scraping",
    "href": "posts/datacollection/index.html#web-scraping",
    "title": "Data collection through Webscraping",
    "section": "Web Scraping",
    "text": "Web Scraping\nWe scrapping is another way of collecting the data for the research if the data is not available in any repositiory. We can collect the data from a website using a library called BeautifulSoup if the website has permision for other people to collect data from the website.\n\nimport bs4                      # library for BeautifulSoup\nfrom bs4 import BeautifulSoup   # import the BeautifulSoup object\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom seaborn import set_style\nset_style(\"whitegrid\")\n\nNow let’s make a html object using BeautifulSoup. Let’s say we have a html website that looks like below\n\nhtml_doc=\"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;title&gt;My Dummy HTML Document&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Welcome to My Dummy HTML Document&lt;/h1&gt;\n    &lt;p&gt;This is a paragraph in my dummy HTML document.&lt;/p&gt;\n    &lt;a href=\"https://mrislambd.github.io/blog\" class=\"blog\" id=\"blog\"&gt; Blog &lt;/a&gt;\n    &lt;a href=\"htpps://mrislambd.github.io/research\" class=\"research\" id=\"research\"&gt; Research &lt;/a&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\nNow we want to grab information from the dummy html documnet above.\n\nsoup=BeautifulSoup(html_doc, features='html.parser')\n\nNow that we have the object soup we can walk through each elements in this object. For example, if we want to grab the title element,\n\nsoup.html.head.title\n\n&lt;title&gt;My Dummy HTML Document&lt;/title&gt;\n\n\nSince the html document has only one title, therefore, we can simply use the following command\n\nsoup.title \n\n&lt;title&gt;My Dummy HTML Document&lt;/title&gt;\n\n\nor this command to get the text only\n\nsoup.title.text\n\n'My Dummy HTML Document'\n\n\nThis soup object is like a family tree. It has parents, children, greatgrand parents etc.\n\nsoup.title.parent\n\n&lt;head&gt;\n&lt;title&gt;My Dummy HTML Document&lt;/title&gt;\n&lt;/head&gt;\n\n\nNow to grab an attribute from the soup object we can use\n\nsoup.a\n\n&lt;a class=\"blog\" href=\"https://mrislambd.github.io/blog\" id=\"blog\"&gt; Blog &lt;/a&gt;\n\n\nor any particular thing from the attribute\n\nsoup.a['class']\n\n['blog']\n\n\nWe can also find multiple attribute of the same kind\n\nsoup.findAll('a')\n\n[&lt;a class=\"blog\" href=\"https://mrislambd.github.io/blog\" id=\"blog\"&gt; Blog &lt;/a&gt;,\n &lt;a class=\"research\" href=\"htpps://mrislambd.github.io/research\" id=\"research\"&gt; Research &lt;/a&gt;]\n\n\nThen if we want any particular object from all a attribute\n\nsoup.findAll('a')[0]['id']\n\n'blog'\n\n\nFor any p tag\n\nsoup.p.text \n\n'This is a paragraph in my dummy HTML document.'\n\n\nSimilarly, if we want to grab all the hrefs from the a tags\n\n[h['href'] for h in soup.findAll('a')]\n\n['https://mrislambd.github.io/blog', 'htpps://mrislambd.github.io/research']"
  },
  {
    "objectID": "posts/datacollection/index.html#example-of-webscraping-from-a-real-website",
    "href": "posts/datacollection/index.html#example-of-webscraping-from-a-real-website",
    "title": "Data collection through Webscraping",
    "section": "Example of Webscraping from a real website",
    "text": "Example of Webscraping from a real website\nIn this example we want to obtain some information from NVIDIA Graduate Fellowship Program. Before accessing this website we need to know if we have permision to access their data through webscraping.\n\nimport requests\nresponse = requests.get(url=\"https://research.nvidia.com/graduate-fellowships/archive\")\nresponse.status_code\n\n200\n\n\nThe status_code \\(200\\) ensures that we have enough permision to acccess their website data. However, if we obtain status_code of \\(403, 400,\\) or \\(500\\) then we do not permision or a bad request. For more about the status codes click here.\n\nsoup = BeautifulSoup(response.text, 'html.parser')\n\nWe want to make an analysis based on the institution of the past graduate fellows. Insepecting the elements in this website we see that the div those have class=\"archive-group\" contains the information of the past graduate fellows.\n\npf = soup.find_all(\"div\", class_=\"archive-group\")\n\nand the first element of this pf contains the information of the graduate fellows in the year of 2021.\n\npf[0]\n\n&lt;div class=\"archive-group\"&gt;\n&lt;h4 class=\"archive-group__title\"&gt;2021 Grad Fellows&lt;/h4&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Alexander Sax&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;University of California, Berkeley&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Hanrui Wang&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Massachusetts Institute of Technology&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Ji Lin&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Massachusetts Institute of Technology&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Krishna Murthy Jatavallabhula&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;University of Montreal&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Rohan Sawhney&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Carnegie Mellon University&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Sana Damani&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Georgia Institute of Technology&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Thierry Tambe&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Harvard University&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Ye Yuan&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Carnegie Mellon University&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Yunzhu Li&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Massachusetts Institute of Technology&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;div class=\"views-row\"&gt;&lt;div class=\"views-field views-field-title\"&gt;&lt;span class=\"field-content\"&gt;Zhiqin Chen&lt;/span&gt;&lt;/div&gt;&lt;div class=\"views-field views-field-field-grad-fellow-institution\"&gt;&lt;div class=\"field-content\"&gt;Simon Fraser University&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;\n&lt;/div&gt;\n\n\nNow let’s make a pandas dataframe using the information in this page. We can make an use of the output from the above chunk. To grab the year, we see that archive-group__title class with a h4 tag contains the year for all years. With strip=True, the text is cleaned by removing extra whitespace from the beginning and end. We need the first element so a split()[0] will do the job. Then we make another group called fellows that contains the fellows in a certian year by using the div and class\"views-row\". Once the new group created, we then iterate through this group to extract their names and corresponding institutions.\n\ndata=[]\n\nfor group in pf:\n    year = group.find(\n        \"h4\",class_=\"archive-group__title\"\n        ).get_text(strip=True).split()[0]\n\n    fellows = group.find_all(\"div\", class_=\"views-row\")\n    for fellow in fellows:\n        name = fellow.find(\n            \"div\", class_=\"views-field-title\"\n            ).get_text(strip=True) \n        institute = fellow.find(\n            \"div\", class_=\"views-field-field-grad-fellow-institution\"\n            ).get_text(strip=True)\n\n        data.append({\"Name\": name, \"Year\": year, \"Institute\": institute})\n\ndata=pd.DataFrame(data)\ndata.head()\n\n\n\n\n\n\n\n\nName\nYear\nInstitute\n\n\n\n\n0\nAlexander Sax\n2021\nUniversity of California, Berkeley\n\n\n1\nHanrui Wang\n2021\nMassachusetts Institute of Technology\n\n\n2\nJi Lin\n2021\nMassachusetts Institute of Technology\n\n\n3\nKrishna Murthy Jatavallabhula\n2021\nUniversity of Montreal\n\n\n4\nRohan Sawhney\n2021\nCarnegie Mellon University\n\n\n\n\n\n\n\nNow let’s perform some Exploratory Data Analysis (EDA). First, we analyze the unique values and distributions.\n\n# Count the number of fellows each year\nyear_counts = data['Year'].value_counts().sort_values(ascending=False)\n# Create a DataFrame where years are columns and counts are values in the next row\nyear_data = {\n    'Year': year_counts.index,\n    'Count': year_counts.values\n}\n# Create the DataFrame\nyear_data_counts = pd.DataFrame(year_data)\n\n# Transpose the DataFrame and reset index to get years as columns\nyear_data_counts = year_data_counts.set_index('Year').T\n\n# Display the DataFrame\nprint(year_data_counts)\n\nYear   2006  2018  2017  2007  2013  2012  2011  2008  2019  2021  2003  2009  \\\nCount    12    11    11    11    11    11    11    10    10    10    10    10   \n\nYear   2010  2005  2015  2004  2016  2002  2020  2014  \nCount     9     8     7     7     6     6     5     5  \n\n\nNext we see that most represented universities\n\nuniversity_counts = data['Institute'].value_counts()\nprint(university_counts.head(10))  # Display the top 10 universities\n\nInstitute\nStanford University                          24\nMassachusetts Institute of Technology        15\nUniversity of California, Berkeley           14\nCarnegie Mellon University                   13\nUniversity of Utah                           10\nUniversity of Washington                      9\nUniversity of Illinois, Urbana-Champaign      9\nUniversity of California, Davis               8\nGeorgia Institute of Technology               8\nUniversity of North Carolina, Chapel Hill     6\nName: count, dtype: int64\n\n\nTo visualize the award distributions per year,\n\nplt.figure(figsize=(9,5))\nsns.countplot(x='Year', data=data, order=sorted(data['Year'].unique()))\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.title('Number of Fellows Per Year')\nplt.show()\n\n\n\n\n\n\n\n\nTop 10 universities visualization\n\nplt.figure(figsize=(6,4))\ntop_universities = data['Institute'].value_counts().head(10)\nsns.barplot(y=top_universities.index, x=top_universities.values)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.title('Top 10 Universities by Number of Fellows')\nplt.xlabel('Number of Fellows')\nplt.ylabel('University')\nplt.show()\n\n\n\n\n\n\n\n\nTrend over time\n\nplt.figure(figsize=(9,5))\ndata['Year'] = data['Year'].astype(int)  \nyearly_trend = data.groupby('Year').size()\nyearly_trend.plot(kind='line', marker='o')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.title('Trend of Fellows Over Time')\nplt.xlabel('Year')\nplt.ylabel('Number of Fellows')\nplt.show()\n\n\n\n\n\n\n\n\nThis is just a simple example of collecting data through webscraping. This BeautifulSoup has endless potentials to use in many projects to collect the data that are not publicly available in cleaned or organized form. Thank you for reading."
  },
  {
    "objectID": "posts/webtemplate/index.html",
    "href": "posts/webtemplate/index.html",
    "title": "Make your personal website, portfolio, and blog using Quarto",
    "section": "",
    "text": "Do you like my website and blog? Let’s make one for you from scratch.\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Inference in Machine Learning: Part 1\n\n\n6 min\n\n\n\nRafiq Islam\n\n\nSunday, July 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData collection through Webscraping\n\n\n7 min\n\n\n\nRafiq Islam\n\n\nWednesday, August 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n\n2 min\n\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to topCitationBibTeX citation:@online{islam2024,\n  author = {Islam, Rafiq},\n  title = {Make Your Personal Website, Portfolio, and Blog Using\n    {Quarto}},\n  date = {2024-08-15},\n  url = {https://mrislambd.github.io/posts/webtemplate/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2024. “Make Your Personal Website, Portfolio, and\nBlog Using Quarto.” August 15, 2024. https://mrislambd.github.io/posts/webtemplate/."
  },
  {
    "objectID": "posts/decisiontree/index.html",
    "href": "posts/decisiontree/index.html",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "",
    "text": "The Decision Tree Classifier is a powerful, interpretable, and widely-used algorithm in machine learning for binary or multi-class classification problems. Its simplicity and visual appeal make it a go-to choice for classification tasks. However, behind this simplicity lies a series of mathematical decisions that guide how the tree is constructed."
  },
  {
    "objectID": "posts/decisiontree/index.html#decision-tree",
    "href": "posts/decisiontree/index.html#decision-tree",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "",
    "text": "The Decision Tree Classifier is a powerful, interpretable, and widely-used algorithm in machine learning for binary or multi-class classification problems. Its simplicity and visual appeal make it a go-to choice for classification tasks. However, behind this simplicity lies a series of mathematical decisions that guide how the tree is constructed."
  },
  {
    "objectID": "posts/decisiontree/index.html#the-core-idea-behind-decision-trees",
    "href": "posts/decisiontree/index.html#the-core-idea-behind-decision-trees",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "The Core Idea Behind Decision Trees",
    "text": "The Core Idea Behind Decision Trees\n\nDecision Tree contains two main type of nodes, decision nodes and leaf nodes. A decision node is a node where a condition is applied to split the data and a leaf node contains the class of a data point. At its heart, a decision tree works by recursively splitting the dataset based on feature values. The goal of each split is to increase the homogeneity of the resulting subgroups, ideally separating the different classes as much as possible. The splitting process relies on a measure of impurity or disorder. The two most common metrics used for this purpose are Gini Impurity and Entropy (used in Information Gain).\n\nGini Impurity\nThe Gini Impurity measures the likelihood of misclassifying a randomly chosen element from the dataset if it were labeled according to the distribution of classes in that subset. Mathematically, the Gini Impurity for a node \\(t\\) is calculated as:\n\\[\\begin{align*}\nG(t) &= 1 - \\sum_{i=1}^{n} p_i^2\n\\end{align*}\\]\nwhere \\(p_i\\) is the proportion of samples belonging to class \\(i\\) at node \\(t\\).\nEntropy and Information Gain\nEntropy, borrowed from information theory, measures the disorder or uncertainty in the dataset. It is defined as:\n\\[H(t) = -\\sum_{i=1}^{n} p_i \\log_2(p_i)\\]\n\n\nCode\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt \n\nx=np.arange(0.01,0.99,0.0001)\ny=[-p*math.log(p,2)-(1-p)*math.log(1-p,2) for p in x]\nplt.plot(x,y)\nplt.xlabel('$p_{\\oplus}$')\nplt.ylabel('$H(t)$')\nplt.title('Entropy')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nInformation Gain is the reduction in entropy after a dataset is split on a feature. It is calculated as:\n\\[IG(D, A) = H(D) - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} H(D_v)\\]\nwhere:\n\n\\(D\\) is the dataset,\n\\(A\\) is the feature on which the split is made,\n\\(D_v\\) is the subset of \\(D\\) for which feature \\(A\\) has value \\(v\\).\n\n\nLet’s explain the math with following example.\nSay, I have the data set like this\n\n\n\n\\(x_0\\)\n\\(x_1\\)\nClass\n\n\n\n\n2\n3\n0\n\n\n3\n4\n0\n\n\n4\n6\n0\n\n\n6\n8\n1\n\n\n7\n10\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\nTotal 20 data points and the scatter plot looks like this\n\n\nCode\ndata = [\n    [2, 3, 0], [3, 4, 0], [4, 6, 0], [6, 8, 1], [7, 10, 1],\n    [8, 12, 1], [5, 7, 1], [2, 5, 0], [9, 15, 1], [1, 2, 0],\n    [11, 3, 0], [4, 13, 1], [8, 14, 1], [1, 5, 0], [6, 2, 1],\n    [9, 3, 1], [15, 13, 0], [7, 5, 0], [5, 9, 0], [8, 3, 1]\n]\n\nx0 = [row[0] for row in data]\nx1 = [row[1] for row in data]\nclasses = [row[2] for row in data]\n\ncolors = ['red' if c == 0 else 'blue' for c in classes]\n\nplt.figure(figsize=(7, 5))\nplt.grid(True)\n\nplt.scatter(x0, x1, color=colors, s=100, edgecolor='black')\n\n# Label points with class values\nfor i in range(len(x0)):\n    plt.text(x0[i] + 0.2, x1[i] + 0.2, str(classes[i]), fontsize=9)\n\n# Set limits for the axes\nplt.xlim(0, 16)\nplt.ylim(0, 16)\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\n# Label axes and show plot\nplt.xlabel('$x_0$')\nplt.ylabel('$x_1$')\nplt.title('Figure 1: Scatter Plot of $x_0$ vs $x_1$ ')\nplt.show()\n\n\n\n\n\n\n\n\n\nAt this point, we see that the classes are not linearly separable, meaning, we can not draw any line that separate the two classes. Notice that the minimum and maximum of feature \\(x_0\\) is 1 and 15, respectively. So, let’s pick a few numbers in between these two numbers. Say, our first number is \\(3.5\\). In the first node, that is the root node, we divide the data based on the feature \\(x_0\\le 3.5\\)\n\n\n\nFigure 2: First Split\n\n\nAt the root node, we have equal number of blue and red points so the proportion of the data class is \\(p_1=p_2=0.5\\), so the entropy\n\\[\\begin{align*}\n    H(\\text{root node})&=-(0.5)\\log_2(0.5)-(0.5)\\log_2(0.5)=1\\\\\n\\end{align*}\\]\nBased on the condition \\(x_0\\le 3.5\\), the left and right child recieves 5 and 15 feature points \\(X=(x_0,x_1)\\), respectively. We see that the left node is a pure node, because it contains only the red points. Therefore, the entropies at these child nodes\n\\[\\begin{align*}\n    H(\\text{left child})&=-1\\log_2(1)-0\\log_2(0)=0\\\\\n    H(\\text{right child})&=-\\frac{5}{15}\\log_2\\left(\\frac{5}{15}\\right)-\\frac{10}{15}\\log_2\\left(\\frac{10}{15}\\right)=0.92\\\\\n\\end{align*}\\]\nand the information gain at this split\n\\[IG(split_1)=1-\\left(\\frac{5}{20}\\cdot 0+\\frac{15}{20}\\cdot 0.92\\right)=0.31\\]\nNow the burning question is how did we select the condition \\(x_0\\le 3.5\\)? It could have been any other number, say we set \\(x_0\\le 6.5\\). Then\n\n\n\nFigure 3: Alternative Split\n\n\nBased on the condition \\(x_0\\le 6.5\\), the left and right child recieves 11 and 9 feature points \\(X=(x_0,x_1)\\), respectively. But in this case we don’t see any pure nodes and the entropies at these child nodes\n\\[\\begin{align*}\n    H(\\text{left child})&=-\\frac{7}{11}\\log_2\\left(\\frac{7}{11}\\right)-\\frac{4}{11}\\log_2\\left(\\frac{4}{11}\\right)=0.95\\\\\n    H(\\text{right child})&=-\\frac{3}{9}\\log_2\\left(\\frac{3}{9}\\right)-\\frac{6}{9}\\log_2\\left(\\frac{6}{9}\\right)=0.92\\\\\n\\end{align*}\\]\nand the information gain at this split\n\\[IG(split_1)=1-\\left(\\frac{11}{20}\\cdot 0.95+\\frac{9}{20}\\cdot 0.92\\right)=0.06\\]\nNote that the information gain is much lower than the first option. Therefore, the first split is better than this alternative split. Because the goal is to have minimum entropy value and/or the maximum information gain. This is where the machine learning gets in the game. The algorithm finds the optimal split based on each feature values.\n\n\n\nFigure 4: Second Split\n\n\nNow say we have a new set of feature values \\((x_0,x_1,Class)=(10,7,1)\\). Based on our tree above, since \\(x_0\\) is NOT less than or equal to \\(3.5\\) so it goes to the right first child. Then it satisfies \\(x_0\\le 10\\). So it moves to the left grand child gradually traverse through the tree and ended up to the very bottom layer left leaf node."
  },
  {
    "objectID": "posts/decisiontree/index.html#building-a-decision-tree",
    "href": "posts/decisiontree/index.html#building-a-decision-tree",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Building a Decision Tree",
    "text": "Building a Decision Tree\n\nChoose the best feature to split on: Calculate Gini impurity or Information Gain for each feature and select the feature that results in the highest Information Gain or lowest Gini impurity.\nSplit the dataset: Partition the data based on the chosen feature and repeat the process for each partition.\nStop conditions: The tree stops growing when all samples in a node belong to the same class, the maximum depth is reached, or further splitting doesn’t add value."
  },
  {
    "objectID": "posts/decisiontree/index.html#implementation-of-decision-tree-scikit-learn",
    "href": "posts/decisiontree/index.html#implementation-of-decision-tree-scikit-learn",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Implementation of Decision Tree: Scikit-learn",
    "text": "Implementation of Decision Tree: Scikit-learn\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier,plot_tree\nfrom sklearn.metrics import accuracy_score\n\nX=pd.DataFrame({'Feature 1':x0, 'Feature 2':x1})\ny=classes\n\n\nclf= DecisionTreeClassifier(criterion=\"entropy\")\nclf.fit(X,y)\n\n\nX_test=pd.DataFrame({'Feature 1':[10,9,11],'Feature 2':[7,9,5]})\ny_test=pd.DataFrame({'Class':[1,0,1]})\n\ntest_data=pd.concat([X_test,y_test], axis=1)\nprint('Test Data \\n')\nprint(test_data)\n\n\ny_prediction=clf.predict(X_test)\nprediction=pd.DataFrame({'Predicted_Class':y_prediction})\nprediction=pd.concat([test_data,prediction],axis=1)\nprint('\\n')\nprint('Result \\n')\nprint(prediction)\nprint('\\n')\nprint('Accuracy score:',round(accuracy_score(y_prediction,y_test),2))\n\nplt.figure(figsize=(11,7))\nplot_tree(clf, filled=True, \n          feature_names=['$x_0$','$x_1$'], \n          class_names=['R', 'B'], impurity=True,\n          )\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\nTest Data \n\n   Feature 1  Feature 2  Class\n0         10          7      1\n1          9          9      0\n2         11          5      1\n\n\nResult \n\n   Feature 1  Feature 2  Class  Predicted_Class\n0         10          7      1                1\n1          9          9      0                0\n2         11          5      1                0\n\n\nAccuracy score: 0.67"
  },
  {
    "objectID": "posts/dsa/index.html",
    "href": "posts/dsa/index.html",
    "title": "Data Structure and Algorithms: Quick Notes",
    "section": "",
    "text": "Code\ndef binary_search(arr, k):\n    \"\"\"\n    arr:: sorted array\n    k:: search item; float or integer\n    \"\"\"\n    low, high=0, len(arr)-1"
  },
  {
    "objectID": "posts/dsa/index.html#binary-search",
    "href": "posts/dsa/index.html#binary-search",
    "title": "Data Structure and Algorithms: Quick Notes",
    "section": "",
    "text": "Code\ndef binary_search(arr, k):\n    \"\"\"\n    arr:: sorted array\n    k:: search item; float or integer\n    \"\"\"\n    low, high=0, len(arr)-1"
  },
  {
    "objectID": "posts/decisiontree/index.html#discussion-on-decision-tree",
    "href": "posts/decisiontree/index.html#discussion-on-decision-tree",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Discussion on Decision Tree",
    "text": "Discussion on Decision Tree\n\nBeing a simple algorithm, it has both pros and cons. It is robust to training data and the training data can contain missing values. However, it is a greedy algorithm, a problem-solving technique that chooses the best option in the current situation, without considering the overall outcome. It also face the overfitting issue. The Decision Tree Classifier is a versatile and intuitive model, but it’s crucial to understand the mathematics that drive its decisions. By implementing it from scratch, you not only gain a deeper understanding but also appreciate the balance between model complexity and interpretability."
  },
  {
    "objectID": "posts/decisiontree/index.html#conclusion",
    "href": "posts/decisiontree/index.html#conclusion",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Conclusion",
    "text": "Conclusion\nThe Decision Tree Classifier is a versatile and intuitive model, but it’s crucial to understand the mathematics that drive its decisions. By implementing it from scratch, you not only gain a deeper understanding but also appreciate the balance between model complexity and interpretability.\nIn the next post, we’ll discuss techniques to improve decision trees, such as pruning, and how ensemble methods like Random Forests can overcome their limitations. Stay tuned!"
  },
  {
    "objectID": "posts/decisiontree/index.html#reference",
    "href": "posts/decisiontree/index.html#reference",
    "title": "Understanding Decision Tree Classifier: A Mathematical Approach",
    "section": "Reference",
    "text": "Reference\nDecision Tree Classification Clearly Explained by Normalized Nerd\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  }
]