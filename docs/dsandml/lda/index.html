<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rafiq Islam">
<meta name="dcterms.date" content="2024-10-17">

<title>Classification: Linear Discriminant Analysis (LDA) – Mohammad Rafiqul Islam</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../..//_assets/images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-listing/list.min.js"></script>
<script src="../../site_libs/quarto-listing/quarto-listing.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-listing .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-image','listing-date','listing-title','listing-author','listing-reading-time',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      page: 18,
    pagination: { item: "<li class='page-item'><a class='page page-link' href='#'></a></li>" },
      searchColumns: ["listing-title","listing-author","listing-date","listing-image","listing-description"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-listing'] = new List('listing-listing', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Z5NP67GHFC"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Z5NP67GHFC', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6878992848042528" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Classification: Linear Discriminant Analysis (LDA) – Mohammad Rafiqul Islam">
<meta property="og:description" content="">
<meta property="og:image" content="https://mrislambd.github.io/dsandml/lda/lda.png">
<meta property="og:site_name" content="Mohammad Rafiqul Islam">
<meta property="og:image:height" content="576">
<meta property="og:image:width" content="768">
<meta name="twitter:title" content="Classification: Linear Discriminant Analysis (LDA) – Mohammad Rafiqul Islam">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://mrislambd.github.io/dsandml/lda/lda.png">
<meta name="twitter:image-height" content="576">
<meta name="twitter:image-width" content="768">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../dsandml/lda/index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../_assets/images/fsu-logo.png" alt="Florida State University" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../dsandml/lda/index.html">
    <span class="navbar-title">Mohammad Rafiqul Islam</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../portfolio.html"> 
<span class="menu-text">Portfolio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cv.html"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/mohammad-rafiqul-islam/" target="_blank"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mrislambd" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#mathematical-foundation-of-lda" id="toc-mathematical-foundation-of-lda" class="nav-link" data-scroll-target="#mathematical-foundation-of-lda">Mathematical Foundation of LDA</a></li>
  <li><a href="#dimensionality-reduction" id="toc-dimensionality-reduction" class="nav-link" data-scroll-target="#dimensionality-reduction">Dimensionality Reduction</a>
  <ul class="collapse">
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a></li>
  <li><a href="#compute-class-means-mu_k-for-each-class" id="toc-compute-class-means-mu_k-for-each-class" class="nav-link" data-scroll-target="#compute-class-means-mu_k-for-each-class">1. Compute Class Means <span class="math inline">\(\mu_k\)</span> for each class:</a></li>
  <li><a href="#compute-overall-mean-mu" id="toc-compute-overall-mean-mu" class="nav-link" data-scroll-target="#compute-overall-mean-mu">2. Compute Overall Mean <span class="math inline">\(\mu\)</span>:</a></li>
  <li><a href="#compute-the-within-class-scatter-matrix-s_w" id="toc-compute-the-within-class-scatter-matrix-s_w" class="nav-link" data-scroll-target="#compute-the-within-class-scatter-matrix-s_w">3. <strong>Compute the Within-Class Scatter Matrix <span class="math inline">\(S_W\)</span></strong>:</a></li>
  <li><a href="#compute-the-between-class-scatter-matrix-s_b" id="toc-compute-the-between-class-scatter-matrix-s_b" class="nav-link" data-scroll-target="#compute-the-between-class-scatter-matrix-s_b">4. Compute the Between-Class Scatter Matrix <span class="math inline">\(S_B\)</span>:</a></li>
  <li><a href="#solve-the-eigenvalue-problem" id="toc-solve-the-eigenvalue-problem" class="nav-link" data-scroll-target="#solve-the-eigenvalue-problem">5. Solve the Eigenvalue Problem:</a></li>
  <li><a href="#step-6-visualizing-the-results" id="toc-step-6-visualizing-the-results" class="nav-link" data-scroll-target="#step-6-visualizing-the-results">Step 6: Visualizing the Results</a></li>
  <li><a href="#summary-of-the-process-of-eigenvalue-problem" id="toc-summary-of-the-process-of-eigenvalue-problem" class="nav-link" data-scroll-target="#summary-of-the-process-of-eigenvalue-problem">Summary of the Process of Eigenvalue Problem</a></li>
  <li><a href="#final-summary" id="toc-final-summary" class="nav-link" data-scroll-target="#final-summary">Final Summary</a></li>
  </ul></li>
  <li><a href="#python-code-example" id="toc-python-code-example" class="nav-link" data-scroll-target="#python-code-example">Python Code Example</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#disclaimer" id="toc-disclaimer" class="nav-link" data-scroll-target="#disclaimer">Disclaimer</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Classification: Linear Discriminant Analysis (LDA)</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">Data Science</div>
    <div class="quarto-category">Machine Learning</div>
    <div class="quarto-category">Artificial Intelligence</div>
    <div class="quarto-category">Data Engineering</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Rafiq Islam </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 17, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p style="text-align: justify">
Linear Discriminant Analysis (LDA) is a supervised machine learning algorithm commonly used for classification tasks. It is widely applied when dealing with datasets where the number of predictors (features) exceeds the number of observations, or when multicollinearity is a concern. LDA works by projecting data onto a lower-dimensional space, maximizing the separation between classes.
</p>
</section>
<section id="mathematical-foundation-of-lda" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-foundation-of-lda">Mathematical Foundation of LDA</h2>
<p style="text-align: justify">
Let’s assume we have a dataset <span class="math inline">\(X \in \mathbb{R}^{n \times p}\)</span> consisting of <span class="math inline">\(n\)</span> data points and <span class="math inline">\(p\)</span> features, and each data point belongs to one of <span class="math inline">\(K\)</span> distinct classes. The goal of LDA is to find a new space (called a discriminant space) in which the classes are maximally separated, i.e.&nbsp;we want to <strong>maximize the separability between classes</strong> while <strong>minimizing the variation within each class</strong>. This can be mathematically expressed as finding a projection that maximizes the ratio of between-class variance to within-class variance.
</p>
<p>For each class <span class="math inline">\(C_k\)</span> (where <span class="math inline">\(k \in \{1, 2, \dots, K\}\)</span>):</p>
<ul>
<li><span class="math inline">\(\mu_k\)</span> is the <strong>mean vector</strong> of class <span class="math inline">\(C_k\)</span>.<br>
</li>
<li><span class="math inline">\(\mu\)</span> is the <strong>overall mean</strong> of the entire dataset.</li>
</ul>
<p><strong>Class Mean</strong>: For each class <span class="math inline">\(C_k\)</span>, the mean is calculated as:</p>
<p><span class="math display">\[
\mu_k = \frac{1}{N_k} \sum_{x_i \in C_k} x_i
\]</span></p>
<p>where <span class="math inline">\(N_k\)</span> is the number of data points in class <span class="math inline">\(C_k\)</span>, and <span class="math inline">\(x_i\)</span> represents individual data points.</p>
<p><strong>Overall Mean</strong>: The mean of the entire dataset is:</p>
<p><span class="math display">\[
\mu = \frac{1}{n} \sum_{i=1}^{n} x_i
\]</span></p>
<p>To understand how well classes are separated, we need two key measures:</p>
<ol type="1">
<li><p><strong>Within-Class Scatter Matrix <span class="math inline">\(S_W\)</span></strong><br>
The within-class scatter matrix measures how the data points of each class deviate from the class mean. It captures the <strong>spread of data points within each class</strong>. For class <span class="math inline">\(C_k\)</span>, the scatter matrix is calculated as:<br>
<span class="math display">\[
S_W = \sum_{k=1}^{K} \sum_{x_i \in C_k} (x_i - \mu_k)(x_i - \mu_k)^T
\]</span></p>
<p>This formula is saying that for each class <span class="math inline">\(C_k\)</span>, we calculate the distance of every point <span class="math inline">\(x_i\)</span> from the mean of its class <span class="math inline">\(\mu_k\)</span>, and then sum these squared distances across all classes.</p></li>
<li><p><strong>Between-Class Scatter Matrix <span class="math inline">\(S_B\)</span></strong><br>
The between-class scatter matrix measures how the <strong>class means deviate from the overall mean</strong>. It captures how well-separated the classes are.</p>
<p><span class="math display">\[
S_B = \sum_{k=1}^{K} N_k (\mu_k - \mu)(\mu_k - \mu)^T
\]</span></p>
<p>In this case, for each class <span class="math inline">\(C_k\)</span>, we calculate the distance between the mean of class <span class="math inline">\(\mu_k\)</span> and the overall mean <span class="math inline">\(\mu\)</span>, then scale this by the number of points in class <span class="math inline">\(C_k\)</span>.</p></li>
</ol>
<hr>
<p style="text-align: justify">
LDA aims to find a transformation that maximizes the separation between classes. This is done by finding a linear projection <span class="math inline">\(\mathbf{w}\)</span> such that the <strong>between-class scatter is maximized</strong> and the <strong>within-class scatter is minimized</strong>. Mathematically, the optimization problem becomes:
</p>
<p><span class="math display">\[
J(\mathbf{w}) = \frac{\mathbf{w}^T S_B \mathbf{w}}{\mathbf{w}^T S_W \mathbf{w}}
\]</span></p>
<ul>
<li><span class="math inline">\(S_B \mathbf{w}\)</span> captures the between-class variance (how well-separated the classes are in the new projection).<br>
</li>
<li><span class="math inline">\(S_W \mathbf{w}\)</span> captures the within-class variance (how tightly packed the points of the same class are in the new projection).</li>
</ul>
<p>This ratio <span class="math inline">\(J(\mathbf{w})\)</span> is known as the <strong>Fisher’s discriminant ratio</strong>. The goal is to find <span class="math inline">\(\mathbf{w}\)</span> that maximizes this ratio. To maximize the Fisher’s discriminant ratio, we need to solve the following generalized eigenvalue problem:</p>
<p><span class="math display">\[
S_W^{-1} S_B \mathbf{w} = \lambda \mathbf{w}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{w}\)</span> is the vector that defines the linear combination of features that maximizes class separation, and <span class="math inline">\(\lambda\)</span> is an eigenvalue that represents how much variance is explained by that direction.</p>
<p>The solution to this equation gives us the eigenvectors (directions) and eigenvalues (variances) of the transformed space. We select the top eigenvectors corresponding to the largest eigenvalues to form the projection matrix <span class="math inline">\(W\)</span>.</p>
<hr>
</section>
<section id="dimensionality-reduction" class="level2">
<h2 class="anchored" data-anchor-id="dimensionality-reduction">Dimensionality Reduction</h2>
<p style="text-align: justify">
The LDA transformation reduces the dimensionality of the data by projecting it onto a subspace spanned by the eigenvectors with the largest eigenvalues. For a dataset with <span class="math inline">\(K\)</span> classes, LDA can reduce the data to at most <span class="math inline">\(K-1\)</span> dimensions because <span class="math inline">\(S_B\)</span> has rank <span class="math inline">\(K-1\)</span>. <br><br> If we have two classes, LDA will reduce the data to a one-dimensional subspace. For three classes, LDA can project the data onto a two-dimensional subspace, and so on.
</p>
<hr>
<p>Now before diving into the python code, let’s do some math by hand so that we can understand the skeleton of the process. Let’s create a small dataset with 6 features and 4 observations divided into 3 classes. We will use this dataset to manually go through the Linear Discriminant Analysis (LDA) process step by step.</p>
<section id="dataset" class="level3">
<h3 class="anchored" data-anchor-id="dataset">Dataset</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th>Observation</th>
<th>Feature 1</th>
<th>Feature 2</th>
<th>Feature 3</th>
<th>Feature 4</th>
<th>Feature 5</th>
<th>Feature 6</th>
<th>Class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(x_1\)</span></td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
<td><span class="math inline">\(C_1\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(x_2\)</span></td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td><span class="math inline">\(C_1\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(x_3\)</span></td>
<td>6</td>
<td>5</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td><span class="math inline">\(C_2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(x_4\)</span></td>
<td>7</td>
<td>6</td>
<td>5</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td><span class="math inline">\(C_3\)</span></td>
</tr>
</tbody>
</table>
<p>Now, we’ll walk through the mathematical steps of LDA for this small dataset.</p>
</section>
<section id="compute-class-means-mu_k-for-each-class" class="level3">
<h3 class="anchored" data-anchor-id="compute-class-means-mu_k-for-each-class">1. Compute Class Means <span class="math inline">\(\mu_k\)</span> for each class:</h3>
<ul>
<li><p>Class <span class="math inline">\(C_1\)</span> (mean of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>): <span class="math display">\[
\mu_1 = \frac{1}{2} \left( \begin{bmatrix} 2 \\ 3 \\ 4 \\ 5 \\ 6 \\ 7 \end{bmatrix} + \begin{bmatrix} 3 \\ 4 \\ 5 \\ 6 \\ 7 \\ 8 \end{bmatrix} \right) = \begin{bmatrix} 2.5 \\ 3.5 \\ 4.5 \\ 5.5 \\ 6.5 \\ 7.5 \end{bmatrix}
\]</span></p></li>
<li><p>Class <span class="math inline">\(C_2\)</span> (only one observation <span class="math inline">\(x_3\)</span>): <span class="math display">\[
\mu_2 = \begin{bmatrix} 6 \\ 5 \\ 4 \\ 3 \\ 2 \\ 1 \end{bmatrix}
\]</span></p></li>
<li><p>Class <span class="math inline">\(C_3\)</span> (only one observation <span class="math inline">\(x_4\)</span>): <span class="math display">\[
\mu_3 = \begin{bmatrix} 7 \\ 6 \\ 5 \\ 4 \\ 3 \\ 2 \end{bmatrix}
\]</span></p></li>
</ul>
</section>
<section id="compute-overall-mean-mu" class="level3">
<h3 class="anchored" data-anchor-id="compute-overall-mean-mu">2. Compute Overall Mean <span class="math inline">\(\mu\)</span>:</h3>
<p>We compute the overall mean <span class="math inline">\(\mu\)</span>, which is the average of all observations from all classes:</p>
<p><span class="math display">\[
\mu = \frac{1}{4} \left( \begin{bmatrix} 2 \\ 3 \\ 4 \\ 5 \\ 6 \\ 7 \end{bmatrix} + \begin{bmatrix} 3 \\ 4 \\ 5 \\ 6 \\ 7 \\ 8 \end{bmatrix} + \begin{bmatrix} 6 \\ 5 \\ 4 \\ 3 \\ 2 \\ 1 \end{bmatrix} + \begin{bmatrix} 7 \\ 6 \\ 5 \\ 4 \\ 3 \\ 2 \end{bmatrix} \right)= \frac{1}{4} \begin{bmatrix} 18 \\ 18 \\ 18 \\ 18 \\ 18 \\ 18 \end{bmatrix} = \begin{bmatrix} 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \end{bmatrix}
\]</span></p>
</section>
<section id="compute-the-within-class-scatter-matrix-s_w" class="level3">
<h3 class="anchored" data-anchor-id="compute-the-within-class-scatter-matrix-s_w">3. <strong>Compute the Within-Class Scatter Matrix <span class="math inline">\(S_W\)</span></strong>:</h3>
<p>For each class <span class="math inline">\(C_k\)</span>, the within-class scatter matrix <span class="math inline">\(S_W\)</span> is computed as:</p>
<p><span class="math display">\[
S_W = \sum_{k=1}^{K} \sum_{x_i \in C_k} (x_i - \mu_k)(x_i - \mu_k)^T
\]</span></p>
<p>For <span class="math inline">\(C_1\)</span>, the within-class scatter matrix is:</p>
<p><span class="math display">\[
(x_1 - \mu_1) = \begin{bmatrix} 2 \\ 3 \\ 4 \\ 5 \\ 6 \\ 7 \end{bmatrix} - \begin{bmatrix} 2.5 \\ 3.5 \\ 4.5 \\ 5.5 \\ 6.5 \\ 7.5 \end{bmatrix} = \begin{bmatrix} -0.5 \\ -0.5 \\ -0.5 \\ -0.5 \\ -0.5 \\ -0.5 \end{bmatrix}; \hspace{6mm} (x_2 - \mu_1) = \begin{bmatrix} 3 \\ 4 \\ 5 \\ 6 \\ 7 \\ 8 \end{bmatrix} - \begin{bmatrix} 2.5 \\ 3.5 \\ 4.5 \\ 5.5 \\ 6.5 \\ 7.5 \end{bmatrix} = \begin{bmatrix} 0.5 \\ 0.5 \\ 0.5 \\ 0.5 \\ 0.5 \\ 0.5 \end{bmatrix}
\]</span></p>
<p>For class <span class="math inline">\(C_1\)</span>, the scatter matrix is:</p>
<p><span class="math display">\[\begin{align*}
S_{W1} &amp;= (x_1 - \mu_1)(x_1 - \mu_1)^T + (x_2 - \mu_1)(x_2 - \mu_1)^T\\
&amp;=\begin{bmatrix} -0.5 \\ -0.5 \\ -0.5 \\ -0.5 \\ -0.5 \\ -0.5 \end{bmatrix} \begin{bmatrix} -0.5 &amp; -0.5 &amp; -0.5 &amp; -0.5 &amp; -0.5 &amp; -0.5 \end{bmatrix} + \begin{bmatrix} 0.5 \\ 0.5 \\ 0.5 \\ 0.5 \\ 0.5 \\ 0.5 \end{bmatrix} \begin{bmatrix} 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \end{bmatrix}\\
&amp;= \begin{bmatrix} 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \end{bmatrix}
\end{align*}\]</span></p>
<p>For classes <span class="math inline">\(C_2\)</span> and <span class="math inline">\(C_3\)</span>, there is only one data point in each, so there is no within-class scatter:</p>
<p><span class="math display">\[
S_{W2} = 0, \quad S_{W3} = 0
\]</span></p>
<p>Thus, the total within-class scatter matrix is:</p>
<p><span class="math display">\[
S_W = S_{W1} + S_{W2} + S_{W3} = S_{W1}
\]</span></p>
<p><span class="math display">\[
S_W = \begin{bmatrix} 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \end{bmatrix}
\]</span></p>
</section>
<section id="compute-the-between-class-scatter-matrix-s_b" class="level3">
<h3 class="anchored" data-anchor-id="compute-the-between-class-scatter-matrix-s_b">4. Compute the Between-Class Scatter Matrix <span class="math inline">\(S_B\)</span>:</h3>
<p>For each class <span class="math inline">\(C_k\)</span>, the between-class scatter matrix is computed as:</p>
<p><span class="math display">\[
S_B = \sum_{k=1}^{K} N_k (\mu_k - \mu)(\mu_k - \mu)^T
\]</span></p>
<p>For class <span class="math inline">\(C_1\)</span> (where <span class="math inline">\(N_1 = 2\)</span>):</p>
<p><span class="math display">\[
(\mu_1 - \mu) = \begin{bmatrix} 2.5 \\ 3.5 \\ 4.5 \\ 5.5 \\ 6.5 \\ 7.5 \end{bmatrix} - \begin{bmatrix} 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \end{bmatrix} = \begin{bmatrix} -2 \\ -1 \\ 0 \\ 1 \\ 2 \\ 3 \end{bmatrix}
\]</span></p>
<p>Thus, for <span class="math inline">\(C_1\)</span>:</p>
<p><span class="math display">\[\begin{align*}
S_{B1} &amp;= 2 \begin{bmatrix} -2 \\ -1 \\ 0 \\ 1 \\ 2 \\ 3 \end{bmatrix} \begin{bmatrix} -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 \end{bmatrix}= 2 \begin{bmatrix} 4 &amp; 2 &amp; 0 &amp; -2 &amp; -4 &amp; -6 \\ 2 &amp; 1 &amp; 0 &amp; -1 &amp; -2 &amp; -3 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 \\ -4 &amp; -2 &amp; 0 &amp; 2 &amp; 4 &amp; 6 \\ -6 &amp; -3 &amp; 0 &amp; 3 &amp; 6 &amp; 9 \end{bmatrix}\\
&amp;=\begin{bmatrix} 8 &amp; 4 &amp; 0 &amp; -4 &amp; -8 &amp; -12 \\ 4 &amp; 2 &amp; 0 &amp; -2 &amp; -4 &amp; -6 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ -4 &amp; -2 &amp; 0 &amp; 2 &amp; 4 &amp; 6 \\ -8 &amp; -4 &amp; 0 &amp; 4 &amp; 8 &amp; 12 \\ -12 &amp; -6 &amp; 0 &amp; 6 &amp; 12 &amp; 18 \end{bmatrix}
\end{align*}\]</span></p>
<p>For <span class="math inline">\(C_2\)</span> (where <span class="math inline">\(N_2 = 1\)</span>):</p>
<p><span class="math display">\[
(\mu_2 - \mu) = \begin{bmatrix} 6 \\ 5 \\ 4 \\ 3 \\ 2 \\ 1 \end{bmatrix} - \begin{bmatrix} 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \end{bmatrix} = \begin{bmatrix} 1.5 \\ 0.5 \\ -0.5 \\ -1.5 \\ -2.5 \\ -3.5 \end{bmatrix}
\]</span></p>
<p>The between-class scatter matrix for <span class="math inline">\(C_2\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
S_{B2} &amp;= \begin{bmatrix} 1.5 \\ 0.5 \\ -0.5 \\ -1.5 \\ -2.5 \\ -3.5 \end{bmatrix} \begin{bmatrix} 1.5 &amp; 0.5 &amp; -0.5 &amp; -1.5 &amp; -2.5 &amp; -3.5 \end{bmatrix}\\
&amp;= \begin{bmatrix} 2.25 &amp; 0.75 &amp; -0.75 &amp; -2.25 &amp; -3.75 &amp; -5.25 \\ 0.75 &amp; 0.25 &amp; -0.25 &amp; -0.75 &amp; -1.25 &amp; -1.75 \\ -0.75 &amp; -0.25 &amp; 0.25 &amp; 0.75 &amp; 1.25 &amp; 1.75 \\ -2.25 &amp; -0.75 &amp; 0.75 &amp; 2.25 &amp; 3.75 &amp; 5.25 \\ -3.75 &amp; -1.25 &amp; 1.25 &amp; 3.75 &amp; 6.25 &amp; 8.75 \\ -5.25 &amp; -1.75 &amp; 1.75 &amp; 5.25 &amp; 8.75 &amp; 12.25 \end{bmatrix}
\end{align*}\]</span></p>
<p>For <span class="math inline">\(C_3\)</span> (where <span class="math inline">\(N_3 = 1\)</span>):</p>
<p><span class="math display">\[
(\mu_3 - \mu) = \begin{bmatrix} 7 \\ 6 \\ 5 \\ 4 \\ 3 \\ 2 \end{bmatrix} - \begin{bmatrix} 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \\ 4.5 \end{bmatrix} = \begin{bmatrix} 2.5 \\ 1.5 \\ 0.5 \\ -0.5 \\ -1.5 \\ -2.5 \end{bmatrix}
\]</span></p>
<p>The between-class scatter matrix for <span class="math inline">\(C_3\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
S_{B3} &amp;= \begin{bmatrix} 2.5 \\ 1.5 \\ 0.5 \\ -0.5 \\ -1.5 \\ -2.5 \end{bmatrix} \begin{bmatrix} 2.5 &amp; 1.5 &amp; 0.5 &amp; -0.5 &amp; -1.5 &amp; -2.5 \end{bmatrix}\\
&amp;=\begin{bmatrix} 6.25 &amp; 3.75 &amp; 1.25 &amp; -1.25 &amp; -3.75 &amp; -6.25 \\ 3.75 &amp; 2.25 &amp; 0.75 &amp; -0.75 &amp; -2.25 &amp; -3.75 \\ 1.25 &amp; 0.75 &amp; 0.25 &amp; -0.25 &amp; -0.75 &amp; -1.25 \\ -1.25 &amp; -0.75 &amp; -0.25 &amp; 0.25 &amp; 0.75 &amp; 1.25 \\ -3.75 &amp; -2.25 &amp; -0.75 &amp; 0.75 &amp; 2.25 &amp; 3.75 \\ -6.25 &amp; -3.75 &amp; -1.25 &amp; 1.25 &amp; 3.75 &amp; 6.25 \end{bmatrix}
\end{align*}\]</span></p>
<p>Total Between-Class Scatter Matrix <span class="math inline">\(S_B\)</span>:</p>
<p><span class="math display">\[\begin{align*}
S_B &amp;= S_{B1} + S_{B2} + S_{B3}\\
&amp;=\begin{bmatrix} 8 &amp; 4 &amp; 0 &amp; -4 &amp; -8 &amp; -12 \\ 4 &amp; 2 &amp; 0 &amp; -2 &amp; -4 &amp; -6 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ -4 &amp; -2 &amp; 0 &amp; 2 &amp; 4 &amp; 6 \\ -8 &amp; -4 &amp; 0 &amp; 4 &amp; 8 &amp; 12 \\ -12 &amp; -6 &amp; 0 &amp; 6 &amp; 12 &amp; 18 \end{bmatrix} + \begin{bmatrix} 2.25 &amp; 0.75 &amp; -0.75 &amp; -2.25 &amp; -3.75 &amp; -5.25 \\ 0.75 &amp; 0.25 &amp; -0.25 &amp; -0.75 &amp; -1.25 &amp; -1.75 \\ -0.75 &amp; -0.25 &amp; 0.25 &amp; 0.75 &amp; 1.25 &amp; 1.75 \\ -2.25 &amp; -0.75 &amp; 0.75 &amp; 2.25 &amp; 3.75 &amp; 5.25 \\ -3.75 &amp; -1.25 &amp; 1.25 &amp; 3.75 &amp; 6.25 &amp; 8.75 \\ -5.25 &amp; -1.75 &amp; 1.75 &amp; 5.25 &amp; 8.75 &amp; 12.25 \end{bmatrix} \\
&amp;\\
&amp; + \begin{bmatrix} 6.25 &amp; 3.75 &amp; 1.25 &amp; -1.25 &amp; -3.75 &amp; -6.25 \\ 3.75 &amp; 2.25 &amp; 0.75 &amp; -0.75 &amp; -2.25 &amp; -3.75 \\ 1.25 &amp; 0.75 &amp; 0.25 &amp; -0.25 &amp; -0.75 &amp; -1.25 \\ -1.25 &amp; -0.75 &amp; -0.25 &amp; 0.25 &amp; 0.75 &amp; 1.25 \\ -3.75 &amp; -2.25 &amp; -0.75 &amp; 0.75 &amp; 2.25 &amp; 3.75 \\ -6.25 &amp; -3.75 &amp; -1.25 &amp; 1.25 &amp; 3.75 &amp; 6.25 \end{bmatrix}
\end{align*}\]</span></p>
<p>Adding the matrices gives:</p>
<p><span class="math display">\[
S_B = \begin{bmatrix} 16.5 &amp; 8.5 &amp; 0.5 &amp; -7.5 &amp; -15.5 &amp; -23.5 \\ 8.5 &amp; 4.5 &amp; 0.5 &amp; -3.5 &amp; -7.5 &amp; -11.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ -7.5 &amp; -3.5 &amp; 0.5 &amp; 4.5 &amp; 8.5 &amp; 12.5 \\ -15.5 &amp; -7.5 &amp; 0.5 &amp; 8.5 &amp; 16.5 &amp; 24.5 \\ -23.5 &amp; -11.5 &amp; 0.5 &amp; 12.5 &amp; 24.5 &amp; 36.5 \end{bmatrix}
\]</span></p>
</section>
<section id="solve-the-eigenvalue-problem" class="level3">
<h3 class="anchored" data-anchor-id="solve-the-eigenvalue-problem">5. Solve the Eigenvalue Problem:</h3>
<p>We now solve the eigenvalue problem:<br>
<span class="math display">\[
S_W^{-1} S_B \mathbf{w} = \lambda \mathbf{w}
\]</span></p>
<p style="text-align:justify">
The solution to this eigenvalue problem gives us the eigenvalues <span class="math inline">\(\lambda\)</span> (which quantify the amount of variance captured in each direction) and the eigenvectors <span class="math inline">\(\mathbf{w}\)</span> (which give the directions of maximum class separation). The eigenvector corresponding to the largest eigenvalue defines the direction of the first discriminant axis, which is the direction that maximally separates the classes.
</p>
<p><span class="math display">\[\begin{align*}
\begin{bmatrix}
6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 \\
6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 \\
6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 \\
6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 \\
6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 \\
6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 \end{bmatrix}\begin{bmatrix} 16.5 &amp; 8.5 &amp; 0.5 &amp; -7.5 &amp; -15.5 &amp; -23.5 \\ 8.5 &amp; 4.5 &amp; 0.5 &amp; -3.5 &amp; -7.5 &amp; -11.5 \\ 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \\ -7.5 &amp; -3.5 &amp; 0.5 &amp; 4.5 &amp; 8.5 &amp; 12.5 \\ -15.5 &amp; -7.5 &amp; 0.5 &amp; 8.5 &amp; 16.5 &amp; 24.5 \\ -23.5 &amp; -11.5 &amp; 0.5 &amp; 12.5 &amp; 24.5 &amp; 36.5 \end{bmatrix}\begin{bmatrix}w_1\\w_2\\w_3\\w_4\\w_5\\w_6\end{bmatrix}&amp;=\lambda\begin{bmatrix}w_1\\w_2\\w_3\\w_4\\w_5\\w_6\end{bmatrix}\\
\implies \begin{bmatrix}
-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 \\
-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 \\
-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 \\
-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 \\
-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 \\
-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 \end{bmatrix}\begin{bmatrix}w_1\\w_2\\w_3\\w_4\\w_5\\w_6\end{bmatrix}&amp;=\lambda\begin{bmatrix}w_1\\w_2\\w_3\\w_4\\w_5\\w_6\end{bmatrix}
\end{align*}\]</span></p>
<p>The eigenvalues of the matrix are:</p>
<p><span class="math display">\[
\lambda_1 = 6.00, \quad \lambda_2 = 1.78 \times 10^{-15}, \quad \lambda_3 = 9.86 \times 10^{-32}, \quad \lambda_4 = 0.00, \quad \lambda_5 = -5.47 \times 10^{-48}, \quad \lambda_6 = -5.95 \times 10^{-16}
\]</span></p>
<p>The two largest eigenvalues are:</p>
<ol type="1">
<li><span class="math inline">\(\lambda_1 = 6.00\)</span></li>
<li><span class="math inline">\(\lambda_2 = 1.78 \times 10^{-15}\)</span></li>
</ol>
<p>The corresponding eigenvectors for the two largest eigenvalues are:</p>
<p><span class="math display">\[
\mathbf{w_1} = \begin{bmatrix}
-0.408 \\
-0.408 \\
-0.408 \\
-0.408 \\
-0.408 \\
-0.408 \end{bmatrix}, \quad
\mathbf{w_2} = \begin{bmatrix}
-0.848 \\
-0.237 \\
-0.237 \\
-0.237 \\
-0.237 \\
-0.237 \end{bmatrix}
\]</span></p>
<p>By projecting the data onto the eigenvector <span class="math inline">\(\mathbf{w}\)</span>, we transform the original dataset into a lower-dimensional space where class separability is maximized. For this dataset, since there are 3 classes, LDA will find up to <span class="math inline">\(K-1 = 2\)</span> discriminant axes. Let’s see how.</p>
<p>The matrix formed by the two largest eigenvectors is:</p>
<p><span class="math display">\[
W=\begin{bmatrix}
-0.408 &amp; -0.848 \\
-0.408 &amp; -0.237 \\
-0.408 &amp; -0.237 \\
-0.408 &amp; -0.237 \\
-0.408 &amp; -0.237 \\
-0.408 &amp; -0.237 \end{bmatrix}
\]</span></p>
<p style="text-align: justify">
This matrix represents the projection directions corresponding to the two largest eigenvalues in the Linear Discriminant Analysis process. With the eigenvectors <span class="math inline">\(\mathbf{w}_1\)</span> and <span class="math inline">\(\mathbf{w}_2\)</span>, we can now project our original dataset onto the new 2D subspace. <br><br> Now, let <span class="math inline">\(X\)</span> represent our original dataset (where each row corresponds to an observation and each column to a feature). The projection of the original data onto the new 2D subspace is given by:
</p>
<p><span class="math display">\[
Y = X W
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(X\)</span> is the <span class="math inline">\(4 \times 6\)</span> matrix (4 observations, 6 features),</li>
<li><span class="math inline">\(W\)</span> is the <span class="math inline">\(6 \times 2\)</span> matrix of eigenvectors.</li>
</ul>
<p>After multiplying <span class="math inline">\(X\)</span> by <span class="math inline">\(W\)</span>, we obtain the projected data matrix <span class="math inline">\(Y\)</span>, which is a <span class="math inline">\(4 \times 2\)</span> matrix (4 observations, 2 features):</p>
<p><span class="math display">\[
Y = \begin{bmatrix}
y_{11} &amp; y_{12} \\
y_{21} &amp; y_{22} \\
y_{31} &amp; y_{32} \\
y_{41} &amp; y_{42}
\end{bmatrix}
\]</span></p>
<p>This matrix <span class="math inline">\(Y\)</span> represents the data in the new 2D space where class separability is maximized. So for our data</p>
<p><span class="math display">\[\begin{align*}
\begin{bmatrix}2&amp;3&amp;4&amp;5&amp;6&amp;7\\3&amp;4&amp;5&amp;6&amp;7&amp;8\\6&amp;5&amp;4&amp;3&amp;2&amp;1\\7&amp;6&amp;5&amp;4&amp;3&amp;2\end{bmatrix}\begin{bmatrix}
-0.408 &amp; -0.848 \\
-0.408 &amp; -0.237 \\
-0.408 &amp; -0.237 \\
-0.408 &amp; -0.237 \\
-0.408 &amp; -0.237 \\
-0.408 &amp; -0.237 \end{bmatrix}&amp;=\begin{bmatrix}-11.016 &amp; -7.621\\-13.464 &amp; -9.654\\ -8.568 &amp; -8.643\\-11.016 &amp;-10.676\end{bmatrix}
\end{align*}\]</span></p>
</section>
<section id="step-6-visualizing-the-results" class="level3">
<h3 class="anchored" data-anchor-id="step-6-visualizing-the-results">Step 6: Visualizing the Results</h3>
<p>If we were to plot the projected data in this new 2D space, we would see the observations from different classes are better separated, which is the ultimate goal of LDA. The two axes of this 2D space correspond to the two linear discriminants that maximize the separation between the classes.</p>
<div id="e52a5b0b" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>],</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                  [<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>],</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                  [<span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>],</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>                  [<span class="dv">7</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>]])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.array([[<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.848</span>],</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                         [<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.237</span>],</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                         [<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.237</span>],</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>                         [<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.237</span>],</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                         [<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.237</span>],</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                         [<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.237</span>]])</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.dot(X, W)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the projection</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(Y.shape[<span class="dv">0</span>]):</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    plt.scatter(Y[i, <span class="dv">0</span>], Y[i, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f'Obs </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    plt.text(Y[i, <span class="dv">0</span>] <span class="op">+</span> <span class="fl">0.02</span>, Y[i, <span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.02</span>, <span class="ss">f'Obs </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Projected Data after LDA"</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'LD1 (First Linear Discriminant)'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'LD2 (Second Linear Discriminant)'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'gray'</span>, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'gray'</span>, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.gcf().patch.set_facecolor(<span class="st">'#f4f4f4'</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.gca().set_facecolor(<span class="st">'#f4f4f4'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-2-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="index_files/figure-html/cell-2-output-1.png" width="670" height="523" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
<section id="summary-of-the-process-of-eigenvalue-problem" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-the-process-of-eigenvalue-problem">Summary of the Process of Eigenvalue Problem</h3>
<ol type="1">
<li><strong>Eigenvalue Calculation</strong>: We found the eigenvalues <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> to be the largest, indicating the directions with the most class separability. We did find only two eigenvaleus since total class is 3.</li>
<li><strong>Eigenvector Calculation</strong>: We computed the eigenvectors <span class="math inline">\(\mathbf{w}_1\)</span> and <span class="math inline">\(\mathbf{w}_2\)</span> corresponding to these eigenvalues. These eigenvectors define the directions in the original feature space along which the class separation is maximized.</li>
<li><strong>Projection</strong>: We projected the original dataset onto the new 2D subspace spanned by the eigenvectors. This resulted in a new dataset in 2D, where the different classes are more separable.</li>
</ol>
<p>This completes the detailed walkthrough of solving the eigenvalue problem in LDA for our example dataset.</p>
<hr>
</section>
<section id="final-summary" class="level3">
<h3 class="anchored" data-anchor-id="final-summary">Final Summary</h3>
<ul>
<li><strong>Within-class scatter matrix</strong> <span class="math inline">\(S_W\)</span> quantifies the spread of data points within each class, and we calculated it for each class.</li>
<li><strong>Between-class scatter matrix</strong> <span class="math inline">\(S_B\)</span> quantifies the separation between the class means, and we calculated it using the mean of each class and the overall mean.</li>
<li>Solving the <strong>eigenvalue problem</strong> <span class="math inline">\(S_W^{-1} S_B \mathbf{w} = \lambda \mathbf{w}\)</span> gives us the directions <span class="math inline">\(\mathbf{w}\)</span> (eigenvectors) that maximize class separation.</li>
</ul>
<p>This is how LDA works step by step, using a small dataset as an example.</p>
</section>
</section>
<section id="python-code-example" class="level2">
<h2 class="anchored" data-anchor-id="python-code-example">Python Code Example</h2>
<p>Let’s now revisit the Python code, with an understanding of the math behind LDA. First build our own classifier</p>
<div id="06728ea8" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomLDA:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,n_components <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">        n_components: int, optional (default=None)</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">                      Number of components to keep. If None, all components are kept</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_components <span class="op">=</span> n_components</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eigenvalues <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eigenvectors <span class="op">=</span> <span class="va">None</span> </span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean_vectors <span class="op">=</span> <span class="va">None</span> </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.class_means <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">        X: ndarray of shape (n_samples, n_features)</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co">        y: ndarray of shape (n_samples,)</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co">           Target labels (must be categorical)</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        class_labels <span class="op">=</span> np.unique(y)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step1: Compute the class means mu_k for each class </span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean_vectors <span class="op">=</span> []</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> class_labels:</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mean_vectors.append(np.mean(X[y<span class="op">==</span>c], axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: Compute the within-class scatter matrix S_W </span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        S_W <span class="op">=</span> np.zeros((n_features, n_features))</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> class_labels:</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>            class_scatter <span class="op">=</span> np.cov(X[y<span class="op">==</span>c].T, bias<span class="op">=</span><span class="va">True</span>) <span class="co"># Covariance matrix for each class</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>            S_W <span class="op">+=</span> class_scatter <span class="op">*</span> (X[y<span class="op">==</span>c].shape[<span class="dv">0</span>])</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: Compute the between-class scatter matrix S_B</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        overall_mean <span class="op">=</span> np.mean(X, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        S_B <span class="op">=</span> np.zeros((n_features, n_features))</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i,mean_vector <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.mean_vectors):</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>            n <span class="op">=</span> X[y <span class="op">==</span> class_labels[i]].shape[<span class="dv">0</span>]</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>            mean_differences <span class="op">=</span> (mean_vector <span class="op">-</span>overall_mean).reshape(n_features,<span class="dv">1</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>            S_B <span class="op">+=</span> n<span class="op">*</span>(mean_differences).dot(mean_differences.T)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Solve the Eigenvalue problem </span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>        eigvalues, eigvectors <span class="op">=</span> np.linalg.eig(np.linalg.pinv(S_W).dot(S_B))</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: Sort the Eigenvalues and corresponding eigenvectors </span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>        eigvalues_sort_idx <span class="op">=</span> np.argsort(np.<span class="bu">abs</span>(eigvalues))[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eigenvalues <span class="op">=</span> eigvalues[eigvalues_sort_idx]</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eigenvectors <span class="op">=</span> eigvectors[:,eigvalues_sort_idx]</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 6: Keep only the top n_components</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.n_components:</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.eigenvectors <span class="op">=</span> <span class="va">self</span>.eigenvectors[:,:<span class="va">self</span>.n_components]</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.class_means <span class="op">=</span> np.dot(<span class="va">self</span>.mean_vectors, <span class="va">self</span>.eigenvectors)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> transform(<span class="va">self</span>,X):</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a><span class="co">        Project the data onto the LDA components </span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a><span class="co">        X: ndarray of shape (n_samples, n_features)</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a><span class="co">        X_transformed: ndarray of shape (n_samples, n_features)</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.dot(X,<span class="va">self</span>.eigenvectors)</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit_transform(<span class="va">self</span>, X, y):</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a><span class="co">        Fit the LDA model and transform the data.</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a><span class="co">        X : ndarray of shape (n_samples, n_features)</span></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a><span class="co">            Training data.</span></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a><span class="co">        y : ndarray of shape (n_samples,)</span></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a><span class="co">            Target labels (must be categorical).</span></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a><span class="co">        X_transformed : ndarray of shape (n_samples, n_components)</span></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a><span class="co">            Transformed data after fitting.</span></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fit(X, y)</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.transform(X)</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a><span class="co">        Predict the class labels for new data points.</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a><span class="co">        X : ndarray of shape (n_samples, n_features)</span></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a><span class="co">            New data to classify.</span></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a><span class="co">        Predictions: ndarray of shape (n_samples,)</span></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a><span class="co">                     Predicted class labels</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>        X_projected <span class="op">=</span> <span class="va">self</span>.transform(X)</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> []</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> X_projected:</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>            distances <span class="op">=</span> np.linalg.norm(x<span class="op">-</span><span class="va">self</span>.class_means, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>            predictions.append(np.argmin(distances))</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(predictions)</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> explained_variance_ratio(<span class="va">self</span>):</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the percentage of variance explained by each of the selected components</span></span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a><span class="co">        explained_variance: ndarray of shape (n_components,)</span></span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a><span class="co">                            Percentage of variance explained by each selected components</span></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> np.<span class="bu">sum</span>(<span class="va">self</span>.eigenvalues)</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [(i<span class="op">/</span>total) <span class="cf">for</span> i <span class="kw">in</span> <span class="va">self</span>.eigenvalues[:<span class="va">self</span>.n_components]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next we apply both the custom classifier and the classifier from the <code>scikit-learn</code> library.</p>
<div id="5ca4d0e4" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.discriminant_analysis <span class="im">import</span> LinearDiscriminantAnalysis <span class="im">as</span> LDA</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> iris.data</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> iris.target</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the dataset (optional but often improves performance)</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test sets</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_scaled, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply LDA from the scikit-learn library</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>lda1 <span class="op">=</span> LDA(n_components<span class="op">=</span><span class="dv">2</span>)  <span class="co"># Reduce to 2 dimensions</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>X_train_lda1 <span class="op">=</span> lda1.fit_transform(X_train, y_train)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>X_test_lda1 <span class="op">=</span> lda1.transform(X_test)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply LDA from the custom built classifier</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>lda2 <span class="op">=</span> CustomLDA(n_components<span class="op">=</span><span class="dv">2</span>)  <span class="co"># Reduce to 2 dimensions</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>X_train_lda2 <span class="op">=</span> lda2.fit_transform(X_train, y_train)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>X_test_lda2 <span class="op">=</span> lda2.transform(X_test)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the LDA-transformed data</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="fl">9.5</span>,<span class="dv">4</span>))</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(X_train_lda1[:, <span class="dv">0</span>], X_train_lda1[:, <span class="dv">1</span>], c<span class="op">=</span>y_train, cmap<span class="op">=</span><span class="st">'rainbow'</span>, edgecolor<span class="op">=</span><span class="st">'k'</span>, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'LD1'</span>)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'LD2'</span>)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Scikit-learn'</span>)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter(X_train_lda2[:, <span class="dv">0</span>], X_train_lda2[:, <span class="dv">1</span>], c<span class="op">=</span>y_train, cmap<span class="op">=</span><span class="st">'rainbow'</span>, edgecolor<span class="op">=</span><span class="st">'k'</span>, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'LD1'</span>)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'LD2'</span>)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Custom'</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axes:</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>    ax.set_facecolor(<span class="st">'#f4f4f4'</span>)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>plt.gcf().patch.set_facecolor(<span class="st">'#f4f4f4'</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'LDA: Projection of the Iris Dataset'</span>)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-4-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="index_files/figure-html/cell-4-output-1.png" width="773" height="394" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Next, apply LDA as a classifiers for the actual classification</p>
<div id="58ef7910" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>lda_classifier1 <span class="op">=</span> LDA()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>lda_classifier1.fit(X_train, y_train)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>y_pred1 <span class="op">=</span> lda_classifier1.predict(X_test)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>lda_classifier2 <span class="op">=</span> CustomLDA()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>lda_classifier2.fit(X_train, y_train)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>y_pred2 <span class="op">=</span> lda_classifier2.predict(X_test)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Check accuracy</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>accuracy1 <span class="op">=</span> accuracy_score(y_test, y_pred1)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>accuracy2 <span class="op">=</span> accuracy_score(y_test, y_pred2)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'sklearn LDA Classifier Accuracy: </span><span class="sc">{</span>accuracy1 <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">% and </span><span class="ch">\n</span><span class="ss">custom LDA Classifier Accuracy: </span><span class="sc">{</span>accuracy2 <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>sklearn LDA Classifier Accuracy: 100.00% and 
custom LDA Classifier Accuracy: 95.56%</code></pre>
</div>
</div>
<p>Not too bad, huh! Let’s see the confusion matrix for our custom classifier</p>
<div id="c6c78cb3" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>conf_mat <span class="op">=</span> confusion_matrix(y_test, y_pred2)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    conf_mat, </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[<span class="st">'Predicted: Setosa'</span>,<span class="st">'Predicted: Virginica'</span>, <span class="st">'Predicted: Versicolor'</span>],</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span>[<span class="st">'Actual: Setosa'</span>,<span class="st">'Actual: Virginica'</span>, <span class="st">'Actual: Versicolor'</span>]</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Predicted: Setosa</th>
<th data-quarto-table-cell-role="th">Predicted: Virginica</th>
<th data-quarto-table-cell-role="th">Predicted: Versicolor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Actual: Setosa</td>
<td>19</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Actual: Virginica</td>
<td>0</td>
<td>11</td>
<td>2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Actual: Versicolor</td>
<td>0</td>
<td>0</td>
<td>13</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<hr>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p style="text-align: justify">
Linear Discriminant Analysis (LDA) is a powerful technique for dimensionality reduction and classification. Its goal is to find directions (linear combinations of the original features) that best separate the classes by maximizing between-class variance while minimizing within-class variance.
</p>
<section id="disclaimer" class="level3">
<h3 class="anchored" data-anchor-id="disclaimer">Disclaimer</h3>
<p style="text-align: justify">
For the mathematical explanation, I used generative AI to produce the matrices and vectors and their manipulations. So it won’t be surprising if a calculation mistake is found. The custom python class was created by the help of ChatGPT4
</p>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li>Fisher, R.A. (1936). “The Use of Multiple Measurements in Taxonomic Problems.” <em>Annals of Eugenics</em>, 7(2), 179–188.<br>
</li>
<li>Murphy, K. P. (2012). <em>Machine Learning: A Probabilistic Perspective</em>. MIT Press.<br>
</li>
<li>Strang, G. (2016). <em>Introduction to Linear Algebra</em> (5th ed.). Wellesley-Cambridge Press.<br>
</li>
<li>Lay, D. C. (2011). <em>Linear Algebra and Its Applications</em> (4th ed.). Pearson.</li>
</ul>
<p><strong>Share on</strong></p>
<div class="columns">
<div class="column" style="width:33%;">
<p><a href="https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#1877F2; text-decoration: none;"></a></p><a href="https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#1877F2; text-decoration: none;">
<p><i class="fa-brands fa-facebook fa-3x" aria-label="facebook"></i></p>
</a><p><a href="https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#1877F2; text-decoration: none;"></a></p>
</div><div class="column" style="width:33%;">
<p><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#0077B5; text-decoration: none;"></a></p><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#0077B5; text-decoration: none;">
<p><i class="fa-brands fa-linkedin fa-3x" aria-label="linkedin"></i></p>
</a><p><a href="https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#0077B5; text-decoration: none;"></a></p>
</div><div class="column" style="width:33%;">
<p><a href="https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#1DA1F2; text-decoration: none;"></a></p><a href="https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#1DA1F2; text-decoration: none;">
<p><i class="fa-brands fa-twitter fa-3x" aria-label="twitter"></i></p>
</a><p><a href="https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#1DA1F2; text-decoration: none;"></a></p>
</div>
</div>
<script src="https://giscus.app/client.js" data-repo="mrislambd/mrislambd.github.io" data-repo-id="R_kgDOMV8crA" data-category="Announcements" data-category-id="DIC_kwDOMV8crM4CjbQW" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<div id="fb-root">

</div>
<script async="" defer="" crossorigin="anonymous" src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&amp;version=v20.0"></script>
<div class="fb-comments" data-href="https://mrislambd.github.io/dsandml/lda/" data-width="800" data-numposts="5">

</div>
<hr>
<p><strong>You may also like</strong></p>



<!-- -->

</section>

<div class="quarto-listing quarto-listing-container-grid" id="listing-listing">
<div class="list grid quarto-listing-cols-3">
<div class="g-col-1" data-index="0" data-categories="Statistics,Data Science,Data Engineering,Machine Learning,Artificial Intelligence" data-listing-date-sort="1728532800000" data-listing-file-modified-sort="1728684641298" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="11" data-listing-word-count-sort="2195">
<a href="../../dsandml/naivebayes/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top"><img src="lda.png" style="height: 150px;"  class="thumbnail-image card-img"/></p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Classification using Naive Bayes algorithm
</h5>
<div class="listing-reading-time card-text text-muted">
11 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Thursday, October 10, 2024
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="1" data-categories="Data Science,Machine Learning,Artificial Intelligence" data-listing-date-sort="1728273600000" data-listing-file-modified-sort="1729145810283" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="9" data-listing-word-count-sort="1737">
<a href="../../dsandml/logreg/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../dsandml/logreg/logreg.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code
</h5>
<div class="listing-reading-time card-text text-muted">
9 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Monday, October 7, 2024
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="2" data-categories="Data Science,Machine Learning,Artificial Intelligence,Data Engineering" data-listing-date-sort="1729137600000" data-listing-file-modified-sort="1729198983638" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="1" data-listing-word-count-sort="167">
<a href="../../dsandml/multiclass/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top"><img src="lda.png" style="height: 150px;"  class="thumbnail-image card-img"/></p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Classification: Techniques to handle multi-class classification problems
</h5>
<div class="listing-reading-time card-text text-muted">
1 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Thursday, October 17, 2024
</div>
</div>
</div>
</div>
</a>
</div>
</div>
<div class="listing-no-matching d-none">
No matching items
</div>
</div><a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{islam2024,
  author = {Islam, Rafiq},
  title = {Classification: {Linear} {Discriminant} {Analysis} {(LDA)}},
  date = {2024-10-17},
  url = {https://mrislambd.github.io/dsandml/lda/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-islam2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Islam, Rafiq. 2024. <span>“Classification: Linear Discriminant Analysis
(LDA).”</span> October 17, 2024. <a href="https://mrislambd.github.io/dsandml/lda/">https://mrislambd.github.io/dsandml/lda/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mrislambd\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Classification: Linear Discriminant Analysis (LDA)"</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-10-17"</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> Rafiq Islam</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Data Science, Machine Learning, Artificial Intelligence, Data Engineering]</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span><span class="co"> true</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="an">search:</span><span class="co"> true</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="an">lightbox:</span><span class="co"> true</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> lda.png</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="an">listing:</span><span class="co"> </span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">    contents: "/../../dsandml"</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">    max-items: 3</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co">    type: grid</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co">    categories: false</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">    date-format: full</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">    fields: [image, date, title, author, reading-time]  </span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">    html: </span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co">      toc: true</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co">---</span>  </span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction  </span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>Linear Discriminant Analysis (LDA) is a supervised machine learning algorithm commonly used for classification tasks. It is widely applied when dealing with datasets where the number of predictors (features) exceeds the number of observations, or when multicollinearity is a concern. LDA works by projecting data onto a lower-dimensional space, maximizing the separation between classes.&lt;/p&gt;</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical Foundation of LDA  </span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>Let’s assume we have a dataset $X \in \mathbb{R}^{n \times p}$ consisting of $n$ data points and $p$ features, and each data point belongs to one of $K$ distinct classes. The goal of LDA is to find a new space (called a discriminant space) in which the classes are maximally separated, i.e. we want to  **maximize the separability between classes** while **minimizing the variation within each class**. This can be mathematically expressed as finding a projection that maximizes the ratio of between-class variance to within-class variance.&lt;/p&gt;</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>For each class $C_k$ (where $k \in <span class="sc">\{</span>1, 2, \dots, K<span class="sc">\}</span>$):  </span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu_k$ is the **mean vector** of class $C_k$.  </span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu$ is the **overall mean** of the entire dataset.</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>**Class Mean**: For each class $C_k$, the mean is calculated as:</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>\mu_k = \frac{1}{N_k} \sum_{x_i \in C_k} x_i</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>where $N_k$ is the number of data points in class $C_k$, and $x_i$ represents individual data points.</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>**Overall Mean**: The mean of the entire dataset is:</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>\mu = \frac{1}{n} \sum_{i=1}^{n} x_i</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>To understand how well classes are separated, we need two key measures:  </span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Within-Class Scatter Matrix $S_W$**  </span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>   The within-class scatter matrix measures how the data points of each class deviate from the class mean. It captures the **spread of data points within each class**. For class $C_k$, the scatter matrix is calculated as:  </span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>    S_W = \sum_{k=1}^{K} \sum_{x_i \in C_k} (x_i - \mu_k)(x_i - \mu_k)^T</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>    This formula is saying that for each class $C_k$, we calculate the distance of every point $x_i$ from the mean of its class $\mu_k$, and then sum these squared distances across all classes.</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Between-Class Scatter Matrix $S_B$**  </span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>   The between-class scatter matrix measures how the **class means deviate from the overall mean**. It captures how well-separated the classes are.</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>    S_B = \sum_{k=1}^{K} N_k (\mu_k - \mu)(\mu_k - \mu)^T</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>    In this case, for each class $C_k$, we calculate the distance between the mean of class $\mu_k$ and the overall mean $\mu$, then scale this by the number of points in class $C_k$.</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>---  </span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>LDA aims to find a transformation that maximizes the separation between classes. This is done by finding a linear projection $\mathbf{w}$ such that the **between-class scatter is maximized** and the **within-class scatter is minimized**. Mathematically, the optimization problem becomes:&lt;/p&gt;</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>J(\mathbf{w}) = \frac{\mathbf{w}^T S_B \mathbf{w}}{\mathbf{w}^T S_W \mathbf{w}}</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$S_B \mathbf{w}$ captures the between-class variance (how well-separated the classes are in the new projection).  </span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$S_W \mathbf{w}$ captures the within-class variance (how tightly packed the points of the same class are in the new projection).</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>This ratio $J(\mathbf{w})$ is known as the **Fisher’s discriminant ratio**. The goal is to find $\mathbf{w}$ that maximizes this ratio. To maximize the Fisher’s discriminant ratio, we need to solve the following generalized eigenvalue problem:</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>S_W^{-1} S_B \mathbf{w} = \lambda \mathbf{w}</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>Here, $\mathbf{w}$ is the vector that defines the linear combination of features that maximizes class separation, and $\lambda$ is an eigenvalue that represents how much variance is explained by that direction.</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>The solution to this equation gives us the eigenvectors (directions) and eigenvalues (variances) of the transformed space. We select the top eigenvectors corresponding to the largest eigenvalues to form the projection matrix $W$.</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a><span class="fu">## Dimensionality Reduction</span></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>The LDA transformation reduces the dimensionality of the data by projecting it onto a subspace spanned by the eigenvectors with the largest eigenvalues. For a dataset with $K$ classes, LDA can reduce the data to at most $K-1$ dimensions because $S_B$ has rank $K-1$.</span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a>If we have two classes, LDA will reduce the data to a one-dimensional subspace. For three classes, LDA can project the data onto a two-dimensional subspace, and so on.&lt;/p&gt;</span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>---  </span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>Now before diving into the python code, let's do some math by hand so that we can understand the skeleton of the process. Let's create a small dataset with 6 features and 4 observations divided into 3 classes. We will use this dataset to manually go through the Linear Discriminant Analysis (LDA) process step by step.</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dataset</span></span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>| Observation | Feature 1 | Feature 2 | Feature 3 | Feature 4 | Feature 5 | Feature 6 | Class  |</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>|-------------|-----------|-----------|-----------|-----------|-----------|-----------|--------|</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>| $x_1$   | 2         | 3         | 4         | 5         | 6         | 7         | $C_1$ |</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>| $x_2$   | 3         | 4         | 5         | 6         | 7         | 8         | $C_1$ |</span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>| $x_3$   | 6         | 5         | 4         | 3         | 2         | 1         | $C_2$ |</span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a>| $x_4$   | 7         | 6         | 5         | 4         | 3         | 2         | $C_3$ |</span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>Now, we’ll walk through the mathematical steps of LDA for this small dataset.</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1. Compute Class Means $\mu_k$ for each class:</span></span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Class $C_1$ (mean of $x_1$ and $x_2$):</span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>  \mu_1 = \frac{1}{2} \left( \begin{bmatrix} 2 <span class="sc">\\</span> 3 <span class="sc">\\</span> 4 <span class="sc">\\</span> 5 <span class="sc">\\</span> 6 <span class="sc">\\</span> 7 \end{bmatrix} + \begin{bmatrix} 3 <span class="sc">\\</span> 4 <span class="sc">\\</span> 5 <span class="sc">\\</span> 6 <span class="sc">\\</span> 7 <span class="sc">\\</span> 8 \end{bmatrix} \right) = \begin{bmatrix} 2.5 <span class="sc">\\</span> 3.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 5.5 <span class="sc">\\</span> 6.5 <span class="sc">\\</span> 7.5 \end{bmatrix}</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Class $C_2$ (only one observation $x_3$):</span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a> $$</span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>  \mu_2 = \begin{bmatrix} 6 <span class="sc">\\</span> 5 <span class="sc">\\</span> 4 <span class="sc">\\</span> 3 <span class="sc">\\</span> 2 <span class="sc">\\</span> 1 \end{bmatrix}</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a> $$</span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Class $C_3$ (only one observation $x_4$):</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a> $$</span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a>  \mu_3 = \begin{bmatrix} 7 <span class="sc">\\</span> 6 <span class="sc">\\</span> 5 <span class="sc">\\</span> 4 <span class="sc">\\</span> 3 <span class="sc">\\</span> 2 \end{bmatrix}</span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a> $$</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2. Compute Overall Mean $\mu$:</span></span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a>We compute the overall mean $\mu$, which is the average of all observations from all classes:</span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a>\mu = \frac{1}{4} \left( \begin{bmatrix} 2 <span class="sc">\\</span> 3 <span class="sc">\\</span> 4 <span class="sc">\\</span> 5 <span class="sc">\\</span> 6 <span class="sc">\\</span> 7 \end{bmatrix} + \begin{bmatrix} 3 <span class="sc">\\</span> 4 <span class="sc">\\</span> 5 <span class="sc">\\</span> 6 <span class="sc">\\</span> 7 <span class="sc">\\</span> 8 \end{bmatrix} + \begin{bmatrix} 6 <span class="sc">\\</span> 5 <span class="sc">\\</span> 4 <span class="sc">\\</span> 3 <span class="sc">\\</span> 2 <span class="sc">\\</span> 1 \end{bmatrix} + \begin{bmatrix} 7 <span class="sc">\\</span> 6 <span class="sc">\\</span> 5 <span class="sc">\\</span> 4 <span class="sc">\\</span> 3 <span class="sc">\\</span> 2 \end{bmatrix} \right)= \frac{1}{4} \begin{bmatrix} 18 <span class="sc">\\</span> 18 <span class="sc">\\</span> 18 <span class="sc">\\</span> 18 <span class="sc">\\</span> 18 <span class="sc">\\</span> 18 \end{bmatrix} = \begin{bmatrix} 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 \end{bmatrix}</span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3. **Compute the Within-Class Scatter Matrix $S_W$**:</span></span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a>For each class $C_k$, the within-class scatter matrix $S_W$ is computed as:</span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>S_W = \sum_{k=1}^{K} \sum_{x_i \in C_k} (x_i - \mu_k)(x_i - \mu_k)^T</span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a>For $C_1$, the within-class scatter matrix is:</span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a>(x_1 - \mu_1) = \begin{bmatrix} 2 <span class="sc">\\</span> 3 <span class="sc">\\</span> 4 <span class="sc">\\</span> 5 <span class="sc">\\</span> 6 <span class="sc">\\</span> 7 \end{bmatrix} - \begin{bmatrix} 2.5 <span class="sc">\\</span> 3.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 5.5 <span class="sc">\\</span> 6.5 <span class="sc">\\</span> 7.5 \end{bmatrix} = \begin{bmatrix} -0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -0.5 \end{bmatrix}; \hspace{6mm} (x_2 - \mu_1) = \begin{bmatrix} 3 <span class="sc">\\</span> 4 <span class="sc">\\</span> 5 <span class="sc">\\</span> 6 <span class="sc">\\</span> 7 <span class="sc">\\</span> 8 \end{bmatrix} - \begin{bmatrix} 2.5 <span class="sc">\\</span> 3.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 5.5 <span class="sc">\\</span> 6.5 <span class="sc">\\</span> 7.5 \end{bmatrix} = \begin{bmatrix} 0.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> 0.5 \end{bmatrix}</span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a>For class $C_1$, the scatter matrix is:</span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a>S_{W1} &amp;= (x_1 - \mu_1)(x_1 - \mu_1)^T + (x_2 - \mu_1)(x_2 - \mu_1)^T<span class="sc">\\</span></span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a> &amp;=\begin{bmatrix} -0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -0.5 \end{bmatrix} \begin{bmatrix} -0.5 &amp; -0.5 &amp; -0.5 &amp; -0.5 &amp; -0.5 &amp; -0.5 \end{bmatrix} + \begin{bmatrix} 0.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> 0.5 \end{bmatrix} \begin{bmatrix} 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a> &amp;= \begin{bmatrix} 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \end{bmatrix}</span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a>For classes $C_2$ and $C_3$, there is only one data point in each, so there is no within-class scatter:</span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a>S_{W2} = 0, \quad S_{W3} = 0</span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a>Thus, the total within-class scatter matrix is:</span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a>S_W = S_{W1} + S_{W2} + S_{W3} = S_{W1}</span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a>S_W = \begin{bmatrix} 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 \end{bmatrix}</span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4. Compute the Between-Class Scatter Matrix $S_B$:</span></span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a>For each class $C_k$, the between-class scatter matrix is computed as:</span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>S_B = \sum_{k=1}^{K} N_k (\mu_k - \mu)(\mu_k - \mu)^T</span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a>For class $C_1$ (where $N_1 = 2$):</span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a>(\mu_1 - \mu) = \begin{bmatrix} 2.5 <span class="sc">\\</span> 3.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 5.5 <span class="sc">\\</span> 6.5 <span class="sc">\\</span> 7.5 \end{bmatrix} - \begin{bmatrix} 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 \end{bmatrix} = \begin{bmatrix} -2 <span class="sc">\\</span> -1 <span class="sc">\\</span> 0 <span class="sc">\\</span> 1 <span class="sc">\\</span> 2 <span class="sc">\\</span> 3 \end{bmatrix}</span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a>Thus, for $C_1$:</span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a>S_{B1} &amp;= 2 \begin{bmatrix} -2 <span class="sc">\\</span> -1 <span class="sc">\\</span> 0 <span class="sc">\\</span> 1 <span class="sc">\\</span> 2 <span class="sc">\\</span> 3 \end{bmatrix} \begin{bmatrix} -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 \end{bmatrix}= 2 \begin{bmatrix} 4 &amp; 2 &amp; 0 &amp; -2 &amp; -4 &amp; -6 <span class="sc">\\</span> 2 &amp; 1 &amp; 0 &amp; -1 &amp; -2 &amp; -3 <span class="sc">\\</span> 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span> -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 <span class="sc">\\</span> -4 &amp; -2 &amp; 0 &amp; 2 &amp; 4 &amp; 6 <span class="sc">\\</span> -6 &amp; -3 &amp; 0 &amp; 3 &amp; 6 &amp; 9 \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a>&amp;=\begin{bmatrix} 8 &amp; 4 &amp; 0 &amp; -4 &amp; -8 &amp; -12 <span class="sc">\\</span> 4 &amp; 2 &amp; 0 &amp; -2 &amp; -4 &amp; -6 <span class="sc">\\</span> 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span> -4 &amp; -2 &amp; 0 &amp; 2 &amp; 4 &amp; 6 <span class="sc">\\</span> -8 &amp; -4 &amp; 0 &amp; 4 &amp; 8 &amp; 12 <span class="sc">\\</span> -12 &amp; -6 &amp; 0 &amp; 6 &amp; 12 &amp; 18 \end{bmatrix}</span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a>For $C_2$ (where $N_2 = 1$):</span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a>(\mu_2 - \mu) = \begin{bmatrix} 6 <span class="sc">\\</span> 5 <span class="sc">\\</span> 4 <span class="sc">\\</span> 3 <span class="sc">\\</span> 2 <span class="sc">\\</span> 1 \end{bmatrix} - \begin{bmatrix} 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 \end{bmatrix} = \begin{bmatrix} 1.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -1.5 <span class="sc">\\</span> -2.5 <span class="sc">\\</span> -3.5 \end{bmatrix}</span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a>The between-class scatter matrix for $C_2$ is:</span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a>S_{B2} &amp;= \begin{bmatrix} 1.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -1.5 <span class="sc">\\</span> -2.5 <span class="sc">\\</span> -3.5 \end{bmatrix} \begin{bmatrix} 1.5 &amp; 0.5 &amp; -0.5 &amp; -1.5 &amp; -2.5 &amp; -3.5 \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a>&amp;= \begin{bmatrix} 2.25 &amp; 0.75 &amp; -0.75 &amp; -2.25 &amp; -3.75 &amp; -5.25 <span class="sc">\\</span> 0.75 &amp; 0.25 &amp; -0.25 &amp; -0.75 &amp; -1.25 &amp; -1.75 <span class="sc">\\</span> -0.75 &amp; -0.25 &amp; 0.25 &amp; 0.75 &amp; 1.25 &amp; 1.75 <span class="sc">\\</span> -2.25 &amp; -0.75 &amp; 0.75 &amp; 2.25 &amp; 3.75 &amp; 5.25 <span class="sc">\\</span> -3.75 &amp; -1.25 &amp; 1.25 &amp; 3.75 &amp; 6.25 &amp; 8.75 <span class="sc">\\</span> -5.25 &amp; -1.75 &amp; 1.75 &amp; 5.25 &amp; 8.75 &amp; 12.25 \end{bmatrix}</span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a>For $C_3$ (where $N_3 = 1$):</span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-225"><a href="#cb7-225" aria-hidden="true" tabindex="-1"></a>(\mu_3 - \mu) = \begin{bmatrix} 7 <span class="sc">\\</span> 6 <span class="sc">\\</span> 5 <span class="sc">\\</span> 4 <span class="sc">\\</span> 3 <span class="sc">\\</span> 2 \end{bmatrix} - \begin{bmatrix} 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 <span class="sc">\\</span> 4.5 \end{bmatrix} = \begin{bmatrix} 2.5 <span class="sc">\\</span> 1.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -1.5 <span class="sc">\\</span> -2.5 \end{bmatrix}</span>
<span id="cb7-226"><a href="#cb7-226" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a>The between-class scatter matrix for $C_3$ is:</span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a>S_{B3} &amp;= \begin{bmatrix} 2.5 <span class="sc">\\</span> 1.5 <span class="sc">\\</span> 0.5 <span class="sc">\\</span> -0.5 <span class="sc">\\</span> -1.5 <span class="sc">\\</span> -2.5 \end{bmatrix} \begin{bmatrix} 2.5 &amp; 1.5 &amp; 0.5 &amp; -0.5 &amp; -1.5 &amp; -2.5 \end{bmatrix}<span class="sc">\\</span></span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a>&amp;=\begin{bmatrix} 6.25 &amp; 3.75 &amp; 1.25 &amp; -1.25 &amp; -3.75 &amp; -6.25 <span class="sc">\\</span> 3.75 &amp; 2.25 &amp; 0.75 &amp; -0.75 &amp; -2.25 &amp; -3.75 <span class="sc">\\</span> 1.25 &amp; 0.75 &amp; 0.25 &amp; -0.25 &amp; -0.75 &amp; -1.25 <span class="sc">\\</span> -1.25 &amp; -0.75 &amp; -0.25 &amp; 0.25 &amp; 0.75 &amp; 1.25 <span class="sc">\\</span> -3.75 &amp; -2.25 &amp; -0.75 &amp; 0.75 &amp; 2.25 &amp; 3.75 <span class="sc">\\</span> -6.25 &amp; -3.75 &amp; -1.25 &amp; 1.25 &amp; 3.75 &amp; 6.25 \end{bmatrix}</span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a>Total Between-Class Scatter Matrix $S_B$:</span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a>S_B &amp;= S_{B1} + S_{B2} + S_{B3}<span class="sc">\\</span></span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a>&amp;=\begin{bmatrix} 8 &amp; 4 &amp; 0 &amp; -4 &amp; -8 &amp; -12 <span class="sc">\\</span> 4 &amp; 2 &amp; 0 &amp; -2 &amp; -4 &amp; -6 <span class="sc">\\</span> 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 <span class="sc">\\</span> -4 &amp; -2 &amp; 0 &amp; 2 &amp; 4 &amp; 6 <span class="sc">\\</span> -8 &amp; -4 &amp; 0 &amp; 4 &amp; 8 &amp; 12 <span class="sc">\\</span> -12 &amp; -6 &amp; 0 &amp; 6 &amp; 12 &amp; 18 \end{bmatrix} + \begin{bmatrix} 2.25 &amp; 0.75 &amp; -0.75 &amp; -2.25 &amp; -3.75 &amp; -5.25 <span class="sc">\\</span> 0.75 &amp; 0.25 &amp; -0.25 &amp; -0.75 &amp; -1.25 &amp; -1.75 <span class="sc">\\</span> -0.75 &amp; -0.25 &amp; 0.25 &amp; 0.75 &amp; 1.25 &amp; 1.75 <span class="sc">\\</span> -2.25 &amp; -0.75 &amp; 0.75 &amp; 2.25 &amp; 3.75 &amp; 5.25 <span class="sc">\\</span> -3.75 &amp; -1.25 &amp; 1.25 &amp; 3.75 &amp; 6.25 &amp; 8.75 <span class="sc">\\</span> -5.25 &amp; -1.75 &amp; 1.75 &amp; 5.25 &amp; 8.75 &amp; 12.25 \end{bmatrix} <span class="sc">\\</span></span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a>&amp;<span class="sc">\\</span></span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a>&amp; + \begin{bmatrix} 6.25 &amp; 3.75 &amp; 1.25 &amp; -1.25 &amp; -3.75 &amp; -6.25 <span class="sc">\\</span> 3.75 &amp; 2.25 &amp; 0.75 &amp; -0.75 &amp; -2.25 &amp; -3.75 <span class="sc">\\</span> 1.25 &amp; 0.75 &amp; 0.25 &amp; -0.25 &amp; -0.75 &amp; -1.25 <span class="sc">\\</span> -1.25 &amp; -0.75 &amp; -0.25 &amp; 0.25 &amp; 0.75 &amp; 1.25 <span class="sc">\\</span> -3.75 &amp; -2.25 &amp; -0.75 &amp; 0.75 &amp; 2.25 &amp; 3.75 <span class="sc">\\</span> -6.25 &amp; -3.75 &amp; -1.25 &amp; 1.25 &amp; 3.75 &amp; 6.25 \end{bmatrix}</span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a>Adding the matrices gives:</span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a>S_B = \begin{bmatrix} 16.5 &amp; 8.5 &amp; 0.5 &amp; -7.5 &amp; -15.5 &amp; -23.5 <span class="sc">\\</span> 8.5 &amp; 4.5 &amp; 0.5 &amp; -3.5 &amp; -7.5 &amp; -11.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> -7.5 &amp; -3.5 &amp; 0.5 &amp; 4.5 &amp; 8.5 &amp; 12.5 <span class="sc">\\</span> -15.5 &amp; -7.5 &amp; 0.5 &amp; 8.5 &amp; 16.5 &amp; 24.5 <span class="sc">\\</span> -23.5 &amp; -11.5 &amp; 0.5 &amp; 12.5 &amp; 24.5 &amp; 36.5 \end{bmatrix}</span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a><span class="fu">### 5. Solve the Eigenvalue Problem:</span></span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a>We now solve the eigenvalue problem:  </span>
<span id="cb7-257"><a href="#cb7-257" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-258"><a href="#cb7-258" aria-hidden="true" tabindex="-1"></a>S_W^{-1} S_B \mathbf{w} = \lambda \mathbf{w}</span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align:justify"&gt;</span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a>The solution to this eigenvalue problem gives us the eigenvalues $\lambda$ (which quantify the amount of variance captured in each direction) and the eigenvectors $\mathbf{w}$ (which give the directions of maximum class separation). The eigenvector corresponding to the largest eigenvalue defines the direction of the first discriminant axis, which is the direction that maximally separates the classes.&lt;/p&gt;</span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb7-265"><a href="#cb7-265" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix} </span>
<span id="cb7-266"><a href="#cb7-266" aria-hidden="true" tabindex="-1"></a>6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 <span class="sc">\\</span></span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a>6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 <span class="sc">\\</span></span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a>6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 <span class="sc">\\</span></span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a>6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 <span class="sc">\\</span></span>
<span id="cb7-270"><a href="#cb7-270" aria-hidden="true" tabindex="-1"></a>6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 <span class="sc">\\</span></span>
<span id="cb7-271"><a href="#cb7-271" aria-hidden="true" tabindex="-1"></a>6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 &amp; 6.67 \end{bmatrix}\begin{bmatrix} 16.5 &amp; 8.5 &amp; 0.5 &amp; -7.5 &amp; -15.5 &amp; -23.5 <span class="sc">\\</span> 8.5 &amp; 4.5 &amp; 0.5 &amp; -3.5 &amp; -7.5 &amp; -11.5 <span class="sc">\\</span> 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 &amp; 0.5 <span class="sc">\\</span> -7.5 &amp; -3.5 &amp; 0.5 &amp; 4.5 &amp; 8.5 &amp; 12.5 <span class="sc">\\</span> -15.5 &amp; -7.5 &amp; 0.5 &amp; 8.5 &amp; 16.5 &amp; 24.5 <span class="sc">\\</span> -23.5 &amp; -11.5 &amp; 0.5 &amp; 12.5 &amp; 24.5 &amp; 36.5 \end{bmatrix}\begin{bmatrix}w_1<span class="sc">\\</span>w_2<span class="sc">\\</span>w_3<span class="sc">\\</span>w_4<span class="sc">\\</span>w_5<span class="sc">\\</span>w_6\end{bmatrix}&amp;=\lambda\begin{bmatrix}w_1<span class="sc">\\</span>w_2<span class="sc">\\</span>w_3<span class="sc">\\</span>w_4<span class="sc">\\</span>w_5<span class="sc">\\</span>w_6\end{bmatrix}<span class="sc">\\</span></span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a>\implies \begin{bmatrix} </span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a>-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 <span class="sc">\\</span></span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a>-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 <span class="sc">\\</span></span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a>-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 <span class="sc">\\</span></span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a>-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 <span class="sc">\\</span></span>
<span id="cb7-277"><a href="#cb7-277" aria-hidden="true" tabindex="-1"></a>-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 <span class="sc">\\</span></span>
<span id="cb7-278"><a href="#cb7-278" aria-hidden="true" tabindex="-1"></a>-2.33 &amp; -1.00 &amp; 0.33 &amp; 1.67 &amp; 3.00 &amp; 4.33 \end{bmatrix}\begin{bmatrix}w_1<span class="sc">\\</span>w_2<span class="sc">\\</span>w_3<span class="sc">\\</span>w_4<span class="sc">\\</span>w_5<span class="sc">\\</span>w_6\end{bmatrix}&amp;=\lambda\begin{bmatrix}w_1<span class="sc">\\</span>w_2<span class="sc">\\</span>w_3<span class="sc">\\</span>w_4<span class="sc">\\</span>w_5<span class="sc">\\</span>w_6\end{bmatrix}</span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a>The eigenvalues of the matrix are:</span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a>\lambda_1 = 6.00, \quad \lambda_2 = 1.78 \times 10^{-15}, \quad \lambda_3 = 9.86 \times 10^{-32}, \quad \lambda_4 = 0.00, \quad \lambda_5 = -5.47 \times 10^{-48}, \quad \lambda_6 = -5.95 \times 10^{-16}</span>
<span id="cb7-285"><a href="#cb7-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-286"><a href="#cb7-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a>The two largest eigenvalues are:</span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-289"><a href="#cb7-289" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\lambda_1 = 6.00$</span>
<span id="cb7-290"><a href="#cb7-290" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\lambda_2 = 1.78 \times 10^{-15}$</span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a>The corresponding eigenvectors for the two largest eigenvalues are:</span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a>\mathbf{w_1} = \begin{bmatrix} </span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a>-0.408 <span class="sc">\\</span></span>
<span id="cb7-297"><a href="#cb7-297" aria-hidden="true" tabindex="-1"></a>-0.408 <span class="sc">\\</span></span>
<span id="cb7-298"><a href="#cb7-298" aria-hidden="true" tabindex="-1"></a>-0.408 <span class="sc">\\</span></span>
<span id="cb7-299"><a href="#cb7-299" aria-hidden="true" tabindex="-1"></a>-0.408 <span class="sc">\\</span></span>
<span id="cb7-300"><a href="#cb7-300" aria-hidden="true" tabindex="-1"></a>-0.408 <span class="sc">\\</span></span>
<span id="cb7-301"><a href="#cb7-301" aria-hidden="true" tabindex="-1"></a>-0.408 \end{bmatrix}, \quad</span>
<span id="cb7-302"><a href="#cb7-302" aria-hidden="true" tabindex="-1"></a>\mathbf{w_2} = \begin{bmatrix} </span>
<span id="cb7-303"><a href="#cb7-303" aria-hidden="true" tabindex="-1"></a>-0.848 <span class="sc">\\</span></span>
<span id="cb7-304"><a href="#cb7-304" aria-hidden="true" tabindex="-1"></a>-0.237 <span class="sc">\\</span></span>
<span id="cb7-305"><a href="#cb7-305" aria-hidden="true" tabindex="-1"></a>-0.237 <span class="sc">\\</span></span>
<span id="cb7-306"><a href="#cb7-306" aria-hidden="true" tabindex="-1"></a>-0.237 <span class="sc">\\</span></span>
<span id="cb7-307"><a href="#cb7-307" aria-hidden="true" tabindex="-1"></a>-0.237 <span class="sc">\\</span></span>
<span id="cb7-308"><a href="#cb7-308" aria-hidden="true" tabindex="-1"></a>-0.237 \end{bmatrix}</span>
<span id="cb7-309"><a href="#cb7-309" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-310"><a href="#cb7-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-311"><a href="#cb7-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-312"><a href="#cb7-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-313"><a href="#cb7-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-314"><a href="#cb7-314" aria-hidden="true" tabindex="-1"></a>By projecting the data onto the eigenvector $\mathbf{w}$, we transform the original dataset into a lower-dimensional space where class separability is maximized. For this dataset, since there are 3 classes, LDA will find up to $K-1 = 2$ discriminant axes. Let's see how. </span>
<span id="cb7-315"><a href="#cb7-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-316"><a href="#cb7-316" aria-hidden="true" tabindex="-1"></a>The matrix formed by the two largest eigenvectors is:</span>
<span id="cb7-317"><a href="#cb7-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-318"><a href="#cb7-318" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-319"><a href="#cb7-319" aria-hidden="true" tabindex="-1"></a>W=\begin{bmatrix} </span>
<span id="cb7-320"><a href="#cb7-320" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.848 <span class="sc">\\</span></span>
<span id="cb7-321"><a href="#cb7-321" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.237 <span class="sc">\\</span></span>
<span id="cb7-322"><a href="#cb7-322" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.237 <span class="sc">\\</span></span>
<span id="cb7-323"><a href="#cb7-323" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.237 <span class="sc">\\</span></span>
<span id="cb7-324"><a href="#cb7-324" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.237 <span class="sc">\\</span></span>
<span id="cb7-325"><a href="#cb7-325" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.237 \end{bmatrix}</span>
<span id="cb7-326"><a href="#cb7-326" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-327"><a href="#cb7-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-328"><a href="#cb7-328" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb7-329"><a href="#cb7-329" aria-hidden="true" tabindex="-1"></a>This matrix represents the projection directions corresponding to the two largest eigenvalues in the Linear Discriminant Analysis process. With the eigenvectors $\mathbf{w}_1$ and $\mathbf{w}_2$, we can now project our original dataset onto the new 2D subspace.</span>
<span id="cb7-330"><a href="#cb7-330" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;&lt;br&gt;</span>
<span id="cb7-331"><a href="#cb7-331" aria-hidden="true" tabindex="-1"></a>Now, let $X$ represent our original dataset (where each row corresponds to an observation and each column to a feature). The projection of the original data onto the new 2D subspace is given by:&lt;/p&gt;</span>
<span id="cb7-332"><a href="#cb7-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-333"><a href="#cb7-333" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-334"><a href="#cb7-334" aria-hidden="true" tabindex="-1"></a>Y = X W</span>
<span id="cb7-335"><a href="#cb7-335" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-336"><a href="#cb7-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-337"><a href="#cb7-337" aria-hidden="true" tabindex="-1"></a>Where:  </span>
<span id="cb7-338"><a href="#cb7-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-339"><a href="#cb7-339" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X$ is the $4 \times 6$ matrix (4 observations, 6 features),</span>
<span id="cb7-340"><a href="#cb7-340" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$W$ is the $6 \times 2$ matrix of eigenvectors.</span>
<span id="cb7-341"><a href="#cb7-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-342"><a href="#cb7-342" aria-hidden="true" tabindex="-1"></a>After multiplying $X$ by $W$, we obtain the projected data matrix $Y$, which is a $4 \times 2$ matrix (4 observations, 2 features):</span>
<span id="cb7-343"><a href="#cb7-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-344"><a href="#cb7-344" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-345"><a href="#cb7-345" aria-hidden="true" tabindex="-1"></a>Y = \begin{bmatrix}</span>
<span id="cb7-346"><a href="#cb7-346" aria-hidden="true" tabindex="-1"></a>y_{11} &amp; y_{12} <span class="sc">\\</span></span>
<span id="cb7-347"><a href="#cb7-347" aria-hidden="true" tabindex="-1"></a>y_{21} &amp; y_{22} <span class="sc">\\</span></span>
<span id="cb7-348"><a href="#cb7-348" aria-hidden="true" tabindex="-1"></a>y_{31} &amp; y_{32} <span class="sc">\\</span></span>
<span id="cb7-349"><a href="#cb7-349" aria-hidden="true" tabindex="-1"></a>y_{41} &amp; y_{42}</span>
<span id="cb7-350"><a href="#cb7-350" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb7-351"><a href="#cb7-351" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-352"><a href="#cb7-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-353"><a href="#cb7-353" aria-hidden="true" tabindex="-1"></a>This matrix $Y$ represents the data in the new 2D space where class separability is maximized. So for our data  </span>
<span id="cb7-354"><a href="#cb7-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-355"><a href="#cb7-355" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb7-356"><a href="#cb7-356" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}2&amp;3&amp;4&amp;5&amp;6&amp;7<span class="sc">\\</span>3&amp;4&amp;5&amp;6&amp;7&amp;8<span class="sc">\\</span>6&amp;5&amp;4&amp;3&amp;2&amp;1<span class="sc">\\</span>7&amp;6&amp;5&amp;4&amp;3&amp;2\end{bmatrix}\begin{bmatrix} </span>
<span id="cb7-357"><a href="#cb7-357" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.848 <span class="sc">\\</span></span>
<span id="cb7-358"><a href="#cb7-358" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.237 <span class="sc">\\</span></span>
<span id="cb7-359"><a href="#cb7-359" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.237 <span class="sc">\\</span></span>
<span id="cb7-360"><a href="#cb7-360" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.237 <span class="sc">\\</span></span>
<span id="cb7-361"><a href="#cb7-361" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.237 <span class="sc">\\</span></span>
<span id="cb7-362"><a href="#cb7-362" aria-hidden="true" tabindex="-1"></a>-0.408 &amp; -0.237 \end{bmatrix}&amp;=\begin{bmatrix}-11.016 &amp; -7.621<span class="sc">\\</span>-13.464 &amp; -9.654<span class="sc">\\</span> -8.568 &amp; -8.643<span class="sc">\\</span>-11.016 &amp;-10.676\end{bmatrix}</span>
<span id="cb7-363"><a href="#cb7-363" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb7-364"><a href="#cb7-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-365"><a href="#cb7-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-366"><a href="#cb7-366" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step 6: Visualizing the Results</span></span>
<span id="cb7-367"><a href="#cb7-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-368"><a href="#cb7-368" aria-hidden="true" tabindex="-1"></a>If we were to plot the projected data in this new 2D space, we would see the observations from different classes are better separated, which is the ultimate goal of LDA. The two axes of this 2D space correspond to the two linear discriminants that maximize the separation between the classes.</span>
<span id="cb7-369"><a href="#cb7-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-372"><a href="#cb7-372" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-373"><a href="#cb7-373" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb7-374"><a href="#cb7-374" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-375"><a href="#cb7-375" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-376"><a href="#cb7-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-377"><a href="#cb7-377" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>],</span>
<span id="cb7-378"><a href="#cb7-378" aria-hidden="true" tabindex="-1"></a>                  [<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>],</span>
<span id="cb7-379"><a href="#cb7-379" aria-hidden="true" tabindex="-1"></a>                  [<span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>],</span>
<span id="cb7-380"><a href="#cb7-380" aria-hidden="true" tabindex="-1"></a>                  [<span class="dv">7</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>]])</span>
<span id="cb7-381"><a href="#cb7-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-382"><a href="#cb7-382" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.array([[<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.848</span>],</span>
<span id="cb7-383"><a href="#cb7-383" aria-hidden="true" tabindex="-1"></a>                         [<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.237</span>],</span>
<span id="cb7-384"><a href="#cb7-384" aria-hidden="true" tabindex="-1"></a>                         [<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.237</span>],</span>
<span id="cb7-385"><a href="#cb7-385" aria-hidden="true" tabindex="-1"></a>                         [<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.237</span>],</span>
<span id="cb7-386"><a href="#cb7-386" aria-hidden="true" tabindex="-1"></a>                         [<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.237</span>],</span>
<span id="cb7-387"><a href="#cb7-387" aria-hidden="true" tabindex="-1"></a>                         [<span class="op">-</span><span class="fl">0.408</span>, <span class="op">-</span><span class="fl">0.237</span>]])</span>
<span id="cb7-388"><a href="#cb7-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-389"><a href="#cb7-389" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.dot(X, W)</span>
<span id="cb7-390"><a href="#cb7-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-391"><a href="#cb7-391" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the projection</span></span>
<span id="cb7-392"><a href="#cb7-392" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb7-393"><a href="#cb7-393" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(Y.shape[<span class="dv">0</span>]):</span>
<span id="cb7-394"><a href="#cb7-394" aria-hidden="true" tabindex="-1"></a>    plt.scatter(Y[i, <span class="dv">0</span>], Y[i, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f'Obs </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb7-395"><a href="#cb7-395" aria-hidden="true" tabindex="-1"></a>    plt.text(Y[i, <span class="dv">0</span>] <span class="op">+</span> <span class="fl">0.02</span>, Y[i, <span class="dv">1</span>] <span class="op">+</span> <span class="fl">0.02</span>, <span class="ss">f'Obs </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb7-396"><a href="#cb7-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-397"><a href="#cb7-397" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Projected Data after LDA"</span>)</span>
<span id="cb7-398"><a href="#cb7-398" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'LD1 (First Linear Discriminant)'</span>)</span>
<span id="cb7-399"><a href="#cb7-399" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'LD2 (Second Linear Discriminant)'</span>)</span>
<span id="cb7-400"><a href="#cb7-400" aria-hidden="true" tabindex="-1"></a>plt.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'gray'</span>, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-401"><a href="#cb7-401" aria-hidden="true" tabindex="-1"></a>plt.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'gray'</span>, lw<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-402"><a href="#cb7-402" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb7-403"><a href="#cb7-403" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'upper right'</span>)</span>
<span id="cb7-404"><a href="#cb7-404" aria-hidden="true" tabindex="-1"></a>plt.gcf().patch.set_facecolor(<span class="st">'#f4f4f4'</span>)</span>
<span id="cb7-405"><a href="#cb7-405" aria-hidden="true" tabindex="-1"></a>plt.gca().set_facecolor(<span class="st">'#f4f4f4'</span>)</span>
<span id="cb7-406"><a href="#cb7-406" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-407"><a href="#cb7-407" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-408"><a href="#cb7-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-409"><a href="#cb7-409" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-410"><a href="#cb7-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-411"><a href="#cb7-411" aria-hidden="true" tabindex="-1"></a><span class="fu">### Summary of the Process of Eigenvalue Problem</span></span>
<span id="cb7-412"><a href="#cb7-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-413"><a href="#cb7-413" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Eigenvalue Calculation**: We found the eigenvalues $\lambda_1$ and $\lambda_2$ to be the largest, indicating the directions with the most class separability. We did find only two eigenvaleus since total class is 3.</span>
<span id="cb7-414"><a href="#cb7-414" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Eigenvector Calculation**: We computed the eigenvectors $\mathbf{w}_1$ and $\mathbf{w}_2$ corresponding to these eigenvalues. These eigenvectors define the directions in the original feature space along which the class separation is maximized.</span>
<span id="cb7-415"><a href="#cb7-415" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Projection**: We projected the original dataset onto the new 2D subspace spanned by the eigenvectors. This resulted in a new dataset in 2D, where the different classes are more separable.</span>
<span id="cb7-416"><a href="#cb7-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-417"><a href="#cb7-417" aria-hidden="true" tabindex="-1"></a>This completes the detailed walkthrough of solving the eigenvalue problem in LDA for our example dataset.</span>
<span id="cb7-418"><a href="#cb7-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-419"><a href="#cb7-419" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-420"><a href="#cb7-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-421"><a href="#cb7-421" aria-hidden="true" tabindex="-1"></a><span class="fu">### Final Summary</span></span>
<span id="cb7-422"><a href="#cb7-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-423"><a href="#cb7-423" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Within-class scatter matrix** $S_W$ quantifies the spread of data points within each class, and we calculated it for each class.</span>
<span id="cb7-424"><a href="#cb7-424" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Between-class scatter matrix** $S_B$ quantifies the separation between the class means, and we calculated it using the mean of each class and the overall mean.</span>
<span id="cb7-425"><a href="#cb7-425" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Solving the **eigenvalue problem** $S_W^{-1} S_B \mathbf{w} = \lambda \mathbf{w}$ gives us the directions $\mathbf{w}$ (eigenvectors) that maximize class separation.</span>
<span id="cb7-426"><a href="#cb7-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-427"><a href="#cb7-427" aria-hidden="true" tabindex="-1"></a>This is how LDA works step by step, using a small dataset as an example.</span>
<span id="cb7-428"><a href="#cb7-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-429"><a href="#cb7-429" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python Code Example</span></span>
<span id="cb7-430"><a href="#cb7-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-431"><a href="#cb7-431" aria-hidden="true" tabindex="-1"></a>Let’s now revisit the Python code, with an understanding of the math behind LDA. First build our own classifier  </span>
<span id="cb7-432"><a href="#cb7-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-435"><a href="#cb7-435" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-436"><a href="#cb7-436" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomLDA:</span>
<span id="cb7-437"><a href="#cb7-437" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,n_components <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb7-438"><a href="#cb7-438" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-439"><a href="#cb7-439" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb7-440"><a href="#cb7-440" aria-hidden="true" tabindex="-1"></a><span class="co">        n_components: int, optional (default=None)</span></span>
<span id="cb7-441"><a href="#cb7-441" aria-hidden="true" tabindex="-1"></a><span class="co">                      Number of components to keep. If None, all components are kept</span></span>
<span id="cb7-442"><a href="#cb7-442" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-443"><a href="#cb7-443" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_components <span class="op">=</span> n_components</span>
<span id="cb7-444"><a href="#cb7-444" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eigenvalues <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-445"><a href="#cb7-445" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eigenvectors <span class="op">=</span> <span class="va">None</span> </span>
<span id="cb7-446"><a href="#cb7-446" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean_vectors <span class="op">=</span> <span class="va">None</span> </span>
<span id="cb7-447"><a href="#cb7-447" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.class_means <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-448"><a href="#cb7-448" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-449"><a href="#cb7-449" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y):</span>
<span id="cb7-450"><a href="#cb7-450" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-451"><a href="#cb7-451" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb7-452"><a href="#cb7-452" aria-hidden="true" tabindex="-1"></a><span class="co">        X: ndarray of shape (n_samples, n_features)</span></span>
<span id="cb7-453"><a href="#cb7-453" aria-hidden="true" tabindex="-1"></a><span class="co">        y: ndarray of shape (n_samples,)</span></span>
<span id="cb7-454"><a href="#cb7-454" aria-hidden="true" tabindex="-1"></a><span class="co">           Target labels (must be categorical)</span></span>
<span id="cb7-455"><a href="#cb7-455" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-456"><a href="#cb7-456" aria-hidden="true" tabindex="-1"></a>        n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb7-457"><a href="#cb7-457" aria-hidden="true" tabindex="-1"></a>        class_labels <span class="op">=</span> np.unique(y)</span>
<span id="cb7-458"><a href="#cb7-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-459"><a href="#cb7-459" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step1: Compute the class means mu_k for each class </span></span>
<span id="cb7-460"><a href="#cb7-460" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean_vectors <span class="op">=</span> []</span>
<span id="cb7-461"><a href="#cb7-461" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> class_labels:</span>
<span id="cb7-462"><a href="#cb7-462" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mean_vectors.append(np.mean(X[y<span class="op">==</span>c], axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb7-463"><a href="#cb7-463" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-464"><a href="#cb7-464" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: Compute the within-class scatter matrix S_W </span></span>
<span id="cb7-465"><a href="#cb7-465" aria-hidden="true" tabindex="-1"></a>        S_W <span class="op">=</span> np.zeros((n_features, n_features))</span>
<span id="cb7-466"><a href="#cb7-466" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> class_labels:</span>
<span id="cb7-467"><a href="#cb7-467" aria-hidden="true" tabindex="-1"></a>            class_scatter <span class="op">=</span> np.cov(X[y<span class="op">==</span>c].T, bias<span class="op">=</span><span class="va">True</span>) <span class="co"># Covariance matrix for each class</span></span>
<span id="cb7-468"><a href="#cb7-468" aria-hidden="true" tabindex="-1"></a>            S_W <span class="op">+=</span> class_scatter <span class="op">*</span> (X[y<span class="op">==</span>c].shape[<span class="dv">0</span>])</span>
<span id="cb7-469"><a href="#cb7-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-470"><a href="#cb7-470" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: Compute the between-class scatter matrix S_B</span></span>
<span id="cb7-471"><a href="#cb7-471" aria-hidden="true" tabindex="-1"></a>        overall_mean <span class="op">=</span> np.mean(X, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-472"><a href="#cb7-472" aria-hidden="true" tabindex="-1"></a>        S_B <span class="op">=</span> np.zeros((n_features, n_features))</span>
<span id="cb7-473"><a href="#cb7-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-474"><a href="#cb7-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-475"><a href="#cb7-475" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i,mean_vector <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.mean_vectors):</span>
<span id="cb7-476"><a href="#cb7-476" aria-hidden="true" tabindex="-1"></a>            n <span class="op">=</span> X[y <span class="op">==</span> class_labels[i]].shape[<span class="dv">0</span>]</span>
<span id="cb7-477"><a href="#cb7-477" aria-hidden="true" tabindex="-1"></a>            mean_differences <span class="op">=</span> (mean_vector <span class="op">-</span>overall_mean).reshape(n_features,<span class="dv">1</span>)</span>
<span id="cb7-478"><a href="#cb7-478" aria-hidden="true" tabindex="-1"></a>            S_B <span class="op">+=</span> n<span class="op">*</span>(mean_differences).dot(mean_differences.T)</span>
<span id="cb7-479"><a href="#cb7-479" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-480"><a href="#cb7-480" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Solve the Eigenvalue problem </span></span>
<span id="cb7-481"><a href="#cb7-481" aria-hidden="true" tabindex="-1"></a>        eigvalues, eigvectors <span class="op">=</span> np.linalg.eig(np.linalg.pinv(S_W).dot(S_B))</span>
<span id="cb7-482"><a href="#cb7-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-483"><a href="#cb7-483" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: Sort the Eigenvalues and corresponding eigenvectors </span></span>
<span id="cb7-484"><a href="#cb7-484" aria-hidden="true" tabindex="-1"></a>        eigvalues_sort_idx <span class="op">=</span> np.argsort(np.<span class="bu">abs</span>(eigvalues))[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb7-485"><a href="#cb7-485" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eigenvalues <span class="op">=</span> eigvalues[eigvalues_sort_idx]</span>
<span id="cb7-486"><a href="#cb7-486" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eigenvectors <span class="op">=</span> eigvectors[:,eigvalues_sort_idx]</span>
<span id="cb7-487"><a href="#cb7-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-488"><a href="#cb7-488" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 6: Keep only the top n_components</span></span>
<span id="cb7-489"><a href="#cb7-489" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.n_components:</span>
<span id="cb7-490"><a href="#cb7-490" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.eigenvectors <span class="op">=</span> <span class="va">self</span>.eigenvectors[:,:<span class="va">self</span>.n_components]</span>
<span id="cb7-491"><a href="#cb7-491" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-492"><a href="#cb7-492" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.class_means <span class="op">=</span> np.dot(<span class="va">self</span>.mean_vectors, <span class="va">self</span>.eigenvectors)</span>
<span id="cb7-493"><a href="#cb7-493" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-494"><a href="#cb7-494" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> transform(<span class="va">self</span>,X):</span>
<span id="cb7-495"><a href="#cb7-495" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-496"><a href="#cb7-496" aria-hidden="true" tabindex="-1"></a><span class="co">        Project the data onto the LDA components </span></span>
<span id="cb7-497"><a href="#cb7-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-498"><a href="#cb7-498" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb7-499"><a href="#cb7-499" aria-hidden="true" tabindex="-1"></a><span class="co">        X: ndarray of shape (n_samples, n_features)</span></span>
<span id="cb7-500"><a href="#cb7-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-501"><a href="#cb7-501" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb7-502"><a href="#cb7-502" aria-hidden="true" tabindex="-1"></a><span class="co">        X_transformed: ndarray of shape (n_samples, n_features)</span></span>
<span id="cb7-503"><a href="#cb7-503" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-504"><a href="#cb7-504" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.dot(X,<span class="va">self</span>.eigenvectors)</span>
<span id="cb7-505"><a href="#cb7-505" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-506"><a href="#cb7-506" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit_transform(<span class="va">self</span>, X, y):</span>
<span id="cb7-507"><a href="#cb7-507" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-508"><a href="#cb7-508" aria-hidden="true" tabindex="-1"></a><span class="co">        Fit the LDA model and transform the data.</span></span>
<span id="cb7-509"><a href="#cb7-509" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb7-510"><a href="#cb7-510" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb7-511"><a href="#cb7-511" aria-hidden="true" tabindex="-1"></a><span class="co">        X : ndarray of shape (n_samples, n_features)</span></span>
<span id="cb7-512"><a href="#cb7-512" aria-hidden="true" tabindex="-1"></a><span class="co">            Training data.</span></span>
<span id="cb7-513"><a href="#cb7-513" aria-hidden="true" tabindex="-1"></a><span class="co">        y : ndarray of shape (n_samples,)</span></span>
<span id="cb7-514"><a href="#cb7-514" aria-hidden="true" tabindex="-1"></a><span class="co">            Target labels (must be categorical).</span></span>
<span id="cb7-515"><a href="#cb7-515" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb7-516"><a href="#cb7-516" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb7-517"><a href="#cb7-517" aria-hidden="true" tabindex="-1"></a><span class="co">        X_transformed : ndarray of shape (n_samples, n_components)</span></span>
<span id="cb7-518"><a href="#cb7-518" aria-hidden="true" tabindex="-1"></a><span class="co">            Transformed data after fitting.</span></span>
<span id="cb7-519"><a href="#cb7-519" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-520"><a href="#cb7-520" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fit(X, y)</span>
<span id="cb7-521"><a href="#cb7-521" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.transform(X)</span>
<span id="cb7-522"><a href="#cb7-522" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-523"><a href="#cb7-523" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb7-524"><a href="#cb7-524" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-525"><a href="#cb7-525" aria-hidden="true" tabindex="-1"></a><span class="co">        Predict the class labels for new data points.</span></span>
<span id="cb7-526"><a href="#cb7-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-527"><a href="#cb7-527" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb7-528"><a href="#cb7-528" aria-hidden="true" tabindex="-1"></a><span class="co">        X : ndarray of shape (n_samples, n_features)</span></span>
<span id="cb7-529"><a href="#cb7-529" aria-hidden="true" tabindex="-1"></a><span class="co">            New data to classify.</span></span>
<span id="cb7-530"><a href="#cb7-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-531"><a href="#cb7-531" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb7-532"><a href="#cb7-532" aria-hidden="true" tabindex="-1"></a><span class="co">        Predictions: ndarray of shape (n_samples,)</span></span>
<span id="cb7-533"><a href="#cb7-533" aria-hidden="true" tabindex="-1"></a><span class="co">                     Predicted class labels</span></span>
<span id="cb7-534"><a href="#cb7-534" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-535"><a href="#cb7-535" aria-hidden="true" tabindex="-1"></a>        X_projected <span class="op">=</span> <span class="va">self</span>.transform(X)</span>
<span id="cb7-536"><a href="#cb7-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-537"><a href="#cb7-537" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> []</span>
<span id="cb7-538"><a href="#cb7-538" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> X_projected:</span>
<span id="cb7-539"><a href="#cb7-539" aria-hidden="true" tabindex="-1"></a>            distances <span class="op">=</span> np.linalg.norm(x<span class="op">-</span><span class="va">self</span>.class_means, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-540"><a href="#cb7-540" aria-hidden="true" tabindex="-1"></a>            predictions.append(np.argmin(distances))</span>
<span id="cb7-541"><a href="#cb7-541" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-542"><a href="#cb7-542" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(predictions)</span>
<span id="cb7-543"><a href="#cb7-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-544"><a href="#cb7-544" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-545"><a href="#cb7-545" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> explained_variance_ratio(<span class="va">self</span>):</span>
<span id="cb7-546"><a href="#cb7-546" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-547"><a href="#cb7-547" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the percentage of variance explained by each of the selected components</span></span>
<span id="cb7-548"><a href="#cb7-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-549"><a href="#cb7-549" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb7-550"><a href="#cb7-550" aria-hidden="true" tabindex="-1"></a><span class="co">        explained_variance: ndarray of shape (n_components,)</span></span>
<span id="cb7-551"><a href="#cb7-551" aria-hidden="true" tabindex="-1"></a><span class="co">                            Percentage of variance explained by each selected components</span></span>
<span id="cb7-552"><a href="#cb7-552" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-553"><a href="#cb7-553" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> np.<span class="bu">sum</span>(<span class="va">self</span>.eigenvalues)</span>
<span id="cb7-554"><a href="#cb7-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-555"><a href="#cb7-555" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [(i<span class="op">/</span>total) <span class="cf">for</span> i <span class="kw">in</span> <span class="va">self</span>.eigenvalues[:<span class="va">self</span>.n_components]]</span>
<span id="cb7-556"><a href="#cb7-556" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb7-557"><a href="#cb7-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-558"><a href="#cb7-558" aria-hidden="true" tabindex="-1"></a>Next we apply both the custom classifier and the classifier from the <span class="in">`scikit-learn`</span> library.</span>
<span id="cb7-559"><a href="#cb7-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-562"><a href="#cb7-562" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-563"><a href="#cb7-563" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-564"><a href="#cb7-564" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb7-565"><a href="#cb7-565" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb7-566"><a href="#cb7-566" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.discriminant_analysis <span class="im">import</span> LinearDiscriminantAnalysis <span class="im">as</span> LDA</span>
<span id="cb7-567"><a href="#cb7-567" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb7-568"><a href="#cb7-568" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb7-569"><a href="#cb7-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-570"><a href="#cb7-570" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb7-571"><a href="#cb7-571" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb7-572"><a href="#cb7-572" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> iris.data</span>
<span id="cb7-573"><a href="#cb7-573" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> iris.target</span>
<span id="cb7-574"><a href="#cb7-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-575"><a href="#cb7-575" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the dataset (optional but often improves performance)</span></span>
<span id="cb7-576"><a href="#cb7-576" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb7-577"><a href="#cb7-577" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb7-578"><a href="#cb7-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-579"><a href="#cb7-579" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test sets</span></span>
<span id="cb7-580"><a href="#cb7-580" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_scaled, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb7-581"><a href="#cb7-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-582"><a href="#cb7-582" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply LDA from the scikit-learn library</span></span>
<span id="cb7-583"><a href="#cb7-583" aria-hidden="true" tabindex="-1"></a>lda1 <span class="op">=</span> LDA(n_components<span class="op">=</span><span class="dv">2</span>)  <span class="co"># Reduce to 2 dimensions</span></span>
<span id="cb7-584"><a href="#cb7-584" aria-hidden="true" tabindex="-1"></a>X_train_lda1 <span class="op">=</span> lda1.fit_transform(X_train, y_train)</span>
<span id="cb7-585"><a href="#cb7-585" aria-hidden="true" tabindex="-1"></a>X_test_lda1 <span class="op">=</span> lda1.transform(X_test)</span>
<span id="cb7-586"><a href="#cb7-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-587"><a href="#cb7-587" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply LDA from the custom built classifier</span></span>
<span id="cb7-588"><a href="#cb7-588" aria-hidden="true" tabindex="-1"></a>lda2 <span class="op">=</span> CustomLDA(n_components<span class="op">=</span><span class="dv">2</span>)  <span class="co"># Reduce to 2 dimensions</span></span>
<span id="cb7-589"><a href="#cb7-589" aria-hidden="true" tabindex="-1"></a>X_train_lda2 <span class="op">=</span> lda2.fit_transform(X_train, y_train)</span>
<span id="cb7-590"><a href="#cb7-590" aria-hidden="true" tabindex="-1"></a>X_test_lda2 <span class="op">=</span> lda2.transform(X_test)</span>
<span id="cb7-591"><a href="#cb7-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-592"><a href="#cb7-592" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the LDA-transformed data</span></span>
<span id="cb7-593"><a href="#cb7-593" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="fl">9.5</span>,<span class="dv">4</span>))</span>
<span id="cb7-594"><a href="#cb7-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-595"><a href="#cb7-595" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].scatter(X_train_lda1[:, <span class="dv">0</span>], X_train_lda1[:, <span class="dv">1</span>], c<span class="op">=</span>y_train, cmap<span class="op">=</span><span class="st">'rainbow'</span>, edgecolor<span class="op">=</span><span class="st">'k'</span>, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb7-596"><a href="#cb7-596" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'LD1'</span>)</span>
<span id="cb7-597"><a href="#cb7-597" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'LD2'</span>)</span>
<span id="cb7-598"><a href="#cb7-598" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Scikit-learn'</span>)</span>
<span id="cb7-599"><a href="#cb7-599" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].scatter(X_train_lda2[:, <span class="dv">0</span>], X_train_lda2[:, <span class="dv">1</span>], c<span class="op">=</span>y_train, cmap<span class="op">=</span><span class="st">'rainbow'</span>, edgecolor<span class="op">=</span><span class="st">'k'</span>, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb7-600"><a href="#cb7-600" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'LD1'</span>)</span>
<span id="cb7-601"><a href="#cb7-601" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'LD2'</span>)</span>
<span id="cb7-602"><a href="#cb7-602" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Custom'</span>)</span>
<span id="cb7-603"><a href="#cb7-603" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axes:</span>
<span id="cb7-604"><a href="#cb7-604" aria-hidden="true" tabindex="-1"></a>    ax.set_facecolor(<span class="st">'#f4f4f4'</span>)</span>
<span id="cb7-605"><a href="#cb7-605" aria-hidden="true" tabindex="-1"></a>plt.gcf().patch.set_facecolor(<span class="st">'#f4f4f4'</span>)</span>
<span id="cb7-606"><a href="#cb7-606" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'LDA: Projection of the Iris Dataset'</span>)</span>
<span id="cb7-607"><a href="#cb7-607" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-608"><a href="#cb7-608" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb7-609"><a href="#cb7-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-610"><a href="#cb7-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-611"><a href="#cb7-611" aria-hidden="true" tabindex="-1"></a>Next, apply LDA as a classifiers for the actual classification  </span>
<span id="cb7-612"><a href="#cb7-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-615"><a href="#cb7-615" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-616"><a href="#cb7-616" aria-hidden="true" tabindex="-1"></a>lda_classifier1 <span class="op">=</span> LDA()</span>
<span id="cb7-617"><a href="#cb7-617" aria-hidden="true" tabindex="-1"></a>lda_classifier1.fit(X_train, y_train)</span>
<span id="cb7-618"><a href="#cb7-618" aria-hidden="true" tabindex="-1"></a>y_pred1 <span class="op">=</span> lda_classifier1.predict(X_test)</span>
<span id="cb7-619"><a href="#cb7-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-620"><a href="#cb7-620" aria-hidden="true" tabindex="-1"></a>lda_classifier2 <span class="op">=</span> CustomLDA()</span>
<span id="cb7-621"><a href="#cb7-621" aria-hidden="true" tabindex="-1"></a>lda_classifier2.fit(X_train, y_train)</span>
<span id="cb7-622"><a href="#cb7-622" aria-hidden="true" tabindex="-1"></a>y_pred2 <span class="op">=</span> lda_classifier2.predict(X_test)</span>
<span id="cb7-623"><a href="#cb7-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-624"><a href="#cb7-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-625"><a href="#cb7-625" aria-hidden="true" tabindex="-1"></a><span class="co"># Check accuracy</span></span>
<span id="cb7-626"><a href="#cb7-626" aria-hidden="true" tabindex="-1"></a>accuracy1 <span class="op">=</span> accuracy_score(y_test, y_pred1)</span>
<span id="cb7-627"><a href="#cb7-627" aria-hidden="true" tabindex="-1"></a>accuracy2 <span class="op">=</span> accuracy_score(y_test, y_pred2)</span>
<span id="cb7-628"><a href="#cb7-628" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'sklearn LDA Classifier Accuracy: </span><span class="sc">{</span>accuracy1 <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">% and </span><span class="ch">\n</span><span class="ss">custom LDA Classifier Accuracy: </span><span class="sc">{</span>accuracy2 <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb7-629"><a href="#cb7-629" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb7-630"><a href="#cb7-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-631"><a href="#cb7-631" aria-hidden="true" tabindex="-1"></a>Not too bad, huh! Let's see the confusion matrix for our custom classifier  </span>
<span id="cb7-632"><a href="#cb7-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-635"><a href="#cb7-635" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-636"><a href="#cb7-636" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb7-637"><a href="#cb7-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-638"><a href="#cb7-638" aria-hidden="true" tabindex="-1"></a>conf_mat <span class="op">=</span> confusion_matrix(y_test, y_pred2)</span>
<span id="cb7-639"><a href="#cb7-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-640"><a href="#cb7-640" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(</span>
<span id="cb7-641"><a href="#cb7-641" aria-hidden="true" tabindex="-1"></a>    conf_mat, </span>
<span id="cb7-642"><a href="#cb7-642" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span>[<span class="st">'Predicted: Setosa'</span>,<span class="st">'Predicted: Virginica'</span>, <span class="st">'Predicted: Versicolor'</span>],</span>
<span id="cb7-643"><a href="#cb7-643" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span>[<span class="st">'Actual: Setosa'</span>,<span class="st">'Actual: Virginica'</span>, <span class="st">'Actual: Versicolor'</span>]</span>
<span id="cb7-644"><a href="#cb7-644" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-645"><a href="#cb7-645" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-646"><a href="#cb7-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-647"><a href="#cb7-647" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb7-648"><a href="#cb7-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-649"><a href="#cb7-649" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb7-650"><a href="#cb7-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-651"><a href="#cb7-651" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb7-652"><a href="#cb7-652" aria-hidden="true" tabindex="-1"></a>Linear Discriminant Analysis (LDA) is a powerful technique for dimensionality reduction and classification. Its goal is to find directions (linear combinations of the original features) that best separate the classes by maximizing between-class variance while minimizing within-class variance. &lt;/p&gt;</span>
<span id="cb7-653"><a href="#cb7-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-654"><a href="#cb7-654" aria-hidden="true" tabindex="-1"></a><span class="fu">### Disclaimer  </span></span>
<span id="cb7-655"><a href="#cb7-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-656"><a href="#cb7-656" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb7-657"><a href="#cb7-657" aria-hidden="true" tabindex="-1"></a>For the mathematical explanation, I used generative AI to produce the matrices and vectors and their manipulations. So it won't be surprising if a calculation mistake is found. The custom python class was created by the help of ChatGPT4 &lt;/p&gt;</span>
<span id="cb7-658"><a href="#cb7-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-659"><a href="#cb7-659" aria-hidden="true" tabindex="-1"></a><span class="fu">## References  </span></span>
<span id="cb7-660"><a href="#cb7-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-661"><a href="#cb7-661" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fisher, R.A. (1936). "The Use of Multiple Measurements in Taxonomic Problems." *Annals of Eugenics*, 7(2), 179–188.  </span>
<span id="cb7-662"><a href="#cb7-662" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.  </span>
<span id="cb7-663"><a href="#cb7-663" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Strang, G. (2016). *Introduction to Linear Algebra* (5th ed.). Wellesley-Cambridge Press.  </span>
<span id="cb7-664"><a href="#cb7-664" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Lay, D. C. (2011). *Linear Algebra and Its Applications* (4th ed.). Pearson.</span>
<span id="cb7-665"><a href="#cb7-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-666"><a href="#cb7-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-667"><a href="#cb7-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-668"><a href="#cb7-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-669"><a href="#cb7-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-670"><a href="#cb7-670" aria-hidden="true" tabindex="-1"></a>**Share on**  </span>
<span id="cb7-671"><a href="#cb7-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-672"><a href="#cb7-672" aria-hidden="true" tabindex="-1"></a>::::{.columns}</span>
<span id="cb7-673"><a href="#cb7-673" aria-hidden="true" tabindex="-1"></a>:::{.column width="33%"}</span>
<span id="cb7-674"><a href="#cb7-674" aria-hidden="true" tabindex="-1"></a>&lt;a href="https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#1877F2; text-decoration: none;"&gt;</span>
<span id="cb7-675"><a href="#cb7-675" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-676"><a href="#cb7-676" aria-hidden="true" tabindex="-1"></a>{{&lt; fa brands facebook size=3x &gt;}}</span>
<span id="cb7-677"><a href="#cb7-677" aria-hidden="true" tabindex="-1"></a>&lt;/a&gt;</span>
<span id="cb7-678"><a href="#cb7-678" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-679"><a href="#cb7-679" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-680"><a href="#cb7-680" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-681"><a href="#cb7-681" aria-hidden="true" tabindex="-1"></a>:::{.column width="33%"}</span>
<span id="cb7-682"><a href="#cb7-682" aria-hidden="true" tabindex="-1"></a>&lt;a href="https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#0077B5; text-decoration: none;"&gt;</span>
<span id="cb7-683"><a href="#cb7-683" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-684"><a href="#cb7-684" aria-hidden="true" tabindex="-1"></a>{{&lt; fa brands linkedin size=3x &gt;}}</span>
<span id="cb7-685"><a href="#cb7-685" aria-hidden="true" tabindex="-1"></a>&lt;/a&gt;</span>
<span id="cb7-686"><a href="#cb7-686" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-687"><a href="#cb7-687" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-688"><a href="#cb7-688" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-689"><a href="#cb7-689" aria-hidden="true" tabindex="-1"></a>:::{.column width="33%"}</span>
<span id="cb7-690"><a href="#cb7-690" aria-hidden="true" tabindex="-1"></a>&lt;a href="https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/lda/" target="_blank" style="color:#1DA1F2; text-decoration: none;"&gt;</span>
<span id="cb7-691"><a href="#cb7-691" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-692"><a href="#cb7-692" aria-hidden="true" tabindex="-1"></a>{{&lt; fa brands twitter size=3x &gt;}}</span>
<span id="cb7-693"><a href="#cb7-693" aria-hidden="true" tabindex="-1"></a>&lt;/a&gt;</span>
<span id="cb7-694"><a href="#cb7-694" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-695"><a href="#cb7-695" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-696"><a href="#cb7-696" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb7-697"><a href="#cb7-697" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-698"><a href="#cb7-698" aria-hidden="true" tabindex="-1"></a>&lt;script src="https://giscus.app/client.js"</span>
<span id="cb7-699"><a href="#cb7-699" aria-hidden="true" tabindex="-1"></a>        data-repo="mrislambd/mrislambd.github.io" </span>
<span id="cb7-700"><a href="#cb7-700" aria-hidden="true" tabindex="-1"></a>        data-repo-id="R_kgDOMV8crA"</span>
<span id="cb7-701"><a href="#cb7-701" aria-hidden="true" tabindex="-1"></a>        data-category="Announcements"</span>
<span id="cb7-702"><a href="#cb7-702" aria-hidden="true" tabindex="-1"></a>        data-category-id="DIC_kwDOMV8crM4CjbQW"</span>
<span id="cb7-703"><a href="#cb7-703" aria-hidden="true" tabindex="-1"></a>        data-mapping="pathname"</span>
<span id="cb7-704"><a href="#cb7-704" aria-hidden="true" tabindex="-1"></a>        data-strict="0"</span>
<span id="cb7-705"><a href="#cb7-705" aria-hidden="true" tabindex="-1"></a>        data-reactions-enabled="1"</span>
<span id="cb7-706"><a href="#cb7-706" aria-hidden="true" tabindex="-1"></a>        data-emit-metadata="0"</span>
<span id="cb7-707"><a href="#cb7-707" aria-hidden="true" tabindex="-1"></a>        data-input-position="bottom"</span>
<span id="cb7-708"><a href="#cb7-708" aria-hidden="true" tabindex="-1"></a>        data-theme="light"</span>
<span id="cb7-709"><a href="#cb7-709" aria-hidden="true" tabindex="-1"></a>        data-lang="en"</span>
<span id="cb7-710"><a href="#cb7-710" aria-hidden="true" tabindex="-1"></a>        crossorigin="anonymous"</span>
<span id="cb7-711"><a href="#cb7-711" aria-hidden="true" tabindex="-1"></a>        async&gt;</span>
<span id="cb7-712"><a href="#cb7-712" aria-hidden="true" tabindex="-1"></a>&lt;/script&gt;</span>
<span id="cb7-713"><a href="#cb7-713" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb7-714"><a href="#cb7-714" aria-hidden="true" tabindex="-1"></a>&lt;div id="fb-root"&gt;&lt;/div&gt;</span>
<span id="cb7-715"><a href="#cb7-715" aria-hidden="true" tabindex="-1"></a>&lt;script async defer crossorigin="anonymous"</span>
<span id="cb7-716"><a href="#cb7-716" aria-hidden="true" tabindex="-1"></a> src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&amp;version=v20.0"&gt;&lt;/script&gt;</span>
<span id="cb7-717"><a href="#cb7-717" aria-hidden="true" tabindex="-1"></a>&lt;div class="fb-comments" data-href="https://mrislambd.github.io/dsandml/lda/" data-width="800" data-numposts="5"&gt;&lt;/div&gt;</span>
<span id="cb7-718"><a href="#cb7-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-719"><a href="#cb7-719" aria-hidden="true" tabindex="-1"></a>---  </span>
<span id="cb7-720"><a href="#cb7-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-721"><a href="#cb7-721" aria-hidden="true" tabindex="-1"></a>**You may also like**  </span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Powered by <a href="https://quarto.org/">Quarto</a> 1.5.54</p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2024 @ Rafiq Islam
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../license.txt">
<p>License</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/rafiqr35" target="_blank">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://youtube.com/@quanttube" target="_blank">
      <i class="bi bi-youtube" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:rafiqfsu@gmail.com?subject&amp;body" target="_blank">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","selector":".lightbox","loop":false,"openEffect":"zoom"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>