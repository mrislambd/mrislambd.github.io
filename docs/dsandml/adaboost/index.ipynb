{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Boosting Algorithm: Adaptive Boosting Method (AdaBoost)\n",
        "\n",
        "Rafiq Islam  \n",
        "2024-12-02\n",
        "\n",
        "## Introduction"
      ],
      "id": "aa834e25-eeac-4629-bd84-ff56ad6e908c"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "e2c06b50-36b6-4f4c-8831-fdd04dab29f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Boosting is a powerful ensemble learning technique that focuses on\n",
        "improving the performance of weak learners to build a robust predictive\n",
        "model. <br> <br> Now the question is what the heck is weak learner?\n",
        "Well, roughly speaking, a statistical learning algorithm is called a\n",
        "weak learner if it is slightly better than just random guess. In\n",
        "contrast, a statistical learning algorithm is called a strong learner if\n",
        "it can be made arbitrarily close to the true value.<br> <br> Unlike\n",
        "bagging (bootstrap aggregating, e.g. random forest), which builds models\n",
        "independently, boosting builds models sequentially, where each new model\n",
        "corrects the errors of its predecessors. This approach ensures that the\n",
        "ensemble concentrates on the difficult-to-predict instances, making\n",
        "boosting highly effective for both classification and regression\n",
        "problems."
      ],
      "id": "962b539e-16fe-40c3-862e-6dd8fb74e0fb"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "c9c35cc9-c9fb-4a8f-a91a-20768d0e69d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Characteristics of Boosting:\n",
        "\n",
        "1.  **Sequential Model Building:** Boosting builds one model at a time,\n",
        "    with each model improving upon the errors of the previous one.\n",
        "2.  **Weight Assignment:** It assigns weights to instances, emphasizing\n",
        "    misclassified or poorly predicted ones in subsequent iterations.\n",
        "3.  **Weak to Strong Learners:** The goal of boosting is to combine\n",
        "    multiple weak learners (models slightly better than random guessing)\n",
        "    into a strong learner.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Adaptive Boosting (AdaBoost)"
      ],
      "id": "fb02648f-d315-4410-a9f0-cd0b8b973d2e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "e92b0a0b-58c5-4e5c-b9c6-b123ab03556e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adaptive Boosting, or AdaBoost, is one of the earliest and most widely\n",
        "used boosting algorithms. It was introduced by Freund and Schapire in\n",
        "1996. AdaBoost combines weak learners, typically decision stumps\n",
        "(single-level decision trees), to form a strong learner."
      ],
      "id": "76ff747b-d2d7-45f5-a1ab-6d9859ec1266"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "56112c7a-35c1-4e91-936f-0fb0968ca277"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How AdaBoost Works:\n",
        "\n",
        "1.  **Initialization:**\n",
        "    -   Assign an initial weight $w_i^{(1)}$ to each training instance\n",
        "        $i$, where $w_i^{(1)} = \\frac{1}{N}$, and $N$ is the total\n",
        "        number of training examples.\n",
        "    -   All instances start with equal weights.\n",
        "2.  **Iterative Model Training:**\n",
        "    -   At each iteration $t$, train a weak learner $h_t(x)$ on the\n",
        "        weighted dataset.\n",
        "\n",
        "    -   Compute the weighted error rate $\\epsilon_t$: $$\n",
        "        \\epsilon_t = \\frac{\\sum_{i=1}^N w_i^{(t)} \\cdot \\mathbb{1}(y_i \\neq h_t(x_i))}{\\sum_{i=1}^N w_i^{(t)}}\n",
        "        $$\n",
        "\n",
        "        Here, $\\mathbb{1}$ is an indicator function that equals 1 when\n",
        "        the prediction is incorrect and 0 otherwise.\n",
        "\n",
        "    -   Calculate the model’s weight $\\alpha_t$: $$\n",
        "        \\alpha_t = \\frac{1}{2} \\ln \\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)\n",
        "        $$ The weight $\\alpha_t$ determines the importance of $h_t(x)$\n",
        "        in the final ensemble. Models with lower error rates receive\n",
        "        higher weights.\n",
        "3.  **Update Weights:**\n",
        "    -   Adjust the weights of the training instances: $$\n",
        "        w_i^{(t+1)} = w_i^{(t)} \\cdot \\exp\\left(-\\alpha_t \\cdot y_i \\cdot h_t(x_i)\\right)\n",
        "        $$ Instances misclassified by $h_t(x)$ have their weights\n",
        "        increased, making them more influential in the next iteration.\n",
        "\n",
        "    -   Normalize the weights to ensure they sum to 1: $$\n",
        "        w_i^{(t+1)} \\leftarrow \\frac{w_i^{(t+1)}}{\\sum_{j=1}^N w_j^{(t+1)}}\n",
        "        $$\n",
        "4.  **Final Model:**\n",
        "    -   Combine the weak learners into a final strong learner: $$\n",
        "        H(x) = \\text{sign}\\left(\\sum_{t=1}^T \\alpha_t \\cdot h_t(x)\\right)\n",
        "        $$ The sign function determines the final class label based on\n",
        "        the weighted sum of weak learners’ predictions.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "#### Mathematical Derivation and Explanation of AdaBoost\n",
        "\n",
        "The core idea of AdaBoost is to minimize an exponential loss function:\n",
        "$$\n",
        "\\mathcal{L} = \\sum_{i=1}^N \\exp\\left(-y_i \\cdot F(x_i)\\right),\n",
        "$$ where $F(x_i)$ is the weighted combination of weak learners: $$\n",
        "F(x_i) = \\sum_{t=1}^T \\alpha_t \\cdot h_t(x_i).\n",
        "$$\n",
        "\n",
        "##### 1. **Weight Update Rule:**\n",
        "\n",
        "-   The exponential term $\\exp(-y_i \\cdot F(x_i))$ increases for\n",
        "    misclassified instances ($y_i \\neq h_t(x_i)$), assigning them more\n",
        "    weight in subsequent iterations.\n",
        "\n",
        "##### 2. **Optimal $\\alpha_t$:**\n",
        "\n",
        "-   The weight $\\alpha_t$ is derived to minimize the weighted error\n",
        "    $\\epsilon_t$. A higher value of $\\alpha_t$ corresponds to a weak\n",
        "    learner with better performance.\n",
        "\n",
        "##### 3. **Boosting as Gradient Descent:**\n",
        "\n",
        "-   AdaBoost can be interpreted as a stage-wise optimization of the\n",
        "    exponential loss function. Each iteration reduces the overall loss\n",
        "    by focusing on misclassified examples.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### Advantages of AdaBoost:\n",
        "\n",
        "-   **Simplicity:** Easy to implement with weak learners like decision\n",
        "    stumps.\n",
        "-   **No Parameter Tuning:** AdaBoost has fewer hyperparameters to tune\n",
        "    compared to other boosting methods.\n",
        "-   **Versatility:** Works well with various types of data and weak\n",
        "    learners.\n",
        "\n",
        "### Limitations of AdaBoost:\n",
        "\n",
        "-   **Sensitivity to Noise:** Outliers can receive disproportionately\n",
        "    high weights, leading to overfitting.\n",
        "-   **Weak Learner Dependency:** The performance heavily depends on the\n",
        "    choice of the weak learner.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### Closing Thoughts\n",
        "\n",
        "AdaBoost is a foundational method that inspired more advanced boosting\n",
        "algorithms like Gradient Boosting and XGBoost. While AdaBoost excels in\n",
        "simplicity and interpretability, other methods address its limitations\n",
        "and enhance performance on larger datasets.\n",
        "\n",
        "In the next post, I will delve into Gradient Boosting, exploring its\n",
        "mechanics, mathematical foundation, and how it builds on the ideas of\n",
        "AdaBoost to improve predictive modeling. Stay tuned!"
      ],
      "id": "7a81fb66-cc81-459c-a88a-074bfaaee30e"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}