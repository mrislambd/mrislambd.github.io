<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rafiq Islam">
<meta name="dcterms.date" content="2024-10-10">

<title>Classification using Naive Bayes algorithm – Mohammad Rafiqul Islam</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../..//_assets/images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-listing/list.min.js"></script>
<script src="../../site_libs/quarto-listing/quarto-listing.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-listing .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-image','listing-date','listing-title','listing-author','listing-reading-time',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      page: 18,
    pagination: { item: "<li class='page-item'><a class='page page-link' href='#'></a></li>" },
      searchColumns: ["listing-title","listing-author","listing-date","listing-image","listing-description"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-listing'] = new List('listing-listing', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Z5NP67GHFC"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Z5NP67GHFC', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6878992848042528" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Classification using Naive Bayes algorithm – Mohammad Rafiqul Islam">
<meta property="og:description" content="">
<meta property="og:site_name" content="Mohammad Rafiqul Islam">
<meta name="twitter:title" content="Classification using Naive Bayes algorithm – Mohammad Rafiqul Islam">
<meta name="twitter:description" content="">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../dsandml/naivebayes/index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../_assets/images/fsu-logo.png" alt="Florida State University" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../dsandml/naivebayes/index.html">
    <span class="navbar-title">Mohammad Rafiqul Islam</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../portfolio.html"> 
<span class="menu-text">Portfolio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cv.html"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/mohammad-rafiqul-islam/" target="_blank"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mrislambd" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#what-is-naive-bayes" id="toc-what-is-naive-bayes" class="nav-link" data-scroll-target="#what-is-naive-bayes">What is Naive Bayes?</a>
  <ul class="collapse">
  <li><a href="#bayes-theorem-the-foundation" id="toc-bayes-theorem-the-foundation" class="nav-link" data-scroll-target="#bayes-theorem-the-foundation">Bayes’ Theorem: The Foundation</a></li>
  <li><a href="#assumptions-and-requirements" id="toc-assumptions-and-requirements" class="nav-link" data-scroll-target="#assumptions-and-requirements">Assumptions and Requirements</a></li>
  </ul></li>
  <li><a href="#types-of-naive-bayes-classifiers" id="toc-types-of-naive-bayes-classifiers" class="nav-link" data-scroll-target="#types-of-naive-bayes-classifiers">Types of Naive Bayes Classifiers</a></li>
  <li><a href="#mathematics-behind-the-process" id="toc-mathematics-behind-the-process" class="nav-link" data-scroll-target="#mathematics-behind-the-process">Mathematics behind the process</a>
  <ul class="collapse">
  <li><a href="#computing-the-probabilities" id="toc-computing-the-probabilities" class="nav-link" data-scroll-target="#computing-the-probabilities">Computing the probabilities</a></li>
  </ul></li>
  <li><a href="#python-implementation" id="toc-python-implementation" class="nav-link" data-scroll-target="#python-implementation">Python Implementation</a>
  <ul class="collapse">
  <li><a href="#gaussian-naive-bayes" id="toc-gaussian-naive-bayes" class="nav-link" data-scroll-target="#gaussian-naive-bayes">Gaussian Naive Bayes</a></li>
  <li><a href="#multinomial-naive-bayes" id="toc-multinomial-naive-bayes" class="nav-link" data-scroll-target="#multinomial-naive-bayes">Multinomial Naive Bayes</a></li>
  <li><a href="#pros-and-cons-of-naive-bayes" id="toc-pros-and-cons-of-naive-bayes" class="nav-link" data-scroll-target="#pros-and-cons-of-naive-bayes"><strong>6. Pros and Cons of Naive Bayes</strong></a></li>
  <li><a href="#python-implementation-of-naive-bayes" id="toc-python-implementation-of-naive-bayes" class="nav-link" data-scroll-target="#python-implementation-of-naive-bayes"><strong>7. Python Implementation of Naive Bayes</strong></a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><strong>8. Conclusion</strong></a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Classification using Naive Bayes algorithm</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">Statistics</div>
    <div class="quarto-category">Data Science</div>
    <div class="quarto-category">Data Engineering</div>
    <div class="quarto-category">Machine Learning</div>
    <div class="quarto-category">Artificial Intelligence</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Rafiq Islam </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 10, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p style="text-align: justify">
Naive Bayes is a family of simple yet powerful probabilistic classifiers based on Bayes’ Theorem, with the assumption of independence among predictors. It is widely used for tasks like spam detection, text classification, and sentiment analysis due to its efficiency and simplicity. Despite being called “naive” for its strong assumption of feature independence, it often performs remarkably well in real-world scenarios.
</p>
</section>
<section id="what-is-naive-bayes" class="level2">
<h2 class="anchored" data-anchor-id="what-is-naive-bayes">What is Naive Bayes?</h2>
<p style="text-align: justify">
Naive Bayes is a probabilistic classifier that leverages Bayes’ Theorem to predict the class of a given data point. It belongs to the family of generative models and works by estimating the posterior probability of a class given a set of features. The term “Naive” refers to the assumption that features are conditionally independent given the class label, which simplifies computation.
</p>
<section id="bayes-theorem-the-foundation" class="level3">
<h3 class="anchored" data-anchor-id="bayes-theorem-the-foundation">Bayes’ Theorem: The Foundation</h3>
<p>Bayes’ Theorem provides a way to update our beliefs about the probability of an event, based on new evidence. The formula for Bayes’ Theorem is:</p>
<p><span class="math display">\[
P(y|X) = \frac{P(X|y) \cdot P(y)}{P(X)}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(P(y|X)\)</span>: Posterior probability of class <span class="math inline">\(y\)</span> given feature set <span class="math inline">\(X\)</span><br>
</li>
<li><span class="math inline">\(P(X|y)\)</span>: Likelihood of feature set <span class="math inline">\(X\)</span> given class <span class="math inline">\(y\)</span><br>
</li>
<li><span class="math inline">\(P(y)\)</span>: Prior probability of class <span class="math inline">\(y\)</span><br>
</li>
<li><span class="math inline">\(P(X)\)</span>: Evidence or probability of feature set <span class="math inline">\(X\)</span></li>
</ul>
<p>In the context of classification:</p>
<ul>
<li>The goal is to predict <span class="math inline">\(y\)</span> (the class) given <span class="math inline">\(X\)</span> (the features).<br>
</li>
<li><span class="math inline">\(P(y)\)</span> is derived from the distribution of classes in the training data.<br>
</li>
<li><span class="math inline">\(P(X|y)\)</span> is derived from the distribution of features for each class.<br>
</li>
<li><span class="math inline">\(P(X)\)</span> is a normalizing constant to ensure probabilities sum to 1, but it can be ignored for classification purposes because it is the same for all classes.</li>
</ul>
</section>
<section id="assumptions-and-requirements" class="level3">
<h3 class="anchored" data-anchor-id="assumptions-and-requirements">Assumptions and Requirements</h3>
<p>The key assumption in Naive Bayes is the <strong>conditional independence</strong> of features. Specifically, it assumes that the likelihood of each feature is independent of the others, given the class label:</p>
<p><span class="math display">\[
P(X_1, X_2, \dots, X_n | y) = P(X_1 | y) \cdot P(X_2 | y) \cdot \dots \cdot P(X_n | y)
\]</span></p>
<p>While this assumption is often violated in real-world data, Naive Bayes can still perform well, especially when certain features dominate the prediction.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li><strong>Numerical Data</strong>: Naive Bayes can handle both numerical and categorical data, though different versions (Gaussian, Multinomial, Bernoulli) of the algorithm handle specific types of data more effectively</li>
<li><strong>Non-Collinear Features</strong>: Highly correlated features can distort predictions since the model assumes independence.<br>
</li>
<li><strong>Sufficient Data</strong>: Naive Bayes relies on probability estimates; thus, insufficient data might lead to unreliable predictions.</li>
</ul>
</section>
</section>
<section id="types-of-naive-bayes-classifiers" class="level2">
<h2 class="anchored" data-anchor-id="types-of-naive-bayes-classifiers">Types of Naive Bayes Classifiers</h2>
<p>There are several variants of Naive Bayes, depending on the nature of the data:</p>
<ol type="1">
<li><strong>Gaussian Naive Bayes</strong>: Assumes features follow a Gaussian distribution (useful for continuous data).<br>
</li>
<li><strong>Multinomial Naive Bayes</strong>: Suitable for discrete data, often used in text classification (e.g., word counts).<br>
</li>
<li><strong>Bernoulli Naive Bayes</strong>: Works well for binary/boolean data, often used in scenarios where the features represent the presence/absence of a characteristic.</li>
</ol>
</section>
<section id="mathematics-behind-the-process" class="level2">
<h2 class="anchored" data-anchor-id="mathematics-behind-the-process">Mathematics behind the process</h2>
<p>To understand the working of Naive Bayes, let’s start with the Bayes’s theorem</p>
<p><span class="math display">\[
P(y_k | X) = \frac{P(X|y_k) \cdot P(y_k)}{P(X)}
\]</span></p>
<p>Where <span class="math inline">\(y_k\)</span> is one of the possible classes. Due to the independence assumption, the likelihood term <span class="math inline">\(P(X|y_k)\)</span> can be factorized as:</p>
<p><span class="math display">\[
P(X|y_k) = P(x_1|y_k) \cdot P(x_2|y_k) \cdot \dots \cdot P(x_n|y_k)
\]</span></p>
<p>Where <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> are the individual features in the feature set <span class="math inline">\(X\)</span>. For each class <span class="math inline">\(y_k\)</span>, compute the posterior probability:</p>
<p><span class="math display">\[
P(y_k | X) \propto P(y_k) \cdot \prod_{i=1}^n P(x_i|y_k)
\]</span></p>
<p>The denominator <span class="math inline">\(P(X)\)</span> is constant for all classes, so we can ignore it during classification. Finally, the class <span class="math inline">\(y_k\)</span> with the highest posterior probability is chosen as the predicted class:</p>
<p><span class="math display">\[\begin{align*}
\hat{y} &amp;= \arg\max_{y_k} P(y_k) \cdot \prod_{i=1}^n P(x_i|y_k)\\
\log{(\hat{y})}&amp;= \log{ \left(\arg\max_{y_k} P(y_k) \cdot \prod_{i=1}^n P(x_i|y_k)\right)}\\
\implies \hat{y} &amp; = \arg\max_{y_k} \left(\log P(y_k)+\sum_{i=1}^{n} P(x_i|y_k)\right)
\end{align*}\]</span></p>
<section id="computing-the-probabilities" class="level3">
<h3 class="anchored" data-anchor-id="computing-the-probabilities">Computing the probabilities</h3>
<section id="prior-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="prior-probabilities">Prior Probabilities</h4>
<p><span class="math inline">\(P(y_k)\)</span> is the prior probability, usually frequency of each class <span class="math inline">\(k\)</span>.<br>
<span class="math display">\[
  P(y_k)=\frac{\text{number of instances in class }y_k}{\text{total number of instances}}
\]</span></p>
</section>
<section id="class-conditional-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="class-conditional-probabilities">Class Conditional Probabilities</h4>
<p><span class="math inline">\(P(x_i|y_k)\)</span> is the class conditional probability. For the</p>
<ol type="1">
<li><p><strong><em>Gaussian Naive Bayes:</em></strong> when the features are continuous and assumed that the features follow a <strong><em>Gaussian</em></strong> distribution, the <code>class conditional</code> probability is given as <span class="math display">\[
P(x_i|y_k) = \frac{1}{\sqrt{2\pi \sigma^2_k}}\exp{\left(-\frac{(x_i-\mu_i)^2}{2\sigma^2_k}\right)}
\]</span></p></li>
<li><p><strong><em>Multinomial Naive Bayes:</em></strong> when the featrues (typically word frequencies) follow a multinomial distribution, the <code>class conditional</code> distribution is given as<br>
<span class="math display">\[
P(x_i|y_k)=\frac{N_{x_i,y_k}+\alpha}{N_{y_k}+\alpha V}
\]</span></p>
<p>where,</p>
<ul>
<li><span class="math inline">\(N_{x_i,y_k}\)</span> is the count of the feature (e.g.&nbsp;word or term) <span class="math inline">\(x_i\)</span> appearing in documents of class <span class="math inline">\(y_k\)</span><br>
</li>
<li><span class="math inline">\(N_{y_k}\)</span> is the total count of all features (e.g.&nbsp;words) in all documents belonging to class <span class="math inline">\(y_k\)</span><br>
</li>
<li><span class="math inline">\(\alpha\)</span> is a smoothing parameter (often called <strong>Laplace smoothing</strong>), used to avoid zero probabilities. If not using smoothing, set <span class="math inline">\(\alpha=0\)</span><br>
</li>
<li><span class="math inline">\(V\)</span> is the size of the vocabulary (i.e., the number of unique words)</li>
</ul></li>
<li><p><strong><em>Bernoulli Naive Bayes:</em></strong> when features are binary/boolean data, often used in scenarios where the features represent the presence/absence of a characteristic, the <code>class conditional</code> distribution is given as<br>
<span class="math display">\[
P(x_i|y_k)=\begin{cases}\frac{N_{x_i,y_k}+\alpha}{N_{y_k}+2\alpha }\hspace{2mm}\text{ if } x_i=1\\
1-\frac{N_{x_i,y_k}+\alpha}{N_{y_k}+2\alpha }\hspace{2mm}\text{ if } x_i=0\end{cases}
\]</span></p></li>
</ol>
</section>
</section>
</section>
<section id="python-implementation" class="level2">
<h2 class="anchored" data-anchor-id="python-implementation">Python Implementation</h2>
<section id="gaussian-naive-bayes" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-naive-bayes">Gaussian Naive Bayes</h3>
<p>Code credit for the custom classifier goes to <a href="https://github.com/AssemblyAI-Community/Machine-Learning-From-Scratch/blob/main/06%20NaiveBayes/naive_bayes.py" target="_blank" style="text-decoration:none">Assembly AI</a></p>
<div id="228f5dac" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GNaiveBayes:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X,y):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">        n_samples: number of observed data n; int;</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        n_features: number of continueous features d; int;</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        _classes: unique classes</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">        n_classes: number of unique classes</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        n_samples, n_features <span class="op">=</span> X.shape</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._classes <span class="op">=</span> np.unique(y)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        n_classes <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>._classes)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate mean, variance, and prior for each class  </span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._mean <span class="op">=</span> np.zeros((n_classes,n_features),dtype<span class="op">=</span>np.float64)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._var <span class="op">=</span> np.zeros((n_classes,n_features),dtype<span class="op">=</span>np.float64)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._prior <span class="op">=</span> np.zeros(n_classes,dtype<span class="op">=</span>np.float64)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, c <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>._classes):</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            X_c <span class="op">=</span> X[y<span class="op">==</span>c]</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._mean[idx,:] <span class="op">=</span> X_c.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._var[idx,:] <span class="op">=</span> X_c.var(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._prior[idx] <span class="op">=</span> X_c.shape[<span class="dv">0</span>]<span class="op">/</span><span class="bu">float</span>(n_samples)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>,X):</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> [<span class="va">self</span>._predict(x) <span class="cf">for</span> x <span class="kw">in</span> X]</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(y_pred)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _predict(<span class="va">self</span>, x):</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        posteriors <span class="op">=</span> []</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the posterior probability for each class  </span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx,c <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>._classes):</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>            prior <span class="op">=</span> np.log(<span class="va">self</span>._prior[idx])</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>            post <span class="op">=</span> np.<span class="bu">sum</span>(np.log(<span class="va">self</span>._pdf(idx,x)))</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>            posterior <span class="op">=</span> post <span class="op">+</span> prior</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>            posteriors.append(posterior)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return the class with the highest posterior</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._classes[np.argmax(posteriors)]</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _pdf(<span class="va">self</span>, class_idx, x):</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> <span class="va">self</span>._mean[class_idx]</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        var <span class="op">=</span> <span class="va">self</span>._var[class_idx]</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        numerator <span class="op">=</span> np.exp(<span class="op">-</span>((x<span class="op">-</span>mean)<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>var))</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        denominator <span class="op">=</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi<span class="op">*</span>var)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> numerator<span class="op">/</span>denominator</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s apply this to the <code>irish</code> data set</p>
<div id="36199d70" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Iris dataset</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> load_iris()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.data  <span class="co"># Features</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data.target  <span class="co"># Target variable (Classes)</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span>data.feature_names)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'target'</span>] <span class="op">=</span> pd.Categorical.from_codes(y, data.target_names)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal length (cm)</th>
<th data-quarto-table-cell-role="th">sepal width (cm)</th>
<th data-quarto-table-cell-role="th">petal length (cm)</th>
<th data-quarto-table-cell-role="th">petal width (cm)</th>
<th data-quarto-table-cell-role="th">target</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
<td>setosa</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4.9</td>
<td>3.0</td>
<td>1.4</td>
<td>0.2</td>
<td>setosa</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4.7</td>
<td>3.2</td>
<td>1.3</td>
<td>0.2</td>
<td>setosa</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4.6</td>
<td>3.1</td>
<td>1.5</td>
<td>0.2</td>
<td>setosa</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5.0</td>
<td>3.6</td>
<td>1.4</td>
<td>0.2</td>
<td>setosa</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="73121e52" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report  </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and testing sets</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>gnb1 <span class="op">=</span> GNaiveBayes()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>gnb1.fit(X_train, y_train)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>pred1 <span class="op">=</span> gnb1.predict(X_test)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>acc1 <span class="op">=</span> accuracy_score(y_test, pred1)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>gnb2 <span class="op">=</span> GaussianNB()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>gnb2.fit(X_train, y_train)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>pred2 <span class="op">=</span> gnb2.predict(X_test)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>acc2 <span class="op">=</span> accuracy_score(y_test, pred2) </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Accuracy from custom classifier = </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(acc1<span class="op">*</span><span class="dv">100</span>))</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix and classification report</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, pred1))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, pred1))</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Accuracy from sklearn classifier = </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(acc2<span class="op">*</span><span class="dv">100</span>))</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, pred2))</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, pred2))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy from custom classifier = 97.78
[[19  0  0]
 [ 0 12  1]
 [ 0  0 13]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      0.92      0.96        13
           2       0.93      1.00      0.96        13

    accuracy                           0.98        45
   macro avg       0.98      0.97      0.97        45
weighted avg       0.98      0.98      0.98        45



Accuracy from sklearn classifier = 97.78
[[19  0  0]
 [ 0 12  1]
 [ 0  0 13]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      0.92      0.96        13
           2       0.93      1.00      0.96        13

    accuracy                           0.98        45
   macro avg       0.98      0.97      0.97        45
weighted avg       0.98      0.98      0.98        45
</code></pre>
</div>
</div>
</section>
<section id="multinomial-naive-bayes" class="level3">
<h3 class="anchored" data-anchor-id="multinomial-naive-bayes">Multinomial Naive Bayes</h3>
<div id="8d6295dd" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MNaiveBayes:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X,y):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Fit the Multinomial Naive Bayes model to the training data.  </span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">        X: input data (n_samples, n_features)</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">        y: target labels (n_samples)</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        n_samples, n_features <span class="op">=</span> X.shape</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._classes <span class="op">=</span> np.unique(y)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        n_classes <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>._classes)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize and count priors </span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._class_feature_count <span class="op">=</span> np.zeros((n_classes, n_features),dtype<span class="op">=</span>np.float64)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._class_count <span class="op">=</span> np.zeros(n_classes, dtype<span class="op">=</span>np.float64)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._prior <span class="op">=</span> np.zeros(n_classes, dtype<span class="op">=</span>np.float64)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx,c <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>._classes):</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>            X_c <span class="op">=</span> X[y<span class="op">==</span>c]</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._class_feature_count[idx,:] <span class="op">=</span> X_c.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._class_count[idx] <span class="op">=</span> X_c.shape[<span class="dv">0</span>]</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._prior[idx] <span class="op">=</span> X_c.shape[<span class="dv">0</span>]<span class="op">/</span><span class="bu">float</span>(n_samples)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Total count of all features accross all classes </span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._total_feature_count <span class="op">=</span> <span class="va">self</span>._class_feature_count.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> [<span class="va">self</span>._predict(x) <span class="cf">for</span> x <span class="kw">in</span> X]</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(y_pred)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _predict(<span class="va">self</span>,x):</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        posteriors <span class="op">=</span> []</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, c <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>._classes):</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>            prior <span class="op">=</span> np.log(<span class="va">self</span>._prior[idx])</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>            likelihood <span class="op">=</span> np.<span class="bu">sum</span>(np.log(<span class="va">self</span>._likelihood(idx,x)))</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>            posterior_prob <span class="op">=</span> prior<span class="op">+</span> likelihood</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>            posteriors.append(posterior_prob)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._classes[np.argmax(posteriors)]</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _likelihood(<span class="va">self</span>, class_idx, x):</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> <span class="va">self</span>.alpha</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>._class_feature_count[class_idx])</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        class_feature_count <span class="op">=</span> <span class="va">self</span>._class_feature_count[class_idx]</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>        total_class_count <span class="op">=</span> <span class="va">self</span>._total_feature_count[class_idx]</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        likelihood <span class="op">=</span> (class_feature_count<span class="op">+</span>alpha)<span class="op">/</span>(total_class_count <span class="op">+</span> alpha <span class="op">*</span> V)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> likelihood<span class="op">**</span>x</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>],</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>],</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>]])</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Corresponding labels (2 classes: 0 and 1)</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and train Multinomial Naive Bayes model</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MNaiveBayes()</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict for new sample</span></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model.predict(X_test)</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(predictions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0 0]</code></pre>
</div>
</div>
</section>
<section id="pros-and-cons-of-naive-bayes" class="level3">
<h3 class="anchored" data-anchor-id="pros-and-cons-of-naive-bayes"><strong>6. Pros and Cons of Naive Bayes</strong></h3>
<section id="pros" class="level4">
<h4 class="anchored" data-anchor-id="pros"><strong>Pros:</strong></h4>
<ul>
<li><strong>Simplicity</strong>: Easy to implement and computationally efficient.</li>
<li><strong>Fast Training and Prediction</strong>: Naive Bayes is especially fast for both training and inference, even on large datasets.</li>
<li><strong>Performs Well with Small Data</strong>: Despite its simplicity, Naive Bayes works well even with relatively small datasets.</li>
<li><strong>Handles Irrelevant Features</strong>: Naive Bayes can often ignore irrelevant features in the data since the independence assumption dilutes their influence.</li>
<li><strong>Multi-Class Classification</strong>: Naturally suited for multi-class classification problems.</li>
</ul>
</section>
<section id="cons" class="level4">
<h4 class="anchored" data-anchor-id="cons"><strong>Cons:</strong></h4>
<ul>
<li><strong>Strong Assumption of Independence</strong>: The assumption that features are independent is rarely true in real-world data, which can limit the model’s effectiveness.</li>
<li><strong>Poor Estimation of Probabilities</strong>: When dealing with very small datasets or unseen feature combinations, Naive Bayes can yield inaccurate probability estimates.</li>
<li><strong>Zero-Frequency Problem</strong>: If a feature value was not present in the training data, Naive Bayes will assign zero probability to the entire class, which can be addressed using Laplace smoothing.</li>
</ul>
<hr>
</section>
</section>
<section id="python-implementation-of-naive-bayes" class="level3">
<h3 class="anchored" data-anchor-id="python-implementation-of-naive-bayes"><strong>7. Python Implementation of Naive Bayes</strong></h3>
<p>Let’s implement Naive Bayes in Python using the Scikit-learn library.</p>
<section id="step-1-importing-the-necessary-libraries" class="level4">
<h4 class="anchored" data-anchor-id="step-1-importing-the-necessary-libraries"><strong>Step 1: Importing the necessary libraries</strong></h4>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-2-load-the-dataset" class="level4">
<h4 class="anchored" data-anchor-id="step-2-load-the-dataset"><strong>Step 2: Load the Dataset</strong></h4>
<p>For this example, let’s use the famous <strong>Iris dataset</strong>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Iris dataset</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> load_iris()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.data  <span class="co"># Features</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data.target  <span class="co"># Target variable (Classes)</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and testing sets</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-3-train-the-naive-bayes-classifier" class="level4">
<h4 class="anchored" data-anchor-id="step-3-train-the-naive-bayes-classifier"><strong>Step 3: Train the Naive Bayes Classifier</strong></h4>
<p>We’ll use Gaussian Naive Bayes since the Iris dataset has continuous features.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Gaussian Naive Bayes classifier</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>gnb <span class="op">=</span> GaussianNB()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>gnb.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-4-making-predictions" class="level4">
<h4 class="anchored" data-anchor-id="step-4-making-predictions"><strong>Step 4: Making Predictions</strong></h4>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test data</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> gnb.predict(X_test)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix and classification report</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred))</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-5-output-and-interpretation" class="level4">
<h4 class="anchored" data-anchor-id="step-5-output-and-interpretation"><strong>Step 5: Output and Interpretation</strong></h4>
<p>The output will provide an accuracy score, confusion matrix, and a classification report summarizing the precision, recall, and F1 scores for each class.</p>
<hr>
</section>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion"><strong>8. Conclusion</strong></h3>
<p>Naive Bayes is a deceptively simple yet powerful algorithm, particularly effective for problems with a clear probabilistic structure. While the independence assumption may seem unrealistic in many cases, the algorithm often performs surprisingly well, especially when working with high-dimensional data such as text. However, users must be aware of its limitations, particularly when handling dependent features.</p>
<hr>
<p>In the next section, we’ll explore more advanced topics related to Naive Bayes, including handling zero frequencies, Laplace smoothing, and real-world applications of the algorithm.</p>



<!-- -->

</section>
</section>

<div class="quarto-listing quarto-listing-container-grid" id="listing-listing">
<div class="list grid quarto-listing-cols-3">
<div class="g-col-1" data-index="0" data-categories="Data Science,Machine Learning,Artificial Intelligence" data-listing-date-sort="1728273600000" data-listing-file-modified-sort="1728356230186" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="7" data-listing-word-count-sort="1345">
<a href="../../dsandml/logreg/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../dsandml/logreg/logreg.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code
</h5>
<div class="listing-reading-time card-text text-muted">
7 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Monday, October 7, 2024
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="1" data-categories="Data Science,Machine Learning,Artificial Intelligence,Data Engineering" data-listing-date-sort="1723608000000" data-listing-file-modified-sort="1726868222575" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="7" data-listing-word-count-sort="1261">
<a href="../../dsandml/datacollection/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../dsandml/datacollection/ws.jpg" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Data collection through Webscraping
</h5>
<div class="listing-reading-time card-text text-muted">
7 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Wednesday, August 14, 2024
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="2" data-categories="Data Science,Machine Learning,Artificial Intelligence,Data Engineering" data-listing-date-sort="1727150400000" data-listing-file-modified-sort="1728438677051" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="8" data-listing-word-count-sort="1429">
<a href="../../dsandml/pca/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../dsandml/pca/pca.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Dimensionality Reduction: Principle Component Analysis (PCA)
</h5>
<div class="listing-reading-time card-text text-muted">
8 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Tuesday, September 24, 2024
</div>
</div>
</div>
</div>
</a>
</div>
</div>
<div class="listing-no-matching d-none">
No matching items
</div>
</div><a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{islam2024,
  author = {Islam, Rafiq},
  title = {Classification Using {Naive} {Bayes} Algorithm},
  date = {2024-10-10},
  url = {https://mrislambd.github.io/dsandml/naivebayes/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-islam2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Islam, Rafiq. 2024. <span>“Classification Using Naive Bayes
Algorithm.”</span> October 10, 2024. <a href="https://mrislambd.github.io/dsandml/naivebayes/">https://mrislambd.github.io/dsandml/naivebayes/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mrislambd\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb11" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Classification using Naive Bayes algorithm"</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-10-10"</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> Rafiq Islam</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Statistics, Data Science, Data Engineering, Machine Learning, Artificial Intelligence]</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span><span class="co"> true</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="an">search:</span><span class="co"> true</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="an">lightbox:</span><span class="co"> true</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="an">listing:</span><span class="co"> </span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">    contents: "/../../dsandml"</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">    max-items: 3</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">    type: grid</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">    categories: false</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">    date-format: full</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">    fields: [image, date, title, author, reading-time]</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">---</span>  </span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction  </span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>Naive Bayes is a family of simple yet powerful probabilistic classifiers based on Bayes' Theorem, with the assumption of independence among predictors. It is widely used for tasks like spam detection, text classification, and sentiment analysis due to its efficiency and simplicity. Despite being called "naive" for its strong assumption of feature independence, it often performs remarkably well in real-world scenarios.&lt;/p&gt;</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="fu">## What is Naive Bayes?</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>Naive Bayes is a probabilistic classifier that leverages Bayes' Theorem to predict the class of a given data point. It belongs to the family of generative models and works by estimating the posterior probability of a class given a set of features. The term "Naive" refers to the assumption that features are conditionally independent given the class label, which simplifies computation.&lt;/p&gt;</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="fu">###  Bayes' Theorem: The Foundation</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>Bayes' Theorem provides a way to update our beliefs about the probability of an event, based on new evidence. The formula for Bayes' Theorem is:</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>P(y|X) = \frac{P(X|y) \cdot P(y)}{P(X)}</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>Where:  </span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$P(y|X)$: Posterior probability of class $y$ given feature set $X$  </span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$P(X|y)$: Likelihood of feature set $X$ given class $y$  </span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$P(y)$: Prior probability of class $y$  </span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$P(X)$: Evidence or probability of feature set $X$</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>In the context of classification:  </span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The goal is to predict $y$ (the class) given $X$ (the features).  </span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$P(y)$ is derived from the distribution of classes in the training data.  </span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$P(X|y)$ is derived from the distribution of features for each class.  </span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$P(X)$ is a normalizing constant to ensure probabilities sum to 1, but it can be ignored for classification purposes because it is the same for all classes.</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a><span class="fu">###  Assumptions and Requirements  </span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>The key assumption in Naive Bayes is the **conditional independence** of features. Specifically, it assumes that the likelihood of each feature is independent of the others, given the class label:</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>P(X_1, X_2, \dots, X_n | y) = P(X_1 | y) \cdot P(X_2 | y) \cdot \dots \cdot P(X_n | y)</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>While this assumption is often violated in real-world data, Naive Bayes can still perform well, especially when certain features dominate the prediction.</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>**Requirements:**  </span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Numerical Data**: Naive Bayes can handle both numerical and categorical data, though different versions (Gaussian, Multinomial, Bernoulli) of the algorithm handle specific types of data more effectively </span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Non-Collinear Features**: Highly correlated features can distort predictions since the model assumes independence.  </span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sufficient Data**: Naive Bayes relies on probability estimates; thus, insufficient data might lead to unreliable predictions.</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a><span class="fu">## Types of Naive Bayes Classifiers</span></span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>There are several variants of Naive Bayes, depending on the nature of the data:  </span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Gaussian Naive Bayes**: Assumes features follow a Gaussian distribution (useful for continuous data).  </span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Multinomial Naive Bayes**: Suitable for discrete data, often used in text classification (e.g., word counts).  </span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Bernoulli Naive Bayes**: Works well for binary/boolean data, often used in scenarios where the features represent the presence/absence of a characteristic.</span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematics behind the process</span></span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a>To understand the working of Naive Bayes, let’s start with the Bayes's theorem</span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a>P(y_k | X) = \frac{P(X|y_k) \cdot P(y_k)}{P(X)}</span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a>Where $y_k$ is one of the possible classes. Due to the independence assumption, the likelihood term $P(X|y_k)$ can be factorized as:</span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a>P(X|y_k) = P(x_1|y_k) \cdot P(x_2|y_k) \cdot \dots \cdot P(x_n|y_k)</span>
<span id="cb11-87"><a href="#cb11-87" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-88"><a href="#cb11-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-89"><a href="#cb11-89" aria-hidden="true" tabindex="-1"></a>Where $x_1, x_2, \dots, x_n$ are the individual features in the feature set $X$. For each class $y_k$, compute the posterior probability:</span>
<span id="cb11-90"><a href="#cb11-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-91"><a href="#cb11-91" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-92"><a href="#cb11-92" aria-hidden="true" tabindex="-1"></a>P(y_k | X) \propto P(y_k) \cdot \prod_{i=1}^n P(x_i|y_k)</span>
<span id="cb11-93"><a href="#cb11-93" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-94"><a href="#cb11-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-95"><a href="#cb11-95" aria-hidden="true" tabindex="-1"></a>The denominator $P(X)$ is constant for all classes, so we can ignore it during classification. Finally, the class $y_k$ with the highest posterior probability is chosen as the predicted class:  </span>
<span id="cb11-96"><a href="#cb11-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-97"><a href="#cb11-97" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb11-98"><a href="#cb11-98" aria-hidden="true" tabindex="-1"></a>\hat{y} &amp;= \arg\max_{y_k} P(y_k) \cdot \prod_{i=1}^n P(x_i|y_k)<span class="sc">\\</span></span>
<span id="cb11-99"><a href="#cb11-99" aria-hidden="true" tabindex="-1"></a>\log{(\hat{y})}&amp;= \log{ \left(\arg\max_{y_k} P(y_k) \cdot \prod_{i=1}^n P(x_i|y_k)\right)}<span class="sc">\\</span></span>
<span id="cb11-100"><a href="#cb11-100" aria-hidden="true" tabindex="-1"></a>\implies \hat{y} &amp; = \arg\max_{y_k} \left(\log P(y_k)+\sum_{i=1}^{n} P(x_i|y_k)\right)</span>
<span id="cb11-101"><a href="#cb11-101" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb11-102"><a href="#cb11-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-103"><a href="#cb11-103" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computing the probabilities    </span></span>
<span id="cb11-104"><a href="#cb11-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-105"><a href="#cb11-105" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Prior Probabilities  </span></span>
<span id="cb11-106"><a href="#cb11-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-107"><a href="#cb11-107" aria-hidden="true" tabindex="-1"></a>$P(y_k)$ is the prior probability, usually frequency of each class $k$.  </span>
<span id="cb11-108"><a href="#cb11-108" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-109"><a href="#cb11-109" aria-hidden="true" tabindex="-1"></a>  P(y_k)=\frac{\text{number of instances in class }y_k}{\text{total number of instances}}</span>
<span id="cb11-110"><a href="#cb11-110" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb11-111"><a href="#cb11-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-112"><a href="#cb11-112" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Class Conditional Probabilities  </span></span>
<span id="cb11-113"><a href="#cb11-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-114"><a href="#cb11-114" aria-hidden="true" tabindex="-1"></a>$P(x_i|y_k)$ is the class conditional probability. For the  </span>
<span id="cb11-115"><a href="#cb11-115" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-116"><a href="#cb11-116" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>***Gaussian Naive Bayes:*** when the features are continuous and assumed that the features follow a ***Gaussian*** distribution, the <span class="in">`class conditional`</span> probability is given as </span>
<span id="cb11-117"><a href="#cb11-117" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-118"><a href="#cb11-118" aria-hidden="true" tabindex="-1"></a> P(x_i|y_k) = \frac{1}{\sqrt{2\pi \sigma^2_k}}\exp{\left(-\frac{(x_i-\mu_i)^2}{2\sigma^2_k}\right)}</span>
<span id="cb11-119"><a href="#cb11-119" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb11-120"><a href="#cb11-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-121"><a href="#cb11-121" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>***Multinomial Naive Bayes:*** when the featrues (typically word frequencies) follow a multinomial distribution, the <span class="in">`class conditional`</span> distribution is given as  </span>
<span id="cb11-122"><a href="#cb11-122" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb11-123"><a href="#cb11-123" aria-hidden="true" tabindex="-1"></a>    P(x_i|y_k)=\frac{N_{x_i,y_k}+\alpha}{N_{y_k}+\alpha V}</span>
<span id="cb11-124"><a href="#cb11-124" aria-hidden="true" tabindex="-1"></a>    $$  </span>
<span id="cb11-125"><a href="#cb11-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-126"><a href="#cb11-126" aria-hidden="true" tabindex="-1"></a>    where,  </span>
<span id="cb11-127"><a href="#cb11-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-128"><a href="#cb11-128" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$N_{x_i,y_k}$ is the count of the feature (e.g. word or term) $x_i$ appearing in documents of class $y_k$  </span>
<span id="cb11-129"><a href="#cb11-129" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$N_{y_k}$ is the total count of all features (e.g. words) in all documents belonging to class $y_k$  </span>
<span id="cb11-130"><a href="#cb11-130" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$\alpha$ is a smoothing parameter (often called **Laplace smoothing**), used to avoid zero probabilities. If not using smoothing, set $\alpha=0$  </span>
<span id="cb11-131"><a href="#cb11-131" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>$V$ is the size of the vocabulary (i.e., the number of unique words)  </span>
<span id="cb11-132"><a href="#cb11-132" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-133"><a href="#cb11-133" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>***Bernoulli Naive Bayes:*** when features are binary/boolean data, often used in scenarios where the features represent the presence/absence of a characteristic, the <span class="in">`class conditional`</span> distribution is given as  </span>
<span id="cb11-134"><a href="#cb11-134" aria-hidden="true" tabindex="-1"></a>   $$</span>
<span id="cb11-135"><a href="#cb11-135" aria-hidden="true" tabindex="-1"></a>    P(x_i|y_k)=\begin{cases}\frac{N_{x_i,y_k}+\alpha}{N_{y_k}+2\alpha }\hspace{2mm}\text{ if } x_i=1<span class="sc">\\</span></span>
<span id="cb11-136"><a href="#cb11-136" aria-hidden="true" tabindex="-1"></a>    1-\frac{N_{x_i,y_k}+\alpha}{N_{y_k}+2\alpha }\hspace{2mm}\text{ if } x_i=0\end{cases}</span>
<span id="cb11-137"><a href="#cb11-137" aria-hidden="true" tabindex="-1"></a>   $$</span>
<span id="cb11-138"><a href="#cb11-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-139"><a href="#cb11-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-140"><a href="#cb11-140" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python Implementation  </span></span>
<span id="cb11-141"><a href="#cb11-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-142"><a href="#cb11-142" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gaussian Naive Bayes  </span></span>
<span id="cb11-143"><a href="#cb11-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-144"><a href="#cb11-144" aria-hidden="true" tabindex="-1"></a>Code credit for the custom classifier goes to <span class="co">[</span><span class="ot">Assembly AI</span><span class="co">](https://github.com/AssemblyAI-Community/Machine-Learning-From-Scratch/blob/main/06%20NaiveBayes/naive_bayes.py)</span>{target="_blank" style="text-decoration:none"}</span>
<span id="cb11-145"><a href="#cb11-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-148"><a href="#cb11-148" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-149"><a href="#cb11-149" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb11-150"><a href="#cb11-150" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-151"><a href="#cb11-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-152"><a href="#cb11-152" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GNaiveBayes:</span>
<span id="cb11-153"><a href="#cb11-153" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X,y):</span>
<span id="cb11-154"><a href="#cb11-154" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb11-155"><a href="#cb11-155" aria-hidden="true" tabindex="-1"></a><span class="co">        n_samples: number of observed data n; int;</span></span>
<span id="cb11-156"><a href="#cb11-156" aria-hidden="true" tabindex="-1"></a><span class="co">        n_features: number of continueous features d; int;</span></span>
<span id="cb11-157"><a href="#cb11-157" aria-hidden="true" tabindex="-1"></a><span class="co">        _classes: unique classes</span></span>
<span id="cb11-158"><a href="#cb11-158" aria-hidden="true" tabindex="-1"></a><span class="co">        n_classes: number of unique classes</span></span>
<span id="cb11-159"><a href="#cb11-159" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb11-160"><a href="#cb11-160" aria-hidden="true" tabindex="-1"></a>        n_samples, n_features <span class="op">=</span> X.shape</span>
<span id="cb11-161"><a href="#cb11-161" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._classes <span class="op">=</span> np.unique(y)</span>
<span id="cb11-162"><a href="#cb11-162" aria-hidden="true" tabindex="-1"></a>        n_classes <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>._classes)</span>
<span id="cb11-163"><a href="#cb11-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-164"><a href="#cb11-164" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate mean, variance, and prior for each class  </span></span>
<span id="cb11-165"><a href="#cb11-165" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._mean <span class="op">=</span> np.zeros((n_classes,n_features),dtype<span class="op">=</span>np.float64)</span>
<span id="cb11-166"><a href="#cb11-166" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._var <span class="op">=</span> np.zeros((n_classes,n_features),dtype<span class="op">=</span>np.float64)</span>
<span id="cb11-167"><a href="#cb11-167" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._prior <span class="op">=</span> np.zeros(n_classes,dtype<span class="op">=</span>np.float64)</span>
<span id="cb11-168"><a href="#cb11-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-169"><a href="#cb11-169" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, c <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>._classes):</span>
<span id="cb11-170"><a href="#cb11-170" aria-hidden="true" tabindex="-1"></a>            X_c <span class="op">=</span> X[y<span class="op">==</span>c]</span>
<span id="cb11-171"><a href="#cb11-171" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._mean[idx,:] <span class="op">=</span> X_c.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb11-172"><a href="#cb11-172" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._var[idx,:] <span class="op">=</span> X_c.var(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb11-173"><a href="#cb11-173" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._prior[idx] <span class="op">=</span> X_c.shape[<span class="dv">0</span>]<span class="op">/</span><span class="bu">float</span>(n_samples)</span>
<span id="cb11-174"><a href="#cb11-174" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-175"><a href="#cb11-175" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>,X):</span>
<span id="cb11-176"><a href="#cb11-176" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> [<span class="va">self</span>._predict(x) <span class="cf">for</span> x <span class="kw">in</span> X]</span>
<span id="cb11-177"><a href="#cb11-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-178"><a href="#cb11-178" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(y_pred)</span>
<span id="cb11-179"><a href="#cb11-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-180"><a href="#cb11-180" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _predict(<span class="va">self</span>, x):</span>
<span id="cb11-181"><a href="#cb11-181" aria-hidden="true" tabindex="-1"></a>        posteriors <span class="op">=</span> []</span>
<span id="cb11-182"><a href="#cb11-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-183"><a href="#cb11-183" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the posterior probability for each class  </span></span>
<span id="cb11-184"><a href="#cb11-184" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx,c <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>._classes):</span>
<span id="cb11-185"><a href="#cb11-185" aria-hidden="true" tabindex="-1"></a>            prior <span class="op">=</span> np.log(<span class="va">self</span>._prior[idx])</span>
<span id="cb11-186"><a href="#cb11-186" aria-hidden="true" tabindex="-1"></a>            post <span class="op">=</span> np.<span class="bu">sum</span>(np.log(<span class="va">self</span>._pdf(idx,x)))</span>
<span id="cb11-187"><a href="#cb11-187" aria-hidden="true" tabindex="-1"></a>            posterior <span class="op">=</span> post <span class="op">+</span> prior</span>
<span id="cb11-188"><a href="#cb11-188" aria-hidden="true" tabindex="-1"></a>            posteriors.append(posterior)</span>
<span id="cb11-189"><a href="#cb11-189" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return the class with the highest posterior</span></span>
<span id="cb11-190"><a href="#cb11-190" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._classes[np.argmax(posteriors)]</span>
<span id="cb11-191"><a href="#cb11-191" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-192"><a href="#cb11-192" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _pdf(<span class="va">self</span>, class_idx, x):</span>
<span id="cb11-193"><a href="#cb11-193" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> <span class="va">self</span>._mean[class_idx]</span>
<span id="cb11-194"><a href="#cb11-194" aria-hidden="true" tabindex="-1"></a>        var <span class="op">=</span> <span class="va">self</span>._var[class_idx]</span>
<span id="cb11-195"><a href="#cb11-195" aria-hidden="true" tabindex="-1"></a>        numerator <span class="op">=</span> np.exp(<span class="op">-</span>((x<span class="op">-</span>mean)<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>var))</span>
<span id="cb11-196"><a href="#cb11-196" aria-hidden="true" tabindex="-1"></a>        denominator <span class="op">=</span> np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi<span class="op">*</span>var)</span>
<span id="cb11-197"><a href="#cb11-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-198"><a href="#cb11-198" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> numerator<span class="op">/</span>denominator</span>
<span id="cb11-199"><a href="#cb11-199" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb11-200"><a href="#cb11-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-201"><a href="#cb11-201" aria-hidden="true" tabindex="-1"></a>Let's apply this to the <span class="in">`irish`</span> data set  </span>
<span id="cb11-202"><a href="#cb11-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-205"><a href="#cb11-205" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-206"><a href="#cb11-206" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb11-207"><a href="#cb11-207" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb11-208"><a href="#cb11-208" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb11-209"><a href="#cb11-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-210"><a href="#cb11-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-211"><a href="#cb11-211" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Iris dataset</span></span>
<span id="cb11-212"><a href="#cb11-212" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> load_iris()</span>
<span id="cb11-213"><a href="#cb11-213" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.data  <span class="co"># Features</span></span>
<span id="cb11-214"><a href="#cb11-214" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data.target  <span class="co"># Target variable (Classes)</span></span>
<span id="cb11-215"><a href="#cb11-215" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span>data.feature_names)</span>
<span id="cb11-216"><a href="#cb11-216" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'target'</span>] <span class="op">=</span> pd.Categorical.from_codes(y, data.target_names)</span>
<span id="cb11-217"><a href="#cb11-217" aria-hidden="true" tabindex="-1"></a>df.head()</span>
<span id="cb11-218"><a href="#cb11-218" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-219"><a href="#cb11-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-222"><a href="#cb11-222" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-223"><a href="#cb11-223" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb11-224"><a href="#cb11-224" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb11-225"><a href="#cb11-225" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb11-226"><a href="#cb11-226" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report  </span>
<span id="cb11-227"><a href="#cb11-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-228"><a href="#cb11-228" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and testing sets</span></span>
<span id="cb11-229"><a href="#cb11-229" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb11-230"><a href="#cb11-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-231"><a href="#cb11-231" aria-hidden="true" tabindex="-1"></a>gnb1 <span class="op">=</span> GNaiveBayes()</span>
<span id="cb11-232"><a href="#cb11-232" aria-hidden="true" tabindex="-1"></a>gnb1.fit(X_train, y_train)</span>
<span id="cb11-233"><a href="#cb11-233" aria-hidden="true" tabindex="-1"></a>pred1 <span class="op">=</span> gnb1.predict(X_test)</span>
<span id="cb11-234"><a href="#cb11-234" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb11-235"><a href="#cb11-235" aria-hidden="true" tabindex="-1"></a>acc1 <span class="op">=</span> accuracy_score(y_test, pred1)</span>
<span id="cb11-236"><a href="#cb11-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-237"><a href="#cb11-237" aria-hidden="true" tabindex="-1"></a>gnb2 <span class="op">=</span> GaussianNB()</span>
<span id="cb11-238"><a href="#cb11-238" aria-hidden="true" tabindex="-1"></a>gnb2.fit(X_train, y_train)</span>
<span id="cb11-239"><a href="#cb11-239" aria-hidden="true" tabindex="-1"></a>pred2 <span class="op">=</span> gnb2.predict(X_test)</span>
<span id="cb11-240"><a href="#cb11-240" aria-hidden="true" tabindex="-1"></a>acc2 <span class="op">=</span> accuracy_score(y_test, pred2) </span>
<span id="cb11-241"><a href="#cb11-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-242"><a href="#cb11-242" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Accuracy from custom classifier = </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(acc1<span class="op">*</span><span class="dv">100</span>))</span>
<span id="cb11-243"><a href="#cb11-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-244"><a href="#cb11-244" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix and classification report</span></span>
<span id="cb11-245"><a href="#cb11-245" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, pred1))</span>
<span id="cb11-246"><a href="#cb11-246" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, pred1))</span>
<span id="cb11-247"><a href="#cb11-247" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb11-248"><a href="#cb11-248" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Accuracy from sklearn classifier = </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(acc2<span class="op">*</span><span class="dv">100</span>))</span>
<span id="cb11-249"><a href="#cb11-249" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, pred2))</span>
<span id="cb11-250"><a href="#cb11-250" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, pred2))</span>
<span id="cb11-251"><a href="#cb11-251" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb11-252"><a href="#cb11-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-253"><a href="#cb11-253" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multinomial Naive Bayes  </span></span>
<span id="cb11-254"><a href="#cb11-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-257"><a href="#cb11-257" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-258"><a href="#cb11-258" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb11-259"><a href="#cb11-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-260"><a href="#cb11-260" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MNaiveBayes:</span>
<span id="cb11-261"><a href="#cb11-261" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb11-262"><a href="#cb11-262" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb11-263"><a href="#cb11-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-264"><a href="#cb11-264" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X,y):</span>
<span id="cb11-265"><a href="#cb11-265" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb11-266"><a href="#cb11-266" aria-hidden="true" tabindex="-1"></a><span class="co">        Fit the Multinomial Naive Bayes model to the training data.  </span></span>
<span id="cb11-267"><a href="#cb11-267" aria-hidden="true" tabindex="-1"></a><span class="co">        X: input data (n_samples, n_features)</span></span>
<span id="cb11-268"><a href="#cb11-268" aria-hidden="true" tabindex="-1"></a><span class="co">        y: target labels (n_samples)</span></span>
<span id="cb11-269"><a href="#cb11-269" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb11-270"><a href="#cb11-270" aria-hidden="true" tabindex="-1"></a>        n_samples, n_features <span class="op">=</span> X.shape</span>
<span id="cb11-271"><a href="#cb11-271" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._classes <span class="op">=</span> np.unique(y)</span>
<span id="cb11-272"><a href="#cb11-272" aria-hidden="true" tabindex="-1"></a>        n_classes <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>._classes)</span>
<span id="cb11-273"><a href="#cb11-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-274"><a href="#cb11-274" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize and count priors </span></span>
<span id="cb11-275"><a href="#cb11-275" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._class_feature_count <span class="op">=</span> np.zeros((n_classes, n_features),dtype<span class="op">=</span>np.float64)</span>
<span id="cb11-276"><a href="#cb11-276" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._class_count <span class="op">=</span> np.zeros(n_classes, dtype<span class="op">=</span>np.float64)</span>
<span id="cb11-277"><a href="#cb11-277" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._prior <span class="op">=</span> np.zeros(n_classes, dtype<span class="op">=</span>np.float64)</span>
<span id="cb11-278"><a href="#cb11-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-279"><a href="#cb11-279" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx,c <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>._classes):</span>
<span id="cb11-280"><a href="#cb11-280" aria-hidden="true" tabindex="-1"></a>            X_c <span class="op">=</span> X[y<span class="op">==</span>c]</span>
<span id="cb11-281"><a href="#cb11-281" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._class_feature_count[idx,:] <span class="op">=</span> X_c.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb11-282"><a href="#cb11-282" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._class_count[idx] <span class="op">=</span> X_c.shape[<span class="dv">0</span>]</span>
<span id="cb11-283"><a href="#cb11-283" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._prior[idx] <span class="op">=</span> X_c.shape[<span class="dv">0</span>]<span class="op">/</span><span class="bu">float</span>(n_samples)</span>
<span id="cb11-284"><a href="#cb11-284" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-285"><a href="#cb11-285" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Total count of all features accross all classes </span></span>
<span id="cb11-286"><a href="#cb11-286" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._total_feature_count <span class="op">=</span> <span class="va">self</span>._class_feature_count.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-287"><a href="#cb11-287" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-288"><a href="#cb11-288" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb11-289"><a href="#cb11-289" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> [<span class="va">self</span>._predict(x) <span class="cf">for</span> x <span class="kw">in</span> X]</span>
<span id="cb11-290"><a href="#cb11-290" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(y_pred)</span>
<span id="cb11-291"><a href="#cb11-291" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-292"><a href="#cb11-292" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _predict(<span class="va">self</span>,x):</span>
<span id="cb11-293"><a href="#cb11-293" aria-hidden="true" tabindex="-1"></a>        posteriors <span class="op">=</span> []</span>
<span id="cb11-294"><a href="#cb11-294" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, c <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>._classes):</span>
<span id="cb11-295"><a href="#cb11-295" aria-hidden="true" tabindex="-1"></a>            prior <span class="op">=</span> np.log(<span class="va">self</span>._prior[idx])</span>
<span id="cb11-296"><a href="#cb11-296" aria-hidden="true" tabindex="-1"></a>            likelihood <span class="op">=</span> np.<span class="bu">sum</span>(np.log(<span class="va">self</span>._likelihood(idx,x)))</span>
<span id="cb11-297"><a href="#cb11-297" aria-hidden="true" tabindex="-1"></a>            posterior_prob <span class="op">=</span> prior<span class="op">+</span> likelihood</span>
<span id="cb11-298"><a href="#cb11-298" aria-hidden="true" tabindex="-1"></a>            posteriors.append(posterior_prob)</span>
<span id="cb11-299"><a href="#cb11-299" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._classes[np.argmax(posteriors)]</span>
<span id="cb11-300"><a href="#cb11-300" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-301"><a href="#cb11-301" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _likelihood(<span class="va">self</span>, class_idx, x):</span>
<span id="cb11-302"><a href="#cb11-302" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> <span class="va">self</span>.alpha</span>
<span id="cb11-303"><a href="#cb11-303" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>._class_feature_count[class_idx])</span>
<span id="cb11-304"><a href="#cb11-304" aria-hidden="true" tabindex="-1"></a>        class_feature_count <span class="op">=</span> <span class="va">self</span>._class_feature_count[class_idx]</span>
<span id="cb11-305"><a href="#cb11-305" aria-hidden="true" tabindex="-1"></a>        total_class_count <span class="op">=</span> <span class="va">self</span>._total_feature_count[class_idx]</span>
<span id="cb11-306"><a href="#cb11-306" aria-hidden="true" tabindex="-1"></a>        likelihood <span class="op">=</span> (class_feature_count<span class="op">+</span>alpha)<span class="op">/</span>(total_class_count <span class="op">+</span> alpha <span class="op">*</span> V)</span>
<span id="cb11-307"><a href="#cb11-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-308"><a href="#cb11-308" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> likelihood<span class="op">**</span>x</span>
<span id="cb11-309"><a href="#cb11-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-310"><a href="#cb11-310" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb11-311"><a href="#cb11-311" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb11-312"><a href="#cb11-312" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>],</span>
<span id="cb11-313"><a href="#cb11-313" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>],</span>
<span id="cb11-314"><a href="#cb11-314" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>]])</span>
<span id="cb11-315"><a href="#cb11-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-316"><a href="#cb11-316" aria-hidden="true" tabindex="-1"></a><span class="co"># Corresponding labels (2 classes: 0 and 1)</span></span>
<span id="cb11-317"><a href="#cb11-317" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb11-318"><a href="#cb11-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-319"><a href="#cb11-319" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and train Multinomial Naive Bayes model</span></span>
<span id="cb11-320"><a href="#cb11-320" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MNaiveBayes()</span>
<span id="cb11-321"><a href="#cb11-321" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)</span>
<span id="cb11-322"><a href="#cb11-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-323"><a href="#cb11-323" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict for new sample</span></span>
<span id="cb11-324"><a href="#cb11-324" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb11-325"><a href="#cb11-325" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model.predict(X_test)</span>
<span id="cb11-326"><a href="#cb11-326" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(predictions)</span>
<span id="cb11-327"><a href="#cb11-327" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-328"><a href="#cb11-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-329"><a href="#cb11-329" aria-hidden="true" tabindex="-1"></a><span class="fu">### **6. Pros and Cons of Naive Bayes**</span></span>
<span id="cb11-330"><a href="#cb11-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-331"><a href="#cb11-331" aria-hidden="true" tabindex="-1"></a><span class="fu">#### **Pros:**</span></span>
<span id="cb11-332"><a href="#cb11-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Simplicity**: Easy to implement and computationally efficient.</span>
<span id="cb11-333"><a href="#cb11-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fast Training and Prediction**: Naive Bayes is especially fast for both training and inference, even on large datasets.</span>
<span id="cb11-334"><a href="#cb11-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Performs Well with Small Data**: Despite its simplicity, Naive Bayes works well even with relatively small datasets.</span>
<span id="cb11-335"><a href="#cb11-335" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Handles Irrelevant Features**: Naive Bayes can often ignore irrelevant features in the data since the independence assumption dilutes their influence.</span>
<span id="cb11-336"><a href="#cb11-336" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-Class Classification**: Naturally suited for multi-class classification problems.</span>
<span id="cb11-337"><a href="#cb11-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-338"><a href="#cb11-338" aria-hidden="true" tabindex="-1"></a><span class="fu">#### **Cons:**</span></span>
<span id="cb11-339"><a href="#cb11-339" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Strong Assumption of Independence**: The assumption that features are independent is rarely true in real-world data, which can limit the model's effectiveness.</span>
<span id="cb11-340"><a href="#cb11-340" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Poor Estimation of Probabilities**: When dealing with very small datasets or unseen feature combinations, Naive Bayes can yield inaccurate probability estimates.</span>
<span id="cb11-341"><a href="#cb11-341" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Zero-Frequency Problem**: If a feature value was not present in the training data, Naive Bayes will assign zero probability to the entire class, which can be addressed using Laplace smoothing.</span>
<span id="cb11-342"><a href="#cb11-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-343"><a href="#cb11-343" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb11-344"><a href="#cb11-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-345"><a href="#cb11-345" aria-hidden="true" tabindex="-1"></a><span class="fu">### **7. Python Implementation of Naive Bayes**</span></span>
<span id="cb11-346"><a href="#cb11-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-347"><a href="#cb11-347" aria-hidden="true" tabindex="-1"></a>Let's implement Naive Bayes in Python using the Scikit-learn library.</span>
<span id="cb11-348"><a href="#cb11-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-349"><a href="#cb11-349" aria-hidden="true" tabindex="-1"></a><span class="fu">#### **Step 1: Importing the necessary libraries**</span></span>
<span id="cb11-350"><a href="#cb11-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-351"><a href="#cb11-351" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb11-352"><a href="#cb11-352" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-353"><a href="#cb11-353" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb11-354"><a href="#cb11-354" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb11-355"><a href="#cb11-355" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb11-356"><a href="#cb11-356" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix, classification_report</span>
<span id="cb11-357"><a href="#cb11-357" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-358"><a href="#cb11-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-359"><a href="#cb11-359" aria-hidden="true" tabindex="-1"></a><span class="fu">#### **Step 2: Load the Dataset**</span></span>
<span id="cb11-360"><a href="#cb11-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-361"><a href="#cb11-361" aria-hidden="true" tabindex="-1"></a>For this example, let's use the famous **Iris dataset**:</span>
<span id="cb11-362"><a href="#cb11-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-363"><a href="#cb11-363" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb11-364"><a href="#cb11-364" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb11-365"><a href="#cb11-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-366"><a href="#cb11-366" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Iris dataset</span></span>
<span id="cb11-367"><a href="#cb11-367" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> load_iris()</span>
<span id="cb11-368"><a href="#cb11-368" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.data  <span class="co"># Features</span></span>
<span id="cb11-369"><a href="#cb11-369" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data.target  <span class="co"># Target variable (Classes)</span></span>
<span id="cb11-370"><a href="#cb11-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-371"><a href="#cb11-371" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and testing sets</span></span>
<span id="cb11-372"><a href="#cb11-372" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb11-373"><a href="#cb11-373" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-374"><a href="#cb11-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-375"><a href="#cb11-375" aria-hidden="true" tabindex="-1"></a><span class="fu">#### **Step 3: Train the Naive Bayes Classifier**</span></span>
<span id="cb11-376"><a href="#cb11-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-377"><a href="#cb11-377" aria-hidden="true" tabindex="-1"></a>We'll use Gaussian Naive Bayes since the Iris dataset has continuous features.</span>
<span id="cb11-378"><a href="#cb11-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-379"><a href="#cb11-379" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb11-380"><a href="#cb11-380" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Gaussian Naive Bayes classifier</span></span>
<span id="cb11-381"><a href="#cb11-381" aria-hidden="true" tabindex="-1"></a>gnb <span class="op">=</span> GaussianNB()</span>
<span id="cb11-382"><a href="#cb11-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-383"><a href="#cb11-383" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb11-384"><a href="#cb11-384" aria-hidden="true" tabindex="-1"></a>gnb.fit(X_train, y_train)</span>
<span id="cb11-385"><a href="#cb11-385" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-386"><a href="#cb11-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-387"><a href="#cb11-387" aria-hidden="true" tabindex="-1"></a><span class="fu">#### **Step 4: Making Predictions**</span></span>
<span id="cb11-388"><a href="#cb11-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-389"><a href="#cb11-389" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb11-390"><a href="#cb11-390" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test data</span></span>
<span id="cb11-391"><a href="#cb11-391" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> gnb.predict(X_test)</span>
<span id="cb11-392"><a href="#cb11-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-393"><a href="#cb11-393" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb11-394"><a href="#cb11-394" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb11-395"><a href="#cb11-395" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb11-396"><a href="#cb11-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-397"><a href="#cb11-397" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix and classification report</span></span>
<span id="cb11-398"><a href="#cb11-398" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_test, y_pred))</span>
<span id="cb11-399"><a href="#cb11-399" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred))</span>
<span id="cb11-400"><a href="#cb11-400" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-401"><a href="#cb11-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-402"><a href="#cb11-402" aria-hidden="true" tabindex="-1"></a><span class="fu">#### **Step 5: Output and Interpretation**</span></span>
<span id="cb11-403"><a href="#cb11-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-404"><a href="#cb11-404" aria-hidden="true" tabindex="-1"></a>The output will provide an accuracy score, confusion matrix, and a classification report summarizing the precision, recall, and F1 scores for each class.</span>
<span id="cb11-405"><a href="#cb11-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-406"><a href="#cb11-406" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb11-407"><a href="#cb11-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-408"><a href="#cb11-408" aria-hidden="true" tabindex="-1"></a><span class="fu">### **8. Conclusion**</span></span>
<span id="cb11-409"><a href="#cb11-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-410"><a href="#cb11-410" aria-hidden="true" tabindex="-1"></a>Naive Bayes is a deceptively simple yet powerful algorithm, particularly effective for problems with a clear probabilistic structure. While the independence assumption may seem unrealistic in many cases, the algorithm often performs surprisingly well, especially when working with high-dimensional data such as text. However, users must be aware of its limitations, particularly when handling dependent features.</span>
<span id="cb11-411"><a href="#cb11-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-412"><a href="#cb11-412" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb11-413"><a href="#cb11-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-414"><a href="#cb11-414" aria-hidden="true" tabindex="-1"></a>In the next section, we'll explore more advanced topics related to Naive Bayes, including handling zero frequencies, Laplace smoothing, and real-world applications of the algorithm.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Powered by <a href="https://quarto.org/">Quarto</a> 1.5.54</p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2024 @ Rafiq Islam
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../license.txt">
<p>License</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/rafiqr35" target="_blank">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://youtube.com/@quanttube" target="_blank">
      <i class="bi bi-youtube" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:rafiqfsu@gmail.com?subject&amp;body" target="_blank">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>