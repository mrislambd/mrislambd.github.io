<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rafiq Islam">
<meta name="dcterms.date" content="2024-10-07">

<title>Ensemble Methods: Random Forest - A detailed overview – Mohammad Rafiqul Islam</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../..//_assets/images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-listing/list.min.js"></script>
<script src="../../site_libs/quarto-listing/quarto-listing.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-listing .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-image','listing-date','listing-title','listing-author','listing-reading-time',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      page: 18,
    pagination: { item: "<li class='page-item'><a class='page page-link' href='#'></a></li>" },
      searchColumns: ["listing-title","listing-author","listing-date","listing-image","listing-description","listing-categories"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-listing'] = new List('listing-listing', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Z5NP67GHFC"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Z5NP67GHFC', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6878992848042528" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Ensemble Methods: Random Forest - A detailed overview – Mohammad Rafiqul Islam">
<meta property="og:description" content="">
<meta property="og:image" content="https://mrislambd.github.io/dsandml/randomforest/gb.webp">
<meta property="og:site_name" content="Mohammad Rafiqul Islam">
<meta name="twitter:title" content="Ensemble Methods: Random Forest - A detailed overview – Mohammad Rafiqul Islam">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://mrislambd.github.io/dsandml/randomforest/gb.webp">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../dsandml/randomforest/index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../_assets/images/fsu-logo.png" alt="Florida State University" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../dsandml/randomforest/index.html">
    <span class="navbar-title">Mohammad Rafiqul Islam</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../portfolio.html"> 
<span class="menu-text">Portfolio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cv.html"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/mohammad-rafiqul-islam/" target="_blank"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mrislambd" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#what-is-random-forest" id="toc-what-is-random-forest" class="nav-link" data-scroll-target="#what-is-random-forest">What is Random Forest?</a></li>
  <li><a href="#mathematics-behind-random-forest" id="toc-mathematics-behind-random-forest" class="nav-link" data-scroll-target="#mathematics-behind-random-forest">Mathematics Behind Random Forest</a>
  <ul class="collapse">
  <li><a href="#decision-tree-recap" id="toc-decision-tree-recap" class="nav-link" data-scroll-target="#decision-tree-recap">Decision Tree Recap</a></li>
  <li><a href="#random-forest-algorithm" id="toc-random-forest-algorithm" class="nav-link" data-scroll-target="#random-forest-algorithm">Random Forest Algorithm</a></li>
  <li><a href="#random-forest-for-classification" id="toc-random-forest-for-classification" class="nav-link" data-scroll-target="#random-forest-for-classification">Random Forest for Classification</a></li>
  <li><a href="#random-forest-for-regression" id="toc-random-forest-for-regression" class="nav-link" data-scroll-target="#random-forest-for-regression">Random Forest for Regression</a></li>
  </ul></li>
  <li><a href="#assumptions-of-random-forest" id="toc-assumptions-of-random-forest" class="nav-link" data-scroll-target="#assumptions-of-random-forest">Assumptions of Random Forest</a></li>
  <li><a href="#advantages-of-random-forest" id="toc-advantages-of-random-forest" class="nav-link" data-scroll-target="#advantages-of-random-forest">Advantages of Random Forest</a></li>
  <li><a href="#disadvantages-of-random-forest" id="toc-disadvantages-of-random-forest" class="nav-link" data-scroll-target="#disadvantages-of-random-forest">Disadvantages of Random Forest</a></li>
  <li><a href="#python-implementation" id="toc-python-implementation" class="nav-link" data-scroll-target="#python-implementation">Python Implementation</a></li>
  <li><a href="#hyperparameter-tuning-for-random-forest" id="toc-hyperparameter-tuning-for-random-forest" class="nav-link" data-scroll-target="#hyperparameter-tuning-for-random-forest">Hyperparameter Tuning for Random Forest</a>
  <ul class="collapse">
  <li><a href="#important-hyperparameters" id="toc-important-hyperparameters" class="nav-link" data-scroll-target="#important-hyperparameters">Important Hyperparameters</a></li>
  <li><a href="#grid-search-for-hyperparameter-tuning" id="toc-grid-search-for-hyperparameter-tuning" class="nav-link" data-scroll-target="#grid-search-for-hyperparameter-tuning">Grid Search for Hyperparameter Tuning</a></li>
  </ul></li>
  <li><a href="#feature-importance-in-random-forest" id="toc-feature-importance-in-random-forest" class="nav-link" data-scroll-target="#feature-importance-in-random-forest">Feature Importance in Random Forest</a>
  <ul class="collapse">
  <li><a href="#computing-feature-importance" id="toc-computing-feature-importance" class="nav-link" data-scroll-target="#computing-feature-importance">Computing Feature Importance</a></li>
  </ul></li>
  <li><a href="#out-of-bag-oob-error-estimate" id="toc-out-of-bag-oob-error-estimate" class="nav-link" data-scroll-target="#out-of-bag-oob-error-estimate">Out-of-Bag (OOB) Error Estimate</a>
  <ul class="collapse">
  <li><a href="#enabling-oob-in-python" id="toc-enabling-oob-in-python" class="nav-link" data-scroll-target="#enabling-oob-in-python">Enabling OOB in Python</a></li>
  </ul></li>
  <li><a href="#dealing-with-imbalanced-data" id="toc-dealing-with-imbalanced-data" class="nav-link" data-scroll-target="#dealing-with-imbalanced-data">Dealing with Imbalanced Data</a></li>
  <li><a href="#random-forest-in-practice-best-practices" id="toc-random-forest-in-practice-best-practices" class="nav-link" data-scroll-target="#random-forest-in-practice-best-practices">Random Forest in Practice: Best Practices</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    <h5 class="quarto-listing-category-title">Categories</h5><div class="quarto-listing-category category-default"><div class="category" data-category="">All <span class="quarto-category-count">(89)</span></div><div class="category" data-category="Algorithms">Algorithms <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Artificial Intelligence">Artificial Intelligence <span class="quarto-category-count">(17)</span></div><div class="category" data-category="Data Engineering">Data Engineering <span class="quarto-category-count">(9)</span></div><div class="category" data-category="Data Science">Data Science <span class="quarto-category-count">(18)</span></div><div class="category" data-category="Machine Learning">Machine Learning <span class="quarto-category-count">(18)</span></div><div class="category" data-category="Programming">Programming <span class="quarto-category-count">(1)</span></div><div class="category" data-category="PyTorch">PyTorch <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Python">Python <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Statistics">Statistics <span class="quarto-category-count">(2)</span></div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Ensemble Methods: Random Forest - A detailed overview</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">Data Science</div>
    <div class="quarto-category">Machine Learning</div>
    <div class="quarto-category">Artificial Intelligence</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Rafiq Islam </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 7, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p style="text-align: justify">
Random Forest is one of the most popular machine learning algorithms, known for its simplicity, versatility, and ability to perform both classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputs the mode of the classes (for classification) or the mean prediction (for regression) of the individual trees.
</p>
</section>
<section id="what-is-random-forest" class="level2">
<h2 class="anchored" data-anchor-id="what-is-random-forest">What is Random Forest?</h2>
<p style="text-align: justify">
Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to obtain a more accurate and stable result. Each tree is built using a different random subset of the data, and at each node, a random subset of features is considered when splitting the data.
</p>
<ul>
<li><strong>Classification:</strong> The final output is determined by majority voting from all the decision trees</li>
<li><strong>Regression:</strong> The output is the average of all tree predictions.</li>
</ul>
</section>
<section id="mathematics-behind-random-forest" class="level2">
<h2 class="anchored" data-anchor-id="mathematics-behind-random-forest">Mathematics Behind Random Forest</h2>
<p>To understand Random Forest, we first need to recap how a decision tree works and then explore how Random Forest extends this idea.</p>
<section id="decision-tree-recap" class="level3">
<h3 class="anchored" data-anchor-id="decision-tree-recap"><a href="../../dsandml/decisiontree/index.html" target="_blank" style="text-decoration:none">Decision Tree Recap</a></h3>
<p>A decision tree is a tree-structured model where each internal node represents a “test” on an attribute (e.g., whether the feature value is above or below a threshold), each branch represents the outcome of the test, and each leaf node represents a class label (classification) or a value (regression).</p>
<ul>
<li>For <strong>classification</strong>, the goal is to partition the data such that the class labels in each partition are as homogeneous as possible.<br>
</li>
<li>For <strong>regression</strong>, the goal is to minimize the variance of the predicted values.</li>
</ul>
<p>Mathematically, the decision tree makes decisions by minimizing the <strong>Gini Index</strong> or <strong>Entropy</strong> for classification tasks and minimizing the <strong>Mean Squared Error (MSE)</strong> for regression tasks.</p>
</section>
<section id="random-forest-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-algorithm">Random Forest Algorithm</h3>
<p>Random Forest enhances decision trees by employing two key concepts:</p>
<ul>
<li><strong>Random Sampling (Bootstrap Sampling):</strong> From the training set of size <span class="math inline">\(N\)</span>, randomly draw <span class="math inline">\(N\)</span> samples with replacement.<br>
</li>
<li><strong>Feature Subsampling:</strong> At each node of the decision tree, a random subset of the features is selected, and the best split is chosen only from these features.</li>
</ul>
<p>The process for building a Random Forest can be summarized as follows:</p>
<ol type="1">
<li>Draw <span class="math inline">\(B\)</span> bootstrap samples from the original dataset.</li>
<li>For each bootstrap sample, grow an unpruned decision tree using a random subset of features at each node.</li>
<li>For <strong>classification</strong>, combine the predictions of all the trees by majority voting.</li>
<li>For <strong>regression</strong>, combine the predictions by averaging the outputs of all trees.</li>
</ol>
</section>
<section id="random-forest-for-classification" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-for-classification">Random Forest for Classification</h3>
<p>For classification tasks, Random Forest works by constructing multiple decision trees, each built on a different subset of the data and a random subset of the features.</p>
<p>Given a dataset <span class="math inline">\(D = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}\)</span>, where <span class="math inline">\(x_i\)</span> is a feature vector and <span class="math inline">\(y_i\)</span> is the class label, Random Forest generates <span class="math inline">\(B\)</span> decision trees <span class="math inline">\(T_1, T_2, ..., T_B\)</span>.</p>
<p>For each test point <span class="math inline">\(x\)</span>, each tree <span class="math inline">\(T_b\)</span> gives a class prediction: <span class="math display">\[
\hat{y}_b(x) = T_b(x)
\]</span> The final prediction is determined by majority voting: <span class="math display">\[
\hat{y}(x) = \text{argmax}_k \sum_{b=1}^{B} I(\hat{y}_b(x) = k)
\]</span> where <span class="math inline">\(I(\cdot)\)</span> is an indicator function that equals 1 if the condition is true and 0 otherwise.</p>
<hr>
</section>
<section id="random-forest-for-regression" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-for-regression">Random Forest for Regression</h3>
<p>In regression tasks, Random Forest builds trees that predict continuous values and averages the results.</p>
<p>Given a dataset <span class="math inline">\(D = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}\)</span>, where <span class="math inline">\(x_i\)</span> is a feature vector and <span class="math inline">\(y_i\)</span> is the continuous target variable, Random Forest generates <span class="math inline">\(B\)</span> decision trees <span class="math inline">\(T_1, T_2, ..., T_B\)</span>.</p>
<p>For each test point <span class="math inline">\(x\)</span>, each tree <span class="math inline">\(T_b\)</span> gives a predicted value: <span class="math display">\[
\hat{y}_b(x) = T_b(x)
\]</span> The final prediction is the average of all the tree predictions: <span class="math display">\[
\hat{y}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{y}_b(x)
\]</span></p>
<hr>
</section>
</section>
<section id="assumptions-of-random-forest" class="level2">
<h2 class="anchored" data-anchor-id="assumptions-of-random-forest">Assumptions of Random Forest</h2>
<p>Random Forest makes few assumptions about the data, making it highly flexible. Some assumptions include:</p>
<ul>
<li><strong>Independent Features:</strong> While Random Forest does not explicitly assume that features are independent, correlated features can reduce its performance slightly.<br>
</li>
<li><strong>Noisy Data:</strong> Random Forest is robust to noise due to its ensemble nature.<br>
</li>
<li><strong>Non-linearity:</strong> Random Forest can handle non-linear relationships between features and the target.</li>
</ul>
</section>
<section id="advantages-of-random-forest" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-random-forest">Advantages of Random Forest</h2>
<ul>
<li><strong>Reduction of Overfitting:</strong> Random Forest reduces overfitting by averaging the predictions of multiple trees.</li>
<li><strong>Handles Missing Data:</strong> It can handle missing values by assigning them to the most frequent class (classification) or mean value (regression).</li>
<li><strong>Robust to Noise:</strong> It is relatively resistant to outliers and noise due to its ensemble nature.</li>
<li><strong>Works with Categorical &amp; Continuous Variables:</strong> Random Forest can handle both categorical and continuous data types.</li>
<li><strong>Feature Importance:</strong> It provides an estimate of feature importance, allowing for better interpretability of models.</li>
</ul>
</section>
<section id="disadvantages-of-random-forest" class="level2">
<h2 class="anchored" data-anchor-id="disadvantages-of-random-forest">Disadvantages of Random Forest</h2>
<ul>
<li><strong>Complexity:</strong> The algorithm is computationally intensive, especially with a large number of trees.</li>
<li><strong>Interpretability:</strong> While decision trees are interpretable, Random Forest is a “black-box” model where it’s hard to understand individual predictions.</li>
<li><strong>Memory Usage:</strong> Random Forest can require more memory to store multiple decision trees.</li>
<li><strong>Bias in Imbalanced Data:</strong> For classification tasks with imbalanced data, Random Forest may be biased toward the majority class.</li>
</ul>
<hr>
</section>
<section id="python-implementation" class="level2">
<h2 class="anchored" data-anchor-id="python-implementation">Python Implementation</h2>
<p>Here is a Python code example of how to implement Random Forest for both classification and regression using <code>scikit-learn</code>.</p>
<div id="244dc847" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier, RandomForestRegressor</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, mean_squared_error</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Classification Example: Iris dataset</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> iris.data, iris.target</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize RandomForest Classifier</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict and evaluate</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.predict(X_test)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Classification Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression Example: Boston Housing dataset</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>data_url <span class="op">=</span> <span class="st">"http://lib.stat.cmu.edu/datasets/boston"</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>raw_df <span class="op">=</span> pd.read_csv(data_url, sep<span class="op">=</span><span class="st">"\s+"</span>, skiprows<span class="op">=</span><span class="dv">22</span>, header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.hstack([raw_df.values[::<span class="dv">2</span>, :], raw_df.values[<span class="dv">1</span>::<span class="dv">2</span>, :<span class="dv">2</span>]])</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> raw_df.values[<span class="dv">1</span>::<span class="dv">2</span>, <span class="dv">2</span>]</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> target</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize RandomForest Regressor</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>reg.fit(X_train, y_train)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict and evaluate</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> reg.predict(X_test)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y_test, y_pred)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Regression Mean Squared Error: </span><span class="sc">{</span>mse<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Classification Accuracy: 1.0
Regression Mean Squared Error: 9.619662013157892</code></pre>
</div>
</div>
</section>
<section id="hyperparameter-tuning-for-random-forest" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-tuning-for-random-forest">Hyperparameter Tuning for Random Forest</h2>
<p>Tuning the hyperparameters of a Random Forest can significantly improve its performance. Here are some important hyperparameters to consider:</p>
<section id="important-hyperparameters" class="level3">
<h3 class="anchored" data-anchor-id="important-hyperparameters">Important Hyperparameters</h3>
<ul>
<li><strong><code>n_estimators</code>:</strong> This is the number of trees in the forest. Increasing this number usually improves performance but also increases computational cost.
<ul>
<li><strong>Tip:</strong> Start with a default value of 100 and increase as needed.<br>
</li>
</ul></li>
<li><strong><code>max_depth</code>:</strong> The maximum depth of each tree. Deeper trees can model more complex relationships, but they also increase the risk of overfitting.
<ul>
<li><strong>Tip:</strong> Use cross-validation to find the optimal depth that balances bias and variance<br>
</li>
</ul></li>
<li><strong><code>min_samples_split</code>:</strong> The minimum number of samples required to split an internal node. Higher values prevent the tree from becoming too specific (overfitting).
<ul>
<li><strong>Tip:</strong> Use higher values (e.g., 5 or 10) to reduce overfitting in noisy datasets.</li>
</ul></li>
<li><strong><code>min_samples_leaf</code>:</strong> The minimum number of samples required to be at a leaf node. Larger leaf sizes reduce model complexity and can help generalization.</li>
<li><strong><code>max_features</code>:</strong> The number of features to consider when looking for the best split. Randomly selecting fewer features can reduce correlation between trees and improve generalization.
<ul>
<li><strong>Tip:</strong> For classification, a common choice is <code>sqrt(number_of_features)</code>. For regression, <code>max_features = number_of_features / 3</code> is often effective.</li>
</ul></li>
<li><strong><code>bootstrap</code>:</strong> Whether to use bootstrap samples when building trees. Set this to <code>True</code> for Random Forest (default) or <code>False</code> for extremely randomized trees (also known as ExtraTrees).</li>
</ul>
</section>
<section id="grid-search-for-hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="grid-search-for-hyperparameter-tuning">Grid Search for Hyperparameter Tuning</h3>
<p>To fine-tune the hyperparameters of a Random Forest, we can use <strong>GridSearchCV</strong> or <strong>RandomizedSearchCV</strong> in <code>scikit-learn</code>. Here’s an example of how to use <code>GridSearchCV</code> for tuning a Random Forest Classifier:</p>
<div id="a5dd86e2" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>],</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="va">None</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>],</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>],</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'sqrt'</span>, <span class="st">'log2'</span>, <span class="va">None</span>]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> iris.data, iris.target</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize Random Forest Classifier</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform grid search</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>clf, param_grid<span class="op">=</span>param_grid, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train, y_train)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Best parameters from grid search</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Hyperparameters:"</span>, grid_search.best_params_)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate with best parameters</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> best_model.predict(X_test)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy with Best Parameters: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best Hyperparameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}
Accuracy with Best Parameters: 1.0</code></pre>
</div>
</div>
<p>Using this technique, we can find the combination of hyperparameters that yields the best model performance.</p>
</section>
</section>
<section id="feature-importance-in-random-forest" class="level2">
<h2 class="anchored" data-anchor-id="feature-importance-in-random-forest">Feature Importance in Random Forest</h2>
<p>One of the appealing aspects of Random Forest is that it provides a measure of <strong>feature importance</strong>, which indicates how much each feature contributes to the model’s predictions.</p>
<section id="computing-feature-importance" class="level3">
<h3 class="anchored" data-anchor-id="computing-feature-importance">Computing Feature Importance</h3>
<p>In Random Forest, feature importance is computed by measuring the <strong>average reduction in impurity</strong> (e.g., Gini impurity or MSE) brought by each feature across all trees. Features that lead to larger reductions are considered more important.</p>
<div id="89b3265a" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train,y_train)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get feature importance from the RandomForest model</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> clf.feature_importances_</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.argsort(importances)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the feature importance</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Feature Importance"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(X.shape[<span class="dv">1</span>]), importances[indices], align<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(X.shape[<span class="dv">1</span>]), iris.feature_names, rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-4-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="index_files/figure-html/cell-4-output-1.png" width="950" height="565" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The bar chart is showing the relative importance of each feature, making it easier to understand which features have the most predictive power.</p>
</section>
</section>
<section id="out-of-bag-oob-error-estimate" class="level2">
<h2 class="anchored" data-anchor-id="out-of-bag-oob-error-estimate">Out-of-Bag (OOB) Error Estimate</h2>
<p>Random Forest uses <strong>Out-of-Bag (OOB)</strong> samples as an alternative to cross-validation. Since each tree is trained on a bootstrap sample, about one-third of the data is left out in each iteration. These “out-of-bag” samples can be used to estimate the model’s performance without the need for a separate validation set.</p>
<section id="enabling-oob-in-python" class="level3">
<h3 class="anchored" data-anchor-id="enabling-oob-in-python">Enabling OOB in Python</h3>
<p>You can enable the out-of-bag error estimate by setting <code>oob_score=True</code> in the <code>RandomForestClassifier</code> or <code>RandomForestRegressor</code>.</p>
<div id="20c711fe" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, oob_score<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Access the OOB score</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"OOB Score: </span><span class="sc">{</span>clf<span class="sc">.</span>oob_score_<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OOB Score: 0.9428571428571428</code></pre>
</div>
</div>
<p>The OOB score is an unbiased estimate of the model’s performance, which is particularly useful when the dataset is small and splitting it further into training/validation sets might reduce training effectiveness.</p>
</section>
</section>
<section id="dealing-with-imbalanced-data" class="level2">
<h2 class="anchored" data-anchor-id="dealing-with-imbalanced-data">Dealing with Imbalanced Data</h2>
<p>For imbalanced classification tasks (where one class is much more frequent than the others), Random Forest may be biased toward predicting the majority class. Several techniques can help mitigate this issue:</p>
<ul>
<li><strong>Class Weights:</strong> You can assign higher weights to the minority class to force the model to pay more attention to it.</li>
</ul>
<div id="8259b391" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestClassifier(class_weight<span class="op">=</span><span class="st">'balanced'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</a></style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomForestClassifier(class_weight='balanced', random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;RandomForestClassifier<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html">?<span>Documentation for RandomForestClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>RandomForestClassifier(class_weight='balanced', random_state=42)</pre></div> </div></div></div></div>
</div>
</div>
<ul>
<li><strong>Resampling:</strong> You can either oversample the minority class or undersample the majority class.</li>
</ul>
<div id="9597558d" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>sm <span class="op">=</span> SMOTE(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>X_resampled, y_resampled <span class="op">=</span> sm.fit_resample(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="random-forest-in-practice-best-practices" class="level2">
<h2 class="anchored" data-anchor-id="random-forest-in-practice-best-practices">Random Forest in Practice: Best Practices</h2>
<ul>
<li><strong>Cross-Validation:</strong> Always perform cross-validation to ensure the model generalizes well</li>
<li><strong>Parallelization:</strong> Random Forest naturally supports parallelization. If using <code>scikit-learn</code>, set <code>n_jobs=-1</code> to utilize all CPU cores for training.<br>
</li>
<li><strong>Ensemble Methods:</strong> For better results, you can combine Random Forest with other ensemble methods, such as boosting (e.g., XGBoost or Gradient Boosting) to further improve performance.</li>
</ul>
<p>Random Forest is a highly flexible, non-parametric machine learning algorithm that can be used for both classification and regression tasks. Its ensemble-based approach reduces overfitting, improves predictive performance, and provides valuable insights like feature importance. Despite its many advantages, Random Forest is computationally intensive and may not be the best choice for real-time applications or datasets with extremely high dimensionality.</p>
<hr>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li>Breiman, L. (2001). “Random Forests”. Machine Learning, 45(1), 5-32.</li>
<li>Pedregosa, F., et al.&nbsp;(2011). “Scikit-learn: Machine Learning in Python”. Journal of Machine Learning Research, 12, 2825-2830.</li>
<li>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). “The Elements of Statistical Learning”. Springer Series in Statistics.</li>
</ol>



<!-- -->

</section>

<div class="quarto-listing quarto-listing-container-grid" id="listing-listing">
<div class="list grid quarto-listing-cols-3">
<div class="g-col-1" data-index="0" data-categories="Statistics,Data Science,Data Engineering,Machine Learning,Artificial Intelligence" data-listing-date-sort="1728518400000" data-listing-file-modified-sort="1729199021940" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="11" data-listing-word-count-sort="2195">
<a href="../../dsandml/naivebayes/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top"><img src="../lda/lda.png" style="height: 150px;"  class="thumbnail-image card-img"/></p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Classification using Naive Bayes algorithm
</h5>
<div class="listing-reading-time card-text text-muted">
11 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Thursday, October 10, 2024
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="1" data-categories="Data Science,Machine Learning,Artificial Intelligence,Data Engineering" data-listing-date-sort="1729123200000" data-listing-file-modified-sort="1729199021940" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="27" data-listing-word-count-sort="5383">
<a href="../../dsandml/lda/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../dsandml/lda/lda.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Classification: Linear Discriminant Analysis (LDA)
</h5>
<div class="listing-reading-time card-text text-muted">
27 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Thursday, October 17, 2024
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="2" data-categories="Data Science,Machine Learning,Artificial Intelligence" data-listing-date-sort="1728259200000" data-listing-file-modified-sort="1729199021940" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="9" data-listing-word-count-sort="1737">
<a href="../../dsandml/logreg/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../dsandml/logreg/logreg.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code
</h5>
<div class="listing-reading-time card-text text-muted">
9 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Monday, October 7, 2024
</div>
</div>
</div>
</div>
</a>
</div>
</div>
<div class="listing-no-matching d-none">
No matching items
</div>
</div><a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{islam2024,
  author = {Islam, Rafiq},
  title = {Ensemble {Methods:} {Random} {Forest} - {A} Detailed
    Overview},
  date = {2024-10-07},
  url = {https://mrislambd.github.io/dsandml/randomforest/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-islam2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Islam, Rafiq. 2024. <span>“Ensemble Methods: Random Forest - A Detailed
Overview.”</span> October 7, 2024. <a href="https://mrislambd.github.io/dsandml/randomforest/">https://mrislambd.github.io/dsandml/randomforest/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mrislambd\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb10" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Ensemble Methods: Random Forest - A detailed overview"</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-10-07"</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> Rafiq Islam</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Data Science, Machine Learning, Artificial Intelligence]</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span><span class="co"> true</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="an">search:</span><span class="co"> true</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> gb.webp</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="an">lightbox:</span><span class="co"> true</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="an">listing:</span><span class="co"> </span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">    contents: "/../../dsandml"</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">    max-items: 3</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">    type: grid</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co">    categories: true</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co">    date-format: full</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co">    fields: [image, date, title, author, reading-time]</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>Random Forest is one of the most popular machine learning algorithms, known for its simplicity, versatility, and ability to perform both classification and regression tasks. It operates by constructing a multitude of decision trees during training and outputs the mode of the classes (for classification) or the mean prediction (for regression) of the individual trees.</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>&lt;/p&gt;</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## What is Random Forest?</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to obtain a more accurate and stable result. Each tree is built using a different random subset of the data, and at each node, a random subset of features is considered when splitting the data.&lt;/p&gt; </span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classification:** The final output is determined by majority voting from all the decision trees </span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Regression:** The output is the average of all tree predictions.</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematics Behind Random Forest</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>To understand Random Forest, we first need to recap how a decision tree works and then explore how Random Forest extends this idea.</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="fu">### [Decision Tree Recap](/dsandml/decisiontree/index.qmd){target="_blank" style="text-decoration:none"}</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>A decision tree is a tree-structured model where each internal node represents a "test" on an attribute (e.g., whether the feature value is above or below a threshold), each branch represents the outcome of the test, and each leaf node represents a class label (classification) or a value (regression).  </span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For **classification**, the goal is to partition the data such that the class labels in each partition are as homogeneous as possible.  </span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For **regression**, the goal is to minimize the variance of the predicted values.</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>Mathematically, the decision tree makes decisions by minimizing the **Gini Index** or **Entropy** for classification tasks and minimizing the **Mean Squared Error (MSE)** for regression tasks.</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Forest Algorithm</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>Random Forest enhances decision trees by employing two key concepts:  </span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random Sampling (Bootstrap Sampling):** From the training set of size $N$, randomly draw $N$ samples with replacement.  </span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feature Subsampling:** At each node of the decision tree, a random subset of the features is selected, and the best split is chosen only from these features.</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>The process for building a Random Forest can be summarized as follows:</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Draw $B$ bootstrap samples from the original dataset.</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>For each bootstrap sample, grow an unpruned decision tree using a random subset of features at each node.</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>For **classification**, combine the predictions of all the trees by majority voting.</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>For **regression**, combine the predictions by averaging the outputs of all trees.</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Forest for Classification</span></span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>For classification tasks, Random Forest works by constructing multiple decision trees, each built on a different subset of the data and a random subset of the features.</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>Given a dataset $D = <span class="sc">\{</span>(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)<span class="sc">\}</span>$, where $x_i$ is a feature vector and $y_i$ is the class label, Random Forest generates $B$ decision trees $T_1, T_2, ..., T_B$.</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>For each test point $x$, each tree $T_b$ gives a class prediction:</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>\hat{y}_b(x) = T_b(x)</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>The final prediction is determined by majority voting:</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>\hat{y}(x) = \text{argmax}_k \sum_{b=1}^{B} I(\hat{y}_b(x) = k)</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>where $I(\cdot)$ is an indicator function that equals 1 if the condition is true and 0 otherwise.</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Forest for Regression</span></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>In regression tasks, Random Forest builds trees that predict continuous values and averages the results.</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>Given a dataset $D = <span class="sc">\{</span>(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)<span class="sc">\}</span>$, where $x_i$ is a feature vector and $y_i$ is the continuous target variable, Random Forest generates $B$ decision trees $T_1, T_2, ..., T_B$.</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>For each test point $x$, each tree $T_b$ gives a predicted value:</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>\hat{y}_b(x) = T_b(x)</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>The final prediction is the average of all the tree predictions:</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>\hat{y}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{y}_b(x)</span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a><span class="fu">## Assumptions of Random Forest</span></span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a>Random Forest makes few assumptions about the data, making it highly flexible. Some assumptions include:  </span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Independent Features:** While Random Forest does not explicitly assume that features are independent, correlated features can reduce its performance slightly.  </span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Noisy Data:** Random Forest is robust to noise due to its ensemble nature.  </span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Non-linearity:** Random Forest can handle non-linear relationships between features and the target.</span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advantages of Random Forest</span></span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Reduction of Overfitting:** Random Forest reduces overfitting by averaging the predictions of multiple trees.</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Handles Missing Data:** It can handle missing values by assigning them to the most frequent class (classification) or mean value (regression).</span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Robust to Noise:** It is relatively resistant to outliers and noise due to its ensemble nature.</span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Works with Categorical &amp; Continuous Variables:** Random Forest can handle both categorical and continuous data types.</span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feature Importance:** It provides an estimate of feature importance, allowing for better interpretability of models.</span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a><span class="fu">## Disadvantages of Random Forest</span></span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Complexity:** The algorithm is computationally intensive, especially with a large number of trees.</span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Interpretability:** While decision trees are interpretable, Random Forest is a "black-box" model where it's hard to understand individual predictions.</span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Memory Usage:** Random Forest can require more memory to store multiple decision trees.</span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bias in Imbalanced Data:** For classification tasks with imbalanced data, Random Forest may be biased toward the majority class.</span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python Implementation</span></span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a>Here is a Python code example of how to implement Random Forest for both classification and regression using <span class="in">`scikit-learn`</span>.</span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier, RandomForestRegressor</span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, mean_squared_error</span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a><span class="co"># Classification Example: Iris dataset</span></span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> iris.data, iris.target</span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize RandomForest Classifier</span></span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict and evaluate</span></span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> clf.predict(X_test)</span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Classification Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression Example: Boston Housing dataset</span></span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a>data_url <span class="op">=</span> <span class="st">"http://lib.stat.cmu.edu/datasets/boston"</span></span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a>raw_df <span class="op">=</span> pd.read_csv(data_url, sep<span class="op">=</span><span class="st">"\s+"</span>, skiprows<span class="op">=</span><span class="dv">22</span>, header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.hstack([raw_df.values[::<span class="dv">2</span>, :], raw_df.values[<span class="dv">1</span>::<span class="dv">2</span>, :<span class="dv">2</span>]])</span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> raw_df.values[<span class="dv">1</span>::<span class="dv">2</span>, <span class="dv">2</span>]</span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data</span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> target</span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize RandomForest Regressor</span></span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a>reg <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a>reg.fit(X_train, y_train)</span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict and evaluate</span></span>
<span id="cb10-169"><a href="#cb10-169" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> reg.predict(X_test)</span>
<span id="cb10-170"><a href="#cb10-170" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y_test, y_pred)</span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Regression Mean Squared Error: </span><span class="sc">{</span>mse<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-173"><a href="#cb10-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-174"><a href="#cb10-174" aria-hidden="true" tabindex="-1"></a><span class="fu">## Hyperparameter Tuning for Random Forest</span></span>
<span id="cb10-175"><a href="#cb10-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-176"><a href="#cb10-176" aria-hidden="true" tabindex="-1"></a>Tuning the hyperparameters of a Random Forest can significantly improve its performance. Here are some important hyperparameters to consider:</span>
<span id="cb10-177"><a href="#cb10-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-178"><a href="#cb10-178" aria-hidden="true" tabindex="-1"></a><span class="fu">### Important Hyperparameters  </span></span>
<span id="cb10-179"><a href="#cb10-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-180"><a href="#cb10-180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**`n_estimators`:** This is the number of trees in the forest. Increasing this number usually improves performance but also increases computational cost.  </span>
<span id="cb10-181"><a href="#cb10-181" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Tip:** Start with a default value of 100 and increase as needed.  </span>
<span id="cb10-182"><a href="#cb10-182" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**`max_depth`:** The maximum depth of each tree. Deeper trees can model more complex relationships, but they also increase the risk of overfitting.  </span>
<span id="cb10-183"><a href="#cb10-183" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Tip:** Use cross-validation to find the optimal depth that balances bias and variance  </span>
<span id="cb10-184"><a href="#cb10-184" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**`min_samples_split`:** The minimum number of samples required to split an internal node. Higher values prevent the tree from becoming too specific (overfitting).</span>
<span id="cb10-185"><a href="#cb10-185" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Tip:** Use higher values (e.g., 5 or 10) to reduce overfitting in noisy datasets.</span>
<span id="cb10-186"><a href="#cb10-186" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**`min_samples_leaf`:** The minimum number of samples required to be at a leaf node. Larger leaf sizes reduce model complexity and can help generalization.</span>
<span id="cb10-187"><a href="#cb10-187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**`max_features`:** The number of features to consider when looking for the best split. Randomly selecting fewer features can reduce correlation between trees and improve generalization.</span>
<span id="cb10-188"><a href="#cb10-188" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Tip:** For classification, a common choice is <span class="in">`sqrt(number_of_features)`</span>. For regression, <span class="in">`max_features = number_of_features / 3`</span> is often effective.</span>
<span id="cb10-189"><a href="#cb10-189" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**`bootstrap`:** Whether to use bootstrap samples when building trees. Set this to <span class="in">`True`</span> for Random Forest (default) or <span class="in">`False`</span> for extremely randomized trees (also known as ExtraTrees).</span>
<span id="cb10-190"><a href="#cb10-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-191"><a href="#cb10-191" aria-hidden="true" tabindex="-1"></a><span class="fu">### Grid Search for Hyperparameter Tuning</span></span>
<span id="cb10-192"><a href="#cb10-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-193"><a href="#cb10-193" aria-hidden="true" tabindex="-1"></a>To fine-tune the hyperparameters of a Random Forest, we can use **GridSearchCV** or **RandomizedSearchCV** in <span class="in">`scikit-learn`</span>. Here's an example of how to use <span class="in">`GridSearchCV`</span> for tuning a Random Forest Classifier:</span>
<span id="cb10-194"><a href="#cb10-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-197"><a href="#cb10-197" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-198"><a href="#cb10-198" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb10-199"><a href="#cb10-199" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span>
<span id="cb10-200"><a href="#cb10-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-201"><a href="#cb10-201" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb10-202"><a href="#cb10-202" aria-hidden="true" tabindex="-1"></a>    <span class="st">'n_estimators'</span>: [<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>],</span>
<span id="cb10-203"><a href="#cb10-203" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_depth'</span>: [<span class="va">None</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>],</span>
<span id="cb10-204"><a href="#cb10-204" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_split'</span>: [<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>],</span>
<span id="cb10-205"><a href="#cb10-205" aria-hidden="true" tabindex="-1"></a>    <span class="st">'min_samples_leaf'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>],</span>
<span id="cb10-206"><a href="#cb10-206" aria-hidden="true" tabindex="-1"></a>    <span class="st">'max_features'</span>: [<span class="st">'sqrt'</span>, <span class="st">'log2'</span>, <span class="va">None</span>]</span>
<span id="cb10-207"><a href="#cb10-207" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-208"><a href="#cb10-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-209"><a href="#cb10-209" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb10-210"><a href="#cb10-210" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> iris.data, iris.target</span>
<span id="cb10-211"><a href="#cb10-211" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-212"><a href="#cb10-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-213"><a href="#cb10-213" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize Random Forest Classifier</span></span>
<span id="cb10-214"><a href="#cb10-214" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-215"><a href="#cb10-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-216"><a href="#cb10-216" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform grid search</span></span>
<span id="cb10-217"><a href="#cb10-217" aria-hidden="true" tabindex="-1"></a>grid_search <span class="op">=</span> GridSearchCV(estimator<span class="op">=</span>clf, param_grid<span class="op">=</span>param_grid, cv<span class="op">=</span><span class="dv">5</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-218"><a href="#cb10-218" aria-hidden="true" tabindex="-1"></a>grid_search.fit(X_train, y_train)</span>
<span id="cb10-219"><a href="#cb10-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-220"><a href="#cb10-220" aria-hidden="true" tabindex="-1"></a><span class="co"># Best parameters from grid search</span></span>
<span id="cb10-221"><a href="#cb10-221" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best Hyperparameters:"</span>, grid_search.best_params_)</span>
<span id="cb10-222"><a href="#cb10-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-223"><a href="#cb10-223" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate with best parameters</span></span>
<span id="cb10-224"><a href="#cb10-224" aria-hidden="true" tabindex="-1"></a>best_model <span class="op">=</span> grid_search.best_estimator_</span>
<span id="cb10-225"><a href="#cb10-225" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> best_model.predict(X_test)</span>
<span id="cb10-226"><a href="#cb10-226" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb10-227"><a href="#cb10-227" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy with Best Parameters: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-228"><a href="#cb10-228" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-229"><a href="#cb10-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-230"><a href="#cb10-230" aria-hidden="true" tabindex="-1"></a>Using this technique, we can find the combination of hyperparameters that yields the best model performance.</span>
<span id="cb10-231"><a href="#cb10-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-232"><a href="#cb10-232" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Importance in Random Forest</span></span>
<span id="cb10-233"><a href="#cb10-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-234"><a href="#cb10-234" aria-hidden="true" tabindex="-1"></a>One of the appealing aspects of Random Forest is that it provides a measure of **feature importance**, which indicates how much each feature contributes to the model’s predictions.</span>
<span id="cb10-235"><a href="#cb10-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-236"><a href="#cb10-236" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computing Feature Importance</span></span>
<span id="cb10-237"><a href="#cb10-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-238"><a href="#cb10-238" aria-hidden="true" tabindex="-1"></a>In Random Forest, feature importance is computed by measuring the **average reduction in impurity** (e.g., Gini impurity or MSE) brought by each feature across all trees. Features that lead to larger reductions are considered more important.</span>
<span id="cb10-239"><a href="#cb10-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-242"><a href="#cb10-242" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-243"><a href="#cb10-243" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb10-244"><a href="#cb10-244" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-245"><a href="#cb10-245" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-246"><a href="#cb10-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-247"><a href="#cb10-247" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train,y_train)</span>
<span id="cb10-248"><a href="#cb10-248" aria-hidden="true" tabindex="-1"></a><span class="co"># Get feature importance from the RandomForest model</span></span>
<span id="cb10-249"><a href="#cb10-249" aria-hidden="true" tabindex="-1"></a>importances <span class="op">=</span> clf.feature_importances_</span>
<span id="cb10-250"><a href="#cb10-250" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.argsort(importances)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb10-251"><a href="#cb10-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-252"><a href="#cb10-252" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the feature importance</span></span>
<span id="cb10-253"><a href="#cb10-253" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb10-254"><a href="#cb10-254" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Feature Importance"</span>)</span>
<span id="cb10-255"><a href="#cb10-255" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(X.shape[<span class="dv">1</span>]), importances[indices], align<span class="op">=</span><span class="st">"center"</span>)</span>
<span id="cb10-256"><a href="#cb10-256" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(X.shape[<span class="dv">1</span>]), iris.feature_names, rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb10-257"><a href="#cb10-257" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-258"><a href="#cb10-258" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-259"><a href="#cb10-259" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-260"><a href="#cb10-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-261"><a href="#cb10-261" aria-hidden="true" tabindex="-1"></a>The bar chart is showing the relative importance of each feature, making it easier to understand which features have the most predictive power.</span>
<span id="cb10-262"><a href="#cb10-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-263"><a href="#cb10-263" aria-hidden="true" tabindex="-1"></a><span class="fu">## Out-of-Bag (OOB) Error Estimate</span></span>
<span id="cb10-264"><a href="#cb10-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-265"><a href="#cb10-265" aria-hidden="true" tabindex="-1"></a>Random Forest uses **Out-of-Bag (OOB)** samples as an alternative to cross-validation. Since each tree is trained on a bootstrap sample, about one-third of the data is left out in each iteration. These "out-of-bag" samples can be used to estimate the model’s performance without the need for a separate validation set.</span>
<span id="cb10-266"><a href="#cb10-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-267"><a href="#cb10-267" aria-hidden="true" tabindex="-1"></a><span class="fu">### Enabling OOB in Python</span></span>
<span id="cb10-268"><a href="#cb10-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-269"><a href="#cb10-269" aria-hidden="true" tabindex="-1"></a>You can enable the out-of-bag error estimate by setting <span class="in">`oob_score=True`</span> in the <span class="in">`RandomForestClassifier`</span> or <span class="in">`RandomForestRegressor`</span>.</span>
<span id="cb10-270"><a href="#cb10-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-273"><a href="#cb10-273" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-274"><a href="#cb10-274" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb10-275"><a href="#cb10-275" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">100</span>, oob_score<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-276"><a href="#cb10-276" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb10-277"><a href="#cb10-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-278"><a href="#cb10-278" aria-hidden="true" tabindex="-1"></a><span class="co"># Access the OOB score</span></span>
<span id="cb10-279"><a href="#cb10-279" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"OOB Score: </span><span class="sc">{</span>clf<span class="sc">.</span>oob_score_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-280"><a href="#cb10-280" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-281"><a href="#cb10-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-282"><a href="#cb10-282" aria-hidden="true" tabindex="-1"></a>The OOB score is an unbiased estimate of the model’s performance, which is particularly useful when the dataset is small and splitting it further into training/validation sets might reduce training effectiveness.</span>
<span id="cb10-283"><a href="#cb10-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-284"><a href="#cb10-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-285"><a href="#cb10-285" aria-hidden="true" tabindex="-1"></a><span class="fu">## Dealing with Imbalanced Data</span></span>
<span id="cb10-286"><a href="#cb10-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-287"><a href="#cb10-287" aria-hidden="true" tabindex="-1"></a>For imbalanced classification tasks (where one class is much more frequent than the others), Random Forest may be biased toward predicting the majority class. Several techniques can help mitigate this issue:  </span>
<span id="cb10-288"><a href="#cb10-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-289"><a href="#cb10-289" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Class Weights:** You can assign higher weights to the minority class to force the model to pay more attention to it.</span>
<span id="cb10-290"><a href="#cb10-290" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-293"><a href="#cb10-293" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-294"><a href="#cb10-294" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb10-295"><a href="#cb10-295" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> RandomForestClassifier(class_weight<span class="op">=</span><span class="st">'balanced'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-296"><a href="#cb10-296" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb10-297"><a href="#cb10-297" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-298"><a href="#cb10-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-299"><a href="#cb10-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Resampling:** You can either oversample the minority class or undersample the majority class.</span>
<span id="cb10-300"><a href="#cb10-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-303"><a href="#cb10-303" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-304"><a href="#cb10-304" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb10-305"><a href="#cb10-305" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb10-306"><a href="#cb10-306" aria-hidden="true" tabindex="-1"></a>sm <span class="op">=</span> SMOTE(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-307"><a href="#cb10-307" aria-hidden="true" tabindex="-1"></a>X_resampled, y_resampled <span class="op">=</span> sm.fit_resample(X_train, y_train)</span>
<span id="cb10-308"><a href="#cb10-308" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-309"><a href="#cb10-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-310"><a href="#cb10-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-311"><a href="#cb10-311" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Forest in Practice: Best Practices  </span></span>
<span id="cb10-312"><a href="#cb10-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-313"><a href="#cb10-313" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cross-Validation:** Always perform cross-validation to ensure the model generalizes well </span>
<span id="cb10-314"><a href="#cb10-314" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Parallelization:** Random Forest naturally supports parallelization. If using <span class="in">`scikit-learn`</span>, set <span class="in">`n_jobs=-1`</span> to utilize all CPU cores for training.  </span>
<span id="cb10-315"><a href="#cb10-315" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ensemble Methods:** For better results, you can combine Random Forest with other ensemble methods, such as boosting (e.g., XGBoost or Gradient Boosting) to further improve performance.</span>
<span id="cb10-316"><a href="#cb10-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-317"><a href="#cb10-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-318"><a href="#cb10-318" aria-hidden="true" tabindex="-1"></a>Random Forest is a highly flexible, non-parametric machine learning algorithm that can be used for both classification and regression tasks. Its ensemble-based approach reduces overfitting, improves predictive performance, and provides valuable insights like feature importance. Despite its many advantages, Random Forest is computationally intensive and may not be the best choice for real-time applications or datasets with extremely high dimensionality.</span>
<span id="cb10-319"><a href="#cb10-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-320"><a href="#cb10-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-321"><a href="#cb10-321" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb10-322"><a href="#cb10-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-323"><a href="#cb10-323" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb10-324"><a href="#cb10-324" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Breiman, L. (2001). "Random Forests". Machine Learning, 45(1), 5-32.</span>
<span id="cb10-325"><a href="#cb10-325" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Pedregosa, F., et al. (2011). "Scikit-learn: Machine Learning in Python". Journal of Machine Learning Research, 12, 2825-2830.</span>
<span id="cb10-326"><a href="#cb10-326" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). "The Elements of Statistical Learning". Springer Series in Statistics.</span>
<span id="cb10-327"><a href="#cb10-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-328"><a href="#cb10-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-329"><a href="#cb10-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-330"><a href="#cb10-330" aria-hidden="true" tabindex="-1"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Powered by <a href="https://quarto.org/">Quarto</a> 1.5.57</p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2024 @ Rafiq Islam
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../license.txt">
<p>License</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/rafiqr35" target="_blank">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://youtube.com/@quanttube" target="_blank">
      <i class="bi bi-youtube" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:rafiqfsu@gmail.com?subject&amp;body" target="_blank">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"descPosition":"bottom","closeEffect":"zoom","loop":false,"selector":".lightbox","openEffect":"zoom"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>