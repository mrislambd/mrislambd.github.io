---
title: "Simple Linear Regression"
date: "2024-08-29"
author: Rafiq Islam
categories: [Data Science, Machine Learning, Artificial Intelligence]
citation: true
search: true
lightbox: true
image: slr.png
listing: 
    contents: "/../../dsandml"
    max-items: 3
    type: grid
    categories: false
    date-format: full
    fields: [image, date, title, author, reading-time]
---  
 

# Simple Linear Regression  

<p style="text-align: justify">
A simple linear regression in multiple predictors/input variables/features/independent variables/ explanatory variables/regressors/ covariates (many names) often takes the form  
</p>

 $$
 y=f(\mathbf{x})+\epsilon =\mathbf{\beta}\mathbf{x}+\epsilon
 $$  

<p style="text-align: justify">
    where $\mathbf{\beta} \in \mathbb{R}^d$ are regression parameters or constant values that we aim to estimate and $\epsilon \sim \mathcal{N}(0,1)$ is a normally distributed error term independent of $x$ or also called the white noise.  
</p> 

In this case, the model:

$$
y=f(x)+\epsilon=\beta_0+\beta_1 x+\epsilon
$$

<p style="text-align: justify">
Therefore, in our model we need to estimate the parameters $\beta_0,\beta_1$. 
The true relationship between the explanatory variables and the dependent variable is $y=f(x)$. But our model is $y=f(x)+\epsilon$. Here, this $f(x)$ is the working model with the data. In other words, $\hat{y}=f(x)=\hat{\beta}_0+\hat{\beta}_1 x$. Therefore, there should be some error in the model prediction which we are calling $\epsilon=\|y-\hat{y}\|$ where $y$ is the true value and $\hat{y}$ is the predicted value. This error term is normally distributed with mean 0 and variance 1. To get the best estimate of the parameters $\beta_0,\beta_1$ we can minimize the error term as much as possible. So, we define the residual sum of squares (RSS) as:
</p>


\begin{align}
RSS &=\epsilon_1^2+\epsilon_2^2+\cdots+\epsilon_{10}^2\\
&= \sum_{i=1}^{10}(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)^2\\
\hat{\mathcal{l}}(\bar{\beta})&=\sum_{i=1}^{10}(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)^2\\
\end{align}

  

Using multivariate calculus we see  


\begin{align}
    \frac{\partial l}{\partial \beta_0}&=\sum_{i=1}^{10} 2(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)(-1)\\
    \frac{\partial l}{\partial \beta_1}&= \sum_{i=1}^{10} 2(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)(-x_i)
\end{align}
  

Setting the partial derivatives to zero we solve for $\hat{\beta_0},\hat{\beta_1}$ as follows 


\begin{align*}
    \frac{\partial l}{\partial \beta_0}&=0\\
    \implies \sum_{i=1}^{10} y_i-10 \hat{\beta_0}-\hat{\beta_1}\left(\sum_{i=1}^{10} x_i\right)&=0\\
    \implies \hat{\beta_0}&=\bar{y}-\hat{\beta_1}\bar{x}
\end{align*}
  

and, 


\begin{align*}
    \frac{\partial l}{\partial \beta_1}&=0\\
    \implies \sum_{i=1}^{10} 2(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)(-x_i)&=0\\
    \implies \sum_{i=1}^{10} (y_i-\hat{\beta}_0-\hat{\beta}_1 x_i)(x_i)&=0\\
    \implies \sum_{i=1}^{10} x_iy_i-\hat{\beta_0}\left(\sum_{i=1}^{10} x_i\right)-\hat{\beta_1}\left(\sum_{i=1}^{10} x_i^2\right)&=0\\
    \implies \sum_{i=1}^{10} x_iy_i-\left(\bar{y}-\hat{\beta_1}\bar{x}\right)\left(\sum_{i=1}^{10} x_i\right)-\hat{\beta_1}\left(\sum_{i=1}^{10} x_i^2\right)&=0\\
    \implies \sum_{i=1}^{10} x_iy_i-\bar{y}\left(\sum_{i=1}^{10} x_i\right)+\hat{\beta_1}\bar{x}\left(\sum_{i=1}^{10} x_i\right)-\hat{\beta_1}\left(\sum_{i=1}^{10} x_i^2\right)&=0\\
    \implies \sum_{i=1}^{10} x_iy_i-\bar{y}\left(\sum_{i=1}^{10} x_i\right) -\hat{\beta_1}\left(\sum_{i=1}^{10}x_i^2-\bar{x}\sum_{i=1}^{10}x_i\right)&=0\\
    \implies \sum_{i=1}^{10} x_iy_i-\bar{y}\left(\sum_{i=1}^{10} x_i\right) -\hat{\beta_1}\left(\sum_{i=1}^{10}x_i^2-10\bar{x}^2\right)&=0\\
    \implies \sum_{i=1}^{10} x_iy_i-\bar{y}\left(\sum_{i=1}^{10} x_i\right) -\hat{\beta_1}\left(\sum_{i=1}^{10}x_i^2-2\times 10\times \bar{x}^2+10\bar{x}^2\right)&=0\\
    \implies \hat{\beta_1}&=\frac{\sum_{i=1}^{10} x_iy_i-10\bar{x}\bar{y}}{\sum_{i=1}^{10}x_i^2-2\times 10\times \bar{x}^2+10\bar{x}^2}\\
    \implies \hat{\beta_1}&=\frac{\sum_{i=1}^{10} x_iy_i -10\bar{x}\bar{y}-10\bar{x}\bar{y}+10\bar{x}\bar{y}}{\sum_{i=1}^{10}x_i^2-2\bar{x}\times 10\times\frac{1}{10}\sum_{i=1}^{10}x_i +10\bar{x}^2}\\
    \implies \hat{\beta_1}&=\frac{\sum_{i=1}^{10} x_iy_i-\bar{y}\left(\sum_{i=1}^{10} x_i\right)-\bar{x}\left(\sum_{i=1}^{10} y_i\right)+10\bar{x}\bar{y}}{\sum_{i=1}^{10}(x_i-\bar{x})^2}\\
    \implies \hat{\beta_1}&=\frac{\sum_{i=1}^{10}\left(x_iy_i-x_i\bar{y}-\bar{x}y_i+\bar{x}\bar{y}\right)}{\sum_{i=1}^{10}(x_i-\bar{x})^2}\\
    \implies \hat{\beta_1}&=\frac{\sum_{i=1}^{10}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{10}(x_i-\bar{x})^2}\\
\end{align*}

  
Therefore, we have the following

\begin{align*}
     \hat{\beta_0}&=\bar{y}-\hat{\beta_1}\bar{x}\\
     \hat{\beta_1}&=\frac{\sum_{i=1}^{10}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{10}(x_i-\bar{x})^2}
\end{align*}

Simple Linear Regression `slr` is applicable for a single feature data set with contineous response variable.  

```{python}
#| code-fold: false
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.linear_model import LinearRegression
```  

## Synthetic Data  

To implement the algorithm, we need some synthetic data. To generate the synthetic data we use the linear equation $y(x)=2x+\frac{1}{2}+\xi$ where $\xi\sim \mathbf{N}(0,1)$  

```{python}
#| code-fold: false
X=np.random.random(100)
y=2*X+0.5+np.random.randn(100)
```  

Note that we used two random number generators, `np.random.random(n)` and `np.random.randn(n)`. The first one generates $n$ random numbers of values from the range (0,1) and the second one generates values from the standard normal distribution with mean 0 and variance or standard deviation 1.  

```{python}
#| code-fold: false
plt.figure(figsize=(9,6))
plt.scatter(X,y)
plt.xlabel('$X$')
plt.ylabel('y')
plt.gca().set_facecolor('#f4f4f4') 
plt.gcf().patch.set_facecolor('#f4f4f4')
plt.show()
```  

## Model
We want to fit a simple linear regression to the above data.  

```{python}
#| code-fold: false
slr=LinearRegression()
```  

Now to fit our data $X$ and $y$ we need to reshape the input variable. Because if we look at $X$,  

```{python}
#| code-fold: false
#| code-overflow: scroll
X
```  

It is a one-dimensional array/vector but the `slr` object accepts input variable as matrix or two-dimensional format.  

```{python}
#| code-fold: false
#| code-overflow: wrap
X=X.reshape(-1,1)
X[:10]
```  

Now we fit the data to our model  
```{python}
#| code-fold: false
slr.fit(X,y)
slr.predict([[2],[3]])
```  

We have our $X=2,3$ and the corresponding $y$ values are from the above cell output, which are pretty close to the model $y=2x+\frac{1}{2}$. 

```{python}
#| code-fold: false
intercept = round(slr.intercept_,4)
slope = slr.coef_
```  

Now our model parameters are: intercept $\beta_0=$ `{python} intercept` and slope $\beta_1=$ `{python} slope`.  

```{python}
#| code-fold: false
plt.figure(figsize=(9,6))
plt.scatter(X,y, alpha=0.7,label="Sample Data")
plt.plot(np.linspace(0,1,100),
    slr.predict(np.linspace(0,1,100).reshape(-1,1)),
    'k',
    label='Model $\hat{f}$'
)
plt.plot(np.linspace(0,1,100),
    2*np.linspace(0,1,100)+0.5,
    'r--',
    label='$f$'
)
plt.xlabel('$X$')
plt.ylabel('y')
plt.legend(fontsize=10)
plt.gca().set_facecolor('#f4f4f4') 
plt.gcf().patch.set_facecolor('#f4f4f4')
plt.show()
```  

So the model fits the data almost perfectly.  

Up next [multiple linear regression](/dsandml/multiplelinreg/index.qmd).  





**Share on**  

<div id="fb-root"></div>
<script async defer crossorigin="anonymous"
 src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0"></script>
 
<div class="share-buttons">
<div class="fb-share-button" data-href="https://mrislambd.github.io/machinelearning/simplelinreg/"
data-layout="button_count" data-size="small"><a target="_blank" 
 href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fmrislambd.github.io%2Fmachinelearning%2Fsimplelinreg%2F&amp;src=sdkpreparse" 
 class="fb-xfbml-parse-ignore">Share</a></div>

<script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
<script type="IN/Share" data-url="https://mrislambd.github.io/machinelearning/simplelinreg/"></script> 
 
<a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" 
 data-url="https://mrislambd.github.io/machinelearning/simplelinreg/" data-show-count="true">Tweet</a>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<div class="fb-comments" data-href="https://mrislambd.github.io/machinelearning/simplelinreg/"
 data-width="" data-numposts="5"></div>



**You may also like**