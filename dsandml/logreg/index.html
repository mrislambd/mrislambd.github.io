<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rafiq Islam">
<meta name="dcterms.date" content="2024-10-07">

<title>Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code – Mohammad Rafiqul Islam</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../..//_assets/images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-listing/list.min.js"></script>
<script src="../../site_libs/quarto-listing/quarto-listing.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>

  window.document.addEventListener("DOMContentLoaded", function (_event) {
    const listingTargetEl = window.document.querySelector('#listing-listing .list');
    if (!listingTargetEl) {
      // No listing discovered, do not attach.
      return; 
    }

    const options = {
      valueNames: ['listing-image','listing-date','listing-title','listing-author','listing-reading-time',{ data: ['index'] },{ data: ['categories'] },{ data: ['listing-date-sort'] },{ data: ['listing-file-modified-sort'] }],
      page: 18,
    pagination: { item: "<li class='page-item'><a class='page page-link' href='#'></a></li>" },
      searchColumns: ["listing-title","listing-author","listing-date","listing-image","listing-description","listing-categories"],
    };

    window['quarto-listings'] = window['quarto-listings'] || {};
    window['quarto-listings']['listing-listing'] = new List('listing-listing', options);

    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  });

  window.addEventListener('hashchange',() => {
    if (window['quarto-listing-loaded']) {
      window['quarto-listing-loaded']();
    }
  })
  </script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Z5NP67GHFC"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Z5NP67GHFC', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6878992848042528" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code – Mohammad Rafiqul Islam">
<meta property="og:description" content="">
<meta property="og:image" content="https://mrislambd.github.io/dsandml/logreg/logreg.png">
<meta property="og:site_name" content="Mohammad Rafiqul Islam">
<meta property="og:image:height" content="464">
<meta property="og:image:width" content="581">
<meta name="twitter:title" content="Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code – Mohammad Rafiqul Islam">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://mrislambd.github.io/dsandml/logreg/logreg.png">
<meta name="twitter:image-height" content="464">
<meta name="twitter:image-width" content="581">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../dsandml/logreg/index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../_assets/images/fsu-logo.png" alt="Florida State University" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../dsandml/logreg/index.html">
    <span class="navbar-title">Mohammad Rafiqul Islam</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html"> 
<span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../portfolio.html"> 
<span class="menu-text">Portfolio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cv.html"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/mohammad-rafiqul-islam/" target="_blank"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mrislambd" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#what-is-logistic-regression" id="toc-what-is-logistic-regression" class="nav-link" data-scroll-target="#what-is-logistic-regression">What is Logistic Regression?</a></li>
  <li><a href="#the-sigmoid-function" id="toc-the-sigmoid-function" class="nav-link" data-scroll-target="#the-sigmoid-function">The Sigmoid Function</a></li>
  <li><a href="#logistic-regression-model" id="toc-logistic-regression-model" class="nav-link" data-scroll-target="#logistic-regression-model">Logistic Regression Model</a></li>
  <li><a href="#cost-function-for-logistic-regression" id="toc-cost-function-for-logistic-regression" class="nav-link" data-scroll-target="#cost-function-for-logistic-regression">Cost Function for Logistic Regression</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient Descent</a></li>
  <li><a href="#python-code-implementation-from-scratch" id="toc-python-code-implementation-from-scratch" class="nav-link" data-scroll-target="#python-code-implementation-from-scratch">Python Code Implementation from Scratch</a></li>
  </ul>
</nav>
    <h5 class="quarto-listing-category-title">Categories</h5><div class="quarto-listing-category category-default"><div class="category" data-category="">All <span class="quarto-category-count">(85)</span></div><div class="category" data-category="Algorithms">Algorithms <span class="quarto-category-count">(1)</span></div><div class="category" data-category="Artificial Intelligence">Artificial Intelligence <span class="quarto-category-count">(12)</span></div><div class="category" data-category="Data Engineering">Data Engineering <span class="quarto-category-count">(6)</span></div><div class="category" data-category="Data Science">Data Science <span class="quarto-category-count">(12)</span></div><div class="category" data-category="Machine Learning">Machine Learning <span class="quarto-category-count">(12)</span></div><div class="category" data-category="Statistics">Statistics <span class="quarto-category-count">(1)</span></div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">Data Science</div>
    <div class="quarto-category">Machine Learning</div>
    <div class="quarto-category">Artificial Intelligence</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Rafiq Islam </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 7, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p style="text-align: justify">
Logistic Regression is a popular classification algorithm used for binary and multi-class classification problems. Unlike Linear Regression, which is used for regression problems, Logistic Regression is used to predict categorical outcomes. In binary classification, the output is either 0 or 1, and the relationship between the input features and the outcome is modeled using a logistic function (also called the sigmoid function).
</p>
</section>
<section id="what-is-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="what-is-logistic-regression">What is Logistic Regression?</h2>
<p style="text-align: justify">
Logistic Regression is a type of regression analysis used when the dependent variable is categorical. In binary logistic regression, the output can have only two possible outcomes (e.g., 0 or 1, pass or fail, spam or not spam). <br> Logistic Regression works by modeling the probability of an event occurring based on one or more input features. It estimates the probability that a given input belongs to a particular category (0 or 1) using the <strong>logistic function (sigmoid function)</strong>.
</p>
</section>
<section id="the-sigmoid-function" class="level2">
<h2 class="anchored" data-anchor-id="the-sigmoid-function">The Sigmoid Function</h2>
<p>The sigmoid function maps any real-valued number to a value between 0 and 1, making it ideal for modeling probabilities.</p>
<p>The sigmoid function is given by the formula:</p>
<p><span class="math display">\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(z\)</span> is the input to the sigmoid function (in logistic regression, <span class="math inline">\(z = \mathbf{x} \cdot \theta\)</span>)</li>
<li><span class="math inline">\(e\)</span> is the base of the natural logarithm</li>
</ul>
<p>The output of the sigmoid function is interpreted as the probability <span class="math inline">\(P(y=1|X)\)</span>.</p>
</section>
<section id="logistic-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-model">Logistic Regression Model</h2>
<p>In Logistic Regression, the hypothesis is modeled as:</p>
<p><span class="math display">\[
h_\theta(X) = \frac{1}{1 + e^{-\theta^T X}}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(X\)</span> is the input feature vector</li>
<li><span class="math inline">\(\theta\)</span> is the parameter vector (weights)</li>
</ul>
</section>
<section id="cost-function-for-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="cost-function-for-logistic-regression">Cost Function for Logistic Regression</h2>
<p style="text-align: justify">
Unlike Linear Regression, which uses the Mean Squared Error (MSE) as the cost function, Logistic Regression uses <strong>log loss</strong> or <strong>binary cross-entropy</strong> as the cost function, as the output is binary (0 or 1).
</p>
<p>So, basically we model probability from the given data. In other words, we can write</p>
<p><span class="math display">\[\begin{align*}
\mathbb{P}(y= 1 \text{ or }0 |\text{ given }X)&amp;=p(\mathbf{x})=\sigma(\mathbf{x}\cdot\theta)=\frac{1}{1+e^{-\mathbf{x}\cdot \theta}}\\
\implies p_{\theta}(\mathbf{x})&amp; = \frac{1}{1+e^{-(\theta_0+\theta_1x_1+\cdots+\theta_dx_d)}}\\
\implies p_{\theta}(\mathbf{x})&amp; = \begin{cases}
                                p_{\theta}(\mathbf{x}) &amp; \text{ if } y=1\\
                                1-p_{\theta}(\mathbf{x}) &amp; \text{ if } y=0
                          \end{cases}
\end{align*}\]</span></p>
<p>Where, <span class="math inline">\(\mathbf{\theta},\mathbf{x}\in \mathbb{R}^{d+1}\)</span> and <span class="math inline">\(d\)</span> is the dimension of the data. For single data vector <span class="math inline">\(\mathbf{x}\)</span> the binary cross-entropy function can be written as</p>
<p><span class="math display">\[
l(\theta) = yp_{\theta}(\mathbf{x})+ (1-y)(1-p_{\theta}(\mathbf{x}))
\]</span></p>
<p>Since we have <span class="math inline">\(n\)</span> of those i.i.d data vectors therefore, we can write</p>
<p><span class="math display">\[
L(\theta) = \prod_{i=1}^{n} \left(y_ip_{\theta}(\mathbf{x_i})+ (1-y_i)(1-p_{\theta}(\mathbf{x_i}))\right)
\]</span></p>
<p>Since our goal is to minimize the loss, we need to perform derivatives of the loss function. Therefore, to change from the product form to addition form we take negative log of the above expression</p>
<p><span class="math display">\[\begin{align*}
\ell (\theta) = -\log{L(\theta)} = -\sum_{i=1}^{n}y_i\log{p_{\theta}(\mathbf{x})}+(1-y_i)\log{(1-p_{\theta}(\mathbf{x}))}
\end{align*}\]</span></p>
<p>For the ease of calculation, let’s rewrite the above equation in terms of <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> where <span class="math inline">\(m\in \mathbb{R}^d = (\theta_1,\theta_2,\cdots,\theta_d)^T\)</span> and <span class="math inline">\(b\in \mathbb{R}\)</span>.</p>
<p><span class="math display">\[
\ell (\theta) = -\sum_{i=1}^{n}y_i\log{p_{m,b}(\mathbf{x})}+(1-y_i)\log{(1-p_{m,b}(\mathbf{x}))}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(n\)</span> is the number of training examples<br>
</li>
<li><span class="math inline">\(m\)</span> is the number of features</li>
<li><span class="math inline">\(y^{(i)}\)</span> is the true label of the <span class="math inline">\(i^{th}\)</span> example</li>
<li><span class="math inline">\(b\)</span> is the bias for the <span class="math inline">\(i^{th}\)</span> example</li>
</ul>
</section>
<section id="gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h2>
<p>To minimize the cost function and find the optimal values for <span class="math inline">\(\theta\)</span>, we use <strong>gradient descent</strong>. We start from the last form of the loss function and convert this to a form that is easy to take the partial dervivatives.</p>
<p><span class="math display">\[\begin{align*}
\ell (\theta) &amp;= -\sum_{i=1}^{n}y_i\log{p_{m,b}(\mathbf{x})}+(1-y_i)\log{(1-p_{m,b}(\mathbf{x}))}\\
              &amp;= -\sum_{i=1}^{n}y_i\log{(\sigma(mx_i+b))}+(1-y_i)\log{(1-\sigma(mx_i+b))}\\
              &amp;= -\sum_{i=1}^{n}y_i\log{(\sigma(mx_i+b))}+(1-y_i)\log{(\sigma(-(mx_i+b)))};\hspace{3mm}\text{ Since } 1-\sigma(x)=\sigma(-x)\\
              &amp;= -\sum_{i=1}^{n}y_i\left[\log{(\sigma(mx_i+b))}-\log{(\sigma(-(mx_i+b)))}\right]+\log{(-\sigma(mx_i+b))}\\
              &amp;= -\sum_{i=1}^{n}y_i\log{\left(\frac{\sigma(mx_i+b)}{\sigma(-(mx_i+b))}\right)}+\log{(-\sigma(mx_i+b))}\\
              &amp;= -\sum_{i=1}^{n}y_i(mx_i+b)+\log{(\sigma(-(mx_i+b)))};\hspace{3mm}\text{ Since }\frac{\sigma(x)}{-\sigma(x)}=e^x\\
\end{align*}\]</span></p>
<p>Now we again use the beautiful features of the sigmoid function<br>
<span class="math display">\[\begin{align*}
\frac{d\sigma(x)}{dx}&amp;=\frac{d}{dx}\left(\frac{1}{1+e^{-x}}\right)=\frac{e^{-x}}{\left(1+e^{-x}\right)^2}=\frac{1}{1+e^{-x}}\cdot \frac{e^{-x}}{1+e^{-x}}\\
&amp;=\sigma(x)\left(1-\frac{1}{1+e^{-x}}\right)=\sigma(x)(1-\sigma(x))\\
&amp;=\sigma(x)\sigma(-x)
\end{align*}\]</span></p>
<p>Finally, we are ready to take the partial derivatives of the loss function with respect to <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span>,</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial \ell}{\partial m} &amp;= -\sum_{i=1}^{n}y_ix_i+\frac{1}{\sigma(-(mx_i+b))}\frac{d}{dx}(\sigma(-(mx_i+b)))\\
&amp; =-\sum_{i=1}^{n}y_ix_i+\frac{1}{\sigma(-(mx_i+b))}\sigma(-(mx_i+b))\sigma(mx_i+b)(-x_i)\\
&amp; = -\sum_{i=1}^{n} x_i(y_i-\sigma(mx_i+b))\\
&amp; = \sum_{i=1}^{n}x_i(p_{m,b}(x_i)-y_i)=\sum_{i=1}^{n} x_i(\hat{y_i}-y_i)\\
&amp; = \mathbf{x_i}\cdot(\mathbf{\hat{y_i}}-\mathbf{y_i})\\
\text{ and } &amp; \\
&amp; \\
\frac{\partial \ell}{\partial b} &amp; = -\sum_{i=1}^{n} y_i +\frac{1}{\sigma(-(mx_i+b))}\frac{d}{dx}(\sigma(-(mx_i+b)))\\
&amp; =  -\sum_{i=1}^{n} y_i - \frac{1}{\sigma(-(mx_i+b))}\sigma(-(mx_i+b))\sigma(mx_i+b)\\
&amp; = \sum_{i=1}^{n} p_{m,b}(x_i)-y_i= \sum_{i=1}^{n} \hat{y}_i-y_i\\
&amp; = \hat{\mathbf{y}}_i-\mathbf{y}_i
\end{align*}\]</span></p>
<p>Using this gradient, we update the parameter vector <span class="math inline">\(\theta\)</span> iteratively:</p>
<p><span class="math display">\[
\theta_{j+1} := \theta_j - \alpha \nabla \ell (\theta_j)
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> is the learning rate</li>
<li><span class="math inline">\(\nabla \ell (\theta_j)\)</span> is the partial derivative of the cost function with respect to <span class="math inline">\(\theta_j\)</span> and <span class="math display">\[
\nabla \ell (\theta) = \begin{bmatrix}\sum_{i=1}^{n} \hat{y}_i-y_i \\
\sum_{i=1}^{n} x_i(\hat{y_i}-y_i) \end{bmatrix}  =\begin{bmatrix}\hat{\mathbf{y}}_i-\mathbf{y}_i \\
\mathbf{x_i}\cdot(\mathbf{\hat{y_i}}-\mathbf{y_i}) \end{bmatrix}= X^T(\hat{\mathbf{y}}_i-\mathbf{y}_i)=X^T(\sigma(X\vec{\theta})-\vec{y})
\]</span></li>
</ul>
<hr>
</section>
<section id="python-code-implementation-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="python-code-implementation-from-scratch">Python Code Implementation from Scratch</h2>
<p>Here’s how to implement Logistic Regression from scratch in Python. We will use two different forms for our class</p>
<div id="579d6959" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression1:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate <span class="op">=</span> <span class="fl">0.1</span>, n_iterations <span class="op">=</span> <span class="dv">1000</span>):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">        Hyper Parameters</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">        - learning_rate: learning rate; float; default 0.01</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        - n_itearations: number of iterations; int; default 1000</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Model Parameters</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        - weights: weights of the features; float or int</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        - bias: bias of the model; float or int</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_iterations <span class="op">=</span> n_iterations </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> <span class="va">None</span> </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _sigmoid(<span class="va">self</span>, x):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X,y):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">        n_sample = number of samples in the data set: the value n</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co">        n_features = number of features or the dimension of the data set: the value d</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        n_sample,n_features <span class="op">=</span> X.shape</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.zeros(n_features) </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_iterations):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>            linear <span class="op">=</span> np.dot(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> <span class="va">self</span>._sigmoid(linear)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>            dw <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n_sample)<span class="op">*</span> np.dot(X.T,(pred<span class="op">-</span>y))</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>            db <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n_sample) <span class="op">*</span> np.<span class="bu">sum</span>(pred<span class="op">-</span>y)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.weights <span class="op">=</span> <span class="va">self</span>.weights <span class="op">-</span> <span class="va">self</span>.learning_rate <span class="op">*</span> dw </span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.bias <span class="op">=</span> <span class="va">self</span>.bias <span class="op">-</span> <span class="va">self</span>.learning_rate <span class="op">*</span> db</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        linear <span class="op">=</span> np.dot(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        predicted_y <span class="op">=</span> <span class="va">self</span>._sigmoid(linear)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        class_of_y <span class="op">=</span> [<span class="dv">0</span> <span class="cf">if</span> y<span class="op">&lt;=</span><span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">1</span> <span class="cf">for</span> y <span class="kw">in</span> predicted_y]</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> class_of_y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let’s use this using the <code>scikit-learn</code> breast cancer data set.</p>
<div id="cf2dd822" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>b_cancer <span class="op">=</span> load_breast_cancer()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>X,y <span class="op">=</span> b_cancer.data, b_cancer.target</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X,y, random_state<span class="op">=</span><span class="dv">123</span>, stratify<span class="op">=</span>y, test_size<span class="op">=</span><span class="fl">0.30</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>clf1 <span class="op">=</span> LogisticRegression1()</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>clf1.fit(X_train, y_train)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>predicted_y <span class="op">=</span> clf1.predict(X_test)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">round</span>(accuracy_score(predicted_y, y_test),<span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.85</code></pre>
</div>
</div>



<!-- -->

</section>

<div class="quarto-listing quarto-listing-container-grid" id="listing-listing">
<div class="list grid quarto-listing-cols-3">
<div class="g-col-1" data-index="0" data-categories="Data Science,Machine Learning,Artificial Intelligence,Data Engineering" data-listing-date-sort="1723593600000" data-listing-file-modified-sort="1728424537808" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="7" data-listing-word-count-sort="1261">
<a href="../../dsandml/datacollection/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../dsandml/datacollection/ws.jpg" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Data collection through Webscraping
</h5>
<div class="listing-reading-time card-text text-muted">
7 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Wednesday, August 14, 2024
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="1" data-categories="Data Science,Machine Learning,Artificial Intelligence,Data Engineering" data-listing-date-sort="1727136000000" data-listing-file-modified-sort="1728424537816" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="8" data-listing-word-count-sort="1429">
<a href="../../dsandml/pca/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../dsandml/pca/pca.png" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Dimensionality Reduction: Principle Component Analysis (PCA)
</h5>
<div class="listing-reading-time card-text text-muted">
8 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Tuesday, September 24, 2024
</div>
</div>
</div>
</div>
</a>
</div>
<div class="g-col-1" data-index="2" data-categories="Data Science,Machine Learning,Artificial Intelligence" data-listing-date-sort="1728259200000" data-listing-file-modified-sort="1728424537812" data-listing-date-modified-sort="NaN" data-listing-reading-time-sort="6" data-listing-word-count-sort="1051">
<a href="../../dsandml/gradientboosting/index.html" class="quarto-grid-link">
<div class="quarto-grid-item card h-100 card-left">
<p class="card-img-top">
<img loading="lazy" src="../../dsandml/gradientboosting/gb.webp" class="thumbnail-image card-img" style="height: 150px;">
</p>
<div class="card-body post-contents">
<h5 class="no-anchor card-title listing-title">
Ensemble Methods: Gradient Boosting - A detailed overview
</h5>
<div class="listing-reading-time card-text text-muted">
6 min
</div>
<div class="card-attribution card-text-small justify">
<div class="listing-author">
Rafiq Islam
</div>
<div class="listing-date">
Monday, October 7, 2024
</div>
</div>
</div>
</div>
</a>
</div>
</div>
<div class="listing-no-matching d-none">
No matching items
</div>
</div><a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{islam2024,
  author = {Islam, Rafiq},
  title = {Classification: {Logistic} {Regression} - {A} {Comprehensive}
    {Guide} with {Mathematical} {Derivation} and {Python} {Code}},
  date = {2024-10-07},
  url = {https://mrislambd.github.io/dsandml/logreg/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-islam2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Islam, Rafiq. 2024. <span>“Classification: Logistic Regression - A
Comprehensive Guide with Mathematical Derivation and Python
Code.”</span> October 7, 2024. <a href="https://mrislambd.github.io/dsandml/logreg/">https://mrislambd.github.io/dsandml/logreg/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mrislambd\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Classification: Logistic Regression - A Comprehensive Guide with Mathematical Derivation and Python Code"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-10-07"</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> Rafiq Islam</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Data Science, Machine Learning, Artificial Intelligence]</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span><span class="co"> true</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="an">search:</span><span class="co"> true</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> logreg.png</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="an">lightbox:</span><span class="co"> true</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="an">listing:</span><span class="co"> </span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">    contents: "/../../dsandml"</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">    max-items: 3</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">    type: grid</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">    categories: true</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">    date-format: full</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">    fields: [image, date, title, author, reading-time]</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span>  </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>Logistic Regression is a popular classification algorithm used for binary and multi-class classification problems. Unlike Linear Regression, which is used for regression problems, Logistic Regression is used to predict categorical outcomes. In binary classification, the output is either 0 or 1, and the relationship between the input features and the outcome is modeled using a logistic function (also called the sigmoid function).</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>&lt;/p&gt;</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="fu">## What is Logistic Regression?  </span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>&lt;p style="text-align: justify"&gt;</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>Logistic Regression is a type of regression analysis used when the dependent variable is categorical. In binary logistic regression, the output can have only two possible outcomes (e.g., 0 or 1, pass or fail, spam or not spam).</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>&lt;br&gt;</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>Logistic Regression works by modeling the probability of an event occurring based on one or more input features. It estimates the probability that a given input belongs to a particular category (0 or 1) using the **logistic function (sigmoid function)**.</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>&lt;/p&gt;</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Sigmoid Function</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>The sigmoid function maps any real-valued number to a value between 0 and 1, making it ideal for modeling probabilities.</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>The sigmoid function is given by the formula:</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>\sigma(z) = \frac{1}{1 + e^{-z}}</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>Where:  </span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$z$ is the input to the sigmoid function (in logistic regression, $z = \mathbf{x} \cdot \theta$)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$e$ is the base of the natural logarithm</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>The output of the sigmoid function is interpreted as the probability $P(y=1|X)$.</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a><span class="fu">## Logistic Regression Model</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>In Logistic Regression, the hypothesis is modeled as:</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>h_\theta(X) = \frac{1}{1 + e^{-\theta^T X}}</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>Where:  </span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X$ is the input feature vector</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta$ is the parameter vector (weights)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a><span class="fu">## Cost Function for Logistic Regression</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>&lt;p style = "text-align: justify"&gt;</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>Unlike Linear Regression, which uses the Mean Squared Error (MSE) as the cost function, Logistic Regression uses **log loss** or **binary cross-entropy** as the cost function, as the output is binary (0 or 1).</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>&lt;/p&gt;  </span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>So, basically we model probability from the given data. In other words, we can write </span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>\mathbb{P}(y= 1 \text{ or }0 |\text{ given }X)&amp;=p(\mathbf{x})=\sigma(\mathbf{x}\cdot\theta)=\frac{1}{1+e^{-\mathbf{x}\cdot \theta}}<span class="sc">\\</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>\implies p_{\theta}(\mathbf{x})&amp; = \frac{1}{1+e^{-(\theta_0+\theta_1x_1+\cdots+\theta_dx_d)}}<span class="sc">\\</span></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>\implies p_{\theta}(\mathbf{x})&amp; = \begin{cases} </span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>                                p_{\theta}(\mathbf{x}) &amp; \text{ if } y=1<span class="sc">\\</span></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>                                1-p_{\theta}(\mathbf{x}) &amp; \text{ if } y=0</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>                          \end{cases}</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>\end{align*}  </span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>Where, $\mathbf{\theta},\mathbf{x}\in \mathbb{R}^{d+1}$ and $d$ is the dimension of the data. For single data vector $\mathbf{x}$ the binary cross-entropy function can be written as  </span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>l(\theta) = yp_{\theta}(\mathbf{x})+ (1-y)(1-p_{\theta}(\mathbf{x}))</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>Since we have $n$ of those i.i.d data vectors therefore, we can write </span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>L(\theta) = \prod_{i=1}^{n} \left(y_ip_{\theta}(\mathbf{x_i})+ (1-y_i)(1-p_{\theta}(\mathbf{x_i}))\right)</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>Since our goal is to minimize the loss, we need to perform derivatives of the loss function. Therefore, to change from the product form to addition form we take negative log of the above expression  </span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>\ell (\theta) = -\log{L(\theta)} = -\sum_{i=1}^{n}y_i\log{p_{\theta}(\mathbf{x})}+(1-y_i)\log{(1-p_{\theta}(\mathbf{x}))}</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>For the ease of calculation, let's rewrite the above equation in terms of $m$ and $b$ where $m\in \mathbb{R}^d = (\theta_1,\theta_2,\cdots,\theta_d)^T$ and $b\in \mathbb{R}$.</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>\ell (\theta) = -\sum_{i=1}^{n}y_i\log{p_{m,b}(\mathbf{x})}+(1-y_i)\log{(1-p_{m,b}(\mathbf{x}))}</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>Where:  </span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$n$ is the number of training examples  </span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$m$ is the number of features</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$y^{(i)}$ is the true label of the $i^{th}$ example</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$b$ is the bias for the $i^{th}$ example</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gradient Descent</span></span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>To minimize the cost function and find the optimal values for $\theta$, we use **gradient descent**. We start from the last form of the loss function and convert this to a form that is easy to take the partial dervivatives.  </span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>\ell (\theta) &amp;= -\sum_{i=1}^{n}y_i\log{p_{m,b}(\mathbf{x})}+(1-y_i)\log{(1-p_{m,b}(\mathbf{x}))}<span class="sc">\\</span></span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>              &amp;= -\sum_{i=1}^{n}y_i\log{(\sigma(mx_i+b))}+(1-y_i)\log{(1-\sigma(mx_i+b))}<span class="sc">\\</span></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>              &amp;= -\sum_{i=1}^{n}y_i\log{(\sigma(mx_i+b))}+(1-y_i)\log{(\sigma(-(mx_i+b)))};\hspace{3mm}\text{ Since } 1-\sigma(x)=\sigma(-x)<span class="sc">\\</span></span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>              &amp;= -\sum_{i=1}^{n}y_i\left<span class="co">[</span><span class="ot">\log{(\sigma(mx_i+b))}-\log{(\sigma(-(mx_i+b)))}\right</span><span class="co">]</span>+\log{(-\sigma(mx_i+b))}<span class="sc">\\</span></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>              &amp;= -\sum_{i=1}^{n}y_i\log{\left(\frac{\sigma(mx_i+b)}{\sigma(-(mx_i+b))}\right)}+\log{(-\sigma(mx_i+b))}<span class="sc">\\</span></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>              &amp;= -\sum_{i=1}^{n}y_i(mx_i+b)+\log{(\sigma(-(mx_i+b)))};\hspace{3mm}\text{ Since }\frac{\sigma(x)}{-\sigma(x)}=e^x<span class="sc">\\</span></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>Now we again use the beautiful features of the sigmoid function  </span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>\frac{d\sigma(x)}{dx}&amp;=\frac{d}{dx}\left(\frac{1}{1+e^{-x}}\right)=\frac{e^{-x}}{\left(1+e^{-x}\right)^2}=\frac{1}{1+e^{-x}}\cdot \frac{e^{-x}}{1+e^{-x}}<span class="sc">\\</span></span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>&amp;=\sigma(x)\left(1-\frac{1}{1+e^{-x}}\right)=\sigma(x)(1-\sigma(x))<span class="sc">\\</span></span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>&amp;=\sigma(x)\sigma(-x)</span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a>\end{align*}  </span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a>Finally, we are ready to take the partial derivatives of the loss function with respect to $m$ and $b$,  </span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ell}{\partial m} &amp;= -\sum_{i=1}^{n}y_ix_i+\frac{1}{\sigma(-(mx_i+b))}\frac{d}{dx}(\sigma(-(mx_i+b)))<span class="sc">\\</span></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a>&amp; =-\sum_{i=1}^{n}y_ix_i+\frac{1}{\sigma(-(mx_i+b))}\sigma(-(mx_i+b))\sigma(mx_i+b)(-x_i)<span class="sc">\\</span></span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>&amp; = -\sum_{i=1}^{n} x_i(y_i-\sigma(mx_i+b))<span class="sc">\\</span></span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a>&amp; = \sum_{i=1}^{n}x_i(p_{m,b}(x_i)-y_i)=\sum_{i=1}^{n} x_i(\hat{y_i}-y_i)<span class="sc">\\</span></span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a>&amp; = \mathbf{x_i}\cdot(\mathbf{\hat{y_i}}-\mathbf{y_i})<span class="sc">\\</span></span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a>\text{ and } &amp; <span class="sc">\\</span></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a>&amp; <span class="sc">\\</span></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>\frac{\partial \ell}{\partial b} &amp; = -\sum_{i=1}^{n} y_i +\frac{1}{\sigma(-(mx_i+b))}\frac{d}{dx}(\sigma(-(mx_i+b)))<span class="sc">\\</span></span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>&amp; =  -\sum_{i=1}^{n} y_i - \frac{1}{\sigma(-(mx_i+b))}\sigma(-(mx_i+b))\sigma(mx_i+b)<span class="sc">\\</span></span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>&amp; = \sum_{i=1}^{n} p_{m,b}(x_i)-y_i= \sum_{i=1}^{n} \hat{y}_i-y_i<span class="sc">\\</span></span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a>&amp; = \hat{\mathbf{y}}_i-\mathbf{y}_i</span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>Using this gradient, we update the parameter vector $\theta$ iteratively:</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a>\theta_{j+1} := \theta_j - \alpha \nabla \ell (\theta_j)</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a>Where:  </span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha$ is the learning rate</span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\nabla \ell (\theta_j)$ is the partial derivative of the cost function with respect to $\theta_j$ and </span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>  \nabla \ell (\theta) = \begin{bmatrix}\sum_{i=1}^{n} \hat{y}_i-y_i <span class="sc">\\</span></span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a>  \sum_{i=1}^{n} x_i(\hat{y_i}-y_i) \end{bmatrix}  =\begin{bmatrix}\hat{\mathbf{y}}_i-\mathbf{y}_i <span class="sc">\\</span></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>  \mathbf{x_i}\cdot(\mathbf{\hat{y_i}}-\mathbf{y_i}) \end{bmatrix}= X^T(\hat{\mathbf{y}}_i-\mathbf{y}_i)=X^T(\sigma(X\vec{\theta})-\vec{y})</span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a><span class="fu">## Python Code Implementation from Scratch</span></span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>Here’s how to implement Logistic Regression from scratch in Python. We will use two different forms for our class  </span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression1:</span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate <span class="op">=</span> <span class="fl">0.1</span>, n_iterations <span class="op">=</span> <span class="dv">1000</span>):</span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a><span class="co">        Hyper Parameters</span></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a><span class="co">        - learning_rate: learning rate; float; default 0.01</span></span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a><span class="co">        - n_itearations: number of iterations; int; default 1000</span></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a><span class="co">        Model Parameters</span></span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a><span class="co">        - weights: weights of the features; float or int</span></span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a><span class="co">        - bias: bias of the model; float or int</span></span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_iterations <span class="op">=</span> n_iterations </span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> <span class="va">None</span> </span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _sigmoid(<span class="va">self</span>, x):</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span>x))</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X,y):</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a><span class="co">        n_sample = number of samples in the data set: the value n</span></span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a><span class="co">        n_features = number of features or the dimension of the data set: the value d</span></span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>        n_sample,n_features <span class="op">=</span> X.shape</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.zeros(n_features) </span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_iterations):</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a>            linear <span class="op">=</span> np.dot(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> <span class="va">self</span>._sigmoid(linear)</span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>            dw <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n_sample)<span class="op">*</span> np.dot(X.T,(pred<span class="op">-</span>y))</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a>            db <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>n_sample) <span class="op">*</span> np.<span class="bu">sum</span>(pred<span class="op">-</span>y)</span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.weights <span class="op">=</span> <span class="va">self</span>.weights <span class="op">-</span> <span class="va">self</span>.learning_rate <span class="op">*</span> dw </span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.bias <span class="op">=</span> <span class="va">self</span>.bias <span class="op">-</span> <span class="va">self</span>.learning_rate <span class="op">*</span> db</span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a>        linear <span class="op">=</span> np.dot(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>        predicted_y <span class="op">=</span> <span class="va">self</span>._sigmoid(linear)</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a>        class_of_y <span class="op">=</span> [<span class="dv">0</span> <span class="cf">if</span> y<span class="op">&lt;=</span><span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">1</span> <span class="cf">for</span> y <span class="kw">in</span> predicted_y]</span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> class_of_y</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a>Now let's use this using the <span class="in">`scikit-learn`</span> breast cancer data set.  </span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a><span class="co">#| warning: false</span></span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_breast_cancer</span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a>b_cancer <span class="op">=</span> load_breast_cancer()</span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a>X,y <span class="op">=</span> b_cancer.data, b_cancer.target</span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X,y, random_state<span class="op">=</span><span class="dv">123</span>, stratify<span class="op">=</span>y, test_size<span class="op">=</span><span class="fl">0.30</span>)</span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a>clf1 <span class="op">=</span> LogisticRegression1()</span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a>clf1.fit(X_train, y_train)</span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a>predicted_y <span class="op">=</span> clf1.predict(X_test)</span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">round</span>(accuracy_score(predicted_y, y_test),<span class="dv">2</span>))</span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a><span class="in">```</span>  </span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Powered by <a href="https://quarto.org/">Quarto</a> 1.5.57</p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2024 @ Rafiq Islam
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../license.txt">
<p>License</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/rafiqr35" target="_blank">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://youtube.com/@quanttube" target="_blank">
      <i class="bi bi-youtube" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:rafiqfsu@gmail.com?subject&amp;body" target="_blank">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>