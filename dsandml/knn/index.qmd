---
title: "K Nearest Neighbors Regression"
date: "2024-08-29"
author: Rafiq Islam
categories: [Data Science, Machine Learning, Artificial Intelligence]
citation: true
image: knn.jpeg
search: true
lightbox: true
listing: 
    contents: "/../../dsandml"
    max-items: 3
    type: grid
    categories: false
    date-format: full
    fields: [image, date, title, author, reading-time]
---  

<p style="text-align: justify">
    Non-parametric model is a statistical model that does not make any assumptions about the underlying data distributions, meaning it does not require specifying functional form for the relationships between variables, instead learning directly from the data points without pre-defined parameters.  
</p>

## $k$NN Regression  

We are given a set of data points $(\bar{x}_i,y_i)$ with $\bar{x}_i\in \mathbb{R}^d$ and $y_i\in \mathbb{R}$  
1. Calculate the distances of the given point $x$ from all the data points  
2. Short the distances in increasing order and select the optimal $k$, the hyperparameter that is used select the nearest distances for the problem at hand. 

This $k$ value controls the fitting of the model. The best value for $k$ is determined from the cross validation and learning curves. Here is the summary regarding the hyperparameter $k$:

- smaller $k$ usually gets low bias but higher variances, which results over fitting.
- larger $k$ usually gets high bias but lower variances, which resutls under fitting.

For classification problems, the class is determined by the vote of its neighbors. For a regression problem the response $y$ is calculated by the weighted average of the sorted $k$-th distances. For example, if $k=4$ and the shorted distances are $d_1<d_3<d_2<d_4$ then $y=(d_1+d_3+d_2+d_4)/2$  

## $k$NN Implementation: Regression

```{python}
#| code-fold: false
import numpy as np 
import matplotlib.pyplot as plt

np.random.seed(123)
X = 2*np.random.normal(size=(100,1))
y = (np.cos(X)+0.3*np.random.normal(size=X.shape)).reshape(-1)
# First 5 entries in X
X[:5]
# First 5 entries in y
y[:5]

# Plot of the data
plt.scatter(X,y)
plt.gca().set_facecolor('#f4f4f4') 
plt.gcf().patch.set_facecolor('#f4f4f4') 
plt.show()
```  

Now we fit two models for two different $k$ values to see how it affects the interpolation.  

```{python}
#| code-fold: false
from sklearn.neighbors import KNeighborsRegressor
knn_3 = KNeighborsRegressor(3)
knn_9 = KNeighborsRegressor(9)

knn_3.fit(X,y)
knn_9.fit(X,y)

fig, ax = plt.subplots(1,2, figsize=(9,5), sharex=True, sharey=True)
ax[0].scatter(X,y, alpha=0.5, label="Sample Data")
ax[1].scatter(X,y, alpha=0.5, label="Sample Data")
ax[0].plot(
    np.linspace(np.min(X)-1,np.max(X)+1,200).reshape(-1,1),
    knn_3.predict(np.linspace(np.min(X)-1,np.max(X)+1,200).reshape(-1,1)),
    'k',
    label='KNR'
)
ax[0].set_title('$k=3$')
ax[0].set_facecolor('#f4f4f4')
ax[0].patch.set_facecolor('#f4f4f4')

ax[1].plot(
    np.linspace(np.min(X)-1,np.max(X)+1,200).reshape(-1,1),
    knn_9.predict(np.linspace(np.min(X)-1,np.max(X)+1,200).reshape(-1,1)),
    'k',
    label='KNR'
)
ax[1].set_title('$k=9$')
ax[1].set_facecolor('#f4f4f4')
ax[1].patch.set_facecolor('#f4f4f4')

plt.show()
```  



## $k$NN Implementation: Classification  

For this classification problem, we choose that famous iris data from the `scikit-learn` library.  

```{python}
#| code-fold: false
from sklearn import datasets


# Load the data
iris = datasets.load_iris()
X,y = iris.data, iris.target

# First 5 entries of the features  
X[:5,:5]
```

```{python}
#| code-fold: false
# First 5 entries of the target values

y[:5]
```

There are total 150 observations of $X$ and $y$ and the scatter plot of the data  

```{python}
#| code-fold: false
# Scatter plot 
_, ax =  plt.subplots()
scatter = ax.scatter(iris.data[:,0],
            iris.data[:,1],
            c=iris.target
        )
ax.set(xlabel=iris.feature_names[0],
       ylabel=iris.feature_names[1]
)
_ = ax.legend(
    scatter.legend_elements()[0],
    iris.target_names, 
    loc="lower right",
    title="Classes"
)
plt.gca().set_facecolor('#f4f4f4') 
plt.gcf().patch.set_facecolor('#f4f4f4') 
```  

We can use the `KNeighborsClassifier` class from the  `sklearn` library or make a custom classifier. 

### KNeighborsClassifier: sklearn Libarary  

```{python}
#| code-fold: false
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42) 

sklern_clf = KNeighborsClassifier(n_neighbors=5) # 5 is the default number
sklern_clf.fit(X_train,y_train)
pred = sklern_clf.predict(X_test)
acc = accuracy_score(y_test,pred)
print('Classification Accuracy: ',acc)
```

### KNeighborsClassifier: custom Libarary  

This is a custom made KNN classifier (code credit goes to [AssemblyAI](https://www.youtube.com/watch?v=rTEtEy5o3X0&t=307s)). Let's use this to classify the iris dataset.  

```python
import numpy as np
from collections import Counter


def distance(x1, x2):
    return np.sqrt(np.sum((x1-x2)**2))


class KNNClassifier:
    def __init__(self, k=3):
        self.k = k

    def fit(self, X, y):
        self.X_train = X
        self.y_train = y

    def predict(self, X):
        predictions = [self._prediction(x) for x in X]
        return predictions

    def _prediction(self, x):
        distances = [distance(x, x_train) for x_train in self.Xtrain]
        knn_indices = np.argsort(distances)[:self.k]
        knn_labels = [self.y_train[i] for i in knn_indices]

        # majority vote
        most_common = Counter(knn_labels).most_common()
        return most_common[0][0]
```  

Save the above file as `knn.py` in the same directory. Then

```{python}
#| code-fold: false
from knn import KNNClassifier

customknn_clf = KNNClassifier(k=5)
customknn_clf.fit(X_train,y_train)
pred = customknn_clf.predict(X_test)
acc = accuracy_score(y_test,pred)
print('Classification Accuracy: ',acc)
```



**Share on**  

<div id="fb-root"></div>
<script async defer crossorigin="anonymous"
 src="https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0"></script>
 
<div class="share-buttons">
<div class="fb-share-button" data-href="https://mrislambd.github.io/machinelearning/knn/"
data-layout="button_count" data-size="small"><a target="_blank" 
 href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fmrislambd.github.io%2Fmachinelearning%2Fknn%2F&amp;src=sdkpreparse" 
 class="fb-xfbml-parse-ignore">Share</a></div>

<script src="https://platform.linkedin.com/in.js" type="text/javascript">lang: en_US</script>
<script type="IN/Share" data-url="https://mrislambd.github.io/machinelearning/knn/"></script> 
 
<a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" 
 data-url="https://mrislambd.github.io/machinelearning/knn/" data-show-count="true">Tweet</a>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<div class="fb-comments" data-href="https://mrislambd.github.io/machinelearning/knn/"
 data-width="" data-numposts="5"></div>


**You may also like**  