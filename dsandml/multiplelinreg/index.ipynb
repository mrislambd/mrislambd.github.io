{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multiple Liear Regression\n",
        "\n",
        "Rafiq Islam  \n",
        "2024-08-29\n",
        "\n",
        "## Multiple Linear Regression\n",
        "\n",
        "The multiple linear regression takes the form  \n",
        "$$\n",
        "y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\cdots +\\beta_d x_d+\\xi=\\vec{x}\\cdot \\vec{\\beta}+\\xi\n",
        "$$\n",
        "\n",
        "with $\\{\\beta_i\\}_{i=0}^{d}\\in \\mathbb{R}$ constants or parameters of\n",
        "the model. In vector notation, $\\vec{\\beta}\\in \\mathbb{R}^{d+1}$,\n",
        "\n",
        "$$\n",
        "\\vec{\\beta}=\\begin{pmatrix}\\beta_0\\\\ \\beta_1\\\\ \\vdots \\\\ \\beta_d \\end{pmatrix};\\hspace{4mm}\\vec{x}=\\begin{pmatrix}1\\\\ x_1\\\\ x_2\\\\ \\vdots\\\\ x_d\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "For $n$ data points, in matrix algebra notation, we can write\n",
        "$y=X\\vec{\\beta}+\\xi$ where $X\\in \\mathcal{M}_{n\\times (d+1)}$ and\n",
        "$y\\in \\mathbb{R}^{d+1}$ with\n",
        "\n",
        "$$\n",
        "X=\\begin{pmatrix}1&x_{11}&x_{12}&\\cdots&x_{1d}\\\\1&x_{21}&x_{22}&\\cdots&x_{2d}\\\\ \\vdots& \\vdots &\\vdots&\\ddots &\\vdots\\\\1&x_{n1}&x_{n2}&\\cdots&x_{nd} \\end{pmatrix};\\hspace{4mm} y=\\begin{pmatrix}y_1\\\\y_2\\\\ \\vdots\\\\ y_n\\end{pmatrix};\\hspace{4mm} \\xi=\\begin{pmatrix}\\xi_1\\\\ \\xi_2\\\\ \\vdots\\\\ \\xi_n\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "We fit the $n$ data points with the objective to minimize the loss\n",
        "function, mean squared error\n",
        "\n",
        "$$\n",
        "MSE(\\vec{\\beta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-f_{\\vec{\\beta}}(\\vec{x}_i)\\right)^2=\\frac{1}{n}\\left|\\vec{y}-X\\vec{\\beta}\\right|^2\n",
        "$$\n",
        "\n",
        "## Ordinary Least Square Method"
      ],
      "id": "a2022681-9f7f-4df1-894b-6d268946502a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "c960d3fe-b5b5-44ad-a015-1db59e974280"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `scikit-learn` library uses ***Ordinary Least Squares (OLS)***\n",
        "method to find the parameters. This method is good for a simple and\n",
        "relatively smaller dataset. Here is a short note on this method.\n",
        "However, when the dimension is very high and the dataset is bigger,\n",
        "`scikit-learn` uses another method called ***Stochastic Gradient\n",
        "Descent*** for optimization which is discussed in the next section."
      ],
      "id": "dd4db71d-e1a9-4d59-b492-31a9a9e0de8b"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "fbd377fe-9ac5-46e2-97b7-1c0259ee71c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of OLS is to find the parameter vector $\\hat{\\beta}$ that\n",
        "minimizes the sum of squared errors (SSE) between the observed target\n",
        "values $y$ and the predicted values $\\hat{y}$:\n",
        "\n",
        "$$\n",
        "\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - X_i\\beta)^2\n",
        "$$\n",
        "\n",
        "This can be expressed in matrix form as:\n",
        "\n",
        "$$\n",
        "\\text{SSE} = (y - X\\beta)^T(y - X\\beta)\n",
        "$$\n",
        "\n",
        "To minimize the SSE, let’s first expand the expression:\n",
        "\n",
        "Since $\\beta^T X^T y$ is a scalar (a 1x1 matrix), it is equal to its\n",
        "transpose. That is\n",
        "\n",
        "and therefore,\n",
        "\n",
        "$$\n",
        "\\text{SSE} = y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta\n",
        "$$\n",
        "\n",
        "To find the minimum of the SSE, we take the derivative with respect to\n",
        "$\\beta$ and set it to zero:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{SSE}}{\\partial \\beta} = -2X^T y + 2X^T X \\beta = 0\n",
        "$$\n",
        "\n",
        "Now, solve for $\\beta$:\n",
        "\n",
        "$$\n",
        "X^T X \\beta = X^T y\n",
        "$$\n",
        "\n",
        "To isolate $\\beta$, we multiply both sides by $(X^T X)^{-1}$ (assuming\n",
        "$X^T X$ is invertible):\n",
        "\n",
        "$$\n",
        "\\beta = (X^T X)^{-1} X^T y\n",
        "$$"
      ],
      "id": "3fffc64d-e206-4c01-8577-5047f86d4985"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "970fb01e-4e22-4c3f-a0a1-e8a337aafa24"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The vector $\\hat{\\beta} = (X^T X)^{-1} X^T y$ gives the estimated\n",
        "coefficients that minimize the sum of squared errors between the\n",
        "observed target values $y$ and the predicted values\n",
        "$\\hat{y} = X\\hat{\\beta}$. This method is exact and works well when\n",
        "$X^T X$ is invertible and the dataset size is manageable. <br> <br> This\n",
        "method is very efficient for small to medium-sized datasets but can\n",
        "become computationally expensive for very large datasets due to the\n",
        "inversion of the matrix $X^TX$."
      ],
      "id": "015f6422-f159-4371-bd39-0b3bcde7a30e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "a58b5de3-6759-44ca-bfc9-e73c23710e4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterative Method\n",
        "\n",
        "### Gradient Descent"
      ],
      "id": "f4a8c815-fd08-4195-b8b5-ac6005be2720"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "5a417949-2ff5-4584-8bec-97163f04731e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img align=\"right\" height=350 width=450 src=\"/dsandml/multiplelinreg/gradient_descent.gif\" alt=\"Collected from gbhat.com\" style=\"margin-left: 20px; margin-bottom: 20px\"/>\n",
        "<br> GIF Credit:\n",
        "<a href=\"https://gbhat.com/machine_learning/gradient_descent_anim.html\" target=\"_blank\" style=\"text-decoration:none\">gbhat.com</a>  \n",
        "<br> Gradient Descent is an optimization algorithm used to minimize the\n",
        "cost function. The cost function $f(\\beta)$ measures how well a model\n",
        "with parameters $\\beta$ fits the data. The goal is to find the values of\n",
        "$\\beta$ that minimize this cost function. In terms of the iterative\n",
        "method, we want to find $\\beta_{k+1}$ and $\\beta_k$ such that\n",
        "$f(\\beta_{k+1})<f(\\beta_k)$. <br> <br> For a small change in $\\beta$, we\n",
        "can approximate $f(\\beta)$ using Taylor series expansion  \n",
        "$$f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}$$"
      ],
      "id": "534bb49e-0b10-44e2-9494-67e97f5f6ae1"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "e9941852-4ad9-4cb0-a8e1-b7e62db09959"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The update rule for vanilla gradient descent is given by:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $\\beta_k$ is the current estimate of the parameters at iteration\n",
        "    $k$.\n",
        "-   $\\eta$ is the learning rate, a small positive scalar that controls\n",
        "    the step size.\n",
        "-   $\\nabla f(\\beta_k)$ is the gradient of the cost function $f$ with\n",
        "    respect to $\\beta$ at the current point $\\beta_k$."
      ],
      "id": "7690c73b-8ad9-43bf-a0b2-429614c129be"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "94002cfb-d9ed-485a-a5f6-8bc44a481c46"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The update rule comes from the idea of moving the parameter vector\n",
        "$\\beta$ in the direction that decreases the cost function the most.\n",
        "\n",
        "1.  **Gradient**: The gradient $\\nabla f(\\beta_k)$ represents the\n",
        "    direction and magnitude of the steepest ascent of the function $f$\n",
        "    at the point $\\beta_k$. Since we want to minimize the function, we\n",
        "    move in the opposite direction of the gradient.\n",
        "\n",
        "2.  **Step Size**: The term $\\eta \\nabla f(\\beta_k)$ scales the gradient\n",
        "    by the learning rate $\\eta$, determining how far we move in that\n",
        "    direction. If $\\eta$ is too large, the algorithm may overshoot the\n",
        "    minimum; if it’s too small, the convergence will be slow.\n",
        "\n",
        "3.  **Iterative Update**: Starting from an initial guess $\\beta_0$, we\n",
        "    repeatedly apply the update rule until the algorithm converges,\n",
        "    meaning that the changes in $\\beta_k$ become negligible, and\n",
        "    $\\beta_k$ is close to the optimal value $\\beta^*$.\n",
        "\n",
        "### Stochastic Gradient Descent (SGD)"
      ],
      "id": "c965645d-f90e-4e16-9b76-28999c4b35d1"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "742e24bd-51bf-44e6-b67c-e0289d7e285b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stochastic Gradient Descent is a variation of the vanilla gradient\n",
        "descent. Instead of computing the gradient using the entire dataset, SGD\n",
        "updates the parameters using only a single data point or a small batch\n",
        "of data points at each iteration. The later one we call it mini batch\n",
        "SGD."
      ],
      "id": "8b56d2b4-328c-44c6-b930-6cd88f36b35e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "11e34ed3-264e-4a2a-afd5-de84527e5720"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose our cost function is defined as the average over a dataset of\n",
        "size $n$:\n",
        "\n",
        "$$\n",
        "f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\beta)\n",
        "$$\n",
        "\n",
        "Where $f_i(\\beta)$ represents the contribution of the $i$-th data point\n",
        "to the total cost function. The gradient of the cost function with\n",
        "respect to $\\beta$ is:\n",
        "\n",
        "$$\n",
        "\\nabla f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\beta)\n",
        "$$\n",
        "\n",
        "Vanilla gradient descent would update the parameters as:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n",
        "$$\n",
        "\n",
        "Instead of using the entire dataset to compute the gradient, SGD\n",
        "approximates the gradient by using only a single data point (or a small\n",
        "batch). The update rule for SGD is:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f_{i_k}(\\beta_k)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $i_k$ is the index of a randomly selected data point at iteration\n",
        "    $k$.\n",
        "-   $\\nabla f_{i_k}(\\beta_k)$ is the gradient of the cost function with\n",
        "    respect to the parameter $\\beta_k$, evaluated only at the data point\n",
        "    indexed by $i_k$.\n",
        "\n",
        "## Python Execution\n",
        "\n",
        "### Synthetic Data"
      ],
      "id": "75114004-37fd-49a6-bbe6-698f3c56fd4c"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "X=np.random.randn(1000,2)\n",
        "y=3*X[:,0]+2*X[:,1]+1+np.random.randn(1000)"
      ],
      "id": "cdc8d809"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So for this project, our known relationship is $y=1+3x_1+2x_2+\\xi$.\n",
        "\n",
        "### Fit the data: Using scikit-learn Library"
      ],
      "id": "eaa2f668-b1c8-4ba9-bd8f-9fbb79829c00"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlr=LinearRegression()\n",
        "mlr.fit(X,y)\n",
        "coefficients=mlr.coef_.tolist()\n",
        "slope=mlr.intercept_.tolist()"
      ],
      "id": "39b9314c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ 1.0353 and coefficients\n",
        "$\\beta_1=$ 2.9829, and $\\beta_2=$ 2.0123\n",
        "\n",
        "### Fit the data: Using Custom Library OLS\n",
        "\n",
        "First we create our custom `NewLinearRegression` using the OLS formula\n",
        "above and save this `python` class as `mlreg.py`\n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NewLinearRegression:\n",
        "    def __init__(self) -> None:\n",
        "        self.beta = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        X_transpose_X = np.dot(X.transpose(), X)\n",
        "        X_transpose_X_inverse = np.linalg.inv(X_transpose_X)\n",
        "        X_transpose_y = np.dot(X.transpose(), y)\n",
        "        self.beta = np.dot(X_transpose_X_inverse, X_transpose_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        return np.dot(X, self.beta)\n",
        "\n",
        "    def coeff_(self):\n",
        "        return self.beta[1:].tolist()\n",
        "\n",
        "    def interceptt_(self):\n",
        "        return self.beta[0].tolist()\n",
        "```\n",
        "\n",
        "Now it’s time to use the new class"
      ],
      "id": "4df32e64-5568-4087-9a91-80656771d46c"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlreg import NewLinearRegression\n",
        "mlr1 = NewLinearRegression()\n",
        "mlr1.fit(X,y)\n",
        "coefficients1=mlr1.coeff_()\n",
        "slope1=mlr1.interceptt_()"
      ],
      "id": "1e50fc77"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ 1.0353 and coefficients\n",
        "$\\beta_1=$ 2.9829, and $\\beta_2=$ 2.0123\n",
        "\n",
        "### Fit the data: Using Gradient Descent\n",
        "\n",
        "We create the class\n",
        "\n",
        "``` python\n",
        "class GDLinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, number_of_iteration=1000) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.number_of_iteration = number_of_iteration\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_of_samples, num_of_features = X.shape\n",
        "        self.weights = np.zeros(num_of_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.number_of_iteration):\n",
        "            y_predicted = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            d_weights = (1 / num_of_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            d_bias = (1 / num_of_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * d_weights\n",
        "            self.bias -= self.learning_rate * d_bias\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_predicted = np.dot(X, self.weights) + self.bias\n",
        "        return y_predicted\n",
        "\n",
        "    def coefff_(self):\n",
        "        return self.weights.tolist()\n",
        "\n",
        "    def intercepttt_(self):\n",
        "        return self.bias\n",
        "```\n",
        "\n",
        "Now we use this similarly as before,"
      ],
      "id": "6b8b040f-97d8-4dd9-8235-85b3ea4a6de4"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlreg import GDLinearRegression\n",
        "mlr2= GDLinearRegression(learning_rate=0.008)\n",
        "mlr2.fit(X,y)\n",
        "coefficients2=mlr2.coefff_()\n",
        "slope2=mlr2.intercepttt_()"
      ],
      "id": "0837fdc7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ 1.0356 and coefficients\n",
        "$\\beta_1=$ 2.9815, and $\\beta_2=$ 2.0115\n",
        "\n",
        "### Fit the data: Using Stochastic Gradient Descent\n",
        "\n",
        "First we define the class\n",
        "\n",
        "``` python\n",
        "class SGDLinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000, batch_size=1) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.theta = None\n",
        "        self.mse_list = None  # Initialize mse_list as an instance attribute\n",
        "\n",
        "    def _loss_function(self, X, y, beta):\n",
        "        num_samples = len(y)\n",
        "        y_predicted = X.dot(beta)\n",
        "        mse = (1/num_samples) * np.sum(np.square(y_predicted - y))\n",
        "        return mse\n",
        "\n",
        "    def _gradient_function(self, X, y, beta):\n",
        "        num_samples = len(y)\n",
        "        y_predicted = X.dot(beta)\n",
        "        grad = (1/num_samples) * X.T.dot(y_predicted - y)\n",
        "        return grad\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Adding the intercept term (bias) as a column of ones\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        num_features = X.shape[1]\n",
        "        self.theta = np.zeros((num_features, 1))\n",
        "\n",
        "        self.mse_list = np.zeros(self.num_iterations)  # Initialize mse_list\n",
        "\n",
        "        for i in range(self.num_iterations):\n",
        "            # Randomly select a batch of data points\n",
        "            indices = np.random.choice(\n",
        "                len(y), size=self.batch_size, replace=False)\n",
        "            X_i = X[indices]\n",
        "            y_i = y[indices].reshape(-1, 1)\n",
        "\n",
        "            # Compute the gradient and update the weights\n",
        "            gradient = self._gradient_function(X_i, y_i, self.theta)\n",
        "            self.theta = self.theta - self.learning_rate * gradient\n",
        "\n",
        "            # Calculate loss for the entire dataset (optional)\n",
        "            self.mse_list[i] = self._loss_function(X, y, self.theta)\n",
        "\n",
        "        return self.theta, self.mse_list\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Adding the intercept term (bias) as a column of ones\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        return X.dot(self.theta)\n",
        "\n",
        "    def coef_(self):\n",
        "        # Return the coefficients (excluding the intercept term)\n",
        "        return self.theta[1:].flatten().tolist()\n",
        "\n",
        "    def intercept_(self):\n",
        "        # Return the intercept term\n",
        "        return self.theta[0].item()\n",
        "\n",
        "    def mse_losses(self):\n",
        "        # Return the mse_list\n",
        "        return self.mse_list.tolist()\n",
        "```\n",
        "\n",
        "Now"
      ],
      "id": "bc489028-2148-4d13-a93c-0dc97b0f79d7"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mlreg import SGDLinearRegression\n",
        "mlr3=SGDLinearRegression(learning_rate=0.01, num_iterations=1000, batch_size=10)\n",
        "theta, _ = mlr3.fit(X, y)"
      ],
      "id": "aeebcaf9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ array(\\[1.01772816\\]) and\n",
        "coefficients $\\beta_1=$ array(\\[3.01503486\\]), and $\\beta_2=$\n",
        "array(\\[2.06061341\\])\n",
        "\n",
        "Up next [knn regression](../../dsandml/knn/index.qmd)\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Share on**\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/multiplelinreg/\" target=\"_blank\" style=\"color:#1877F2; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/multiplelinreg/\" target=\"_blank\" style=\"color:#0077B5; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/multiplelinreg/\" target=\"_blank\" style=\"color:#1DA1F2; text-decoration: none;\">\n",
        "\n",
        "</a>"
      ],
      "id": "a096b712-9f9e-43d5-bd72-39657179174e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script src=\"https://giscus.app/client.js\"\n",
        "        data-repo=\"mrislambd/mrislambd.github.io\" \n",
        "        data-repo-id=\"R_kgDOMV8crA\"\n",
        "        data-category=\"Announcements\"\n",
        "        data-category-id=\"DIC_kwDOMV8crM4CjbQW\"\n",
        "        data-mapping=\"pathname\"\n",
        "        data-strict=\"0\"\n",
        "        data-reactions-enabled=\"1\"\n",
        "        data-emit-metadata=\"0\"\n",
        "        data-input-position=\"bottom\"\n",
        "        data-theme=\"light\"\n",
        "        data-lang=\"en\"\n",
        "        crossorigin=\"anonymous\"\n",
        "        async>\n",
        "</script>"
      ],
      "id": "a424e716-7ed3-405f-8514-43faafdb811d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [],
      "id": "94f30617-9824-496b-90b2-fafbabb077d9"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script async defer crossorigin=\"anonymous\"\n",
        " src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"></script>"
      ],
      "id": "f80d8171-f6c9-48c0-a807-4f104d32e310"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**You may also like**"
      ],
      "id": "05e6dfee-296f-43cb-b7b3-df5907e09dc4"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/Users/macpc/Library/CloudStorage/OneDrive-FloridaStateUniversity/OnlineLearning/python_environments/pytorch-env/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  }
}