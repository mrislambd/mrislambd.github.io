{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multiple Liear Regression\n",
        "\n",
        "Rafiq Islam  \n",
        "2024-08-29\n",
        "\n",
        "## Multiple Linear Regression\n",
        "\n",
        "The multiple linear regression takes the form  \n",
        "$$\n",
        "y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\cdots +\\beta_d x_d+\\xi=\\vec{x}\\cdot \\vec{\\beta}+\\xi\n",
        "$$\n",
        "\n",
        "with $\\{\\beta_i\\}_{i=0}^{d}\\in \\mathbb{R}$ constants or parameters of\n",
        "the model. In vector notation, $\\vec{\\beta}\\in \\mathbb{R}^{d+1}$,\n",
        "\n",
        "$$\n",
        "\\vec{\\beta}=\\begin{pmatrix}\\beta_0\\\\ \\beta_1\\\\ \\vdots \\\\ \\beta_d \\end{pmatrix};\\hspace{4mm}\\vec{x}=\\begin{pmatrix}1\\\\ x_1\\\\ x_2\\\\ \\vdots\\\\ x_d\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "For $n$ data points, in matrix algebra notation, we can write\n",
        "$y=X\\vec{\\beta}+\\xi$ where $X\\in \\mathcal{M}_{n\\times (d+1)}$ and\n",
        "$y\\in \\mathbb{R}^{d+1}$ with\n",
        "\n",
        "$$\n",
        "X=\\begin{pmatrix}1&x_{11}&x_{12}&\\cdots&x_{1d}\\\\1&x_{21}&x_{22}&\\cdots&x_{2d}\\\\ \\vdots& \\vdots &\\vdots&\\ddots &\\vdots\\\\1&x_{n1}&x_{n2}&\\cdots&x_{nd} \\end{pmatrix};\\hspace{4mm} y=\\begin{pmatrix}y_1\\\\y_2\\\\ \\vdots\\\\ y_n\\end{pmatrix};\\hspace{4mm} \\xi=\\begin{pmatrix}\\xi_1\\\\ \\xi_2\\\\ \\vdots\\\\ \\xi_n\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "We fit the $n$ data points with the objective to minimize the loss\n",
        "function, mean squared error\n",
        "\n",
        "$$\n",
        "MSE(\\vec{\\beta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i-f_{\\vec{\\beta}}(\\vec{x}_i)\\right)^2=\\frac{1}{n}\\left|\\vec{y}-X\\vec{\\beta}\\right|^2\n",
        "$$\n",
        "\n",
        "## Ordinary Least Square Method"
      ],
      "id": "e2f8c953-616a-49bb-aedc-f649e629145a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "42c1bc43-c90b-4f09-a1a7-99e5a86ea025"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `scikit-learn` library uses ***Ordinary Least Squares (OLS)***\n",
        "method to find the parameters. This method is good for a simple and\n",
        "relatively smaller dataset. Here is a short note on this method.\n",
        "However, when the dimension is very high and the dataset is bigger,\n",
        "`scikit-learn` uses another method called ***Stochastic Gradient\n",
        "Descent*** for optimization which is discussed in the next section."
      ],
      "id": "59d9dd40-a6a2-4277-a587-625db5c085e5"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "af8ec21c-9370-46a5-89e5-e05f7dbc50ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of OLS is to find the parameter vector $\\hat{\\beta}$ that\n",
        "minimizes the sum of squared errors (SSE) between the observed target\n",
        "values $y$ and the predicted values $\\hat{y}$:\n",
        "\n",
        "$$\n",
        "\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - X_i\\beta)^2\n",
        "$$\n",
        "\n",
        "This can be expressed in matrix form as:\n",
        "\n",
        "$$\n",
        "\\text{SSE} = (y - X\\beta)^T(y - X\\beta)\n",
        "$$\n",
        "\n",
        "To minimize the SSE, let’s first expand the expression:\n",
        "\n",
        "Since $\\beta^T X^T y$ is a scalar (a 1x1 matrix), it is equal to its\n",
        "transpose. That is\n",
        "\n",
        "and therefore,\n",
        "\n",
        "$$\n",
        "\\text{SSE} = y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta\n",
        "$$\n",
        "\n",
        "To find the minimum of the SSE, we take the derivative with respect to\n",
        "$\\beta$ and set it to zero:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{SSE}}{\\partial \\beta} = -2X^T y + 2X^T X \\beta = 0\n",
        "$$\n",
        "\n",
        "Now, solve for $\\beta$:\n",
        "\n",
        "$$\n",
        "X^T X \\beta = X^T y\n",
        "$$\n",
        "\n",
        "To isolate $\\beta$, we multiply both sides by $(X^T X)^{-1}$ (assuming\n",
        "$X^T X$ is invertible):\n",
        "\n",
        "$$\n",
        "\\beta = (X^T X)^{-1} X^T y\n",
        "$$"
      ],
      "id": "c82fbd1d-9d14-4f47-9fe6-0b53b8fe671a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "68e17812-dd38-4920-a381-94c77994709e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The vector $\\hat{\\beta} = (X^T X)^{-1} X^T y$ gives the estimated\n",
        "coefficients that minimize the sum of squared errors between the\n",
        "observed target values $y$ and the predicted values\n",
        "$\\hat{y} = X\\hat{\\beta}$. This method is exact and works well when\n",
        "$X^T X$ is invertible and the dataset size is manageable. <br> <br> This\n",
        "method is very efficient for small to medium-sized datasets but can\n",
        "become computationally expensive for very large datasets due to the\n",
        "inversion of the matrix $X^TX$."
      ],
      "id": "aacadff3-a78c-4198-8326-b3ec2b0629c7"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "9eca5685-92f4-4271-8bfd-273b85c30590"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterative Method\n",
        "\n",
        "### Gradient Descent"
      ],
      "id": "b6319c52-e06f-40af-b95b-db2724705b11"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "bd1d5811-b12c-476d-9a18-d9730bc73388"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img align=\"right\" height=350 width=450 src=\"/dsandml/multiplelinreg/gradient_descent.gif\" alt=\"Collected from gbhat.com\" style=\"margin-left: 20px; margin-bottom: 20px\"/>\n",
        "<br> GIF Credit:\n",
        "<a href=\"https://gbhat.com/machine_learning/gradient_descent_anim.html\" target=\"_blank\" style=\"text-decoration:none\">gbhat.com</a>  \n",
        "<br> Gradient Descent is an optimization algorithm used to minimize the\n",
        "cost function. The cost function $f(\\beta)$ measures how well a model\n",
        "with parameters $\\beta$ fits the data. The goal is to find the values of\n",
        "$\\beta$ that minimize this cost function. In terms of the iterative\n",
        "method, we want to find $\\beta_{k+1}$ and $\\beta_k$ such that\n",
        "$f(\\beta_{k+1})<f(\\beta_k)$. <br> <br> For a small change in $\\beta$, we\n",
        "can approximate $f(\\beta)$ using Taylor series expansion  \n",
        "$$f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}$$"
      ],
      "id": "2925a491-349d-4863-863a-80d5d3e7b3a3"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "b7bb2ce9-b0e6-4147-bc63-e5f1bcfbaaac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The update rule for vanilla gradient descent is given by:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $\\beta_k$ is the current estimate of the parameters at iteration\n",
        "    $k$.\n",
        "-   $\\eta$ is the learning rate, a small positive scalar that controls\n",
        "    the step size.\n",
        "-   $\\nabla f(\\beta_k)$ is the gradient of the cost function $f$ with\n",
        "    respect to $\\beta$ at the current point $\\beta_k$."
      ],
      "id": "7c54db45-bffa-4af0-b642-4ed1486e3712"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "af8be7c2-4f28-4804-b963-d747b7a6d0df"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The update rule comes from the idea of moving the parameter vector\n",
        "$\\beta$ in the direction that decreases the cost function the most.\n",
        "\n",
        "1.  **Gradient**: The gradient $\\nabla f(\\beta_k)$ represents the\n",
        "    direction and magnitude of the steepest ascent of the function $f$\n",
        "    at the point $\\beta_k$. Since we want to minimize the function, we\n",
        "    move in the opposite direction of the gradient.\n",
        "\n",
        "2.  **Step Size**: The term $\\eta \\nabla f(\\beta_k)$ scales the gradient\n",
        "    by the learning rate $\\eta$, determining how far we move in that\n",
        "    direction. If $\\eta$ is too large, the algorithm may overshoot the\n",
        "    minimum; if it’s too small, the convergence will be slow.\n",
        "\n",
        "3.  **Iterative Update**: Starting from an initial guess $\\beta_0$, we\n",
        "    repeatedly apply the update rule until the algorithm converges,\n",
        "    meaning that the changes in $\\beta_k$ become negligible, and\n",
        "    $\\beta_k$ is close to the optimal value $\\beta^*$.\n",
        "\n",
        "### Stochastic Gradient Descent (SGD)"
      ],
      "id": "27068d0f-0218-4ecf-8025-42d557a64e05"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p style=\"text-align: justify\">"
      ],
      "id": "fd05587a-974c-42b1-876a-b60ec2071915"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stochastic Gradient Descent is a variation of the vanilla gradient\n",
        "descent. Instead of computing the gradient using the entire dataset, SGD\n",
        "updates the parameters using only a single data point or a small batch\n",
        "of data points at each iteration. The later one we call it mini batch\n",
        "SGD."
      ],
      "id": "f0795efb-0bb4-4b82-acd0-045d8a03240d"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "fb840041-dccf-40cf-afd3-51ca75c9fdd3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose our cost function is defined as the average over a dataset of\n",
        "size $n$:\n",
        "\n",
        "$$\n",
        "f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\beta)\n",
        "$$\n",
        "\n",
        "Where $f_i(\\beta)$ represents the contribution of the $i$-th data point\n",
        "to the total cost function. The gradient of the cost function with\n",
        "respect to $\\beta$ is:\n",
        "\n",
        "$$\n",
        "\\nabla f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\beta)\n",
        "$$\n",
        "\n",
        "Vanilla gradient descent would update the parameters as:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n",
        "$$\n",
        "\n",
        "Instead of using the entire dataset to compute the gradient, SGD\n",
        "approximates the gradient by using only a single data point (or a small\n",
        "batch). The update rule for SGD is:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - \\eta \\nabla f_{i_k}(\\beta_k)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "-   $i_k$ is the index of a randomly selected data point at iteration\n",
        "    $k$.\n",
        "-   $\\nabla f_{i_k}(\\beta_k)$ is the gradient of the cost function with\n",
        "    respect to the parameter $\\beta_k$, evaluated only at the data point\n",
        "    indexed by $i_k$.\n",
        "\n",
        "## Python Execution\n",
        "\n",
        "### Synthetic Data"
      ],
      "id": "046a2d54-f5e2-4908-8a1c-de17ac63eed8"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "X=np.random.randn(1000,2)\n",
        "y=3*X[:,0]+2*X[:,1]+1+np.random.randn(1000)"
      ],
      "id": "4b7f408f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So for this project, our known relationship is $y=1+3x_1+2x_2+\\xi$.\n",
        "\n",
        "### Fit the data: Using scikit-learn Library"
      ],
      "id": "c49ffabd-2cdc-4c8b-89fb-56ba22391ddd"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlr=LinearRegression()\n",
        "mlr.fit(X,y)\n",
        "coefficients=mlr.coef_.tolist()\n",
        "slope=mlr.intercept_.tolist()"
      ],
      "id": "13e864c2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ 0.9619 and coefficients\n",
        "$\\beta_1=$ 3.0206, and $\\beta_2=$ 2.0246\n",
        "\n",
        "### Fit the data: Using Custom Library OLS\n",
        "\n",
        "First we create our custom `NewLinearRegression` using the OLS formula\n",
        "above and save this `python` class as `mlreg.py`\n",
        "\n",
        "``` python\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NewLinearRegression:\n",
        "    def __init__(self) -> None:\n",
        "        self.beta = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        X_transpose_X = np.dot(X.transpose(), X)\n",
        "        X_transpose_X_inverse = np.linalg.inv(X_transpose_X)\n",
        "        X_transpose_y = np.dot(X.transpose(), y)\n",
        "        self.beta = np.dot(X_transpose_X_inverse, X_transpose_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        return np.dot(X, self.beta)\n",
        "\n",
        "    def coeff_(self):\n",
        "        return self.beta[1:].tolist()\n",
        "\n",
        "    def interceptt_(self):\n",
        "        return self.beta[0].tolist()\n",
        "```\n",
        "\n",
        "Now it’s time to use the new class"
      ],
      "id": "3d84d0d6-0912-459f-8823-85b2e6926210"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlreg import NewLinearRegression\n",
        "mlr1 = NewLinearRegression()\n",
        "mlr1.fit(X,y)\n",
        "coefficients1=mlr1.coeff_()\n",
        "slope1=mlr1.interceptt_()"
      ],
      "id": "eac2dcb4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ 0.9619 and coefficients\n",
        "$\\beta_1=$ 3.0206, and $\\beta_2=$ 2.0246\n",
        "\n",
        "### Fit the data: Using Gradient Descent\n",
        "\n",
        "We create the class\n",
        "\n",
        "``` python\n",
        "class GDLinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, number_of_iteration=1000) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.number_of_iteration = number_of_iteration\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_of_samples, num_of_features = X.shape\n",
        "        self.weights = np.zeros(num_of_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.number_of_iteration):\n",
        "            y_predicted = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            d_weights = (1 / num_of_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            d_bias = (1 / num_of_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * d_weights\n",
        "            self.bias -= self.learning_rate * d_bias\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_predicted = np.dot(X, self.weights) + self.bias\n",
        "        return y_predicted\n",
        "\n",
        "    def coefff_(self):\n",
        "        return self.weights.tolist()\n",
        "\n",
        "    def intercepttt_(self):\n",
        "        return self.bias\n",
        "```\n",
        "\n",
        "Now we use this similarly as before,"
      ],
      "id": "21a11d96-52d4-4f40-b46d-745ff725d49f"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlreg import GDLinearRegression\n",
        "mlr2= GDLinearRegression(learning_rate=0.008)\n",
        "mlr2.fit(X,y)\n",
        "coefficients2=mlr2.coefff_()\n",
        "slope2=mlr2.intercepttt_()"
      ],
      "id": "23ebc53d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ 0.9622 and coefficients\n",
        "$\\beta_1=$ 3.0195, and $\\beta_2=$ 2.0238\n",
        "\n",
        "### Fit the data: Using Stochastic Gradient Descent\n",
        "\n",
        "First we define the class\n",
        "\n",
        "``` python\n",
        "class SGDLinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000, batch_size=1) -> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.batch_size = batch_size\n",
        "        self.theta = None\n",
        "        self.mse_list = None  # Initialize mse_list as an instance attribute\n",
        "\n",
        "    def _loss_function(self, X, y, beta):\n",
        "        num_samples = len(y)\n",
        "        y_predicted = X.dot(beta)\n",
        "        mse = (1/num_samples) * np.sum(np.square(y_predicted - y))\n",
        "        return mse\n",
        "\n",
        "    def _gradient_function(self, X, y, beta):\n",
        "        num_samples = len(y)\n",
        "        y_predicted = X.dot(beta)\n",
        "        grad = (1/num_samples) * X.T.dot(y_predicted - y)\n",
        "        return grad\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Adding the intercept term (bias) as a column of ones\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        num_features = X.shape[1]\n",
        "        self.theta = np.zeros((num_features, 1))\n",
        "\n",
        "        self.mse_list = np.zeros(self.num_iterations)  # Initialize mse_list\n",
        "\n",
        "        for i in range(self.num_iterations):\n",
        "            # Randomly select a batch of data points\n",
        "            indices = np.random.choice(\n",
        "                len(y), size=self.batch_size, replace=False)\n",
        "            X_i = X[indices]\n",
        "            y_i = y[indices].reshape(-1, 1)\n",
        "\n",
        "            # Compute the gradient and update the weights\n",
        "            gradient = self._gradient_function(X_i, y_i, self.theta)\n",
        "            self.theta = self.theta - self.learning_rate * gradient\n",
        "\n",
        "            # Calculate loss for the entire dataset (optional)\n",
        "            self.mse_list[i] = self._loss_function(X, y, self.theta)\n",
        "\n",
        "        return self.theta, self.mse_list\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Adding the intercept term (bias) as a column of ones\n",
        "        X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
        "        return X.dot(self.theta)\n",
        "\n",
        "    def coef_(self):\n",
        "        # Return the coefficients (excluding the intercept term)\n",
        "        return self.theta[1:].flatten().tolist()\n",
        "\n",
        "    def intercept_(self):\n",
        "        # Return the intercept term\n",
        "        return self.theta[0].item()\n",
        "\n",
        "    def mse_losses(self):\n",
        "        # Return the mse_list\n",
        "        return self.mse_list.tolist()\n",
        "```\n",
        "\n",
        "Now"
      ],
      "id": "aeaecb95-c560-4b26-bffd-ac76b8dbcfd1"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mlreg import SGDLinearRegression\n",
        "mlr3=SGDLinearRegression(learning_rate=0.01, num_iterations=1000, batch_size=10)\n",
        "theta, _ = mlr3.fit(X, y)"
      ],
      "id": "1706919b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So the model parameters: slope $\\beta_0=$ array(\\[0.96890933\\]) and\n",
        "coefficients $\\beta_1=$ array(\\[3.00262882\\]), and $\\beta_2=$\n",
        "array(\\[2.01517029\\])\n",
        "\n",
        "Up next [knn regression](../../dsandml/knn/index.qmd)\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Share on**\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer.php?u=https://mrislambd.github.io/dsandml/multiplelinreg/\" target=\"_blank\" style=\"color:#1877F2; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://mrislambd.github.io/dsandml/multiplelinreg/\" target=\"_blank\" style=\"color:#0077B5; text-decoration: none;\">\n",
        "\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.twitter.com/intent/tweet?url=https://mrislambd.github.io/dsandml/multiplelinreg/\" target=\"_blank\" style=\"color:#1DA1F2; text-decoration: none;\">\n",
        "\n",
        "</a>"
      ],
      "id": "8aae2978-88a2-4322-bbe9-08dced990481"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script src=\"https://giscus.app/client.js\"\n",
        "        data-repo=\"mrislambd/mrislambd.github.io\" \n",
        "        data-repo-id=\"R_kgDOMV8crA\"\n",
        "        data-category=\"Announcements\"\n",
        "        data-category-id=\"DIC_kwDOMV8crM4CjbQW\"\n",
        "        data-mapping=\"pathname\"\n",
        "        data-strict=\"0\"\n",
        "        data-reactions-enabled=\"1\"\n",
        "        data-emit-metadata=\"0\"\n",
        "        data-input-position=\"bottom\"\n",
        "        data-theme=\"light\"\n",
        "        data-lang=\"en\"\n",
        "        crossorigin=\"anonymous\"\n",
        "        async>\n",
        "</script>"
      ],
      "id": "d31e4d68-d6eb-4b5d-bff4-98b050365eee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [],
      "id": "3d5f4c33-8a21-49bd-aa64-88332ab034ad"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<script async defer crossorigin=\"anonymous\"\n",
        " src=\"https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v20.0\"></script>"
      ],
      "id": "536827bf-ce2f-44fd-a1e7-f9ecf754cf4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**You may also like**"
      ],
      "id": "713306dd-53d3-4625-b592-a2d226f495fc"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/opt/hostedtoolcache/Python/3.10.16/x64/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  }
}