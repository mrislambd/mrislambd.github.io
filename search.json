[
  {
    "objectID": "publication/pub2/index.html",
    "href": "publication/pub2/index.html",
    "title": "GJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies",
    "section": "",
    "text": "Cryptocurrencies are currently traded worldwide, with hundreds of different currencies in existence and even more on the way. This study implements some statistical and machine learning approaches for cryptocurrency investments. First, we implement GJR-GARCH over the GARCH model to estimate the volatility of ten popular cryptocurrencies based on market capitalization: Bitcoin, Bitcoin Cash, Bitcoin SV, Chainlink, EOS, Ethereum, Litecoin, TETHER, Tezos, and XRP. Then, we use Monte Carlo simulations to generate the conditional variance of the cryptocurrencies using the GJR-GARCH model, and calculate the value at risk (VaR) of the simulations. We also estimate the tail-risk using VaR backtesting. Finally, we use an artificial neural network (ANN) for predicting the prices of the ten cryptocurrencies. The graphical analysis and mean square errors (MSEs) from the ANN models confirmed that the predicted prices are close to the market prices. For some cryptocurrencies, the ANN models perform better than traditional ARIMA models.\n\n\n    \n        \n    \n    \n        \n    \n\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@article{mostafa2021,\n  author = {Mostafa, Fahad and Saha, Pritam and Rafiqul Islam, Mohammad\n    and Nguyen, Nguyet},\n  title = {GJR-GARCH {Volatility} {Modeling} Under {NIG} and {ANN} for\n    {Predicting} {Top} {Cryptocurrencies}},\n  journal = {Journal of Risk and Financial Management},\n  date = {2021-09-03},\n  url = {https://mrislambd.github.io/publication/pub2/},\n  doi = {10.3390/jrfm14090421},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMostafa, Fahad, Pritam Saha, Mohammad Rafiqul Islam, and Nguyet Nguyen.\n2021. “GJR-GARCH Volatility Modeling Under NIG and ANN for\nPredicting Top Cryptocurrencies.” Journal of Risk and\nFinancial Management, September. https://doi.org/10.3390/jrfm14090421."
  },
  {
    "objectID": "banglablog/welcomepost/index.html",
    "href": "banglablog/welcomepost/index.html",
    "title": "বাংলা ব্লগে আপনাকে স্বাগতম",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "talks/2023-10-05-st-op.html",
    "href": "talks/2023-10-05-st-op.html",
    "title": "Decentralized Stochastic Gradient Langevin Dynamics andHamiltonian Monte Carlo",
    "section": "",
    "text": "This presentation is based on the paper Decentralized Stochastic Gradient Langevin Dynamics andHamiltonian Monte Carlo by Dr. Lingjiong Zhu\nA copy of the presentation can be found here\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html",
    "href": "portfolio/dsp/ecommerce/index.html",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "",
    "text": "Notebook"
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#project-overview",
    "href": "portfolio/dsp/ecommerce/index.html#project-overview",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Project Overview",
    "text": "Project Overview\n\nThis is a preliminary level linear regression based machine learning project to investigate the feature importance for an e-commerce based company or simply building a predictive model to generate insights on different features."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#dataset",
    "href": "portfolio/dsp/ecommerce/index.html#dataset",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Dataset",
    "text": "Dataset\nThe data is collected from kaggle.com. It contains 500 observations with the following columns\n\nEmail: Email address of the customers\nAddress: Physical mailing address of the customers\n\nAvatar: The fancy avater of the customers\n\nAvg. Session Length: Average session lenght spent either on app or web\n\nLength of Membership: Length of the membership of the customers with the e-commerce company\n\nTime on App: Time spent on the mobile app\n\nTime on Website: Time spent on web based browser\n\nYearly Amount Spent: This is the dependent variable."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#stakeholders",
    "href": "portfolio/dsp/ecommerce/index.html#stakeholders",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Stakeholders",
    "text": "Stakeholders\nIf the company wants to decide whether to focus their efforts on the mobile app or the website."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#key-performance-indicators-kpis",
    "href": "portfolio/dsp/ecommerce/index.html#key-performance-indicators-kpis",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Key Performance Indicators (KPIs)",
    "text": "Key Performance Indicators (KPIs)\n\nAll the quantitative features were considered to find their importance on the Yearly Amount Spent variable. However, it was found that Length of Membership, Time on App, Avg. Session Length have the highest impact on the dependent variable in decreasing order."
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#modeling",
    "href": "portfolio/dsp/ecommerce/index.html#modeling",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Modeling",
    "text": "Modeling\n\\[\\begin{align*}\n\\text{Yearly Amount Spent}&=-1054.215476+25.362665\\times (\\text{Avg. Session Length})\\\\\n& +38.823679\\times (\\text{Time on App})+0.803568\\times (\\text{Time on Website})\\\\\n& + 61.549053\\times (\\text{Length of Membership})\n\\end{align*}\\]"
  },
  {
    "objectID": "portfolio/dsp/ecommerce/index.html#results-and-outcome",
    "href": "portfolio/dsp/ecommerce/index.html#results-and-outcome",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Results and Outcome",
    "text": "Results and Outcome\n\nModel Explanation\nBased on the model above, we can sumerize as follows\n\nIf everything else remain unchanged, a 1 unit increase in Avg. Session Length is associated with an increase of \\(25.36\\) in total Yearly Amount Spent\n\nIf everything else remain unchanged, a 1 unit increase in Time on App is associated with an increase of \\(38.82\\) in total Yearly Amount Spent\n\nIf everything else remain unchanged, a 1 unit increase in Time on Website is associated with an increase of \\(0.80\\) in total Yearly Amount Spent\n\nIf everything else remain unchanged, a 1 unit increase in Length of Membership is associated with an increase of \\(61.55\\) in total Yearly Amount Spent\n\nNow the key question, should the company focus more on Time on App more?\n\nThe answer to the question above is a little bit tricky. Based on the modeling approach, appearantly it may seems that time on app has more impact than the time on web. However, the most significant factor seems the Length of Memberhsip. So we need further analysis of this two features to properly answer if the company should focus more on app.\n\n\n\nModel Accuracy\nThe model above returns a MAE of 7.99, MSE of 102.72, RMSE of 10.14, and \\(R^2=98.46\\%\\)"
  },
  {
    "objectID": "portfolio/dsp/dp-nlp/index.html",
    "href": "portfolio/dsp/dp-nlp/index.html",
    "title": "Disease diagnosis using classification and NLP",
    "section": "",
    "text": "Team Members\nRebecca Ceppas de Castro, Fulya Tastan, Philip Barron, Mohammad Rafiqul Islam, Nina Adhikari, Viraj Meruliya\nAutomatic Symptom Detection (ASD) and Automatic Diagnosis (AD) have seen several advances in recent years. Patients and medical professionals would benefit from tools that can aid in diagnosing diseases based on antecedents and presenting symptoms. The lack of quality healthcare in many parts of the world makes solving this problem a matter of utmost urgency. The aim of this project is to build a tool that can diagnose a disease based on a list of symptoms and contribute to our understanding of automatic diagnosis.\nProject Details\nSlides\nExecutive Summary\nGitHub Repo\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{ceppas_de_castro,_fulya_tastan,_philip_barron,_mohammad_rafiqul_islam,_nina_adhikari,_viraj_meruliya_2024,\n  author = {Ceppas de Castro, Fulya Tastan, Philip Barron, Mohammad\n    Rafiqul Islam, Nina Adhikari, Viraj Meruliya , Rebecca},\n  title = {Disease Diagnosis Using Classification and {NLP}},\n  date = {2024-06-18},\n  url = {https://mrislambd.github.io/portfolio/dsp/dp-nlp/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCeppas de Castro, Fulya Tastan, Philip Barron, Mohammad Rafiqul Islam,\nNina Adhikari, Viraj Meruliya, Rebecca. 2024. “Disease Diagnosis\nUsing Classification and NLP.” June 18, 2024. https://mrislambd.github.io/portfolio/dsp/dp-nlp/."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html",
    "href": "portfolio/dsp/medicalcost/index.html",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "Notebook GitHub WebApp"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#project-overview",
    "href": "portfolio/dsp/medicalcost/index.html#project-overview",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Project Overview",
    "text": "Project Overview\n\nThis predictive modeling project involves personal medical data to predict the medical insurance charge by using a linear regression model."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#dataset",
    "href": "portfolio/dsp/medicalcost/index.html#dataset",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Dataset",
    "text": "Dataset\nThe dataset used in this project is collected from Kaggle\nColumns\nage: age of primary beneficiary\nsex: insurance contractor gender, female, male\nbmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight \\(\\frac{kg}{m^ 2}\\) using the ratio of height to weight, ideally \\(18.5\\) to \\(24.9\\)\nchildren: Number of children covered by health insurance / Number of dependents\nsmoker: Smoking\nregion: the beneficiary’s residential area in the US, northeast, southeast, southwest, northwest.\ncharges: Individual medical costs billed by health insurance\nAcknowledgements\nThe dataset is available on GitHub here."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#stakeholders",
    "href": "portfolio/dsp/medicalcost/index.html#stakeholders",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Stakeholders",
    "text": "Stakeholders\nCan we accurately predict insurance costs?"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#key-performance-indicators-kpis",
    "href": "portfolio/dsp/medicalcost/index.html#key-performance-indicators-kpis",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Key Performance Indicators (KPIs)",
    "text": "Key Performance Indicators (KPIs)\n\nAll the features were considered for the modeling purposes. However, from the exploratory data analysis and mathematical analysis, it was found that the charges usually goes up for the factors such as increase in age, living in certain region, having certain number of children. But this is not always the same depending on the smoker variable. Also, there is a strong correlation between age and bmi variable. Age a result new features such as age_bmi and age_bmi_smoker features were created to see how the charges interact."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#modeling",
    "href": "portfolio/dsp/medicalcost/index.html#modeling",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Modeling",
    "text": "Modeling\n\nModeling Approaches\nWe consider the following models\n\nBaseline model: Assumption that the charges variable can be modeled with the mean value of this charges variable.\n\\[\n\\text{charges}=\\mathbb{E}[\\text{charges}]+\\xi\n\\]\nLinear Regression with age-bmi-smoke interaction\n\\[\n\\text{charges}=\\beta_0+\\beta_1 (\\text{age\\_bmi})+\\beta_2 (\\text{male})+\\beta_3 (\\text{smoke})+\\beta_4 (\\text{children})+\\beta_5 (\\text{region})+\\beta_6 (\\text{age-bmi-smoke})+\\xi\n\\]\nK-Neighbor Regression\n\\(k\\)NN using all the original feature with \\(k=10\\)\n\n\n\nFinal Model\nFinally the modeling was done based on the lowest MSE value found from the 5-fold cross validation and the model has the following form\n\\[\\begin{align*}\n\\text{charges} &=10621.25+ 3346.14\\times \\text{Age\\_BMI}+4570.76\\times \\text{Male}+ 479.61\\times \\text{Smoke}-315.12\\times \\text{Children}\\\\\n&+13274.48\\times \\text{Region}-212.22\\times \\text{Age\\_BMI\\_Smoke}\n\\end{align*}\\]"
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#results-and-outcomes",
    "href": "portfolio/dsp/medicalcost/index.html#results-and-outcomes",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Results and Outcomes",
    "text": "Results and Outcomes\n\nModel Accuracy\nThe model above returns an RMSE of \\(5853.0\\) on the training set and an RMSE of \\(5600.0\\) on the test set with an \\(R^2=80\\%\\).\n\n\nWeb Application\nThe final model was developed and deployed using Streamlit. To try a single instance, fill out the following form and then click predict charges."
  },
  {
    "objectID": "portfolio/dsp/medicalcost/index.html#future-directions",
    "href": "portfolio/dsp/medicalcost/index.html#future-directions",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Future Directions",
    "text": "Future Directions\nFuture project on the same data could be adding a neural network and compare the relative performances of the two models.\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet"
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html",
    "href": "portfolio/dsp/autoloan/index.html",
    "title": "Auto Loan Decision Model",
    "section": "",
    "text": "Report Presentation"
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#objective",
    "href": "portfolio/dsp/autoloan/index.html#objective",
    "title": "Auto Loan Decision Model",
    "section": "Objective",
    "text": "Objective\n\nThe Auto Loan Credit Decisioning Model project aimed to enhance the application decision process for auto loans by leveraging machine learning techniques to classify applicants into approved or rejected categories. The project focused on improving prediction accuracy and ensuring fairness across demographic groups while addressing challenges like class imbalance and missing data."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#data-overview",
    "href": "portfolio/dsp/autoloan/index.html#data-overview",
    "title": "Auto Loan Decision Model",
    "section": "Data Overview",
    "text": "Data Overview\n\nDataset: Auto loan account data with 21,000 training records and 5,400 test records.\nFeatures: 43 columns, including borrower creditworthiness, loan application attributes, and demographics.\nTarget Variable: ‘Bad Flag’ (binary), indicating ‘Poor Credit Quality’ (95.5%) or ‘Good Credit Quality’ (4.5%).\nChallenges: Significant class imbalance and high proportion of missing values in several predictors."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#methodology",
    "href": "portfolio/dsp/autoloan/index.html#methodology",
    "title": "Auto Loan Decision Model",
    "section": "Methodology",
    "text": "Methodology\n\nExploratory Data Analysis (EDA):\n\nInvestigated class distributions and correlations with the target variable.\nAddressed missing values using mode and median imputations based on feature types.\nAnalyzed key predictors such as FICO scores, loan-to-value ratios, and credit utilization rates.\n\nModel Development:\n\nBuilt models using Logistic Regression, Decision Tree, and Random Forest classifiers.\nConducted hyperparameter tuning via GridSearchCV for optimal model settings.\nAddressed class imbalance with resampling techniques like SMOTE.\n\nEvaluation Metrics:\n\nPrioritized ROC-AUC, F1-Score, and classification reports over accuracy to account for imbalanced data."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#results",
    "href": "portfolio/dsp/autoloan/index.html#results",
    "title": "Auto Loan Decision Model",
    "section": "Results",
    "text": "Results\n\nBest Model: Random Forest Classifier with an ROC-AUC score of 0.8078.\nPerformance:\n\nTest data accuracy: 94.08%\nPrecision (Class 0): 96.16%\nRecall (Class 0): 97.70%\n\nFairness Analysis:\n\nGender-neutral approval rates.\nNo significant racial bias in decisions."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#key-insights",
    "href": "portfolio/dsp/autoloan/index.html#key-insights",
    "title": "Auto Loan Decision Model",
    "section": "Key Insights",
    "text": "Key Insights\n\nFICO scores, loan-to-value ratios, and credit utilization rates were strong predictors of credit quality.\nHigher class imbalance and overfitting issues were observed with resampling techniques."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#innovations",
    "href": "portfolio/dsp/autoloan/index.html#innovations",
    "title": "Auto Loan Decision Model",
    "section": "Innovations",
    "text": "Innovations\n\nImplemented Local Interpretable Model-agnostic Explanations (LIME) for model transparency, allowing stakeholders to understand prediction outcomes."
  },
  {
    "objectID": "portfolio/dsp/autoloan/index.html#conclusion",
    "href": "portfolio/dsp/autoloan/index.html#conclusion",
    "title": "Auto Loan Decision Model",
    "section": "Conclusion",
    "text": "Conclusion\nThe Random Forest Classifier demonstrated strong predictive performance and fairness, providing a reliable foundation for auto loan decisioning. Opportunities for further improvements include advanced resampling methods, enhanced feature engineering, and exploring models like XGBoost or LightGBM for better results.\nKeywords: Auto Loan, Predictive Modeling, Random Forest, Class Imbalance, Machine Learning, LIME."
  },
  {
    "objectID": "portfolio/spd/webapp/index.html",
    "href": "portfolio/spd/webapp/index.html",
    "title": "Streamlit Web App",
    "section": "",
    "text": "Welcome to MiCharge Predictor! This web app is a part of my data science project Insurance Cost Forecast by using Linear Regression, aimed at predicting the medical cost based on various personal and lifestyle factors. By leveraging advanced machine learning techniques, MiCharge Predictor provides an approximate estimates to help users understand potential medical expances.\n\n\n\n\nUser-Friendly Interface: Easily input your personal details and receive instant predictions.\n\nCopmprehensive Data Analysis: Utilizes sophisticated algorithms to analyze factors such as age, BMI, smoking habits, and more.\n\nAccessibility: Available both on web and mobile platforms.\n\n\n\n\n\nInput Your Data: Enter details such as age, gender, BMI, number of children, smoking status, and region.\n\nage: Minimum 18, maximum 100\ngender: Male or Female\nBMI: Minimum 15.0, maximum 60\nnumber of children: Takes values from 0 to 5\nsmoking: Yes or No\nregion: Takes four string input: Northeast, Northwest, Southeast, Southwest\n\nAnalyze: The algorithm process the given input\nGet Prediction: Based on the input, you get the output."
  },
  {
    "objectID": "portfolio/spd/webapp/index.html#about",
    "href": "portfolio/spd/webapp/index.html#about",
    "title": "Streamlit Web App",
    "section": "",
    "text": "Welcome to MiCharge Predictor! This web app is a part of my data science project Insurance Cost Forecast by using Linear Regression, aimed at predicting the medical cost based on various personal and lifestyle factors. By leveraging advanced machine learning techniques, MiCharge Predictor provides an approximate estimates to help users understand potential medical expances.\n\n\n\n\nUser-Friendly Interface: Easily input your personal details and receive instant predictions.\n\nCopmprehensive Data Analysis: Utilizes sophisticated algorithms to analyze factors such as age, BMI, smoking habits, and more.\n\nAccessibility: Available both on web and mobile platforms.\n\n\n\n\n\nInput Your Data: Enter details such as age, gender, BMI, number of children, smoking status, and region.\n\nage: Minimum 18, maximum 100\ngender: Male or Female\nBMI: Minimum 15.0, maximum 60\nnumber of children: Takes values from 0 to 5\nsmoking: Yes or No\nregion: Takes four string input: Northeast, Northwest, Southeast, Southwest\n\nAnalyze: The algorithm process the given input\nGet Prediction: Based on the input, you get the output."
  },
  {
    "objectID": "portfolio/spd/webapp/index.html#usage",
    "href": "portfolio/spd/webapp/index.html#usage",
    "title": "Streamlit Web App",
    "section": "Usage",
    "text": "Usage\nYou can use the app directly from streamlit web using this link or just here.."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\nSpring 2025: MAC2311 Calculus With Analytic Geometry I\n\n\n\n\n\n\n\n\n\nSpring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\n\n\n\nFall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\n\n\n\nSpring 2023: MAC1140 PreCalculus Algebra\n\n\n\n\n\n\n\n\n\nFall 2022: MAC2311 Calculus and Analytic Geometry I\n\n\n\n\n\n\n\n\n\nFall 2021 and Spring 2022: PreCalculus and Algebra\n\n\n\n\n\n\n\n\n\nFall 2018 to Spring 2020: College Algebra, Trigonometry\n\n\n\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "codepages/ecommerce/index.html",
    "href": "codepages/ecommerce/index.html",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nsalesdata = pd.read_csv('Ecommerce Customers')\nsalesdata.head()\n\n\n\n\n\n\n\n\nEmail\nAddress\nAvatar\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\n\n0\nmstephenson@fernandez.com\n835 Frank Tunnel\\nWrightmouth, MI 82180-9605\nViolet\n34.497268\n12.655651\n39.577668\n4.082621\n587.951054\n\n\n1\nhduke@hotmail.com\n4547 Archer Common\\nDiazchester, CA 06566-8576\nDarkGreen\n31.926272\n11.109461\n37.268959\n2.664034\n392.204933\n\n\n2\npallen@yahoo.com\n24645 Valerie Unions Suite 582\\nCobbborough, D...\nBisque\n33.000915\n11.330278\n37.110597\n4.104543\n487.547505\n\n\n3\nriverarebecca@gmail.com\n1414 David Throughway\\nPort Jason, OH 22070-1220\nSaddleBrown\n34.305557\n13.717514\n36.721283\n3.120179\n581.852344\n\n\n4\nmstephens@davidson-herman.com\n14023 Rodriguez Passage\\nPort Jacobville, PR 3...\nMediumAquaMarine\n33.330673\n12.795189\n37.536653\n4.446308\n599.406092"
  },
  {
    "objectID": "codepages/ecommerce/index.html#load-the-data",
    "href": "codepages/ecommerce/index.html#load-the-data",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nsalesdata = pd.read_csv('Ecommerce Customers')\nsalesdata.head()\n\n\n\n\n\n\n\n\nEmail\nAddress\nAvatar\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\n\n0\nmstephenson@fernandez.com\n835 Frank Tunnel\\nWrightmouth, MI 82180-9605\nViolet\n34.497268\n12.655651\n39.577668\n4.082621\n587.951054\n\n\n1\nhduke@hotmail.com\n4547 Archer Common\\nDiazchester, CA 06566-8576\nDarkGreen\n31.926272\n11.109461\n37.268959\n2.664034\n392.204933\n\n\n2\npallen@yahoo.com\n24645 Valerie Unions Suite 582\\nCobbborough, D...\nBisque\n33.000915\n11.330278\n37.110597\n4.104543\n487.547505\n\n\n3\nriverarebecca@gmail.com\n1414 David Throughway\\nPort Jason, OH 22070-1220\nSaddleBrown\n34.305557\n13.717514\n36.721283\n3.120179\n581.852344\n\n\n4\nmstephens@davidson-herman.com\n14023 Rodriguez Passage\\nPort Jacobville, PR 3...\nMediumAquaMarine\n33.330673\n12.795189\n37.536653\n4.446308\n599.406092"
  },
  {
    "objectID": "codepages/ecommerce/index.html#eda",
    "href": "codepages/ecommerce/index.html#eda",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "EDA",
    "text": "EDA\n\nDescriptive Statistics\n\nsalesdata.describe()\n\n\n\n\n\n\n\n\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\n\ncount\n500.000000\n500.000000\n500.000000\n500.000000\n500.000000\n\n\nmean\n33.053194\n12.052488\n37.060445\n3.533462\n499.314038\n\n\nstd\n0.992563\n0.994216\n1.010489\n0.999278\n79.314782\n\n\nmin\n29.532429\n8.508152\n33.913847\n0.269901\n256.670582\n\n\n25%\n32.341822\n11.388153\n36.349257\n2.930450\n445.038277\n\n\n50%\n33.082008\n11.983231\n37.069367\n3.533975\n498.887875\n\n\n75%\n33.711985\n12.753850\n37.716432\n4.126502\n549.313828\n\n\nmax\n36.139662\n15.126994\n40.005182\n6.922689\n765.518462\n\n\n\n\n\n\n\n\nsalesdata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 500 entries, 0 to 499\nData columns (total 8 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   Email                 500 non-null    object \n 1   Address               500 non-null    object \n 2   Avatar                500 non-null    object \n 3   Avg. Session Length   500 non-null    float64\n 4   Time on App           500 non-null    float64\n 5   Time on Website       500 non-null    float64\n 6   Length of Membership  500 non-null    float64\n 7   Yearly Amount Spent   500 non-null    float64\ndtypes: float64(5), object(3)\nmemory usage: 31.4+ KB\n\n\n\n\nVisualization\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.9, 5))\n\n# Scatter plot with regression line for 'Time on Website' vs 'Yearly Amount Spent'\nsns.scatterplot(\n    x='Time on Website', y='Yearly Amount Spent', \n    data=salesdata, ax=ax1\n    )\nsns.regplot(\n    x='Time on Website', y='Yearly Amount Spent', \n    data=salesdata, ax=ax1, scatter=False, color='blue'\n    )\nax1.set_title('Time on Website vs Yearly Amount Spent')\n\n# Scatter plot with regression line for 'Time on App' vs 'Yearly Amount Spent'\nsns.scatterplot(\n    x='Time on App', y='Yearly Amount Spent', \n    data=salesdata, ax=ax2\n    )\nsns.regplot(\n    x='Time on App', y='Yearly Amount Spent',\n    data=salesdata, ax=ax2, scatter=False, color='blue'\n    )\nax2.set_title('Time on App vs Yearly Amount Spent')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSo, from this plot, we see that Time on Website has no significant trend or pattern on Yearly Amount Spent variable. However, Time on App seems to have a linear relationship on Yearly Amount Spent.\nNext, we see the relationship between Avg. Session Length vs Yearly Amount Spent, and Length of Membership vs Yearly Amount Spent.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.9, 5))\n\n# Scatter plot with regression line for 'Time on Website' vs 'Yearly Amount Spent'\nsns.scatterplot(\n    x='Avg. Session Length', y='Yearly Amount Spent', \n    data=salesdata, ax=ax1\n    )\nsns.regplot(\n    x='Avg. Session Length', y='Yearly Amount Spent', \n    data=salesdata, ax=ax1, scatter=False, color='blue'\n    )\nax1.set_title('Avg. Session Length vs Yearly Amount Spent')\n\n# Scatter plot with regression line for 'Time on App' vs 'Yearly Amount Spent'\nsns.scatterplot(\n    x='Length of Membership', y='Yearly Amount Spent', \n    data=salesdata, ax=ax2\n    )\nsns.regplot(\n    x='Length of Membership', y='Yearly Amount Spent', \n    data=salesdata, ax=ax2, scatter=False, color='blue'\n    )\nax2.set_title('Length of Membership vs Yearly Amount Spent')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBoth of these features have impact on the dependent variable. However, Length of Membership seems to have the most significant impact on Yearly Amount Spent.\n\nsns.pairplot(salesdata)"
  },
  {
    "objectID": "codepages/ecommerce/index.html#modeling",
    "href": "codepages/ecommerce/index.html#modeling",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Modeling",
    "text": "Modeling\n\nTraining\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nX = salesdata[\n    ['Avg. Session Length', 'Time on App',\n    'Time on Website', 'Length of Membership']\n    ]\ny = salesdata['Yearly Amount Spent']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size = 0.30, random_state=123\n)\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\nprint('Coefficients: \\n', linreg.coef_)\n\nCoefficients: \n [25.36266491 38.82367921  0.80356799 61.54905291]\n\n\n\n\nTesting\n\npred = linreg.predict(X_test)\nplt.scatter(y_test, pred)\nplt.xlabel('y test')\nplt.ylabel('predicted y')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nModel Evaluation\n\nfrom sklearn import metrics\n\nprint('MAE', metrics.mean_absolute_error(y_test, pred))\nprint('MSE', metrics.mean_squared_error(y_test, pred))\nprint('RMSE', metrics.root_mean_squared_error(y_test, pred))\nprint('R-squared:', metrics.r2_score(y_test, pred))\n\nMAE 7.9880791942451\nMSE 102.72313941866005\nRMSE 10.135242444986703\nR-squared: 0.9845789607829495\n\n\n\n\nResidual Analysis\n\nsns.displot(y_test-pred, bins= 60, kde=True)"
  },
  {
    "objectID": "codepages/ecommerce/index.html#conclusion",
    "href": "codepages/ecommerce/index.html#conclusion",
    "title": "Feature Selection: A linear regression approach to find the impact of the features of e-commerce sales data",
    "section": "Conclusion",
    "text": "Conclusion\n\ncoeff = pd.DataFrame({\n    'Feature': ['Intercept'] + list(X.columns), \n    'Coefficient': [linreg.intercept_] + list(linreg.coef_) \n})\n\ncoeff\n\n\n\n\n\n\n\n\nFeature\nCoefficient\n\n\n\n\n0\nIntercept\n-1054.215476\n\n\n1\nAvg. Session Length\n25.362665\n\n\n2\nTime on App\n38.823679\n\n\n3\nTime on Website\n0.803568\n\n\n4\nLength of Membership\n61.549053"
  },
  {
    "objectID": "codepages/lendingclub/index.html#data",
    "href": "codepages/lendingclub/index.html#data",
    "title": "Lendingclub’s loan default prediction",
    "section": "Data",
    "text": "Data\nkaggle.api.authenticate()\nkaggle.api.dataset_download_files(\n    'jeandedieunyandwi/lending-club-dataset', path='.', unzip=True\n)"
  },
  {
    "objectID": "codepages/lendingclub/index.html#primary-libraries",
    "href": "codepages/lendingclub/index.html#primary-libraries",
    "title": "Lendingclub’s loan default prediction",
    "section": "Primary Libraries",
    "text": "Primary Libraries\n\nfrom mywebstyle import plot_style\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport kaggle\nplot_style('#f4f4f4')\nloandata = pd.read_csv('lending_club_loan_two.csv')\nfirst_13_cols = loandata.iloc[:, :13]\nfirst_13_cols.head()\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nint_rate\ninstallment\ngrade\nsub_grade\nemp_title\nemp_length\nhome_ownership\nannual_inc\nverification_status\nissue_d\nloan_status\n\n\n\n\n0\n10000\n36 months\n11.44\n329.48\nB\nB4\nMarketing\n10+ years\nRENT\n117000.0\nNot Verified\nJan-15\nFully Paid\n\n\n1\n8000\n36 months\n11.99\n265.68\nB\nB5\nCredit analyst\n4 years\nMORTGAGE\n65000.0\nNot Verified\nJan-15\nFully Paid\n\n\n2\n15600\n36 months\n10.49\n506.97\nB\nB3\nStatistician\n&lt; 1 year\nRENT\n43057.0\nSource Verified\nJan-15\nFully Paid\n\n\n3\n7200\n36 months\n6.49\n220.65\nA\nA2\nClient Advocate\n6 years\nRENT\n54000.0\nNot Verified\nNov-14\nFully Paid\n\n\n4\n24375\n60 months\n17.27\n609.33\nC\nC5\nDestiny Management Inc.\n9 years\nMORTGAGE\n55000.0\nVerified\nApr-13\nCharged Off\n\n\n\n\n\n\n\n\nsecond_13_cols = loandata.iloc[:, 13:]\nsecond_13_cols.head()\n\n\n\n\n\n\n\n\npurpose\ntitle\ndti\nearliest_cr_line\nopen_acc\npub_rec\nrevol_bal\nrevol_util\ntotal_acc\ninitial_list_status\napplication_type\nmort_acc\npub_rec_bankruptcies\naddress\n\n\n\n\n0\nvacation\nVacation\n26.24\nJun-90\n16\n0\n36369\n41.8\n25\nw\nINDIVIDUAL\n0.0\n0.0\n0174 Michelle Gateway\\nMendozaberg, OK 22690\n\n\n1\ndebt_consolidation\nDebt consolidation\n22.05\nJul-04\n17\n0\n20131\n53.3\n27\nf\nINDIVIDUAL\n3.0\n0.0\n1076 Carney Fort Apt. 347\\nLoganmouth, SD 05113\n\n\n2\ncredit_card\nCredit card refinancing\n12.79\nAug-07\n13\n0\n11987\n92.2\n26\nf\nINDIVIDUAL\n0.0\n0.0\n87025 Mark Dale Apt. 269\\nNew Sabrina, WV 05113\n\n\n3\ncredit_card\nCredit card refinancing\n2.60\nSep-06\n6\n0\n5472\n21.5\n13\nf\nINDIVIDUAL\n0.0\n0.0\n823 Reid Ford\\nDelacruzside, MA 00813\n\n\n4\ncredit_card\nCredit Card Refinance\n33.95\nMar-99\n13\n0\n24584\n69.8\n43\nf\nINDIVIDUAL\n1.0\n0.0\n679 Luna Roads\\nGreggshire, VA 11650"
  },
  {
    "objectID": "codepages/lendingclub/index.html#exploratory-data-analysis",
    "href": "codepages/lendingclub/index.html#exploratory-data-analysis",
    "title": "Lendingclub’s loan default prediction",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nDescriptive Statistics\n\nloandata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 395900 entries, 0 to 395899\nData columns (total 27 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   loan_amnt             395900 non-null  int64  \n 1   term                  395900 non-null  object \n 2   int_rate              395900 non-null  float64\n 3   installment           395900 non-null  float64\n 4   grade                 395900 non-null  object \n 5   sub_grade             395900 non-null  object \n 6   emp_title             372982 non-null  object \n 7   emp_length            377608 non-null  object \n 8   home_ownership        395900 non-null  object \n 9   annual_inc            395900 non-null  float64\n 10  verification_status   395900 non-null  object \n 11  issue_d               395900 non-null  object \n 12  loan_status           395900 non-null  object \n 13  purpose               395900 non-null  object \n 14  title                 394145 non-null  object \n 15  dti                   395900 non-null  float64\n 16  earliest_cr_line      395900 non-null  object \n 17  open_acc              395900 non-null  int64  \n 18  pub_rec               395900 non-null  int64  \n 19  revol_bal             395900 non-null  int64  \n 20  revol_util            395624 non-null  float64\n 21  total_acc             395900 non-null  int64  \n 22  initial_list_status   395900 non-null  object \n 23  application_type      395900 non-null  object \n 24  mort_acc              358117 non-null  float64\n 25  pub_rec_bankruptcies  395365 non-null  float64\n 26  address               395900 non-null  object \ndtypes: float64(7), int64(5), object(15)\nmemory usage: 81.6+ MB\n\n\n\nsns.heatmap(loandata.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n\n\n\n\n\n\n\n\nSeems like there are some missing data in the emp_length, emp_title, and mort_acc columns. Around 4.62\\(\\%\\) missing data in emp_length column, 5.79\\(\\%\\) missing data in emp_title column, and 9.54\\(\\%\\) in the mort_acc coulumn. Also, there is very small proportion, 0.07\\(\\%\\) of missing data in the revol_util column.   We need to take care of this missing observations. We may drop the missing values if it is insignificantly missing or doesn’t seem to be very strongly related to the predictive/dependent variable.\n\n\nloandata.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nloan_amnt\n395900.0\n14114.249305\n8357.637338\n500.00\n8000.00\n12000.00\n20000.00\n40000.00\n\n\nint_rate\n395900.0\n13.639385\n4.472112\n5.32\n10.49\n13.33\n16.49\n30.99\n\n\ninstallment\n395900.0\n431.859947\n250.733444\n16.08\n250.33\n375.43\n567.30\n1533.81\n\n\nannual_inc\n395900.0\n74206.819251\n61645.032777\n0.00\n45000.00\n64000.00\n90000.00\n8706582.00\n\n\ndti\n395900.0\n17.379187\n18.021550\n0.00\n11.28\n16.91\n22.98\n9999.00\n\n\nopen_acc\n395900.0\n11.311081\n5.137591\n0.00\n8.00\n10.00\n14.00\n90.00\n\n\npub_rec\n395900.0\n0.178204\n0.530716\n0.00\n0.00\n0.00\n0.00\n86.00\n\n\nrevol_bal\n395900.0\n15844.331435\n20589.846553\n0.00\n6026.00\n11181.00\n19620.00\n1743266.00\n\n\nrevol_util\n395624.0\n53.793449\n24.452575\n0.00\n35.80\n54.80\n72.90\n892.30\n\n\ntotal_acc\n395900.0\n25.414622\n11.887279\n2.00\n17.00\n24.00\n32.00\n151.00\n\n\nmort_acc\n358117.0\n1.814091\n2.148006\n0.00\n0.00\n1.00\n3.00\n34.00\n\n\npub_rec_bankruptcies\n395365.0\n0.121647\n0.356176\n0.00\n0.00\n0.00\n0.00\n8.00\n\n\n\n\n\n\n\n\n\nData Visualization\n\nLoan Status\nFirst, let’s look at the class balance in the dependent variable\n\nsns.countplot(x='loan_status', data=loandata)\nfp = np.round(\n    len(loandata[loandata['loan_status'] == 'Fully Paid'])/len(loandata)*100, 2\n)\n\nco = np.round(\n    len(loandata[loandata['loan_status'] == 'Charged Off']) /\n    len(loandata)*100, 2\n)\n\n\n\n\n\n\n\n\nSo, 80.39\\(\\%\\) are Fully Paid category where as 19.61\\(\\%\\) is labeled as Charged Off or defaulter.\n\nInsights:  With 80.39 % labeled as “Fully Paid” and 19.61 % as “Charged Off,” this dataset qualifies as moderately imbalanced. The minority class (“Charged Off”) is well underrepresented, but not severely so. This level of imbalance can still impact model performance, especially if the model is biased towards predicting the majority class . Techniques like resampling(oversampling the minority class or undersampling the majority), using metrics like F1-score or AUC-ROC, or even experimenting with algorithms specifically designed for imbalanced datasets(such as XGBoost or balanced random forests) could be effective ways to address this imbalance in the predictive modeling process.\n\n\n\nLoan Amount and Installment Interaction With Loan Status\nNext, let’s see how the features interacts with the dependent variable\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nloandata[loandata['loan_status'] == 'Charged Off']['loan_amnt'].hist(\n    alpha=0.5, color='red', bins=30, ax=ax1,\n    label='Charged Off'\n)\nloandata[loandata['loan_status'] == 'Fully Paid']['loan_amnt'].hist(\n    alpha=0.5, color='blue', bins=30, ax=ax1,\n    label='Fully Paid'\n)\nax1.set_title('Loan Amount Distribution')\nax1.set_xlabel('Loan Amount')\nax1.legend()\n\nax2 = fig.add_subplot(122)\nloandata[loandata['loan_status'] == 'Charged Off']['installment'].hist(\n    alpha=0.5, color='red', bins=30, ax=ax2,\n    label='Charged Off'\n)\nloandata[loandata['loan_status'] == 'Fully Paid']['installment'].hist(\n    alpha=0.5, color='blue', bins=30, ax=ax2,\n    label='Fully Paid'\n)\nax2.set_title('Installment Distribution')\nax2.set_xlabel('Installment')\nax2.legend()\n\n\n\n\n\n\n\n\nBoth the loan_amnt and installment are slightly positively skewed. How about their mean, upper quartile, lower quartile based on loan_status?\n\nfig = plt.figure(figsize=(8.8, 4))\nax1 = fig.add_subplot(121)\nsns.boxplot(\n    x='loan_status', y='loan_amnt', hue='loan_status',\n    data=loandata, ax=ax1, palette='winter'\n)\nax1.set_title('Loan Amount Boxplot')\n\nax2 = fig.add_subplot(122)\nsns.boxplot(\n    x='loan_status', y='installment', hue='loan_status',\n    data=loandata, ax=ax2, palette='winter'\n)\nax2.set_title('Installment Boxplot')\n\nText(0.5, 1.0, 'Installment Boxplot')\n\n\n\n\n\n\n\n\n\nInsights:  From the above plot we see that mean loan amount of the fully paid and charged off categories are \\(\\$12, 500\\) and \\(\\$14, 000\\), respectively.\n\n\nTerm, Grade, and Sub-grade Interaction With Loan Status\n\nfig = plt.figure(figsize=(8.8, 4))\nax1 = fig.add_subplot(121)\nsns.countplot(\n    x='loan_status',\n    hue='term', data=loandata,\n    palette='RdBu_r', ax=ax1\n)\nax2 = fig.add_subplot(122)\nsns.countplot(\n    x='loan_status',\n    hue='grade', data=loandata,\n    palette='winter', ax=ax2\n)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.histplot(x='sub_grade', hue='loan_status', data=loandata, ax=ax)\n\n\n\n\n\n\n\n\n\n\nEmployment Title and Employment Length\n\nloandata['emp_title'] = loandata['emp_title'].str.lower()\nloandata.emp_title.value_counts()[:25]\n\nemp_title\nmanager                     5635\nteacher                     5426\nregistered nurse            2626\nsupervisor                  2589\nsales                       2381\ndriver                      2306\nowner                       2200\nrn                          2072\nproject manager             1776\noffice manager              1638\ngeneral manager             1460\ntruck driver                1288\ndirector                    1192\nengineer                    1187\npolice officer              1041\nvice president               961\nsales manager                961\noperations manager           960\nstore manager                941\npresident                    877\nadministrative assistant     865\naccountant                   845\naccount manager              845\ntechnician                   839\nmechanic                     753\nName: count, dtype: int64\n\n\nLet’s work with the employment length column\n\nloandata['emp_length'] = loandata['emp_length'].replace({\n    '&lt; 1 year': 0,\n    '1 year': 1,\n    '2 years': 2,\n    '3 years': 3,\n    '4 years': 4,\n    '5 years': 5,\n    '6 years': 6,\n    '7 years': 7,\n    '8 years': 8,\n    '9 years': 9,\n    '10+ years': 10\n}\n).infer_objects(copy=False)\n\nloandata['emp_length_group'] = pd.cut(\n    loandata['emp_length'],\n    bins=[-1, 2, 7, 10],  # Bins: &lt;3 years, 3-7 years, &gt; 7 years\n    labels=['Short-term', 'Mid-term', 'Long-term']\n)\n\nsns.countplot(\n    x='emp_length_group',\n    hue='loan_status',\n    data=loandata,\n    palette='winter',\n    stat='count'\n)\n\n/tmp/ipykernel_3242/3832649202.py:1: FutureWarning:\n\nDowncasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n\n\n\n\n\n\n\n\n\n\n\nInsights:  So, from this plot we can see a trend. As employment length goes up, the chances of fully paid gets higher, but the charged of situation remains almost same, irrespective of the employment length. Therefore, it may not be very useful when we may think of data imputation for the missing values.\n\n\n\nHome Ownership and Annual Income\n\nfig = plt.figure(figsize=(8, 10))\nax1 = fig.add_subplot(211)\nsns.boxplot(\n    x='loan_status', y='annual_inc',\n    hue='loan_status', palette='winter',\n    data=loandata, ax=ax1\n)\nax1.set_title('Income Distribution')\nax1.set_xlabel('Loan Status')\nax1.set_ylabel('Annual Income')\n\nax2 = fig.add_subplot(212)\nsns.countplot(\n    x='home_ownership', hue='loan_status',\n    data=loandata, ax=ax2\n)\n\n\n\n\n\n\n\n\nInsights:  From the Annual Income column, there is not enough insights based on the plot. But for the Home Ownership plot shows that, if the house is owned, it’s less likely to be charged off.\n\n\nIssue Date and Verification Status\n\nloandata['issue_d'] = pd.to_datetime(\n    loandata['issue_d'], format='%b-%y'\n)\nloandata = loandata.sort_values('issue_d')\nloan_status_trend = loandata.groupby(\n    ['issue_d', 'loan_status']).size().unstack()\n\nfig = plt.figure(figsize=(8, 10))\nax1 = fig.add_subplot(211)\nloan_status_trend.plot(\n    kind='line', marker='o', ax=ax1\n)\nax1.set_title('Loan Status Over Time by Issue Date')\nax1.set_xlabel('Issue Date(mm-yyyy)')\nax1.set_ylabel('Number of Loans')\nax1.legend(title='Loan Status')\n\nax2 = fig.add_subplot(212)\nsns.countplot(\n    x='verification_status', hue='loan_status',\n    data=loandata, palette='winter', ax=ax2\n)\n\n\n\n\n\n\n\n\n\nInsights:  From issue date plot we see that most of loans that were marked as charged off happened during the year 2012 to 2016, with the pick in in 2015. For the verification status, we can see the less charged off incidents when the source was not varified.\n\n\n\nPurpose and Debt-to-Income Ratio\n\nloandata['purpose'].value_counts()\nfig = plt.figure(figsize=(7.9, 4))\nax1 = fig.add_subplot(121)\nsns.countplot(\n    y='purpose', hue='loan_status',\n    data=loandata, palette='coolwarm'\n)\nax1.set_title('Loan Purpose')\n\nax2 = fig.add_subplot(122)\ndti_threshold = loandata['dti'].quantile(0.95)\nfiltereddata = loandata[loandata['dti'] &lt;= dti_threshold]\n\nsns.boxplot(\n    x='loan_status', y='dti',\n    hue='loan_status', data=filtereddata,\n    palette='coolwarm', ax=ax2\n)\nax2.set_title('Debt-to-Income Ratio on Loan Status')\n\nText(0.5, 1.0, 'Debt-to-Income Ratio on Loan Status')\n\n\n\n\n\n\n\n\n\n\nInsights: From the purpose column, we see that most of the loans that were charged off were used to make debt consolidation. Therefore, debt consolidation may have been a significant factor when a loan is charged off. Another insight we obtain from the debt-to-income ratio is that the charged off loans have higher dti ratio\n\n\n\nNumber of Credit Accounts and Number of Public Records\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nfiltered_open_account_threshold = loandata['open_acc'].quantile(0.98)\nfiltered_open_account = loandata[loandata['open_acc']\n                                 &lt;= filtered_open_account_threshold]\nfiltered_open_account[filtered_open_account['loan_status'] == 'Fully Paid']['open_acc'].hist(\n    alpha=0.5, color='green', bins=30,label='Fully Paid' ,ax=ax1\n)\nfiltered_open_account[filtered_open_account['loan_status'] == 'Charged Off']['open_acc'].hist(\n    alpha=0.5, color='red', bins=30,label='Charged Off' ,ax=ax1\n)\nax1.set_xlabel('Number of opened credit acc.')\nax1.legend()\nax1.set_title('Number of open credit accounts and Loan Status')\n\nloandata['pub_rec_group'] = pd.cut(\n    loandata['pub_rec'], bins=[-1, 0, 1, 3, loandata['pub_rec'].max()],\n    labels=['0', '1', '2-3', '4+']\n)\nloan_status_by_pub_rec = loandata.groupby(\n    ['pub_rec_group', 'loan_status'], observed=False\n).size().unstack()\n\nax2 = fig.add_subplot(122)\nloan_status_by_pub_rec.plot(\n    kind='bar', stacked=False, edgecolor='black',\n    color=['#1f77b4', '#ff7f0e'], ax=ax2\n)\nax2.set_title('Public Records and Loan Status')\nax2.set_xlabel('Public Records')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n\n\n\n\n\n\n\n\n\nsns.boxplot(\n    y='open_acc', x='loan_status',\n    hue='loan_status',\n    data=filtered_open_account\n)\n\n\n\n\n\n\n\n\n\nInsights: Number of credit account seems normally distributed among both groups except for some outliers. However, the mean number of credit accounts are slightly higher for the charged off category than the fully paid category. So, higher credit account has some sort of relation with loan being charged off. Also, people who doesn’t have any public record seems to have higher chance of loan status being charged off\n\n\n\nRevolving Balance and Utilization\n\nfig = plt.figure(figsize=(9,4))\nax1 = fig.add_subplot(121)\nrevol_bal_threshold = loandata['revol_bal'].quantile(0.95)\nfiltered_revol_bal = loandata[loandata['revol_bal']&lt;=revol_bal_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_bal',\n    data=filtered_revol_bal, hue='loan_status',\n    palette='Set2',ax=ax1\n)\nax1.set_xlabel('Loan Status')\nax1.set_ylabel('Revolving Balance')\nax1.set_title('Revolving Balance vs Loan Status')\n\nax2 = fig.add_subplot(122)\nrevol_util_threshold = loandata['revol_util'].quantile(0.95)\nfiltered_revol_util = loandata[loandata['revol_util']&lt;=revol_util_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_util',\n    hue='loan_status', data=filtered_revol_util,\n    palette='Set1',ax=ax2\n)\nax2.set_xlabel('Loan Status')\nax2.set_ylabel('Revolving Utilization')\nax2.set_title('Revolving Utilization vs Loan Status')\n\nText(0.5, 1.0, 'Revolving Utilization vs Loan Status')\n\n\n\n\n\n\n\n\n\n\nInsights: From the above plots, we don’t see much difference between fully paid or charged off status regarding the revolving balance, however, for revolving utilization, it’s higher for the charged off category.\n\n\n\nTotal Account and Initial List Status\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nfiltered_total_account_threshold = loandata['total_acc'].quantile(0.95)\nfiltered_total_account = loandata[loandata['total_acc']\n                                 &lt;= filtered_total_account_threshold]\nfiltered_total_account[filtered_total_account['loan_status'] == 'Fully Paid']['total_acc'].hist(\n    alpha=0.5, color='green', bins=30,label='Fully Paid' ,ax=ax1\n)\nfiltered_total_account[filtered_total_account['loan_status'] == 'Charged Off']['total_acc'].hist(\n    alpha=0.5, color='red', bins=30,label='Charged Off' ,ax=ax1\n)\nax1.set_xlabel('Number of Total Credit Acc.')\nax1.legend()\nax1.set_title('Number of Total credit accounts and Loan Status')\n\n\nax2 = fig.add_subplot(122)\nsns.countplot(\n    x='initial_list_status',hue='loan_status',\n    data=loandata, palette='winter'\n)\nax2.set_title('Initial Status and Loan Status')\nax2.set_xlabel('Initial Status (Funded or Withdrawn)')\n\nText(0.5, 0, 'Initial Status (Funded or Withdrawn)')\n\n\n\n\n\n\n\n\n\nnext,\n\nsns.boxplot(\n    y='total_acc', x='loan_status',\n    hue='loan_status',\n    data=filtered_total_account\n)\n\n\n\n\n\n\n\n\n\nInsights: The number of total credit account seems to have no impact on loan status based on the plottings above. Similarly, Initial Loan Status also doesn’t give much information.\n\n\n\nNumber of Mortgage Account and Number of Public Record of Bankruptcies\n\nfig = plt.figure(figsize=(9,4))\n\nax1 = fig.add_subplot(121)\n\nloandata['mort_acc_group'] = pd.cut(\n    loandata['mort_acc'], bins=[-1, 0, 2, 5, 10, loandata['mort_acc'].max()],\n    labels=['0', '1-2', '3-5', '6-10', '10+']\n)\n\nmort_acc_counts = loandata.groupby(\n    ['mort_acc_group', 'loan_status'],observed=False\n).size().unstack()\n\nmort_acc_counts.plot(\n    kind='bar', stacked=False, \n    colormap='viridis', ax=ax1\n)\nax1.set_title('Loan Status by the # of Mort. Acc')\nax1.set_xlabel('Number of Mortgage Accounts (Grouped)')\nax1.set_ylabel('Number of Loans')\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)\nax1.legend(title='Loan Status')\n\nax2 = fig.add_subplot(122)\n\nloandata['pub_rec_bankruptcies_group'] = pd.cut(\n    loandata['pub_rec_bankruptcies'], bins=[-1, 0, loandata['pub_rec_bankruptcies'].max()],\n    labels=['0', '1 or More']\n)\n\n# Plot the grouped bar chart\npub_rec_bankruptcies_counts = loandata.groupby(\n    ['pub_rec_bankruptcies_group', 'loan_status'],observed=False\n).size().unstack()\n\npub_rec_bankruptcies_counts.plot(\n    kind='bar', stacked=False,\n    colormap='coolwarm', ax=ax2\n)\nax2.set_title('Loan Status by the # of Pub Rec of Bankruptcies')\nax2.set_xlabel('Public Record of Bankruptcies (Grouped)')\nax2.set_ylabel('Number of Loans')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\nax2.legend(title='Loan Status')\nplt.show()\n\n\n\n\n\n\n\n\nInsigths: Again, loans where the applicants has less number of mortgage acc or public record of bankruptcies are more likely to be tagged as charged off.\n\n\nApplication Type\n\nsns.countplot(\n    x='loan_status', hue= 'application_type', \n    data=loandata, palette='winter'\n)\n\n\n\n\n\n\n\n\nWe see only the individual type applications have impact on loans being charged off.\nNow that we have analyzed all the features, how about the correlations among the features?\n\nnumeric_loandata = loandata.select_dtypes(include=['float64','int64'])\nplt.figure(figsize=(10,8))\nsns.heatmap(numeric_loandata.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nConclusion"
  },
  {
    "objectID": "codepages/medicalcost/index.html",
    "href": "codepages/medicalcost/index.html",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "import pandas as pd\n\ninsurance = pd.read_csv('insurance.csv')\n\ninsurance.sample(5, random_state=111)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n1000\n30\nmale\n22.99\n2\nyes\nnorthwest\n17361.7661\n\n\n53\n36\nmale\n34.43\n0\nyes\nsoutheast\n37742.5757\n\n\n432\n42\nmale\n26.90\n0\nno\nsouthwest\n5969.7230\n\n\n162\n54\nmale\n39.60\n1\nno\nsouthwest\n10450.5520\n\n\n1020\n51\nmale\n37.00\n0\nno\nsouthwest\n8798.5930\n\n\n\n\n\n\n\n\n\n\n\n\n\ninsurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\nNo missing data. Total 1338 observations.\n\n\n\nStatistical properties of the non-categorical variables\n\nprint(insurance.describe())\n\n               age          bmi     children       charges\ncount  1338.000000  1338.000000  1338.000000   1338.000000\nmean     39.207025    30.663397     1.094918  13270.422265\nstd      14.049960     6.098187     1.205493  12110.011237\nmin      18.000000    15.960000     0.000000   1121.873900\n25%      27.000000    26.296250     0.000000   4740.287150\n50%      39.000000    30.400000     1.000000   9382.033000\n75%      51.000000    34.693750     2.000000  16639.912515\nmax      64.000000    53.130000     5.000000  63770.428010\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2,3,figsize = (15,9))\n\nsns.histplot(insurance['age'], color='red', kde=True, ax= axes[0, 0]).set_title('Age Distribution')\n\nsns.histplot(insurance['bmi'], color='green', kde=True, ax= axes[0,1]).set_title('BMI Distribution')\n\nsns.histplot(insurance['charges'],color='blue', kde=True, ax= axes[0,2]).set_title('Charge Distribution')\n\nsns.countplot(x='smoker', data=insurance, hue='sex', palette='Set2', ax=axes[1,0]).set_title('Smoker vs Gender')\n\nsns.countplot(x=insurance['region'], hue=insurance['region'], palette='Set1', ax=axes[1,1]).set_title('Region Distribution')\n\nsns.countplot(x=insurance['children'], hue=insurance['children'],legend=False,palette='Set2', ax=axes[1,2]).set_title('Children Distribution')\n\nplt.gcf().patch.set_facecolor('#f4f4f4')\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1,2, figsize=(9.5,4))\ng=sns.stripplot(data=insurance, x='smoker', y='charges', hue='smoker' ,palette=['blue', 'orange'], legend=True, ax=axes[0])\ng.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].scatter(insurance.loc[insurance.smoker=='yes'].bmi,\n            insurance.loc[insurance.smoker=='yes'].charges, label=\"yes\", marker='o',\n            s=60,edgecolors='black', c='orange'\n            )\naxes[1].set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].scatter(insurance.loc[insurance.smoker=='no'].bmi,\n            insurance.loc[insurance.smoker=='no'].charges, label=\"no\", marker='v',\n            s=60,edgecolors='black', c='lightblue'\n            )\naxes[1].set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].set_xlabel('bmi')\naxes[1].set_ylabel('charges')\naxes[1].legend()\n\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nClearly from the plots above we can see that the somoking status has effect on the insurance charges in relation with bmi\n\nfig, axes = plt.subplots(1,2,figsize=(9.5,4))\n\ng1=sns.stripplot(x='region', y='charges', data=insurance, ax=axes[0])\ng1.set_xticklabels(['SW', 'SE', 'NW','NE'])\ng1.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng2=sns.scatterplot(x='age', y='charges', data=insurance, hue='smoker' ,ax=axes[1])\ng2.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\n\ng1=sns.stripplot(x='children', y='charges',data=insurance,hue='children',palette='Set1', ax=axes[0])\ng1.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng1.set_facecolor('#f4f4f4')\ng2=sns.boxplot(x='sex', y='charges', data=insurance, hue='sex', palette='Set2', ax=axes[1])\ng2.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng2.set_facecolor('#f4f4f4')\n\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nTo see the combined effect of all the features\n\nplt.figure(figsize=(12,6))\ng = sns.FacetGrid(insurance, col='smoker', row='sex',hue='region', margin_titles=True, height=2.4, aspect=1.5)\ng.map(sns.scatterplot, 'age','charges')\n\ng.fig.patch.set_facecolor('#f4f4f4')\ng.add_legend()\nplt.show()\n\n&lt;Figure size 1152x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above plots, we can see that age feature stacks in three layers for charges. It maybe depending on other categorical features such as smoking status.\n\n\n\n\n\n\n\ncorr_matrix = insurance[['age','bmi','charges']].corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Matrix')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats as st\nanova_sex, p_value1 = st.f_oneway(\n    insurance[insurance['sex']=='male']['charges'],\n    insurance[insurance['sex']=='female']['charges']\n)\n\nanova_smoker, p_value2 = st.f_oneway(\n    insurance[insurance['smoker']=='yes']['charges'],\n    insurance[insurance['smoker']=='no']['charges']\n)\n\nanova_region, p_value3 = st.f_oneway(\n    insurance[insurance['region']=='southwest']['charges'],\n    insurance[insurance['region']=='southeast']['charges'],\n    insurance[insurance['region']=='northwest']['charges'],\n    insurance[insurance['region']=='northeast']['charges']\n)\n\nanova_children, p_value4 = st.f_oneway(\n    insurance[insurance['children']==0]['charges'],\n    insurance[insurance['children']==1]['charges'],\n    insurance[insurance['children']==2]['charges'],\n    insurance[insurance['children']==3]['charges'],\n    insurance[insurance['children']==4]['charges'],\n    insurance[insurance['children']==5]['charges']\n)\n\nanova_results = {\n    'feature_name': ['sex', 'smoker', 'region', 'children'],\n    'F-Statistic':[anova_sex, anova_smoker,anova_region,anova_children],\n    'p-value':[p_value1, p_value2, p_value3, p_value4]\n}\n\nanova = pd.DataFrame(anova_results)\nprint(anova)\n\n  feature_name  F-Statistic        p-value\n0          sex     4.399702   3.613272e-02\n1       smoker  2177.614868  8.271436e-283\n2       region     2.969627   3.089336e-02\n3     children     3.296920   5.785681e-03\n\n\n\n\n\nBoth age and bmi features are positively correlated to charges with correlation coefficients \\(0.3\\) and \\(0.2\\), respectively. Since the \\(p\\)-values are less thatn \\(0.05\\), therefore, all the categorical features have impact on the target features.\n\n\n\n\n\n\n\n\n# Binary Encoding for the variables with two categories\nfrom sklearn.preprocessing import LabelEncoder\n\ninsurance['male'] = pd.get_dummies(insurance.sex, dtype=int)['male']\ninsurance['smoke'] = pd.get_dummies(insurance.smoker, dtype=int)['yes']\ninsurance.drop(['sex','smoker'],axis=1, inplace=True)\n\nlabel_encoder = LabelEncoder()\ninsurance['region']=label_encoder.fit_transform(insurance['region'])\n\nnew_order = ['age', 'bmi', 'male', 'smoke','children','region', 'charges']\ninsurance = insurance[new_order]\ninsurance['charges'] = insurance['charges'].round(2)\ninsurance.sample(5)\n\n\n\n\n\n\n\n\nage\nbmi\nmale\nsmoke\nchildren\nregion\ncharges\n\n\n\n\n561\n54\n32.68\n0\n0\n0\n0\n10923.93\n\n\n212\n24\n28.50\n1\n0\n2\n1\n3537.70\n\n\n532\n59\n29.70\n1\n0\n2\n2\n12925.89\n\n\n1047\n22\n52.58\n1\n1\n1\n2\n44501.40\n\n\n643\n23\n34.96\n0\n0\n3\n1\n4466.62\n\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = insurance.drop('charges', axis=1)\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values,i) for i in range(len(X.columns))]\nprint(vif_data)\n\n    feature        VIF\n0       age   7.551348\n1       bmi  10.371829\n2      male   2.001061\n3     smoke   1.256837\n4  children   1.801245\n5    region   2.924528\n\n\nSince BMI and Age have higher values for the multicolinearity, therefore we adopt the following methods\n\n\n\n\n\n\nplt.scatter(insurance.age,insurance.bmi)\nplt.xlabel('AGE')\nplt.ylabel('BMI')\nplt.title('BMI vs AGE')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nSince there is no clear linear relationship or any pattern, the Multicollinearity issue can be ignored. However, older individuals with a certain BMI range might have different risks or costs associated with their health. We could explore interaction terms like age * bmi in our model to capture any potential synergistic effects.\n\n\ninsurance.insert(6,'age_bmi',insurance.age*insurance.bmi)\ninsurance.insert(7,'age_bmi_smoke',insurance.age_bmi*insurance.smoke)\ninsurance.sample(5,random_state=111)\n\n\n\n\n\n\n\n\nage\nbmi\nmale\nsmoke\nchildren\nregion\nage_bmi\nage_bmi_smoke\ncharges\n\n\n\n\n1000\n30\n22.99\n1\n1\n2\n1\n689.70\n689.70\n17361.77\n\n\n53\n36\n34.43\n1\n1\n0\n2\n1239.48\n1239.48\n37742.58\n\n\n432\n42\n26.90\n1\n0\n0\n3\n1129.80\n0.00\n5969.72\n\n\n162\n54\n39.60\n1\n0\n1\n3\n2138.40\n0.00\n10450.55\n\n\n1020\n51\n37.00\n1\n0\n0\n3\n1887.00\n0.00\n8798.59\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX = insurance.drop('charges',axis=1)\ny = insurance['charges'].to_frame()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42)\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\nconts_features = ['age','bmi','age_bmi']\ncateg_features = ['male','smoke', 'children','region']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), conts_features)\n    ],\n    remainder= 'passthrough'\n)\nX_train_sc = preprocessor.fit_transform(X_train)\nX_test_sc = preprocessor.fit(X_test)"
  },
  {
    "objectID": "codepages/medicalcost/index.html#data-loading",
    "href": "codepages/medicalcost/index.html#data-loading",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "import pandas as pd\n\ninsurance = pd.read_csv('insurance.csv')\n\ninsurance.sample(5, random_state=111)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n1000\n30\nmale\n22.99\n2\nyes\nnorthwest\n17361.7661\n\n\n53\n36\nmale\n34.43\n0\nyes\nsoutheast\n37742.5757\n\n\n432\n42\nmale\n26.90\n0\nno\nsouthwest\n5969.7230\n\n\n162\n54\nmale\n39.60\n1\nno\nsouthwest\n10450.5520\n\n\n1020\n51\nmale\n37.00\n0\nno\nsouthwest\n8798.5930"
  },
  {
    "objectID": "codepages/medicalcost/index.html#exploratory-data-analysis-eda",
    "href": "codepages/medicalcost/index.html#exploratory-data-analysis-eda",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "insurance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\nNo missing data. Total 1338 observations.\n\n\n\nStatistical properties of the non-categorical variables\n\nprint(insurance.describe())\n\n               age          bmi     children       charges\ncount  1338.000000  1338.000000  1338.000000   1338.000000\nmean     39.207025    30.663397     1.094918  13270.422265\nstd      14.049960     6.098187     1.205493  12110.011237\nmin      18.000000    15.960000     0.000000   1121.873900\n25%      27.000000    26.296250     0.000000   4740.287150\n50%      39.000000    30.400000     1.000000   9382.033000\n75%      51.000000    34.693750     2.000000  16639.912515\nmax      64.000000    53.130000     5.000000  63770.428010\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2,3,figsize = (15,9))\n\nsns.histplot(insurance['age'], color='red', kde=True, ax= axes[0, 0]).set_title('Age Distribution')\n\nsns.histplot(insurance['bmi'], color='green', kde=True, ax= axes[0,1]).set_title('BMI Distribution')\n\nsns.histplot(insurance['charges'],color='blue', kde=True, ax= axes[0,2]).set_title('Charge Distribution')\n\nsns.countplot(x='smoker', data=insurance, hue='sex', palette='Set2', ax=axes[1,0]).set_title('Smoker vs Gender')\n\nsns.countplot(x=insurance['region'], hue=insurance['region'], palette='Set1', ax=axes[1,1]).set_title('Region Distribution')\n\nsns.countplot(x=insurance['children'], hue=insurance['children'],legend=False,palette='Set2', ax=axes[1,2]).set_title('Children Distribution')\n\nplt.gcf().patch.set_facecolor('#f4f4f4')\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1,2, figsize=(9.5,4))\ng=sns.stripplot(data=insurance, x='smoker', y='charges', hue='smoker' ,palette=['blue', 'orange'], legend=True, ax=axes[0])\ng.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].scatter(insurance.loc[insurance.smoker=='yes'].bmi,\n            insurance.loc[insurance.smoker=='yes'].charges, label=\"yes\", marker='o',\n            s=60,edgecolors='black', c='orange'\n            )\naxes[1].set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].scatter(insurance.loc[insurance.smoker=='no'].bmi,\n            insurance.loc[insurance.smoker=='no'].charges, label=\"no\", marker='v',\n            s=60,edgecolors='black', c='lightblue'\n            )\naxes[1].set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\n\naxes[1].set_xlabel('bmi')\naxes[1].set_ylabel('charges')\naxes[1].legend()\n\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nClearly from the plots above we can see that the somoking status has effect on the insurance charges in relation with bmi\n\nfig, axes = plt.subplots(1,2,figsize=(9.5,4))\n\ng1=sns.stripplot(x='region', y='charges', data=insurance, ax=axes[0])\ng1.set_xticklabels(['SW', 'SE', 'NW','NE'])\ng1.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng2=sns.scatterplot(x='age', y='charges', data=insurance, hue='smoker' ,ax=axes[1])\ng2.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\nfor ax in axes:\n    ax.set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\n\ng1=sns.stripplot(x='children', y='charges',data=insurance,hue='children',palette='Set1', ax=axes[0])\ng1.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng1.set_facecolor('#f4f4f4')\ng2=sns.boxplot(x='sex', y='charges', data=insurance, hue='sex', palette='Set2', ax=axes[1])\ng2.set_yticklabels(['0k','10k','20k','30k','40k','50k','60k','65k'])\ng2.set_facecolor('#f4f4f4')\n\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\nTo see the combined effect of all the features\n\nplt.figure(figsize=(12,6))\ng = sns.FacetGrid(insurance, col='smoker', row='sex',hue='region', margin_titles=True, height=2.4, aspect=1.5)\ng.map(sns.scatterplot, 'age','charges')\n\ng.fig.patch.set_facecolor('#f4f4f4')\ng.add_legend()\nplt.show()\n\n&lt;Figure size 1152x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above plots, we can see that age feature stacks in three layers for charges. It maybe depending on other categorical features such as smoking status.\n\n\n\n\n\n\n\ncorr_matrix = insurance[['age','bmi','charges']].corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Matrix')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats as st\nanova_sex, p_value1 = st.f_oneway(\n    insurance[insurance['sex']=='male']['charges'],\n    insurance[insurance['sex']=='female']['charges']\n)\n\nanova_smoker, p_value2 = st.f_oneway(\n    insurance[insurance['smoker']=='yes']['charges'],\n    insurance[insurance['smoker']=='no']['charges']\n)\n\nanova_region, p_value3 = st.f_oneway(\n    insurance[insurance['region']=='southwest']['charges'],\n    insurance[insurance['region']=='southeast']['charges'],\n    insurance[insurance['region']=='northwest']['charges'],\n    insurance[insurance['region']=='northeast']['charges']\n)\n\nanova_children, p_value4 = st.f_oneway(\n    insurance[insurance['children']==0]['charges'],\n    insurance[insurance['children']==1]['charges'],\n    insurance[insurance['children']==2]['charges'],\n    insurance[insurance['children']==3]['charges'],\n    insurance[insurance['children']==4]['charges'],\n    insurance[insurance['children']==5]['charges']\n)\n\nanova_results = {\n    'feature_name': ['sex', 'smoker', 'region', 'children'],\n    'F-Statistic':[anova_sex, anova_smoker,anova_region,anova_children],\n    'p-value':[p_value1, p_value2, p_value3, p_value4]\n}\n\nanova = pd.DataFrame(anova_results)\nprint(anova)\n\n  feature_name  F-Statistic        p-value\n0          sex     4.399702   3.613272e-02\n1       smoker  2177.614868  8.271436e-283\n2       region     2.969627   3.089336e-02\n3     children     3.296920   5.785681e-03\n\n\n\n\n\nBoth age and bmi features are positively correlated to charges with correlation coefficients \\(0.3\\) and \\(0.2\\), respectively. Since the \\(p\\)-values are less thatn \\(0.05\\), therefore, all the categorical features have impact on the target features."
  },
  {
    "objectID": "codepages/medicalcost/index.html#pre-processing",
    "href": "codepages/medicalcost/index.html#pre-processing",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "",
    "text": "# Binary Encoding for the variables with two categories\nfrom sklearn.preprocessing import LabelEncoder\n\ninsurance['male'] = pd.get_dummies(insurance.sex, dtype=int)['male']\ninsurance['smoke'] = pd.get_dummies(insurance.smoker, dtype=int)['yes']\ninsurance.drop(['sex','smoker'],axis=1, inplace=True)\n\nlabel_encoder = LabelEncoder()\ninsurance['region']=label_encoder.fit_transform(insurance['region'])\n\nnew_order = ['age', 'bmi', 'male', 'smoke','children','region', 'charges']\ninsurance = insurance[new_order]\ninsurance['charges'] = insurance['charges'].round(2)\ninsurance.sample(5)\n\n\n\n\n\n\n\n\nage\nbmi\nmale\nsmoke\nchildren\nregion\ncharges\n\n\n\n\n561\n54\n32.68\n0\n0\n0\n0\n10923.93\n\n\n212\n24\n28.50\n1\n0\n2\n1\n3537.70\n\n\n532\n59\n29.70\n1\n0\n2\n2\n12925.89\n\n\n1047\n22\n52.58\n1\n1\n1\n2\n44501.40\n\n\n643\n23\n34.96\n0\n0\n3\n1\n4466.62\n\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = insurance.drop('charges', axis=1)\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values,i) for i in range(len(X.columns))]\nprint(vif_data)\n\n    feature        VIF\n0       age   7.551348\n1       bmi  10.371829\n2      male   2.001061\n3     smoke   1.256837\n4  children   1.801245\n5    region   2.924528\n\n\nSince BMI and Age have higher values for the multicolinearity, therefore we adopt the following methods\n\n\n\n\n\n\nplt.scatter(insurance.age,insurance.bmi)\nplt.xlabel('AGE')\nplt.ylabel('BMI')\nplt.title('BMI vs AGE')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nSince there is no clear linear relationship or any pattern, the Multicollinearity issue can be ignored. However, older individuals with a certain BMI range might have different risks or costs associated with their health. We could explore interaction terms like age * bmi in our model to capture any potential synergistic effects.\n\n\ninsurance.insert(6,'age_bmi',insurance.age*insurance.bmi)\ninsurance.insert(7,'age_bmi_smoke',insurance.age_bmi*insurance.smoke)\ninsurance.sample(5,random_state=111)\n\n\n\n\n\n\n\n\nage\nbmi\nmale\nsmoke\nchildren\nregion\nage_bmi\nage_bmi_smoke\ncharges\n\n\n\n\n1000\n30\n22.99\n1\n1\n2\n1\n689.70\n689.70\n17361.77\n\n\n53\n36\n34.43\n1\n1\n0\n2\n1239.48\n1239.48\n37742.58\n\n\n432\n42\n26.90\n1\n0\n0\n3\n1129.80\n0.00\n5969.72\n\n\n162\n54\n39.60\n1\n0\n1\n3\n2138.40\n0.00\n10450.55\n\n\n1020\n51\n37.00\n1\n0\n0\n3\n1887.00\n0.00\n8798.59\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX = insurance.drop('charges',axis=1)\ny = insurance['charges'].to_frame()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42)\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\nconts_features = ['age','bmi','age_bmi']\ncateg_features = ['male','smoke', 'children','region']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), conts_features)\n    ],\n    remainder= 'passthrough'\n)\nX_train_sc = preprocessor.fit_transform(X_train)\nX_test_sc = preprocessor.fit(X_test)"
  },
  {
    "objectID": "codepages/medicalcost/index.html#modelling-approaches",
    "href": "codepages/medicalcost/index.html#modelling-approaches",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Modelling Approaches",
    "text": "Modelling Approaches\nWe consider the following models\n\nBaseline model: Assumption that the charges variable can be modeled with the mean value of this charges variable.\n\\[\n\\text{charges}=\\mathbb{E}[\\text{charges}]+\\xi\n\\]\nLinear Regression with age-bmi-smoke interaction\n\\[\n\\text{charges}=\\beta_0+\\beta_1 (\\text{age\\_bmi})+\\beta_2 (\\text{male})+\\beta_3 (\\text{smoke})+\\beta_4 (\\text{children})+\\beta_5 (\\text{region})+\\beta_6 (\\text{age-bmi-smoke})+\\xi\n\\]\nK-Neighbor Regression\n\\(k\\)NN using all the original feature\n\n\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\n\nkfold = KFold(n_splits=5,shuffle=True, random_state=111)\n\nmses = np.zeros((3,5))\n\nk = 10\n\nfor i, (train_index, test_index) in enumerate(kfold.split(X_train_sc)):\n    X_train_sc_train = X_train_sc[train_index]\n    X_train_sc_holdout = X_train_sc[test_index]\n\n    y_train_train = y_train.iloc[train_index]\n    y_train_holdout = y_train.iloc[test_index]\n\n    pred0 = y_train_train.charges.mean()*np.ones(len(test_index))\n\n    model1 = LinearRegression()\n    model2 = KNeighborsRegressor(k)\n\n    model1.fit(X_train_sc_train[:,2:], y_train_train)\n    model2.fit(X_train_sc_train[:,:6], y_train_train)\n\n    pred1 = model1.predict(X_train_sc_holdout[:,2:])\n    pred2 = model2.predict(X_train_sc_holdout[:,:6])\n\n\n    mses[0,i] = mean_squared_error(y_train_holdout, pred0)\n    mses[1,i] = mean_squared_error(y_train_holdout, pred1)\n    mses[2,i] = mean_squared_error(y_train_holdout, pred2)\n\nplt.scatter(np.zeros(5), mses[0,:],s=60, c='white', edgecolors='black', label='Single Split')\nplt.scatter(np.ones(5), mses[1,:], s=60, c='white', edgecolors='black')\nplt.scatter(2*np.ones(5), mses[1,:], s=60, c='white', edgecolors='black')\nplt.scatter([0,1,2],np.mean(mses, axis=1),s=60, c='r', marker='X', label='Mean')\nplt.legend(loc='upper right', fontsize=12)\nplt.xticks([0,1,2],['Baseline','LinReg','KNN Reg'])\nplt.yticks(fontsize=10)\nplt.ylabel('MSE',fontsize=12)\nplt.gca().set_facecolor('#f4f4f4')\nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(np.mean(np.sqrt(mses),axis=1))\nprint('\\n')\nprint('Minimum RMSE={} \\n Model {}'.format(min(np.mean(np.sqrt(mses),axis=1)),np.argmin(np.mean(np.sqrt(mses),axis=1))) )\n\n[12101.66177575  5938.57275531  7124.33629511]\n\n\nMinimum RMSE=5938.572755308773 \n Model 1"
  },
  {
    "objectID": "codepages/medicalcost/index.html#final-model",
    "href": "codepages/medicalcost/index.html#final-model",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Final Model",
    "text": "Final Model\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\ndata = pd.read_csv('insurance.csv')\n\nclass FeatureEngineering(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        # Initialize OneHotEncoder for 'smoker' and 'sex'\n        self.ohe_smoker_sex = OneHotEncoder(\n            drop='first', dtype=int, sparse_output=False)\n        self.label_encoder = LabelEncoder()\n\n    def fit(self, X, y=None):\n        # Fit the OneHotEncoder on smoker and sex\n        self.ohe_smoker_sex.fit(X[['smoker', 'sex']])\n        self.label_encoder.fit(X['region'])\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n\n        # Apply OneHotEncoder to 'smoker' and 'sex'\n        smoker_sex_encoded = self.ohe_smoker_sex.transform(\n            X[['smoker', 'sex']])\n        smoker_sex_columns = ['smoker_yes', 'sex_male']\n\n        # Create DataFrame for encoded variables and merge with original data\n        smoker_sex_df = pd.DataFrame(\n            smoker_sex_encoded, columns=smoker_sex_columns, index=X.index)\n        X = pd.concat([X, smoker_sex_df], axis=1)\n\n        # Label encode the 'region' column\n        X['region'] = self.label_encoder.transform(X['region'])\n\n        # Create new features\n        X['age_bmi'] = X['age'] * X['bmi']\n        X['age_bmi_smoker'] = X['age_bmi'] * X['smoker_yes']\n\n        # Drop original columns\n        X = X.drop(columns=['age', 'bmi', 'smoker', 'sex'])\n\n        return X\n\ndata = pd.read_csv('insurance.csv')\ndata['charges'] = data['charges'].round(2)\n\nX = data.drop('charges', axis=1)\ny = data['charges']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('scale', StandardScaler(), ['age_bmi', 'age_bmi_smoker'])\n    ],\n    remainder='passthrough'\n)\n\npipe = Pipeline(steps=[\n    ('feature_engineering', FeatureEngineering()),\n    ('preprocess', preprocessor),\n    ('model', LinearRegression())\n])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, shuffle=True, random_state=111\n)\npipe.fit(X_train, y_train)\nprint(np.round(pipe['model'].intercept_,2))\n\n10621.25"
  },
  {
    "objectID": "codepages/medicalcost/index.html#model-validation",
    "href": "codepages/medicalcost/index.html#model-validation",
    "title": "Insurance Cost Forecast by using Linear Regression",
    "section": "Model Validation",
    "text": "Model Validation\n\nRoot Mean Squared Error (RMSE)\n\ntrain_prediction = pipe.predict(X_train)\ntest_prediction = pipe.predict(X_test)\n\nprint(\"Training set RMSE:\",\n    np.round(np.sqrt(mean_squared_error(train_prediction,y_train)))\n)\nprint(\"Test set RMSE:\",\n    np.round(np.sqrt(mean_squared_error(test_prediction,y_test)))\n)\n\nTraining set RMSE: 5853.0\nTest set RMSE: 5600.0\n\n\n\n\nR-Squared (\\(R^2\\))\n\nfrom sklearn.metrics import r2_score\ny_pred = pipe.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nprint(f'R-squared: {r2:.4f}')\n\nR-squared: 0.8008\n\n\n\n\nResiduals\n\nres = y_test - y_pred\n\nplt.scatter(y_pred, res)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residuals Plot')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.displot(res,kind='kde')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()"
  },
  {
    "objectID": "posts/matmul/index.html",
    "href": "posts/matmul/index.html",
    "title": "Matrix multiplication: Let’s make it less expensive!",
    "section": "",
    "text": "Have you ever wondered why your code takes forever to run? Sometimes a simple code may take significant time because of an inefficient implementation approach. Let’s take a simple example of matrix multiplication, and explore the time and space complexity, specifically focusing on multiplying matrices where one of the matrices is formed as an outer product of a vector with itself.\nMatrix multiplication is a fundamental operation in many areas such as computer graphics, machine learning, and scientific computing. Given two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), the product \\(\\mathbf{AB}\\) or \\(\\mathbf{BA}\\) is a new matrix where each element is computed as the dot product of the corresponding row of \\(\\mathbf{A}\\) and the column of \\(\\mathbf{B}\\) or the other way around.\nConsider the scenario where \\(\\mathbf{A}\\) is an outer product of a column vector \\(\\mathbf{a}\\) with itself, i.e.,\n\\[\\begin{align*}\n\\mathbf{A}=\\mathbf{a} \\mathbf{a}^T&=\\begin{pmatrix}a_1\\\\a_2\\\\\\vdots \\\\a_n\\end{pmatrix}\\begin{pmatrix}a_1&a_2&\\cdots &a_n\\end{pmatrix}\\\\\n&=\\begin{pmatrix}\na_1a_1 & a_1a_2& \\cdots &a_1a_n\\\\\na_2a_1& a_2a_2&\\cdots &a_2a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_na_1 & a_na_2 &\\cdots& a_na_n\\end{pmatrix}\\\\\n\\end{align*}\n\\]\nNow simply, if \\(\\mathbf{B}\\) is another \\(n\\times n\\) matrix, then\n\\[\n\\begin{align*}\n\\mathbf{BA}&=\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1n}\\\\\nb_{21} & b_{22} & \\cdots & b_{2n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{nn}\\\\\n\\end{pmatrix}\\begin{pmatrix}\na_1a_1 & a_1a_2& \\cdots &a_1a_n\\\\\na_2a_1& a_2a_2&\\cdots &a_2a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_na_1 & a_na_2 &\\cdots& a_na_n\\end{pmatrix}\n\\end{align*}\n\\]\nLet’s analyze the complexity of this matrix matrix multiplication.\nWorst Case: The worst case scenario would be performing the multiplication naively without exploiting the rank-1 structure. How? When we compute any element in the resultant matrix \\(\\mathbf{BA}\\) or \\(\\mathbf{AB}\\) we precisely perform \\(n\\) multiplication and there are total \\(n^2\\) elements to compute for a matrix of \\(n\\times n\\). This would result in the standard matrix multiplication time complexity of \\(O(n^3)\\).\n\\[\n\\begin{align*}\n\\mathbf{BA}&=\\begin{pmatrix}\nb_{11}a_1a_1+\\cdots+b_{1n}a_na_1& b_{11}a_1a_2+\\cdots+b_{1n}a_na_2 &\\cdots &\nb_{11}a_1a_n+\\cdots+b_{1n}a_na_n\\\\\nb_{21}a_1a_1+\\cdots+b_{2n}a_na_1&b_{21}a_1a_2+\\cdots+b_{2n}a_na_2 &\\cdots &\nb_{21}a_1a_n+\\cdots+b_{2n}a_na_n\\\\\n\\vdots & \\vdots &\\ddots & \\vdots\\\\\nb_{n1}a_1a_1+\\cdots+b_{nn}a_na_1&b_{n1}a_1a_2+\\cdots+b_{nn}a_na_2 &\\cdots &\nb_{n1}a_1a_n+\\cdots+b_{nn}a_na_n\\end{pmatrix}\n\\end{align*}\n\\]\nBest Case: The best case scenario in terms of time complexity occurs when we exploit the structure of \\(\\mathbf{A}\\). Since \\(\\mathbf{A}\\) is a rank-1 matrix, we can simplify the multiplication: \\[\n\\begin{align*}\n\\mathbf{BA}&=\\mathbf{B}\\begin{pmatrix}\na_1a_1 & a_1a_2& \\cdots &a_1a_n\\\\\na_2a_1& a_2a_2&\\cdots &a_2a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_na_1 & a_na_2 &\\cdots& a_na_n\\end{pmatrix}\\\\\n&\\\\\n&=\\mathbf{B}\\begin{pmatrix}a_1 \\mathbf{a} & a_2 \\mathbf{a} &\\cdots a_n \\mathbf{a}\\end{pmatrix}\\\\\n&=\\begin{pmatrix}a_1 \\mathbf{B}\\mathbf{a} & a_2 \\mathbf{B}\\mathbf{a} &\\cdots a_n \\mathbf{B}\\mathbf{a}\\end{pmatrix}\\\\\n&= (\\mathbf{Ba}) a^T\n\\end{align*}\n\\]\nWe break this algorithm in to two steps.\nStep 1: Since \\(\\mathbf{B}\\) is a matrix of \\(n\\times n\\) and \\(\\mathbf{a}\\) is a matrix of \\(n\\times 1\\), therefore \\(\\mathbf{Ba}\\) is a matrix of size \\(n\\times 1\\) or just a vector of size \\(n\\).\n\\[\n\\begin{align*}\n\\mathbf{Ba}&=\\begin{pmatrix}\nb_{11} & b_{12} & \\cdots & b_{1n}\\\\\nb_{21} & b_{22} & \\cdots & b_{2n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{n1} & b_{n2} & \\cdots & b_{nn}\\\\\n\\end{pmatrix}\\begin{pmatrix}a_1\\\\a_2\\\\ \\vdots \\\\a_n \\end{pmatrix}\\\\\n&\\\\\n&=\\begin{pmatrix}\n    b_{11}a_1+b_{12}a_2+\\cdots b_{1n}a_n\\\\\n    b_{21}a_1+b_{22}a_2+\\cdots b_{2n}a_n\\\\\n    \\vdots\\\\\n    b_{n1}a_1+b_{n2}a_2+\\cdots b_{nn}a_n\\\\\n\\end{pmatrix}\n\\end{align*}\n\\] The matrix \\(\\mathbf{Ba}\\) contains \\(n\\) elements where each element takes \\(n\\) multiplications. Thus, computing \\(\\mathbf{Ba}\\) takes \\(O(n^2)\\) time.\nStep 2: Next, we compute \\((\\mathbf{Ba})\\mathbf{a}^T\\).\n\\[\n\\begin{align*}\n(\\mathbf{Ba})\\mathbf{a}^T&=\\begin{pmatrix}ba_1\\\\ ba_2\\\\ \\vdots\\\\ ba_n \\end{pmatrix}\n\\begin{pmatrix}a_{1}& a_{2}& \\cdots a_{n} \\end{pmatrix}\\\\\n&\\\\\n&=\\begin{pmatrix}\n(ba_1)a_1 & (ba_1)a_2 &\\cdots &(ba_1)a_n\\\\\n(ba_2)a_1 & (ba_2)a_2 &\\cdots &(ba_2)a_n\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n(ba_n)a_1 & (ba_n)a_2 &\\cdots &(ba_n)a_n\\\\\n\\end{pmatrix}\n\\end{align*}\n\\] Forming the outer product of \\(\\mathbf{Ba}\\) and \\(\\mathbf{a}^T\\) also takes \\(O(n^2)\\) time. Thus, the best case time complexity is \\(O(n^2)\\).\nWell, how about the other way around? What’s the optimal strategy for \\(\\mathbf{AB}\\)? We can reach similar results in the following way \\[\n\\begin{align*}\n\\mathbf{AB}&=(\\mathbf{a} \\mathbf{a}^T) \\mathbf{B} = \\mathbf{a} (\\mathbf{a}^T \\mathbf{B})\n\\end{align*}\n\\]\nHere, \\(\\mathbf{a}^T \\mathbf{B}\\) is a row vector of size \\(n\\). Computing \\(\\mathbf{a}^T \\mathbf{B}\\) takes \\(O(n^2)\\) time. Then, multiplying the column vector \\(\\mathbf{a}\\) by the resulting row vector forms an \\(n \\times n\\) matrix, also in \\(O(n^2)\\) time. Thus, the best case time complexity is \\(O(n^2)\\). Note, that \\(\\mathbf{AB}\\ne \\mathbf{BA}\\).\nComparison: So, what’s the big difference? There is a significant difference in two algorithms. In the first algorithm the time complexity is \\(O(n^3)\\) where as in the second algorithm the time complexity is \\(O(n^2)+O(n^2)\\) or \\(2O(n^2)\\) or just \\(C\\hspace{1mm} O(n^2)\\). For example, if \\(n=500\\) then the first algorithm requires 125 million multiplications and the second one just takes 500,000 multiplications which is 250 times faster.\nUnderstanding the structure of the matrices involved in multiplication can significantly optimize the performance of our code. By exploiting the rank-1 structure of the outer product matrix \\(\\mathbf{a} = \\mathbf{a} \\mathbf{a}^T\\), we can reduce the time complexity from \\(O(n^3)\\) to \\(O(n^2)\\) in the best case scenario. This optimization can lead to considerable performance improvements, especially for large matrices.\nSpace Complexity: Regardless of the case, the space complexity remains \\(O(n^2)\\) since we need to store the resulting \\(n \\times n\\) matrix \\(\\mathbf{BA}\\).\nPython Code:"
  },
  {
    "objectID": "posts/matmul/index.html#output",
    "href": "posts/matmul/index.html#output",
    "title": "Matrix multiplication: Let’s make it less expensive!",
    "section": "Output",
    "text": "Output\nNaive Multiplication Time: 27.198208 seconds\nOptimized Multiplication Time: 0.001841 seconds\nWhat about when \\(\\mathbf{A}\\) is not given as \\(\\mathbf{A}=\\mathbf{aa}^T\\) (i.e., it’s not a rank-1 matrix)? We simply cannot exploit the same optimization based on the outer product. In this case, we have to use the general matrix multiplication approach, which typically has a time complexity of \\(O(n^3)\\) for naive multiplication. However, there are optimized algorithms that can reduce the time complexity:\n\nStrassen’s Algorithm: Reduces the time complexity to approximately \\(O(n^{2.81})\\)\n\nCoppersmith-Winograd Algorithm: Further reduces the time complexity to approximately \\(O(n^{2.376})\\)\n\nParallel Algorithms: Use parallel computing techniques to perform matrix multiplication more efficiently.\n\nMay be some other day we can talk about these algorithms.\n\n\n\n\n\n\n\n\nShare on\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/matrixrep/index.html",
    "href": "posts/matrixrep/index.html",
    "title": "Matrix Representation: Change of Basis",
    "section": "",
    "text": "Let \\(\\alpha: \\mathcal{P}_2(\\mathbb{R}) \\longrightarrow M_{2\\times 2}(\\mathbb{R})\\) be defined by\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\nFirst, let’s show that \\(\\alpha\\) is a linear transformation. Let \\(f(x),g(x) \\in \\mathcal{P}_2(\\mathbb{R})\\) and \\(a,b\\in \\mathbb{R}\\). Then by definition, we have\n\n\n\\(\\alpha(af(x)+bg(x))=\\left(\\begin{array}{cc}af'(0)+bg'(0)& 2af(1)+2bg(1)\\\\0& af''(3)+bg''(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1.6in}\\)=\\(\\left(\\begin{array}{cc}af'(0)& 2af(1)\\\\0& af''(3)\\end{array}\\right)\\)+\\(\\left(\\begin{array}{cc}bg'(0)& 2bg(1)\\\\0& bg''(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1.6in}\\)=\\(a\\left(\\begin{array}{cc}f'(0)& 2f(1)\\\\0& f''(3)\\end{array}\\right)\\)+\\(b\\left(\\begin{array}{cc}g'(0)& 2g(1)\\\\0& g''(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1.6in}=a\\alpha(f(x))+b\\alpha(g(x))\\)\n\n\nSo that \\(\\alpha\\) is a linear transformation.\n\n\nSecond, we find the kernel space \\(ker(\\alpha)\\), then use the Dimension Theorem (formula) to decide the rank of \\(\\alpha\\)\n\n\nThe kernel of \\(\\alpha\\) is defined as\n\n\n\\(ker(\\alpha)=\\{v\\in V|\\alpha(v)=0_{M_{2\\times2}(\\mathbb{R})}\\}\\)\n\n\n\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)=[0]\\)\n\n\n\\(\\implies f'(0)=0, 2f(1)=0, f''(3)=0\\)\n\n\nIf \\(f(x)=a+bx+cx^2\\) then we have,\n\n\n\\(\\begin{array}{c}f'(0)\\implies b=0\\\\2f(1)=0\\implies 2(a+b+c)=0\\\\f''(3)=0\\implies 2c=0\\end{array}\\)\n\n\n\\(\\implies a=b=c=0 \\implies ker(\\alpha)=\\{0_{\\mathcal{P}_2(\\mathbb{R})}\\}\\)\n\n\nThen \\(nullity(\\alpha)=\\dim ker(\\alpha)=0\\) and if we use the dimension formula then, \\(rank(\\alpha)=\\dim \\mathcal{P}_2(\\mathbb{R})-nullity(\\alpha)=3-0=3\\)\n\n\nThird, we will find the representation matrix \\(\\phi_{BD}(\\alpha)\\), where \\(B=\\{1+x,1-x,x^2\\}\\) is an ordered basis for \\(\\mathcal{P}_2(\\mathbb{R})\\)\n\n\nand\n\n\\(D=\\begin{Bmatrix}\\begin{bmatrix}1 & 0\\\\0 &0\\end{bmatrix},\\begin{bmatrix}0 & 1\\\\0 &0\\end{bmatrix},\\begin{bmatrix}0 & 0\\\\1 &0\\end{bmatrix},\\begin{bmatrix}0 & 0\\\\0 &1\\end{bmatrix}\\end{Bmatrix}\\)\nis an ordered basis for \\(\\mathbf{M}_{2\\times 2}(\\mathbb{R})\\)\n\n\n\nNow if \\(f(x)=1+x\\) then\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{cc}1& 4\\\\0& 0\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{c}1\\\\4\\\\0\\\\0\\end{array}\\right)\\)\n\n\n\n\n\n\nNow if \\(f(x)=1-x\\) then\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{cc}-1& 0\\\\0& 0\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{c}-1\\\\0\\\\0\\\\0\\end{array}\\right)\\)\n\n\n\n\n\n\nNow if \\(f(x)=x^2\\) then\n\n\n\\(\\alpha(f(x))=\\left(\\begin{array}{cc}f^{'}(0)& 2f(1)\\\\0& f^{''}(3)\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{cc}0& 2\\\\0& 2\\end{array}\\right)\\)\n\n\n\n\n\n\\(\\hspace{1in}=\\left(\\begin{array}{c}-2\\\\2\\\\0\\\\2\\end{array}\\right)\\)\n\n\n\n\n\nbecause,\n\n\n\n\n\n\n\\(\\left(\\begin{array}{cc}0& 2\\\\0& 2\\end{array}\\right)\\)\\(=-2\\left(\\begin{array}{cc}1& 0\\\\0& 0\\end{array}\\right)\\)\\(+2\\left(\\begin{array}{cc}0& 1\\\\0& 0\\end{array}\\right)\\)\\(+0\\left(\\begin{array}{cc}0& 0\\\\1& 0\\end{array}\\right)\\)\\(+2\\left(\\begin{array}{cc}1& 0\\\\0& 1\\end{array}\\right)\\)\n\n\n\n\n\nTherefore, \\(\\phi_{BD}(\\alpha)=\\)\\(\\left(\\begin{array}{ccc}1& -1& -2\\\\4& 0& 2\\\\0& 0& 0\\\\0& 0& 2\\end{array}\\right)\\)\n\n\n\n\n\n\n\n\n\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n2 min\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)\n\n14 min\n\n\nRafiq Islam\n\n\nThursday, September 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n26 min\n\n\nRafiq Islam\n\n\nTuesday, November 9, 2021\n\n\n\n\n\n\nNo matching items\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Matrix {Representation:} {Change} of {Basis}},\n  date = {2021-01-21},\n  url = {https://mrislambd.github.io/posts/matrixrep/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Matrix Representation: Change of\nBasis.” January 21, 2021. https://mrislambd.github.io/posts/matrixrep/."
  },
  {
    "objectID": "posts/sgd/index.html",
    "href": "posts/sgd/index.html",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "",
    "text": "GIF Credit: gbhat.com\n Gradient Descent is an optimization algorithm used to minimize the cost function. The cost function \\(f(\\beta)\\) measures how well a model with parameters \\(\\beta\\) fits the data. The goal is to find the values of \\(\\beta\\) that minimize this cost function. In terms of the iterative method, we want to find \\(\\beta_{k+1}\\) and \\(\\beta_k\\) such that \\(f(\\beta_{k+1})&lt;f(\\beta_k)\\).   For a small change in \\(\\beta\\), we can approximate \\(f(\\beta)\\) using Taylor series expansion\n\\[f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}\\]\n\nThe update rule for vanilla gradient descent is given by:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nWhere:\n\n\\(\\beta_k\\) is the current estimate of the parameters at iteration \\(k\\).\n\\(\\eta\\) is the learning rate, a small positive scalar that controls the step size.\n\\(\\nabla f(\\beta_k)\\) is the gradient of the cost function \\(f\\) with respect to \\(\\beta\\) at the current point \\(\\beta_k\\).\n\n\nThe update rule comes from the idea of moving the parameter vector \\(\\beta\\) in the direction that decreases the cost function the most.\n\nGradient: The gradient \\(\\nabla f(\\beta_k)\\) represents the direction and magnitude of the steepest ascent of the function \\(f\\) at the point \\(\\beta_k\\). Since we want to minimize the function, we move in the opposite direction of the gradient.\nStep Size: The term \\(\\eta \\nabla f(\\beta_k)\\) scales the gradient by the learning rate \\(\\eta\\), determining how far we move in that direction. If \\(\\eta\\) is too large, the algorithm may overshoot the minimum; if it’s too small, the convergence will be slow.\nIterative Update: Starting from an initial guess \\(\\beta_0\\), we repeatedly apply the update rule until the algorithm converges, meaning that the changes in \\(\\beta_k\\) become negligible, and \\(\\beta_k\\) is close to the optimal value \\(\\beta^*\\)."
  },
  {
    "objectID": "posts/sgd/index.html#gradient-descent",
    "href": "posts/sgd/index.html#gradient-descent",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "",
    "text": "GIF Credit: gbhat.com\n Gradient Descent is an optimization algorithm used to minimize the cost function. The cost function \\(f(\\beta)\\) measures how well a model with parameters \\(\\beta\\) fits the data. The goal is to find the values of \\(\\beta\\) that minimize this cost function. In terms of the iterative method, we want to find \\(\\beta_{k+1}\\) and \\(\\beta_k\\) such that \\(f(\\beta_{k+1})&lt;f(\\beta_k)\\).   For a small change in \\(\\beta\\), we can approximate \\(f(\\beta)\\) using Taylor series expansion\n\\[f(\\beta_{k+1})=f(\\beta_k +\\Delta\\beta_k)\\approx f(\\beta_k)+\\nabla f(\\beta_k)^T \\Delta \\beta_k+\\text{higher-order terms}\\]\n\nThe update rule for vanilla gradient descent is given by:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nWhere:\n\n\\(\\beta_k\\) is the current estimate of the parameters at iteration \\(k\\).\n\\(\\eta\\) is the learning rate, a small positive scalar that controls the step size.\n\\(\\nabla f(\\beta_k)\\) is the gradient of the cost function \\(f\\) with respect to \\(\\beta\\) at the current point \\(\\beta_k\\).\n\n\nThe update rule comes from the idea of moving the parameter vector \\(\\beta\\) in the direction that decreases the cost function the most.\n\nGradient: The gradient \\(\\nabla f(\\beta_k)\\) represents the direction and magnitude of the steepest ascent of the function \\(f\\) at the point \\(\\beta_k\\). Since we want to minimize the function, we move in the opposite direction of the gradient.\nStep Size: The term \\(\\eta \\nabla f(\\beta_k)\\) scales the gradient by the learning rate \\(\\eta\\), determining how far we move in that direction. If \\(\\eta\\) is too large, the algorithm may overshoot the minimum; if it’s too small, the convergence will be slow.\nIterative Update: Starting from an initial guess \\(\\beta_0\\), we repeatedly apply the update rule until the algorithm converges, meaning that the changes in \\(\\beta_k\\) become negligible, and \\(\\beta_k\\) is close to the optimal value \\(\\beta^*\\)."
  },
  {
    "objectID": "posts/sgd/index.html#stochastic-gradient-descent-sgd",
    "href": "posts/sgd/index.html#stochastic-gradient-descent-sgd",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Stochastic Gradient Descent (SGD)",
    "text": "Stochastic Gradient Descent (SGD)\n\nStochastic Gradient Descent is a variation of the vanilla gradient descent. Instead of computing the gradient using the entire dataset, SGD updates the parameters using only a single data point or a small batch of data points at each iteration. The later one we call it mini batch SGD.\n\nSuppose our cost function is defined as the average over a dataset of size \\(n\\):\n\\[\nf(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\beta)\n\\]\nWhere \\(f_i(\\beta)\\) represents the contribution of the \\(i\\)-th data point to the total cost function. The gradient of the cost function with respect to \\(\\beta\\) is:\n\\[\n\\nabla f(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\beta)\n\\]\nVanilla gradient descent would update the parameters as:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f(\\beta_k)\n\\]\nInstead of using the entire dataset to compute the gradient, SGD approximates the gradient by using only a single data point (or a small batch). The update rule for SGD is:\n\\[\n\\beta_{k+1} = \\beta_k - \\eta \\nabla f_{i_k}(\\beta_k)\n\\]\nWhere:\n\n\\(i_k\\) is the index of a randomly selected data point at iteration \\(k\\).\n\\(\\nabla f_{i_k}(\\beta_k)\\) is the gradient of the cost function with respect to the parameter \\(\\beta_k\\), evaluated only at the data point indexed by \\(i_k\\)."
  },
  {
    "objectID": "posts/sgd/index.html#implementation-of-gd",
    "href": "posts/sgd/index.html#implementation-of-gd",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Implementation of GD",
    "text": "Implementation of GD\n\nIn 1D\nAssume that we have a function \\(f(x)=x^2-3x+\\frac{13}{4}\\) which we want to minimize. Meaning, we want to find \\(x\\) that minimizes the function.\n\n\n\n\n\n\n\n\n\nNext, let’s implement the GD1\n\n# Define the function\ndef f(x):\n    return x**2-3*x+(13/4)\n# Define the gradient function \ndef grad_f(x):\n    return 2*x-3\n\n# Take a random guess from the domain\nlocal_min = np.random.choice(x)\nprint('Initial guess: ',local_min )\n\n# Hyper-parameters \nlearning_rate = 0.01\ntraining_epochs = 200\n\nmodel_params = np.zeros((training_epochs,2))\nfor i in range(training_epochs):\n    grad = grad_f(local_min)\n    local_min =local_min - learning_rate*grad \n    model_params[i,:] = [local_min, grad]\n\nprint('Empirical/estimated local minimum:',local_min)\n\nfig, axs = plt.subplots(1,2, figsize=(8.5,3.8))\nfor i in range(2):\n    axs[i].plot(model_params[:,i],'-')\n    axs[i].set_xlabel('Iteration')\n    axs[i].set_title(f'Final estimated Min: {local_min:.5f}')\naxs[0].set_ylabel('Local Min')\naxs[1].set_ylabel('Derivative')\nplt.show()\n\nInitial guess:  -0.6546546546546548\nEmpirical/estimated local minimum: 1.4621040489801647\n\n\n\n\n\n\n\n\n\nAlternatively, if we want to set a tolerance, this is how we set that\n\n# Take a random guess from the domain\nlocal_min = np.random.choice(x)\nprint('Initial guess:', local_min)\n\n# Hyper-parameters \nlearning_rate = 0.01\ntolerance = 1e-2  # Stop if gradient is smaller than this value\n\n# Lists to store optimization progress\nlocal_min_vals = []\ngrad_vals = []\niteration = 0  # Track the number of iterations\n\n# Gradient Descent Loop (Runs until gradient is small enough)\nwhile True:\n    grad = grad_f(local_min)\n    # Stop when gradient is smaller than tolerance\n    if abs(grad) &lt; tolerance:\n        print(f\"Stopping at iteration {iteration} (grad={grad:.5f})\")\n        break\n    # Update local minimum\n    local_min = local_min - learning_rate * grad  \n    # Store values\n    local_min_vals.append(local_min)\n    grad_vals.append(grad)\n    iteration += 1  # Increment iteration count\n\nprint('Empirical/estimated local minimum:', local_min)\n\n# Convert lists to numpy arrays\nlocal_min_vals = np.array(local_min_vals)\ngrad_vals = np.array(grad_vals)\n\n# Plot Results\nfig, axs = plt.subplots(1, 2, figsize=(8.5, 3.8))\n\naxs[0].plot(local_min_vals, '.', label=\"Local Min Estimate\")\naxs[0].set_xlabel('Iteration')\naxs[0].set_ylabel('Local Min')\naxs[0].legend()\n\naxs[1].plot(grad_vals, '.', label=\"Gradient Value\")\naxs[1].set_xlabel('Iteration')\naxs[1].set_ylabel('Derivative')\naxs[1].legend()\n\nplt.suptitle(f'Final estimated Min: {local_min:.5f}')\nplt.show()\n\nInitial guess: -1.1381381381381381\nStopping at iteration 311 (grad=-0.00985)\nEmpirical/estimated local minimum: 1.4950727219254853\n\n\n\n\n\n\n\n\n\n\nParametric variation\nLet’s consider a different problem, \\(f(x)=-e^{-\\frac{x^2}{10}}(0.2x\\cos x+\\sin x)\\). We want to find the \\(x\\) values and optimal hyper-parameters that minimizes \\(f(x)\\).\n\nx = np.linspace(-3*np.pi, 3*np.pi, 400)\ny = -np.exp(-(x**2/10))*(0.2*x*np.cos(x)+np.sin(x))\n\ndy = np.exp(-(x**2/10))*((0.04*x**2-1.2)*np.cos(x)+0.4*x*np.sin(x))\nplt.plot(x,y, x, dy)\nplt.legend(['f(x)','df'])\nplt.show()\n\n\n\n\n\n\n\n\nClearly, the global minimum is somewhere in between 0 to 2.5, maybe around 1.25. Now let’s apply GD with varying parameters\n\ndef f(x):\n    f = -np.exp(-(x**2/10))*(0.2*x*np.cos(x)+np.sin(x))\n    return f\ndef df(x):\n    ddx_of_f = np.exp(-(x**2/10))*((0.04*x**2-1.2)*np.cos(x)+0.4*x*np.sin(x))\n    return ddx_of_f\n\nlearning_rate = 0.003\ntraining_epochs = 1000\nlocal_min = np.random.choice(x,1)\nprint('The chosen initial point: ', local_min)\n\nfor i in range(training_epochs):\n    grad = df(local_min)\n    local_min = local_min - learning_rate*grad \n\nplt.plot(x,f(x), x, df(x),'--')\nplt.plot(local_min, df(local_min),'ro')\nplt.plot(local_min, f(local_min),'ro')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend(['f(x)','df','f(x) min'])\nplt.title('Iterated local min at x = %s'%local_min[0])\nplt.show()\n\nThe chosen initial point:  [-4.88954646]\n\n\n\n\n\n\n\n\n\nNow let’s see how we can pick the right initial point\n\n# Vary the starting point \nstarting_points = np.linspace(-5,5,50)\nstopping_points = np.zeros(len(starting_points))\n\ntraining_epochs = 1000\nlearning_rate = 0.01\nfor idx, local_min in enumerate(starting_points):\n    for i in range(training_epochs):\n        grad = df(local_min)\n        local_min = local_min - learning_rate * grad \n    stopping_points[idx] = local_min\nplt.plot(starting_points, stopping_points, 's-')\nplt.xlabel('Starting points')\nplt.ylabel('Stopping points')\nplt.show()\n\n\n\n\n\n\n\n\nBased on this, if we start with any point roughly between \\((-1,3.5)\\) we will end up with the global minimum. However, points outside this interval may take the iterative process to other local minima.\n\n# varying learning rates \nlrs = np.linspace(1e-10, 1e-2, 30)\nstopping_points = np.zeros(len(lrs))\n\ntraining_epochs = 1000\n\nfor idx, lr  in enumerate(lrs):\n    # fixed initial point \n    local_min = -0.03\n    for i in range(training_epochs):\n        grad = df(local_min)\n        local_min = local_min - lr*grad \n    stopping_points[idx] = local_min\nplt.plot(lrs, stopping_points, 's-')\nplt.xlabel('learning rates')\nplt.ylabel('Stopping points')\nplt.show()\n\n\n\n\n\n\n\n\nSo this suggests that a learning rate between 0.003 to 0.005 is a good start.\n\nlrs = np.linspace(1e-10, 1e-2, 30)\ntraining_epochs = np.round(np.linspace(10,1000,50))\n\nstopping_points = np.zeros((len(lrs), len(training_epochs)))\n\nfor lr_idx, lr in enumerate(lrs):\n    for tr_idx, tr_epoch in enumerate(training_epochs):\n        local_min = -0.03\n        \n        for i in range(int(tr_epoch)):\n            grad = df(local_min)\n            local_min = local_min - lr * grad \n\n        stopping_points[lr_idx, tr_idx] = local_min\nplt.imshow(stopping_points, extent=[lrs[0], lrs[-1],\\\n     training_epochs[0], training_epochs[-1]], aspect='auto',\\\n        vmin=0, vmax=1.5)\nplt.xlabel('learning rate')\nplt.ylabel('training epochs')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\nWe know that the expected minimum at \\(x \\approx 1.17\\) so the color corresponding to this is light. Therefore, if we choose a very small learning rate and/or a larger training epoch then we end up to the darker area which is not a good approximation.\n\nplt.plot(lrs, stopping_points)\nplt.xlabel('learning rate')\nplt.ylabel('Iterated solution')\nplt.title('Each line is a training epoch N')\nplt.show\n\n\n\n\n\n\n\n\n\n\n\nIn 2D\n\nLet’s imagine a hypothetical scenario, Walmart Inc. wants to explore their business in a new twon. They want to have their store in location so that the total distance of the store from all the houses in the neighborhood is the smallest possible. If they have the data of \\(n\\) houses with corresponding coordinates of the houses, return the optimized location for the store.\n\nThe Euclidean distance between two points \\((x_1,y_1)\\) and \\((x_2,y_2)\\) is given by\n\\[d=\\sqrt{(x_1-x_2)^2+(y_1-y_2)}\\]\nAssume that \\(P=(x,y)\\) is the coordinate of Walmart. So for a total of \\(n\\) such points the total distance \\(D\\) from the point \\(P\\) is a function of two variable \\(x\\) and \\(y\\) of the following form\n\\[D=f(x,y)=\\sum_{i=1}^{n}\\sqrt{(x-x_i)^2+(y-y_i)^2}\\]\n\nimport plotly.offline as iplot\nimport plotly as py\npy.offline.init_notebook_mode(connected=True)\nimport plotly.graph_objects as go\n\n\ndef f(x,y, c, d):\n    return np.sqrt((x-c)**2+(y-d)**2)\n\nx = np.linspace(-10,10, 400)\ny = np.linspace(-10,10, 400)\nx, y = np.meshgrid(x,y)\n\nc, d = 0,0\n\nz = f(x, y, c, d)\n\nfig = go.Figure(data=[go.Surface(z=z, x=x, y=y)])\nfig.update_layout(\n    title='3D plot',\n    scene=dict(\n        xaxis_title = 'x',\n        yaxis_title = 'y',\n        zaxis_title = 'z'\n    )\n)\n\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nall we need to do is to minimize the function \\(f(x,y)\\) and to do that we need to calculate the gradient vector which is the partial derivative of \\(f(x,y)\\) with respect to \\(x\\) and \\(y\\). So,\n\\[\\begin{align*}\n\\frac{\\partial f}{\\partial x}& = \\sum_{i=1}^{n} \\frac{x-x_i}{\\sqrt{(x-x_i)^2+(y-y_i)^2}}\\\\\n\\frac{\\partial f}{\\partial y}& = \\sum_{i=1}^{n} \\frac{y-y_i}{\\sqrt{(x-x_i)^2+(y-y_i)^2}}\\\\\n& \\\\\n\\implies \\nabla f(x,y) &= \\begin{bmatrix}\\frac{\\partial f}{\\partial x}\\\\\\frac{\\partial f}{\\partial y}\\end{bmatrix}\n\\end{align*}\\]\nThen the algorithm\n\\[\\begin{align*}\n\\begin{bmatrix}x_{i+1}\\\\y_{i+1}\\end{bmatrix}&= \\begin{bmatrix}x_{i}\\\\y_{i}\\end{bmatrix} - \\eta_i \\begin{bmatrix}\\frac{\\partial f}{\\partial x}|_{x_i}\\\\\\frac{\\partial f}{\\partial y}|_{y_i}\\end{bmatrix}\n\\end{align*}\\]\nwhere, the \\(\\eta\\) is the step size or learning rate that scales the size of the move towards the opposite of the gradient direction.\nNext, how do we control the numerical stability of the algorithm? We need to decrease the step size at each iteration which. This is called the rate of decay. We also need a termination factor or tolerance level that determines if we can stop the iteration. Sometimes, for a deep down convex function, the process oscillates back and forth around a range of values. In this case, applying a damping factor increases the chance for a smooth convergence.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\nclass GDdistanceMin:\n    def __init__(self, step_size=1, decay_rate=0.99, tolerance=1e-7, damping_rate=0.75, points=[]):\n        self.step_size = step_size\n        self.decay_rate = decay_rate\n        self.tolerance = tolerance\n        self.damping_rate = damping_rate\n        self.points = points\n        self.x = sum(x for x, y in points) / len(points)  # Initialization\n        self.y = sum(y for x, y in points) / len(points)  # Initialization\n        self.x_updates = []\n        self.y_updates = []\n\n    def _partial_derivative_x(self, x, y):\n        grad_x = 0\n        for xi, yi in self.points:\n            if x != xi or y != yi:\n                grad_x += (x - xi) / math.sqrt((x - xi)**2 + (y - yi)**2)\n        return grad_x\n\n    def _partial_derivative_y(self, x, y):\n        grad_y = 0\n        for xi, yi in self.points:\n            if x != xi or y != yi:\n                grad_y += (y - yi) / math.sqrt((x - xi)**2 + (y - yi)**2)\n        return grad_y\n\n    def gradient_descent(self):\n        dx, dy = 0, 0\n        while self.step_size &gt; self.tolerance:\n            dx = self._partial_derivative_x(self.x, self.y) + self.damping_rate * dx \n            dy = self._partial_derivative_y(self.x, self.y) + self.damping_rate * dy \n            self.x -= self.step_size * dx \n            self.x_updates.append(self.x)\n            self.y -= self.step_size * dy \n            self.y_updates.append(self.y)\n            self.step_size *= self.decay_rate\n        return (self.x, self.y)\n\ndef f(x, y, c, d):\n    return np.sqrt((x - c)**2 + (y - d)**2)\n\n# Define points\npoints = [(1, 3), (-2, 4), (3, 4), (-2, 1), (9, 2), (-5, 2)]\ngd_min = GDdistanceMin(points=points)\nmin_pt = gd_min.gradient_descent()\nxs = gd_min.x_updates\nys = gd_min.y_updates\nprint(\"Minimum point:\", min_pt)\n\nc, d = min_pt\n\n# Create a grid for plotting\nx = np.linspace(-10, 10, 400)\ny = np.linspace(-10, 10, 400)\nx_grid, y_grid = np.meshgrid(x, y)\nz = f(x_grid, y_grid, c, d)\n\n# Calculate z values for the updates\nzs = [f(xi, yi, c, d) for xi, yi in zip(xs, ys)]\n\n# Plotting\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(x_grid, y_grid, z, cmap='viridis', alpha=0.6)\nax.scatter(xs, ys, zs, color='red', s=50, label=\"Updates\", marker='o')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\nMinimum point: (0.9999997458022071, 2.9999999769909707)\n\n\n\n\n\n\n\n\n\nDifferent approach2\n\nimport sympy as sym \nsym.init_printing()\nfrom IPython.display import display, Math\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nset_matplotlib_formats('svg') \n\n\ndef distance_function(x,y,c,d):\n    x,y = np.meshgrid(x,y)\n    z = np.sqrt((x-c)**2+(y-d)**2)\n    return z\n\nc,d = 0,0\nx = np.linspace(-5,5, 400)\ny = np.linspace(-5,5, 400)\nz = distance_function(x,y,c,d)\n\nplt.imshow(z, extent=[x[0],x[-1], y[0],y[-1]],vmin=-5, vmax=5, origin='lower')\nplt.show()\n\n\n\n\n\n\n\n\nSo, the minimum is in the center of the deep color area. Let’s compute the derivatives using sympy library\n\nsx, sy = sym.symbols('sx,sy')\nsz = sym.sqrt((sx-c)**2+(sy-d)**2)\n\ndf_sx = sym.lambdify((sx,sy), sym.diff(sz, sx), 'sympy')\ndf_sy = sym.lambdify((sx,sy), sym.diff(sz, sy), 'sympy')\n\ndisplay(Math(f\"\\\\frac{{\\\\partial f}}{{\\\\partial x}}={sym.latex(sym.diff(sz,sx))}\\\n     \\\\text{{ and }}  \\\\frac{{\\\\partial f}}{{\\\\partial x}}\\\\mid_{{(1,1)}} = {df_sx(1,1).evalf()}\"))\ndisplay(Math(f\"\\\\frac{{\\\\partial f}}{{\\\\partial y}}={sym.latex(sym.diff(sz,sy))} \\\n     \\\\text{{ and }}  \\\\frac{{\\\\partial f}}{{\\\\partial y}}\\\\mid_{{(1,1)}} = {df_sy(1,1).evalf()}\"))\n\n\\(\\displaystyle \\frac{\\partial f}{\\partial x}=\\frac{sx}{\\sqrt{sx^{2} + sy^{2}}}     \\text{ and }  \\frac{\\partial f}{\\partial x}\\mid_{(1,1)} = 0.707106781186548\\)\n\n\n\\(\\displaystyle \\frac{\\partial f}{\\partial y}=\\frac{sy}{\\sqrt{sx^{2} + sy^{2}}}      \\text{ and }  \\frac{\\partial f}{\\partial y}\\mid_{(1,1)} = 0.707106781186548\\)\n\n\nNow let’s implement the GD\n\n# Let's pick a random starting point(uniformly distributed between -2 and 2)\nlocal_min = np.random.rand(2)*4-2\nstart_point = local_min[:] # make a copy\n\nlearning_rate = 0.01\ntraining_epochs = 1000\n\ntrajectory = np.zeros((training_epochs,2))\n\nfor i in range(training_epochs):\n    grad = np.array([\n        df_sx(local_min[0],local_min[1]).evalf(),\n        df_sy(local_min[0],local_min[1]).evalf()\n    ])\n    local_min = local_min - learning_rate*grad\n    trajectory[i,:] = local_min\n\nprint('Starting point ',start_point)\nprint('Local min found ',local_min)\n\nplt.imshow(z, extent=[x[0],x[-1], y[0],y[-1]],vmin=-5, vmax=5, origin='lower')\nplt.plot(start_point[0], start_point[1], 'bs')\nplt.plot(local_min[0],local_min[1],'rs')\nplt.plot(trajectory[:,0],trajectory[:,1], 'b', linewidth=3)\nplt.legend(['random start', 'local min'])\nplt.colorbar()\nplt.show()\n\nStarting point  [-0.13212455 -0.41329171]\nLocal min found  [0.00185828751128861 0.00581280962477011]"
  },
  {
    "objectID": "posts/sgd/index.html#implementation-of-sgd",
    "href": "posts/sgd/index.html#implementation-of-sgd",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Implementation of SGD",
    "text": "Implementation of SGD\nPersonally, I think this webpage is one of the best resources for learning how to implement SGD.\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/sgd/index.html#footnotes",
    "href": "posts/sgd/index.html#footnotes",
    "title": "Implementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe implementation is taken from the the Udemy course A Deep Understanding of Deep Learning↩︎\nThe implementation is taken from the the Udemy course A Deep Understanding of Deep Learning↩︎"
  },
  {
    "objectID": "research/extrasgld/index.html",
    "href": "research/extrasgld/index.html",
    "title": "Generalized EXTRA stochastic gradient Langevin dynamics",
    "section": "",
    "text": "Langevin algorithms are popular Markov Chain Monte Carlo methods for Bayesian learning, particularly when the aim is to sample from the posterior distribution of a parametric model, given the input data and the prior distribution over the model parameters. Their stochastic versions such as stochastic gradient Langevin dynamics (SGLD) allow iterative learning based on randomly sampled mini-batches of large datasets and are scalable to large datasets. However, when data is decentralized across a network of agents subject to communication and privacy constraints, standard SGLD algorithms cannot be applied. Instead, we employ decentralized SGLD (DE-SGLD) algorithms, where Bayesian learning is performed collaboratively by a network of agents without sharing individual data.      Nonetheless, existing DE-SGLD algorithms induce a bias at every agent that can negatively impact performance; this bias persists even when using full batches and is attributable to network effects. Motivated by the EXTRA algorithm and its generalizations for decentralized optimization, we propose the generalized EXTRA stochastic gradient Langevin dynamics, which eliminates this bias in the full-batch setting. Moreover, we show that, in the mini-batch setting, our algorithm provides performance bounds that significantly improve upon those of standard DE-SGLD algorithms in the literature. Our numerical results also demonstrate the efficiency of the proposed approach.     Performance of the EXTRA SGLD for Bayesian linear regression on four different network structures. Out of 20 agents, we report only the first 4 agents and the mean of the nodes \\(\\bar{\\beta}^{(k)}=\\frac{1}{N}\\sum_{i=1}^{N}\\beta_i^{(k)}\\)      Comparative accuracy distribution of the DE-SGLD and EXTRA SGLD method across different network structures on Breast Cancer data set. The plots are from a randomly selected node.\n\n\n    \n        \n    \n\n    \n        \n    \n\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{gurbuzbalaban2024,\n  author = {Gurbuzbalaban, Mert and Rafiqul Islam, Mohammad and Wang,\n    Xiaoyu and Zhu, Lingjiong},\n  title = {Generalized {EXTRA} Stochastic Gradient {Langevin} Dynamics},\n  date = {2024-02-12},\n  url = {https://mrislambd.github.io/research/extrasgld/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGurbuzbalaban, Mert, Mohammad Rafiqul Islam, Xiaoyu Wang, and Lingjiong\nZhu. 2024. “Generalized EXTRA Stochastic Gradient Langevin\nDynamics.” February 12, 2024. https://mrislambd.github.io/research/extrasgld/."
  },
  {
    "objectID": "research/holmc/index.html",
    "href": "research/holmc/index.html",
    "title": "Higher-order Langevin Algorithms",
    "section": "",
    "text": "Langevin algorithms are popular Markov chain Monte Carlo (MCMC) methods for large-scale sampling problems that often arise in data science. We propose Monte Carlo algorithms based on \\(P\\)-th order Langevin dynamics for any \\(P\\geq 3\\). Our design of \\(P\\)-th order Langevin Monte Carlo (LMC) algorithms is by combining splitting and accurate integration methods. We obtain Wasserstein convergence guarantees for sampling from distributions with log-concave and smooth densities. Specifically, the mixing time of the \\(P\\)-th order LMC algorithm scales as \\(O\\left(d^{\\frac{1}{\\mathcal{R}}}/\\epsilon^{\\frac{1}{2\\mathcal{R}}} \\right)\\) for \\(\\mathcal{R}=4\\cdot\\mathbf{1}_{\\{ P=3\\}}+ (2P-1)\\cdot\\mathbf{1}_{\\{ P\\geq 4\\}}\\), which have better dependence on the dimension and the accuracy level as \\(P\\) grows. Numerical experiments illustrate the efficiency of our proposed algorithms.\n\n\n  \n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{l._dang2025,\n  author = {L. Dang, Thanh and Gurbuzbalaban, Mert and Rafiqul Islam,\n    Mohammad and Yao, Nihan and Zhu, Lingjiong},\n  title = {Higher-Order {Langevin} {Algorithms}},\n  date = {2025-07-23},\n  url = {https://mrislambd.github.io/research/holmc/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nL. Dang, Thanh, Mert Gurbuzbalaban, Mohammad Rafiqul Islam, Nihan Yao,\nand Lingjiong Zhu. 2025. “Higher-Order Langevin\nAlgorithms.” July 23, 2025. https://mrislambd.github.io/research/holmc/."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/fall22.html",
    "href": "teaching/fall22.html",
    "title": "Fall 2022: MAC2311 Calculus and Analytic Geometry I",
    "section": "",
    "text": "Students who have substantial knowledge of precalculus and algebra may require to take this course as a mathematics requirement depending on their majors. The topic of this course includes but is not limited to Foundation for calculus: Functions and Limits, Derivative, The Definite Integral, and Constructing Antiderivatives.  As a recitation instructor for this course, I ran two poster presentation sessions of 30 students in each group where they presented mathematical problems and their solutions step by step to their peer classmates followed by a group activity where they solved another set of problems. I also graded their exam scripts and weekly posters.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/sp23.html",
    "href": "teaching/sp23.html",
    "title": "Spring 2023: MAC1140 PreCalculus Algebra",
    "section": "",
    "text": "PreCalculus and Algebra are one of the many important foundation math courses that open doors to many upper-level math and science courses. The topic of this course includes but not is limited to Complex Numbers, Piecewise Functions, Quadratic Functions, Polynomial Functions, Polynomial Division, Zeros of Polynomials, Rational Functions, Polynomial and Rational Inequalities, Inverse Functions, Exponential Functions, Logarithmic Functions, Properties of Logarithms, Exponential and Logarithmic Equations, and so on.  As an Instructor of Record for this course, I taught a class of 27 undergraduate students from different majors. I also proctor their lab classes where they take their quizzes and tests online and other application based lab activities.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/fall21.html",
    "href": "teaching/fall21.html",
    "title": "Fall 2021 and Spring 2022: PreCalculus and Algebra",
    "section": "",
    "text": "As a lecture TA, my job was to facilitate the instructor during the class. This included helping students in class activities such as answering short questions that counted as class attendance, checking students’ eligibility forms for taking this course, and others as needed by the instructor.  As a lab TA I worked in a computer lab where students take their weekly quizzes and midterm tests.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/fall23.html",
    "href": "teaching/fall23.html",
    "title": "Fall 2023: MAP4170 Introduction to Actuarial Mathematics",
    "section": "",
    "text": "Worked as a greader for this course.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n\n\nLinear Algebra\n\nMathematics\n\n\n\n\n\n\n\n\n\nJan 25, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)\n\n\n\nData Science\n\nMachine Learning\n\nStochastic Gradient Descent\n\nOptimization\n\n\n\n\n\n\n\n\n\nSep 19, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n\n\nData Science\n\nMachine Learning\n\nComputational Mathematics\n\nAlgorithmic Complexity\n\nProgramming\n\nComputer Science\n\n\n\n\n\n\n\n\n\nNov 9, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Representation: Change of Basis\n\n\n\nLinear Algebra\n\nMathematics\n\n\n\n\n\n\n\n\n\nJan 21, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix multiplication: Let’s make it less expensive!\n\n\n\nData Science\n\nMachine Learning\n\nComputational Mathematics\n\nAlgorithmic Complexity\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nModeling viral disease\n\n\n\nApplied Mathematics\n\nMath Biology\n\nMathematical Modeling\n\n\n\n\n\n\n\n\n\nFeb 23, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nSome Linear Algebra Proofs\n\n\n\nLinear Algebra\n\nMathematics\n\n\n\n\n\n\n\n\n\nJan 24, 2021\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nবাংলা ভাষায় আমার লেখা || My Blog in Benglali Language\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\nমোহাম্মদ রকিবুল ইসলাম\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "teaching/f18-f21.html",
    "href": "teaching/f18-f21.html",
    "title": "Fall 2018 to Spring 2020: College Algebra, Trigonometry",
    "section": "",
    "text": "Responsible for the preparation and delivery of all lectures, making question paper for all exams, and the grading of tests and homework assignments for the following courses: - College Algebra, Fall 2018 - Trigonometry, Fall 2019 and Spring 2020\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/sp25.html",
    "href": "teaching/sp25.html",
    "title": "Spring 2025: MAC2311 Calculus With Analytic Geometry I",
    "section": "",
    "text": "COURSE MEETING SCHEDULE:\nMondays, Wednesdays, and Fridays\nTime: 10:40 AM - 11:30 AM Location: HTL 215 Thursdays\nTime: 11:35 AM - 12:50 PM Location: HTL 215\nCREDIT HOURS: 4\nTime Zone: This course is a Main Campus FSU course and all times provided for this course (class meetings, due dates, etc) will be in Eastern Standard Time.\nCOURSE INSTRUCTOR\nRAFIQ ISLAM\nrislam@fsu.edu\nOffice Hours: Monday 12:00-1:00, Tuesday 11:30-12:30, Wednesday  12:00 - 1:00  Office: Lov 331\nCOURSE DESCRIPTION\nIn this course, students will develop problem solving skills, critical thinking, computational proficiency, and contextual fluency through the study of limits, derivatives, and definite and indefinite integrals of functions of one variable, including algebraic, exponential, logarithmic, and trigonometric functions, and applications. Topics will include limits, continuity, differentiation and rates of change, optimization, curve sketching, and introduction to integration and area.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "teaching/sp24.html",
    "href": "teaching/sp24.html",
    "title": "Spring 2024: MAP4170 Introduction to Actuarial Mathematics",
    "section": "",
    "text": "One of the course objectives is for each student to develop a mastery of financial mathematics used by actuaries, based on the mathematics of interest theory. Other course objectives are for each student to understand the long-term individual study commitment necessary to achieve a designation within one of the actuarial societies and for each student to increase their knowledge of the actuarial profession\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Download a PDF"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\n\nPh.D in Mathematics, Florida State University; Florida, USA 2026 (expected)\nM.S. in Mathematics, Youngstown State University; Ohio, USA 2020\nM.S. in Applied Mathematics, University of Dhaka; Dhaka, Bangladesh 2016\nB.S. in Mathematics, University of Dhaka; Dhaka, Bangladesh 2014"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "CV",
    "section": "Work experience",
    "text": "Work experience\n\nGraduate Teaching Assistant (Fall 2021- To Date)\n\nFlorida State University\nDuties includes: Teaching, Proctoring, and Grading\nSupervisor: Penelope Kirby, Ph.D\n\nGraduate Teaching Assistant (Fall 2018 - Spring 2020)\n\nYoungstown State University University\nDuties included: Teaching, Proctoring, and Grading\nSupervisor: G. Jay Kerns, Ph.D\n\nAssistant Vice President (September 2017 - July 2018)\n\nDelta Life Insurance Company Ltd. Dhaka, Bangladesh\nDuties included: Calculated all types of claims (death, surrender, and maturity) using excel spreadsheets.\nProcessed approximately 500 claims each week and submitted corresponding statistical reports to the higher authority.\nWorked in a team to develop a new short-term endowment assurance product which played an important role to increase the company’s new business.\nRefurbished a without risk endowment product which was out of the sale. Priced insurance premiums based on different risk factors for bigger clients which impacted our life fund significantly.\nCalculated reserves for group endowment, term and premium back policies which was a vital part of the final valuation report.\nLiaised directly with the consulting actuary and provided all sorts of technical and documental supports during actuarial valuation\nSupervisor: Md. Salahuddin Soud, VP"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "CV",
    "section": "Skills",
    "text": "Skills\n\nLanguage\n\nBengali: Native\nEnglish: Fluent\n\nComputer Literacy\n\nProgramming Languages: Python, FORTRAN, Julia, R, MATLAB, Mathematica\nSoftware Development Tools: Git, GitHub, PyPi\n\nMusical Instrument: Amateur/Novice Bamboo flute player"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "CV",
    "section": "Publications",
    "text": "Publications\n\nGeneralized EXTRA stochastic gradient Langevin dynamics\nMert, Gurbuzbalaban; Islam, Mohammad Rafiqul; Wang, Xiaoyu; Zhu, Lingjiong (2024) “Generalized EXTRA stochastic gradient Langevin dynamics.” arXiv preprint arXiv.2412.01993\n\nGJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies \nMostafa, F; Saha, P; Islam, Mohammad R.; Nguyen, N. (2020) “GJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies.” Journal of Risk and Financial Management.\nComparison of Financial Models for Stock Price Prediction \nIslam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management."
  },
  {
    "objectID": "cv.html#talks-and-presentations",
    "href": "cv.html#talks-and-presentations",
    "title": "CV",
    "section": "Talks and Presentations",
    "text": "Talks and Presentations\n\n The Heavy-Tail Phenomenon in Decentralized Stochastic Gradient Descent\nNovember 20, 2023\nPresentation at James J Love Building, Florida State University, Tallahassee, Florida\n\n Decentralized Stochastic Gradient Langevin Dynamics and Hamiltonian Monte Carlo\nOctober 05, 2023\nPresentation at James J Love Building, Florida State University, Tallahassee, Florida\n\n Sensitivity analysis for Monte Carlo and Quasi Monte Carlo option pricing\nApril 28, 2020\nPresentation at Cafaro Hall, Youngstown State University, Youngstown, Ohio"
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "CV",
    "section": "Teaching",
    "text": "Teaching\n\n Spring 2025: MAC2311 Calculus and Analytic Geometry I\n Spring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n Fall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n Spring 2023: MAC1140 PreCalculus Algebra\n\n Fall 2022: MAC2311 Calculus and Analytic Geometry I\n\n Fall 2021 and Spring 2022: PreCalculus and Algebra\n\n Fall 2018 to Spring 2020: College Algebra, Trigonometry"
  },
  {
    "objectID": "cv.html#awards-and-affiliations",
    "href": "cv.html#awards-and-affiliations",
    "title": "CV",
    "section": "Awards and Affiliations",
    "text": "Awards and Affiliations\n\nAwards\n\nBettye Anne Busbee Case Graduate Fellowship & Doctoral Mentorship Recognition 2024\n\nOutstanding Graduate Student in Statistics Award for the 2019-2020 academic year, Youngstown State University.\n\nGraduate College Premiere Scholarship, Youngstown State University.\n\nMetLife Bangladesh Actuarial Study Program 2015\n\n\n\nAffiliations\n\nBangladesh Mathematical Society: Life Member\nSociety of Actuaries, SOA: Student Member\nAmerican Mathematical Society, AMS\nSociety for Industrial and Applied Mathematics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "index.html#rafiq-islam",
    "href": "index.html#rafiq-islam",
    "title": "",
    "section": "Rafiq Islam",
    "text": "Rafiq Islam\nPh.D. Candidate in Mathematics\nFlorida State University\nContact\nEmail:  rislam@fsu.edu\nOffice:  Love Building: Room 331A\nOffice Hours:  M, W: 12:00 pm -1:00 pm  T: 11:30 am - 12:30 pm\nFind me on"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "",
    "section": "About",
    "text": "About\n\nHi, thank you and welcome to my website. My name is Mohammad Rafiqul Islam (Rafiq Islam), and I am a Ph.D. candidate in the Mathematics Department at Florida State University. My research focuses on Theoretical Machine Learning, particularly optimization and sampling techniques in Bayesian learning using Markov-Chain Monte Carlo (MCMC) algorithms. I work on developing scalable algorithms for collaborative learning under regularization and privacy constraints. My research is conducted under the supervision of professor  Lingjiong Zhu.  I hold a Bachelor of Science (BS) in Mathematics degree from the  University of Dhaka, Bangladesh, with minors in Statistics, Physics, and Computer Science. I also completed an integrated master’s degree in Applied Mathematics from the same institution. During my master’s studies, I pursued actuarial exams and passed several professional exams from the Institute and Faculty of Actuaries (IFoA), UK  which are equivalent to the exams P, FM, and Life Contingencies of  Society of Actuaries (SOA), USA . With this background, I began my professional career in the life insurance industries in Bangladesh, working for two years at two prominent life insurance companies.  In 2018, I moved to the United States to pursue a second master’s degree in Mathematics, specializing in Statistics and Data Analytics, at Youngstown State University, Ohio. During my studies and professional work, I developed a deeper interest in data science, machine learning, and research, which ultimately led me to pursue a Ph.D. in Mathematics with a focus on Theoretical Machine Learning."
  },
  {
    "objectID": "research/rulmc/index.html",
    "href": "research/rulmc/index.html",
    "title": "Reflected Underdamped Langevin Monte Carlo",
    "section": "",
    "text": "Ongoing research. The details will be available soon.\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/bengalitrial/index.html",
    "href": "posts/bengalitrial/index.html",
    "title": "বাংলা ভাষায় আমার লেখা || My Blog in Benglali Language",
    "section": "",
    "text": "“বাংলায় ব্লগিং করতে পারলে ভালই হতো” এমন ভাবনা থেকেই ঘাটাঘটি শুরু করলাম কিভাবে নিজের ব্লগে বাংলায় লিখতে পারি। বাংলায় মনের ভাব প্রকাশের অসংখ্য মাধ্যম রয়েছে। সামাজিক যোগাযোগের মাধ্যম, পত্রিকা, কিংবা অন্যান্য প্রতিষ্ঠিত ব্লগ। কিন্তু নিজের ব্লগে নিজে বাংলায় লিখতে পারবো কিনা তা নিয়ে একটু সংশয় ছিল কারিগরি দিকটা নিয়ে। কোয়ার্তো দিয়ে আমার এই ব্লগ সাইট বানানো। তাই কোয়ার্তোর ওয়েবসাইট ঘাঁটতে ঘাঁটতে আজ পেয়ে গেলাম কিভাবে ইউনিকোড দিয়ে লিখা যায়। এখন থেকে মাঝে মধ্যেই এখানে বাংলায় পোষ্ট করবো। দেখা যাক।"
  },
  {
    "objectID": "posts/bengalitrial/index.html#আপনক-সবগতম",
    "href": "posts/bengalitrial/index.html#আপনক-সবগতম",
    "title": "বাংলা ভাষায় আমার লেখা || My Blog in Benglali Language",
    "section": "",
    "text": "“বাংলায় ব্লগিং করতে পারলে ভালই হতো” এমন ভাবনা থেকেই ঘাটাঘটি শুরু করলাম কিভাবে নিজের ব্লগে বাংলায় লিখতে পারি। বাংলায় মনের ভাব প্রকাশের অসংখ্য মাধ্যম রয়েছে। সামাজিক যোগাযোগের মাধ্যম, পত্রিকা, কিংবা অন্যান্য প্রতিষ্ঠিত ব্লগ। কিন্তু নিজের ব্লগে নিজে বাংলায় লিখতে পারবো কিনা তা নিয়ে একটু সংশয় ছিল কারিগরি দিকটা নিয়ে। কোয়ার্তো দিয়ে আমার এই ব্লগ সাইট বানানো। তাই কোয়ার্তোর ওয়েবসাইট ঘাঁটতে ঘাঁটতে আজ পেয়ে গেলাম কিভাবে ইউনিকোড দিয়ে লিখা যায়। এখন থেকে মাঝে মধ্যেই এখানে বাংলায় পোষ্ট করবো। দেখা যাক।"
  },
  {
    "objectID": "posts/lu/index.html",
    "href": "posts/lu/index.html",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "",
    "text": "! This program factors a full rank matrix A into lower triangular (or trapezoidal) L and upper\n! triangular matrix U\nprogram LU_decompostion\n    implicit none\n    !############# #################List of main program variable##################################\n    integer::m,n\n    ! m is the # of rows of the matrix that we are working with\n    ! n is the # of columns that we are working with\n    doubleprecision, allocatable,dimension(:,:)::A,A1\n    ! A is the working matrix, A1 is the original matrix preserved to check correctness of factoring\n    integer,allocatable,dimension(:):: P,Q\n    ! P, Q are the row and column permutation vectors for partial and complete pivoting\n    character::method\n    !################################################################################################\n\n    ! ############################ Open an Input Data File###########################################\n    open(unit=1,file=&quot;data.txt&quot;)\n    ! ###############################################################################################\n\n    ! ################################# Read m, n of the matrix A ###################################\n    write(*,*)&quot;Input the number of rows of the matrix A, m&quot;\n    read(*,*) m\n    write(*,*)&quot;Input the number of columns of the matrix A, n&quot;\n    read(*,*)n\n    ! ##############################################################################################\n\n    ! ########################### Allocate Space ###################################################\n    allocate(A(m,n),A1(m,n),P(m),Q(n))\n    ! ##############################################################################################\n\n    ! Create the matrix A\n    print*,\n    call matrixA(m,n,A,A1)\n    print*,\n\n    !##################################### Choose the method #######################################\n    print*,&quot;What method you want to apply?&quot;\n    print*,&quot;For No Pivot input: N&quot;\n    print*, &quot;For Partial Pivot input: P&quot;\n    print*, &quot;For Complete Pivot input: C&quot;\n    read(*,*) method\n    !###############################################################################################\n\n    ! ############################### Execution of the methods #####################################\n    IF(method.eq.&quot;C&quot;.or.method.eq.&quot;c&quot;) THEN\n        print*, &quot;Complete Pivoting method has been selected&quot;\n        print*,\n        call completePivot(m,n,A,A1,P,Q)\n    ELSE IF(method.eq.&quot;P&quot;.or.method.eq.&quot;p&quot;) then\n        print*,&quot;Partial Pivoting method has been selected&quot;\n        print*,\n        call partialPivot(m,n,A,A1,P)\n    ELSE IF (method.eq.&quot;N&quot;.or.method.eq.&quot;n&quot;) then\n        print*, &quot;No Pivoting method has been selected&quot;\n        print*,\n        call noPivot(m,n,A,A1)\n    END IF\n\nend program\n\nsubroutine matrixA(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer::i\n    print*,\n    print*, &quot;This is the provided working matrix&quot;\n    print*\n    do i=1,m\n        read(1,*)A(i,:)\n        A1(i,:)=A(i,:)\n        print*,A(i,:)\n    end do\n    do i=1,n\n        IF(A(i,i)==0) then\n            print*,&quot;A 0 entry was found in the main diagonal.&quot;\n            print*, &quot;Therefore, pivoting is a must required&quot;\n        else if(A(i,i).lt.0.0001) then\n            print*, &quot;WARNING!! Diagonal Element is too small.&quot;\n        END IF\n    end do\nend subroutine\n\nsubroutine completePivot(m,n,A,A1,P,Q)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer,dimension(m),intent(out)::P\n    integer,dimension(n),intent(out)::Q\n    integer::i,j,k,row,col\n    doubleprecision::temp\n\n    do k=1,n\n       call max_val(A,m,n,k,row,col)\n       do i=k,n\n            temp=A(i,col)\n            A(i,col)=A(i,k)\n            A(i,k)=temp\n        end do\n        Q(k)=col\n        do j=k,n\n            temp=A(k,j)\n            A(k,j)=A(row,j)\n            A(row,j)=temp\n        end do\n        P(k)=row\n\n        A(k+1:n,k)=A(k+1:n,k)/A(k,k)\n        do j=k+1,n\n            do i=k+1,n\n                A(i,j)=A(i,j)-A(i,k)*A(k,j)\n            end do\n        end do\n    end do\n    print*,\n    print*,&quot;------------------------------------------------------&quot;\n    print*,&quot;         Complete Pivot A=LU factorized array&quot;\n    print*,&quot;-------------------------------------------------------&quot;\n    print*,\n    do i=1,m\n        print*,A(i,:)\n    end do\n    print*,\n    print*, &quot;Permutation vector P=(&quot;,(P(i),i=1,m-1),&quot;)&quot;\n    print*,\n    print*, &quot;Permutation vector Q=(&quot;,(Q(i),i=1,n-1),&quot;)&quot;\n    print*,\n    print*,&quot;*******************************************************&quot;\n    print*,&quot;       Checking Correctness of the factorization&quot;\n    print*,&quot;*******************************************************&quot;\n    print*,\n    call CheckingCompletePivot(m,n,A,A1)\n\nend subroutine\n\n\nsubroutine CheckingCompletePivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    doubleprecision,dimension(n,n):: U\n    doubleprecision,dimension(m,n)::L,A2\n    integer::i,j\n\n    do i=1,m\n        do j=1,n\n            if(i.le.j)then\n                L(i,j)=0\n            else if (i.gt.j) then\n                L(i,j)=A(i,j)\n            end if\n            L(i,i)=1\n        end do\n    end do\n    do i=1,n\n        do j=1,n\n            if(i.le.j)then\n                U(i,j)=A(i,j)\n            else\n                U(i,j)=0.0\n            end if\n        end do\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Complete Pivot Upper triangular matrix U&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,n\n        print*,U(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Complete Pivot Lower triangular matrix L&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,L(i,:)\n    end do\n\n    A2=matmul(L,U)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;              Product of L U=&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,A2(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;  Factoring Accuracy with the Frobenius-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,Frobenius(m,n,A1-A2)/Frobenius(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, Frobenius(m,n,matmul(abs(L),abs(U)))/Frobenius(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the 1-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,norm1(m,n,A1-A2)/norm1(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, norm1(m,n,matmul(abs(L),abs(U)))/norm1(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the Infinity-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,infinityNorm(m,n,A1-A2)/infinityNorm(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, infinityNorm(m,n,matmul(abs(L),abs(U)))/infinityNorm(m,n,A1)\nend subroutine\n\nsubroutine partialPivot(m,n,A,A1,P)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer,dimension(m),intent(out)::P\n    integer::i,j,k,row\n    doubleprecision::temp\n\n    do k=1,n\n       call max_valP(A,m,n,k,row)\n        do j=k,n\n            temp=A(k,j)\n            A(k,j)=A(row,j)\n            A(row,j)=temp\n        end do\n        P(k)=row\n\n        A(k+1:n,k)=A(k+1:n,k)/A(k,k)\n        do j=k+1,n\n            do i=k+1,n\n                A(i,j)=A(i,j)-A(i,k)*A(k,j)\n            end do\n        end do\n    end do\n    print*,\n    print*,&quot;------------------------------------------------------&quot;\n    print*,&quot;          Partial Pivot A=LU factorized array&quot;\n    print*,&quot;-------------------------------------------------------&quot;\n    print*,\n    do i=1,m\n        print*,A(i,:)\n    end do\n    print*,\n    print*, &quot;Permutation vector P=(&quot;,(P(i),i=1,m-1),&quot;)&quot;\n    print*,\n    print*,&quot;*******************************************************&quot;\n    print*,&quot;       Checking Correctness of the factorization&quot;\n    print*,&quot;*******************************************************&quot;\n    print*,\n    call CheckingPartialPivot(m,n,A,A1)\nend subroutine\n\nsubroutine CheckingPartialPivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    doubleprecision,dimension(n,n):: U\n    doubleprecision,dimension(m,n)::L,A2\n    integer::i,j\n\n    do i=1,m\n        do j=1,n\n            if(i.le.j)then\n                L(i,j)=0\n            else if (i.gt.j) then\n                L(i,j)=A(i,j)\n            end if\n            L(i,i)=1\n        end do\n    end do\n    do i=1,n\n        do j=1,n\n            if(i.le.j)then\n                U(i,j)=A(i,j)\n            else\n                U(i,j)=0.0\n            end if\n        end do\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Partial Pivot Upper triangular matrix U&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,n\n        print*,U(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  Partial Pivot Lower triangular matrix L&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,L(i,:)\n    end do\n\n    A2=matmul(L,U)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;       Partial Pivot Product of L U=&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,A2(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot; Factoring Accuracy with the Frobenius-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,Frobenius(m,n,A1-A2)/Frobenius(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, Frobenius(m,n,matmul(abs(L),abs(U)))/Frobenius(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the 1-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,norm1(m,n,A1-A2)/norm1(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, norm1(m,n,matmul(abs(L),abs(U)))/norm1(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the Infinity-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,infinityNorm(m,n,A1-A2)/infinityNorm(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, infinityNorm(m,n,matmul(abs(L),abs(U)))/infinityNorm(m,n,A1)\nend subroutine\n\nsubroutine noPivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    integer::i,j,k\n\n    do k=1,n\n        A(k+1:n,k)=A(k+1:n,k)/A(k,k)\n        do j=k+1,n\n            do i=k+1,n\n                A(i,j)=A(i,j)-A(i,k)*A(k,j)\n            end do\n        end do\n    end do\n    print*,\n    print*,&quot;------------------------------------------------------&quot;\n    print*,&quot;          No Pivot A=LU factorized array&quot;\n    print*,&quot;-------------------------------------------------------&quot;\n    print*,\n    do i=1,m\n        print*,A(i,:)\n    end do\n    print*,\n    print*,&quot;*******************************************************&quot;\n    print*,&quot;       Checking Correctness of the factorization&quot;\n    print*,&quot;*******************************************************&quot;\n    print*,\n    call CheckingNoPivot(m,n,A,A1)\nend subroutine\n\nsubroutine CheckingNoPivot(m,n,A,A1)\n    integer,intent(in)::m,n\n    doubleprecision,dimension(m,n),intent(inout)::A,A1\n    doubleprecision,dimension(n,n):: U\n    doubleprecision,dimension(m,n)::L,A2\n    integer::i,j\n\n    do i=1,m\n        do j=1,n\n            if(i.le.j)then\n                L(i,j)=0\n            else if (i.gt.j) then\n                L(i,j)=A(i,j)\n            end if\n            L(i,i)=1\n        end do\n    end do\n    do i=1,n\n        do j=1,n\n            if(i.le.j)then\n                U(i,j)=A(i,j)\n            else\n                U(i,j)=0.0\n            end if\n        end do\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  No Pivot Upper triangular matrix U&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,n\n        print*,U(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;  No Pivot Lower triangular matrix L&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,L(i,:)\n    end do\n\n    A2=matmul(L,U)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*,&quot;       No Pivot Product of L U=&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    do i=1,m\n        print*,A2(i,:)\n    end do\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;  Factoring Accuracy with the Frobenius Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,Frobenius(m,n,A1-A2)/Frobenius(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, Frobenius(m,n,matmul(abs(L),abs(U)))/Frobenius(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the 1-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,norm1(m,n,A1-A2)/norm1(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, norm1(m,n,matmul(abs(L),abs(U)))/norm1(m,n,A1)\n    print*,\n    print*, &#39;----------------------------------------------&#39;\n    print*, &quot;     Factoring Accuracy with the Infinity-Norm&quot;\n    print*, &#39;----------------------------------------------&#39;\n    print*,\n    print*,&quot;Relative Error=&quot;,infinityNorm(m,n,A1-A2)/infinityNorm(m,n,A1)\n    print*\n    print*, &quot;Growth Factor=&quot;, infinityNorm(m,n,matmul(abs(L),abs(U)))/infinityNorm(m,n,A1)\nend subroutine\n\nsubroutine max_val(A,m,n,k,row,col)\n    implicit none\n    integer,intent(in)::m,n,k\n    integer,intent(out)::row,col\n    doubleprecision,dimension(m,n),intent(inout)::A\n    doubleprecision::maximum\n    integer::i,j\n\n    maximum=maxval(A(k:m,k:n))\n\n    do i=k,m\n        do j=k,n\n            if(A(i,j)==maximum) then\n                row=i\n                col=j\n                goto 100\n            end if\n        end do\n    end do\n100 end subroutine\n\nsubroutine max_valP(A,m,n,k,row)\n    implicit none\n    integer,intent(in)::m,n,k\n    integer,intent(out)::row\n    doubleprecision,dimension(m,n),intent(inout)::A\n    doubleprecision::maximum\n    integer::i,j\n\n    maximum=maxval(A(k:m,k))\n\n    do i=k,m\n        if(A(i,k)==maximum) then\n            row=i\n            goto 101\n        end if\n    end do\n101 end subroutine\n\n\nfunction norm1(m,n,A)\n    integer::m,n,i,j\n    doubleprecision,dimension(m,n)::A\n    doubleprecision,dimension(n):: normvector\n    doubleprecision::normval\n\n    normval=0\n    do j=1,n\n        do i=1,m\n            normval=normval+abs(A(i,j))\n        end do\n        normvector(j)=normval\n    end do\n    norm1=maxval(normvector(1:n))\n    return\nend function\n\nfunction infinityNorm(m,n,A)\n    integer::m,n,i,j\n    doubleprecision,dimension(m,n)::A\n    doubleprecision,dimension(n):: normvector\n    doubleprecision:: normval\n\n    normval=0\n    do j=1,n\n        do i=1,m\n            normval=normval+abs(A(i,j))\n            normvector(j)=normval\n        end do\n    end do\n    infinityNorm=maxval(normvector(1:m))\n    return\nend function\n\nfunction Frobenius(m,n,A)\n    integer::m,n,i,j\n    doubleprecision,dimension(m,n)::A\n    doubleprecision:: normval\n\n    normval=0\n    do i=1,m\n        do j=1,n\n            normval=normval+(abs(A(i,j)))**2\n        end do\n    end do\n    Frobenius=sqrt(normval)\n    return\nend function"
  },
  {
    "objectID": "posts/lu/index.html#no-pivot",
    "href": "posts/lu/index.html#no-pivot",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "No Pivot",
    "text": "No Pivot\nInput Matrix (Stored in a data file in the same directory where the program file .f90 located)\n8 2 9\n4 9 4\n6 7 9\n\nComand prompt: \n Input the number of rows of the matrix A, m\n3\n Input the number of columns of the matrix A, n\n3\n\n\n This is the provided working matrix\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n   4.0000000000000000        9.0000000000000000        4.0000000000000000\n   6.0000000000000000        7.0000000000000000        9.0000000000000000\n\n What method you want to apply?\n For No Pivot input: N\n For Partial Pivot input: P\n For Complete Pivot input: C\nn\n No Pivoting method has been selected\n\n\n ------------------------------------------------------\n           No Pivot A=LU factorized array\n -------------------------------------------------------\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n  0.50000000000000000        8.0000000000000000      -0.50000000000000000\n  0.75000000000000000       0.68750000000000000        2.5937500000000000\n\n *******************************************************\n        Checking Correctness of the factorization\n *******************************************************\n\n\n ----------------------------------------------\n   No Pivot Upper triangular matrix U\n ----------------------------------------------\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n   0.0000000000000000        8.0000000000000000      -0.50000000000000000\n   0.0000000000000000        0.0000000000000000        2.5937500000000000\n\n ----------------------------------------------\n   No Pivot Lower triangular matrix L\n ----------------------------------------------\n\n   1.0000000000000000        0.0000000000000000        0.0000000000000000\n  0.50000000000000000        1.0000000000000000        0.0000000000000000\n  0.75000000000000000       0.68750000000000000        1.0000000000000000\n\n ----------------------------------------------\n        No Pivot Product of L U=\n ----------------------------------------------\n\n   8.0000000000000000        2.0000000000000000        9.0000000000000000\n   4.0000000000000000        9.0000000000000000        4.0000000000000000\n   6.0000000000000000        7.0000000000000000        9.0000000000000000\n\n ----------------------------------------------\n   Factoring Accuracy with the Frobenius Norm\n ----------------------------------------------\n\n Relative Error=   0.00000000\n\n Growth Factor=   1.02520537\n\n ----------------------------------------------\n      Factoring Accuracy with the 1-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\n\n ----------------------------------------------\n      Factoring Accuracy with the Infinity-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1"
  },
  {
    "objectID": "posts/lu/index.html#partial-pivot",
    "href": "posts/lu/index.html#partial-pivot",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "Partial Pivot",
    "text": "Partial Pivot\nInput Matrix (Stored in a data file in the same directory where the program file .f90 located)\n1 2 4\n2 1 3\n3 2 4\n\nComand Prompt: \n Input the number of rows of the matrix A, m\n3\n Input the number of columns of the matrix A, n\n3\n\n\n This is the provided working matrix\n\n   1.0000000000000000        2.0000000000000000        4.0000000000000000\n   2.0000000000000000        1.0000000000000000        3.0000000000000000\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n\n What method you want to apply?\n For No Pivot input: N\n For Partial Pivot input: P\n For Complete Pivot input: C\np\n Partial Pivoting method has been selected\n\n\n ------------------------------------------------------\n           Partial Pivot A=LU factorized array\n -------------------------------------------------------\n\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n  0.66666666666666663        1.3333333333333335        2.6666666666666670\n  0.33333333333333331      -0.24999999999999992        1.0000000000000000\n\n Permutation vector P=(           3           3 )\n\n *******************************************************\n        Checking Correctness of the factorization\n *******************************************************\n\n\n ----------------------------------------------\n   Partial Pivot Upper triangular matrix U\n ----------------------------------------------\n\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n   0.0000000000000000        1.3333333333333335        2.6666666666666670\n   0.0000000000000000        0.0000000000000000        1.0000000000000000\n\n ----------------------------------------------\n   Partial Pivot Lower triangular matrix L\n ----------------------------------------------\n\n   1.0000000000000000        0.0000000000000000        0.0000000000000000\n  0.66666666666666663        1.0000000000000000        0.0000000000000000\n  0.33333333333333331      -0.24999999999999992        1.0000000000000000\n\n ----------------------------------------------\n        Partial Pivot Product of L U=\n ----------------------------------------------\n\n   3.0000000000000000        2.0000000000000000        4.0000000000000000\n   2.0000000000000000        2.6666666666666670        5.3333333333333339\n   1.0000000000000000       0.33333333333333337        1.6666666666666667\n\n ----------------------------------------------\n  Factoring Accuracy with the Frobenius-Norm\n ----------------------------------------------\n\n Relative Error=  0.618016541\n\n Growth Factor=   1.11492395\n\n ----------------------------------------------\n      Factoring Accuracy with the 1-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\n\n ----------------------------------------------\n      Factoring Accuracy with the Infinity-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1"
  },
  {
    "objectID": "posts/lu/index.html#complete-pivot",
    "href": "posts/lu/index.html#complete-pivot",
    "title": "LU Factorization of a Full rank Matrix using Fortran",
    "section": "Complete Pivot",
    "text": "Complete Pivot\nInput Matrix (Stored in a data file in the same directory where the program file .f90 located)\n  \n2 3 4\n4 7 5\n4 9 5\n\nCommand Prompt:\n Input the number of rows of the matrix A, m\n3\n Input the number of columns of the matrix A, n\n3\n\n\n This is the provided working matrix\n\n   2.0000000000000000        3.0000000000000000        4.0000000000000000\n   4.0000000000000000        7.0000000000000000        5.0000000000000000\n   4.0000000000000000        9.0000000000000000        5.0000000000000000\n\n What method you want to apply?\n For No Pivot input: N\n For Partial Pivot input: P\n For Complete Pivot input: C\nc\n Complete Pivoting method has been selected\n\n\n ------------------------------------------------------\n          Complete Pivot A=LU factorized array\n -------------------------------------------------------\n\n   9.0000000000000000        4.0000000000000000        5.0000000000000000\n  0.77777777777777779        2.3333333333333335       0.66666666666666674\n  0.33333333333333331       0.47619047619047616       0.57142857142857140\n\n Permutation vector P=(           3           3 )\n\n Permutation vector Q=(           2           3 )\n\n *******************************************************\n        Checking Correctness of the factorization\n *******************************************************\n\n\n ----------------------------------------------\n   Complete Pivot Upper triangular matrix U\n ----------------------------------------------\n\n   9.0000000000000000        4.0000000000000000        5.0000000000000000\n   0.0000000000000000        2.3333333333333335       0.66666666666666674\n   0.0000000000000000        0.0000000000000000       0.57142857142857140\n\n ----------------------------------------------\n   Complete Pivot Lower triangular matrix L\n ----------------------------------------------\n\n   1.0000000000000000        0.0000000000000000        0.0000000000000000\n  0.77777777777777779        1.0000000000000000        0.0000000000000000\n  0.33333333333333331       0.47619047619047616        1.0000000000000000\n\n ----------------------------------------------\n               Product of L U=\n ----------------------------------------------\n\n   9.0000000000000000        4.0000000000000000        5.0000000000000000\n   7.0000000000000000        5.4444444444444446        4.5555555555555554\n   3.0000000000000000        2.4444444444444442        2.5555555555555554\n\n ----------------------------------------------\n   Factoring Accuracy with the Frobenius-Norm\n ----------------------------------------------\n\n Relative Error=  0.683437467\n\n Growth Factor=   1.00393677\n\n ----------------------------------------------\n      Factoring Accuracy with the 1-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\n\n ----------------------------------------------\n      Factoring Accuracy with the Infinity-Norm\n ----------------------------------------------\n\n Relative Error=           0\n\n Growth Factor=           1\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/mathbiology/index.html",
    "href": "posts/mathbiology/index.html",
    "title": "Modeling viral disease",
    "section": "",
    "text": "Consider the spreading of a highly communicable disease on an isolated island with population size \\(N\\). A portion of the population travels abroad and returns to the island infected with the disease. You would like to predict the number of people \\(X\\) who will have been infected by some time \\(t\\). Consider the following model, where \\(k &gt; 0\\) is constant:\n\\[\\begin{equation*}\n  \\frac{dX}{dt}=k\\textcolor{red}{X}(N-X)\n\\end{equation*}\\]\n\nList two major assumptions implicit in the preceding model. How reasonable are your assumptions?\nAnswer: Here are two major assumptions:\n\n\n\nFixed population \\(\\implies\\) all infected. We assume the population size remain unchanged that is no one gets in the island or no one gets out of the island. This will lead everyone affected by the disease eventually.\nNo immediate cure or vaccination. We also assume that there is no immediate hard immunity build up among the population or invention of vaccination.\n\n\nGraph \\(\\frac{dX}{dt}\\) versus \\(X\\)\n\n\n\n\nPhoto\n\n\n\nGraph \\(X\\) versus \\(t\\) if the initial number of infections is \\(X_1 &lt; \\frac{N}{2}\\). Graph \\(X\\) versus \\(t\\) if the initial number of infections is \\(X_2 &gt;\\frac{N}{2}\\).\nAnswer: For equilibrium of the model\n\n\\[\\begin{align*}\n  f(X)&=kX(N-X)=0\\\\\n  \\implies kX&=0 & N-X=0\\\\\n  \\implies X=&0  & X=N\n\\end{align*}\\] For the stability analysis:\n\\[\\begin{align*}\nf(X)&=(kN)X-kX^2 & \\implies f'(X)=kN-2kX\n\\end{align*}\\]\nNow, \\(f'(0)=kN&gt;0\\) therefore, \\(X=0\\) is an unstable equilibrium. And \\(f'(N)=kN-2kN=-kN&lt;0\\) since \\(k, N&gt;0\\). So, \\(X=N\\) is a stable equilibrium.\n\n\n\nequilibrium\n\n\nIf the initial infection \\(X_1&lt;\\frac{N}{2}\\) it might decrease and reach to 0 but that is not a stable equilibrium. So eventually it will hit \\(N\\).\n\nSolve the model given earlier for \\(X\\) as a function of \\(t\\).\nAnswer: Solving the ODE we have\n\n\\[\\begin{align*}\n  \\frac{dX}{dt}&=kX(N-X)\\\\\n  \\text{Since}\\hspace{2mm} X&&gt;0\\\\\n  \\frac{dX}{X(N-X)}&=kdt\\\\\n  \\implies \\int \\frac{dX}{X(N-X)}&=\\int kdt\\\\\n  \\implies \\frac{1}{N}\\int \\left(\\frac{1}{X}+\\frac{1}{N-X}\\right)dX&= \\int kdt\\\\\n  \\implies \\frac{1}{N} \\ln\\left(\\frac{X}{N-X}\\right)&=kt+c\\\\\n  \\implies \\ln\\left(\\frac{X}{N-X}\\right)&=Nkt+Nc\\\\\n  \\implies \\frac{X}{N-X}&=e^{Nkt+Nc}\\\\\n  \\implies X&=Ne^{Nkt+Nc}-Xe^{Nkt+Nc}\\\\\n  \\implies X\\left(1+e^{Nkt+Nc}\\right)&=Ne^{Nkt+Nc}\\\\\n  \\implies X(t)&=\\frac{Ne^{Nkt+Nc}}{1+e^{Nkt+Nc}}\\\\\n  \\implies X(t)&=\\frac{N}{1+e^{-(Nkt+Nc)}}\n\\end{align*}\\]\n\nFrom part (d), find the limit of \\(X\\) as \\(t\\) approaches infinity.\nAnswer:\n\n\\[\\begin{align*}\n\\lim_{t\\longrightarrow \\infty} X(t)&=\\lim_{t\\longrightarrow \\infty} \\frac{N}{1+e^{-(Nkt+Nc)}}=N\n\\end{align*}\\]\n\nConsider an island with a population of \\(5000\\). At various times during the epidemic the number of people infected was recorded as follows:\n\n\n\n\n\\(t\\) (days)\n2\n6\n\n\n\n\n\\(X\\) (People infected)\n\\(1887\\)\n\\(4087\\)\n\n\n\\(\\ln{\\left(\\frac{X}{N-X}\\right)}\\)\n\\(-0.5\\)\n\\(1.5\\)\n\n\n\nDo the collected data support the given model?\nAnswer: If we look at part (d) we have\n\\[\\begin{align*}\n\\ln\\left(\\frac{X}{N-X}\\right)&=Nkt+Nc & \\text(And),\\\\\nX(t)&=\\frac{N}{1+e^{-(Nkt+Nc)}}\n\\end{align*}\\]\nSo we get if \\(2Nk+Nc=-0.5\\) then \\(X(2)=\\frac{5000}{1+e^{0.5}}=1887.703\\), if \\(6Nk+Nc=1.5\\) then \\(X(6)=\\frac{5000}{e^{-1.5}}=4087.87\\), and if \\(10Nk+Nc=3.5\\) then \\(X(10)=\\frac{5000}{e^{-3.5}}=4853.44\\)\nTherefore, the collected data supports the model.\n\nUse the results in part (f) to estimate the constants in the model, and predict the number of people who will be infected by \\(t = 12\\) days.\nAnswer: We have\n\n\\[\\begin{align*}\n2Nk+Nc&=-0.5\\\\\n6Nk+Nc&=1.5\n\\end{align*}\\]\nSolving the above system we have \\(k=\\frac{1}{2N}\\) and \\(c=\\frac{-1.5}{N}\\). If we substitute these values in the solution we got in part (d) we have\n\\[\\begin{align*}\n  X(t)&=\\frac{N}{1+e^{-\\left(\\frac{t}{2}-1.5\\right)}}\n\\end{align*}\\]\nSo, \\(X(12)\\approx 4945\\)\n\n\n\n\n\n\n\n\nShare on\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized eigenvectors and eigenspaces\n\n2 min\n\n\nRafiq Islam\n\n\nMonday, January 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)\n\n14 min\n\n\nRafiq Islam\n\n\nThursday, September 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n26 min\n\n\nRafiq Islam\n\n\nTuesday, November 9, 2021\n\n\n\n\n\n\nNo matching items\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Modeling Viral Disease},\n  date = {2021-02-23},\n  url = {https://mrislambd.github.io/posts/mathbiology/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Modeling Viral Disease.” February 23,\n2021. https://mrislambd.github.io/posts/mathbiology/."
  },
  {
    "objectID": "posts/someproofs/index.html",
    "href": "posts/someproofs/index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "posts/someproofs/index.html#let-n-be-a-positive-integer.-show-that-every-matrix-a-in-m_n-times-nmathbbr-can-be-written-as-the-sum-of-two-non-singular-matrices.",
    "href": "posts/someproofs/index.html#let-n-be-a-positive-integer.-show-that-every-matrix-a-in-m_n-times-nmathbbr-can-be-written-as-the-sum-of-two-non-singular-matrices.",
    "title": "",
    "section": "1. Let \\(n\\) be a positive integer. Show that every matrix \\(A \\in M_{n \\times n}(\\mathbb{R})\\) can be written as the sum of two non-singular matrices.",
    "text": "1. Let \\(n\\) be a positive integer. Show that every matrix \\(A \\in M_{n \\times n}(\\mathbb{R})\\) can be written as the sum of two non-singular matrices.\nProof: To prove this, we will use two known properties of matrices.\n\n\\(det(A)=\\) Product of the eigenvalues of \\(A\\)\n\nIf \\(\\lambda\\) is an eigenvalue of \\(A\\) then \\(\\lambda+n\\) is an eigenvalue of \\(A+nI\\) matrix.\n\nSince, \\(A\\in M_{n\\times n}(\\mathbb{R}),\\) let \\(\\lambda_i\\) for \\(1\\le i\\le n\\) be the eigenvalues of \\(A\\). The matrix \\(A+(n+1)I\\) has eigenvalues \\(\\lambda_{i+n+1}\\) for \\(1\\le i\\le n\\).\nLet,\n\\[\\begin{align*}\nn&=max\\{|\\lambda_i| : 1\\le i \\le n\\}\\\\\n\\implies& -n\\le \\lambda_i \\le n \\text{ for all }1\\le i \\le n\\\\\n\\implies& -n+n+1\\le \\lambda_i+n+1 \\le n+n+1\\text{ for all }1\\le i \\le n\\\\\n\\implies& 1\\le \\lambda_i+n+1 \\le 2n+1\\text{ for all }1\\le i \\le n\n\\end{align*}\\] Thus, \\(\\lambda_i+n+1\\ge 1\\) that is \\(\\lambda_i+n+1 \\ne 0\\) and \\(0\\) is not an eigenvalue of \\(A\\).\nNow, from property (1), we have,\n\\(det(A)=\\prod_{i=1}^{n}\\lambda_i\\)\nand\n\\[\\begin{align*}\ndet(A+(n+1)I)&=\\prod_{i=1}^{n}(\\lambda_i+n+1)\\ne 0\\\\\n\\end{align*}\\] \\(\\implies A\\text{ or }A+(n+1)I\\) both are non-singular.\n\\(-(n+1)I\\) is of course non-singular.\nThen\n\\(A=(A+(n+1)I)+(-(n+1)I)\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#let-alpha-in-mathcallv-and-dim-vn-infty",
    "href": "posts/someproofs/index.html#let-alpha-in-mathcallv-and-dim-vn-infty",
    "title": "",
    "section": "2. Let \\(\\alpha \\in \\mathcal{L}(V)\\) and \\(\\dim V=n< \\infty\\)",
    "text": "2. Let \\(\\alpha \\in \\mathcal{L}(V)\\) and \\(\\dim V=n&lt; \\infty\\)\n\nSuppose that \\(\\alpha\\) has two distinct eigenvalues \\(\\lambda\\) and \\(\\mu\\). Prove that if \\(\\dim E_{\\lambda}=n-1\\) then \\(\\alpha\\) is diagonalizable.\n\n\nProof: Since \\(\\mu\\) and \\(\\lambda\\) are two distinct eigenvalues associated with \\(\\alpha\\), so \\(V=E_{\\lambda}\\bigoplus E_{\\mu}\\) and \\(\\dim E_{\\lambda}(\\alpha)+\\dim E_{\\mu}(\\alpha)=n\\).\n\n\nHere, \\(\\dim E_{\\mu}(\\alpha)\\ge 1\\) and \\(\\dim E_{\\lambda}(\\alpha)=n-1\\). So,\n\n\n\\(\\dim E_{\\lambda}(\\alpha)+\\dim E_{\\mu}(\\alpha)=n-1+1=n\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#let-alphain-mathcallv-and-0ne-vin-v-where-dim-vn-infty.",
    "href": "posts/someproofs/index.html#let-alphain-mathcallv-and-0ne-vin-v-where-dim-vn-infty.",
    "title": "",
    "section": "3. Let \\(\\alpha\\in \\mathcal{L}(V)\\) and \\(0\\ne v\\in V\\) where \\(\\dim V=n< \\infty\\).",
    "text": "3. Let \\(\\alpha\\in \\mathcal{L}(V)\\) and \\(0\\ne v\\in V\\) where \\(\\dim V=n&lt; \\infty\\).\n\n\nProve that there is a unique monic polynomial \\(p(t)\\) of the smallest degree such that \\(p(\\alpha)(v)=0\\)\n\n\nProof: Since \\(V\\) is finite-dimensional so there exists smallest \\(k\\) such that \\(\\{v,\\alpha(v),\\cdots,\\alpha^{k-1}(v)\\}\\) is linearly independent but \\(\\{v,\\alpha(v),\\cdots,\\alpha^{k-1}(v),\\alpha^k(v)\\}\\) is linearly dependent. So there exists \\(c_0,c_1,\\cdots,c_k\\in \\mathbb{F}\\) not all zero such that\n\n\n\\(c_0v+c_1\\alpha(v)+c_2\\alpha^2(v)+\\cdots+c_{k-1}\\alpha^{k-1}(v)+c_k\\alpha^k(v)=0\\)\n\n\nWithout loss of generality, let’s assume that \\(c_k\\ne 0\\). Then\n\n\n\\(a_0v+a_1\\alpha(v)+a_2\\alpha^2(v)+\\cdots+a_{k-1}\\alpha^{k-1}(v)+\\alpha^k(v)=0\\)\n\n\nwhere, \\(a_i=\\frac{c_i}{c_k}\\) for \\(1\\le i \\le k\\).\n\n\nThus, \\(p(t)=a_0+a_1t+a_2t^2+\\cdots+a_{k-1}t^{k-1}+t^k\\), a unique monic polynomial such that \\(p(\\alpha)(v)=0\\)\n\n\n\n\n\n(ii) Prove that \\(p(t)\\) from (i) divides the minimal polynomial of \\(\\alpha\\)\n\n\nProof: By polynomial division we have,\n\n\n\\(m(t)=p(t)h(t)+r(t)\\) where, \\(m(t)\\) is the minimal polynomial.\n\n\nThen, \\(m(\\alpha)(v)=p(\\alpha)h(\\alpha)(v)+r(\\alpha)(v)\\)\n\n\n\\(\\implies 0=0+r(\\alpha)(v)\\)\n\n\n\\(\\implies r(\\alpha)(v)=0\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#let-abin-m_ntimes-nmathbbf-such-that-there-exists-an-invertible-matrix-sin-m_ntimes-nmathbbf-such-that-sas-1-and-sbs-1-are-upper-triangular-matrices.-show-that-every-eigenvalue-of-ab-ba-is-zero",
    "href": "posts/someproofs/index.html#let-abin-m_ntimes-nmathbbf-such-that-there-exists-an-invertible-matrix-sin-m_ntimes-nmathbbf-such-that-sas-1-and-sbs-1-are-upper-triangular-matrices.-show-that-every-eigenvalue-of-ab-ba-is-zero",
    "title": "",
    "section": "4. Let \\(A,B\\in M_{n\\times n}(\\mathbb{F})\\) such that there exists an invertible matrix \\(S\\in M_{n\\times n}(\\mathbb{F})\\) such that \\(SAS^{-1}\\) and \\(SBS^{-1}\\) are upper triangular matrices. Show that every eigenvalue of \\(AB-BA\\) is zero",
    "text": "4. Let \\(A,B\\in M_{n\\times n}(\\mathbb{F})\\) such that there exists an invertible matrix \\(S\\in M_{n\\times n}(\\mathbb{F})\\) such that \\(SAS^{-1}\\) and \\(SBS^{-1}\\) are upper triangular matrices. Show that every eigenvalue of \\(AB-BA\\) is zero\n\nProof: To prove the above statement, it is enough to show that \\(spec(AB-BA)=\\{0\\}\\)\n\n\nWe know that if \\(C\\) and \\(D\\) are upper triangular matrices then \\(spec(CD-DC)=\\{0\\}\\). Now let’s assume that \\(C=SAS^{-1}\\) and \\(D=SBS^{-1}\\). Then,\n\n\n\\(CD-DC=SAS^{-1}SBS^{-1}-SBS^{-1}SAS^{-1}\\)\n\n\n\\(\\implies CD-DC=SABS^{-1}-SBAS^{-1}\\)\n\n\n\\(\\implies CD-DC=S(AB-BA)S^{-1}\\)\n\n\nHence \\(CD-DC\\) and \\(AB-BA\\) are similar matrices. So they have the same eigenvalues, that is \\(spec(AB-BA)=\\{0\\}\\)."
  },
  {
    "objectID": "posts/someproofs/index.html#caley-hamilton-theorem",
    "href": "posts/someproofs/index.html#caley-hamilton-theorem",
    "title": "",
    "section": "5. Caley-Hamilton Theorem",
    "text": "5. Caley-Hamilton Theorem\n\nTheorem: Let \\(p(t)\\) be the characteristic polynomial of a matrix \\(A\\). Then \\(p(A)=0\\)\n\n\nBefore we start proving the theorem, we need to discuss some basics.\n\n\nFor Linear Operator: If \\(\\alpha\\in \\mathcal{L}(V)\\) and \\(A=\\Phi_{BB}(\\alpha)\\) is a representation matrix of \\(\\alpha\\) with respect to the basis \\(B\\), then \\(p(A)\\) is the representation matrix of \\(p(\\alpha)\\). Thus we also have \\(p(\\alpha)=0\\) if \\(p\\) is the characteristic polynomial of \\(\\alpha\\)\n\n\nAdjoint Matrix Method: If we have a matrix \\(A=[a_{ij}]\\in M_{n\\times n}(\\mathbb{F})\\) then we define the \\(\\textit{adjoint}\\) of \\(A\\) to be the matrix \\(adj(A)=[b_{ij}]\\in M_{n\\times n}(\\mathbb{F})\\), where \\(b_{ij}=(-1)^{i+j}|A_{ji}|\\) for all \\(1 \\le i,j \\le n\\)\n\n\nAnd, \\(A_{ji}\\) is the matrix obtained by deleting the i-th row and j-th column.\n\n\nExample: Let \\(A=\\left(\\begin{array}{ccc}1 &4 &7\\\\2 &5 &8\\\\3 &6 &9\\end{array}\\right)\\)\n\n\n\\(b_{11}=(-1)^{1+1}|A_{11}|=det\\left(\\begin{array}{cc}5 &8\\\\6 &9\\end{array}\\right)=-3\\)\n\n\n\\(b_{12}=(-1)^{1+2}|A_{21}|=-det\\left(\\begin{array}{cc}4 &7\\\\6 &9\\end{array}\\right)=6\\)\n\n\n\\(b_{13}=(-1)^{1+3}|A_{21}|=det\\left(\\begin{array}{cc}4 &7\\\\5 &8\\end{array}\\right)=-3\\)\n\n\n\\(b_{21}=(-1)^{2+1}|A_{12}|=-det\\left(\\begin{array}{cc}2 &8\\\\3 &9\\end{array}\\right)=6\\)\n\n\n\\(b_{22}=(-1)^{2+2}|A_{22}|=det\\left(\\begin{array}{cc}1 &7\\\\3 &9\\end{array}\\right)=-12\\)\n\n\n\\(b_{23}=(-1)^{2+3}|A_{32}|=-det\\left(\\begin{array}{cc}1 &7\\\\2 &8\\end{array}\\right)=6\\)\n\n\n\\(b_{31}=(-1)^{3+1}|A_{13}|=det\\left(\\begin{array}{cc}2 &5\\\\3 &6\\end{array}\\right)=-3\\)\n\n\n\\(b_{32}=(-1)^{3+2}|A_{23}|=-det\\left(\\begin{array}{cc}1 &4\\\\3 &6\\end{array}\\right)=6\\)\n\n\n\\(b_{33}=(-1)^{3+3}|A_{33}|=-det\\left(\\begin{array}{cc}1 &4\\\\2 &5\\end{array}\\right)=-3\\)\n\n\nThus,\n\n\n\\(adj(A)=\\left(\\begin{array}{ccc}-3 &6 &-3\\\\6 &-12 &6\\\\-3 &6 &-3\\end{array}\\right)\\).\n\n\nThe important formula that we are going to use is that,\n\n\n\\(AA^{-1}=I \\implies A\\frac{adj(A)}{det(A)}=I \\implies A.adj(A)=det(A).I\\) (*)\n\n\n\n\n\nProof: Let \\(A\\in M_{n\\times n}(\\mathbb{F})\\) have the minimal polynomial \\(p(t)=t^n+\\sum_{i=0}^{n-1}a_it^i\\).\n\n\nNow let, \\(adj(tI-A)=[g_{ij}(t)]=\\left(\\begin{array}{cccc}g_{11}(t) &g_{12}(t) &\\cdots &g_{1n}(t)\\\\g_{21}(t) &g_{22}(t) &\\cdots &g_{2n}(t)\\\\\\vdots &\\vdots &\\ddots &\\vdots\\\\g_{n1}(t) &g_{n2}(t) &\\cdots &g_{nn}(t)\\end{array}\\right)\\) be the adjoint matrix of \\((tI-A)\\).\n\n\nSince each \\(g_{ij}(t)\\) is a polynomial of degree at most \\(n-1\\), we can write this as, \\(adj(tI-A)=\\sum_{i=1}^{n}B_it^{n-i}\\) where \\(B_i\\in M_{n\\times n}(\\mathbb{F})\\).\n\n\nThen by (*) we have,\n\n\\[\\begin{align*}\np(t)I&=det(tI-A).I=(tI-A)adj(tI-A)=(tI-A)\\sum_{i=1}^{n}B_it^{n-i}\\\\\n\\implies& (a_0+a_1t+a_2t^2+\\cdots+a_{n-1}t^{n-1}+t^n)I=(tI-A)B_1t^{n-1}+\\cdots+(tI-A)B_{n-1}t+(tI-A)B_n\\\\\n\\implies& (a_0+a_1t+a_2t^2+\\cdots+a_{n-1}t^{n-1}+t^n)I=B_1t^n-AB_1t^{n-1}+B_2t^{n-1}-AB_2t^{n-2}+\\cdots+B_{n-1}t^2-AB_n\n\\end{align*}\\]\n\nBy comparing the coefficients, we get\n\n\n\\(B_1=I\\)\n\n\n\\(B_2-AB_1=a_{n-1}I\\)\n\n\n\\(B_3-AB_2=a_{n-2}I\\)\n\n\n\\(\\vdots\\)\n\n\n\\(B_n-AB_{n-1}=a_1I\\)\n\n\n\\(-AB_n=a_0I\\)\n\n\nNow multiply \\(A^{n+1-j}\\) to the \\(j-th\\) equation, and then sum up both sides we get,\n\n\n\\(A^{n+1-1}B_1=IA^{n+1-1}\\hspace{2.3in} \\implies A^nB_1=A^n\\)\n\n\n\\(A^{n+1-2}(B_2-AB_1)=a_{n-1}A^{n+1-2}\\hspace{1in} \\implies A^{n-1}B_2-AB_1=a_{n-1}A^{n-1}\\)\n\n\n\\(A^{n+1-3}(B_3-AB_2)=a_{n-2}A^{n+1-3}\\hspace{1in} \\implies A^{n-2}B_3-A^{n-1}B_2=a_{n-1}A^{n-2}\\)\n\n\n\\(\\vdots\\hspace{5in} \\vdots\\)\n\n\n\\(A^{n+1-n}(B_n-AB_{n-1})=a_1A^{n+1-n}\\hspace{1in} \\implies AB_n-A^2B_{n-1}=a_1A\\)\n\n\n\\(-AB_n=a_0I\\hspace{3.2in}\\implies -AB_n=a_0I\\)\n\n\nIf we add both sides then we obtain, \\(p(A)=0\\)"
  },
  {
    "objectID": "posts/someproofs/index.html#prove-that-the-spectral-radius-of-the-textitmarkov-matrix-is-less-than-or-equal-to-1",
    "href": "posts/someproofs/index.html#prove-that-the-spectral-radius-of-the-textitmarkov-matrix-is-less-than-or-equal-to-1",
    "title": "",
    "section": "6. Prove that the spectral radius of the \\(\\textit{Markov}\\) matrix is less than or equal to 1",
    "text": "6. Prove that the spectral radius of the \\(\\textit{Markov}\\) matrix is less than or equal to 1\n\nWe need to prove that if \\(A\\) is a Markov matrix then \\(\\rho(A)\\le 1\\). Now, what is a Markov matrix?\n\n\nMarkov Matrix: A matrix \\(A=[a_{i,j}]_{n\\times n}\\) is called a Markov matrix if \\(a_{i,j}\\ge 0\\) for all \\(1\\le i,j \\le n\\) and \\(\\sum_{j=1}^{n} a_i=1\\), that is the sum of the elements in any row is equal to 1.\n\n\nExample: If we have a matrix like this, \\(A=\\left(\\begin{array}{ccc}0.2 &0.4 &0.4\\\\0.1 &0.4 &0.5\\\\0.9 &0.1 &0\\end{array}\\right)\\) then \\(A\\) is a Markov matrix.\n\n\n\n\n\nProof: Let \\(\\lambda \\in spec(A)\\) and \\(x=\\left(\\begin{array}{c}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{array}\\right)\\ne 0\\) be a column vector. Then we define, \\(x_h=max\\{|x_i|: 1\\le i \\le n\\}\\) & \\(&gt;0\\). Here we are assuming \\(x_h &gt;0\\) because \\(x\\ne 0\\), as a result at least one of the coordinate of \\(x\\) must be greater than \\(0\\).\n\n\nNow,\n\n\n\\(Ax=\\lambda x=\\left(\\begin{array}{c}\\lambda x_1\\\\ \\lambda x_2 \\\\ \\vdots \\\\ \\lambda x_n\\end{array}\\right)\\)\n\n\n\\(\\implies \\lambda x_h =\\sum_{j=1}^{n} a_{hj}x_j\\)\n\n\n\\(\\implies |\\lambda x_h|=|\\lambda |.|x_h|=|\\sum_{j=1}^{n} a_{hj}x_j|\\le \\sum_{j=1}^{n} |a_{hj}| |x_j|\\)\n\n\n\\(\\implies |\\lambda |.|x_h| \\le (\\sum_{j=1}^{n} |a_{hj}|) |x_h|=1. |x_h|\\)\n\n\n\\(\\implies |\\lambda| \\le 1\\)\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/eigen/index.html",
    "href": "posts/eigen/index.html",
    "title": "Generalized eigenvectors and eigenspaces",
    "section": "",
    "text": "Definition: Let \\(\\alpha\\in End(V)\\) and \\(\\lambda\\in spec(\\alpha)\\). A non-zero vector \\(v\\) is called a generalized eigenvector vector of \\(\\alpha\\) associated with \\(\\lambda\\) if \\((\\alpha-\\lambda I)^{k}(v)=0\\) and \\((\\alpha-\\lambda I)^{k-1}(v)\\ne 0\\) for some \\(k\\ge 1\\) where \\(k\\) is called the degree of nilpotence for \\(v\\).\n\n\nLet \\(\\lambda\\in spec(\\alpha)\\). Then, \\(M_{\\lambda}=\\bigcup\\limits_{m=1} ker(\\lambda I-\\alpha)^m\\) is what we call it the generalized eigenspace corresponding to \\(\\lambda\\). Clearly, \\(M_{\\lambda}\\) is the union of the zero vector and the set of all generalized eigenvectors of \\(\\alpha\\) associated with \\(\\lambda\\)\n\n\nFact: \\(M_{\\lambda}\\) is a subspace and \\(\\alpha-\\)invariant and if \\(v\\) is a generalized vector of index \\(k\\) then \\(\\{v,(\\alpha-\\lambda I)v,\\cdots, (\\alpha-\\lambda I)^{k-1}v\\}\\) is linearly independent.\n\n\nProof: Let \\(a\\in \\mathbb{F}\\) and let \\(v,w\\in V\\) be generalized eigenvectors of \\(\\alpha\\) associated with \\(\\lambda\\) of degrees \\(k\\) and \\(h\\) respectively. Then,\n\n\n\\((\\alpha-\\lambda I)^k(v)=0\\) and \\((\\alpha-\\lambda I)^h(w)=0\\)\n\n\n\\(\\implies v\\in ker (\\alpha-\\lambda I)^k\\) and \\(w\\in ker(\\alpha-\\lambda I)^h\\)\n\n\n\\(\\implies v\\in ker (\\alpha-\\lambda I)^{k+h}\\) and \\(w\\in ker(\\alpha-\\lambda I)^{k+h}\\) because \\((\\alpha-\\lambda I)^{k+h}(v)=0\\) and \\((\\alpha-\\lambda I)^{k+h}(w)=0\\)\n\n\n\\(\\implies v+w \\in ker(\\alpha-\\lambda)^{k+h}\\)\n\n\nAnd, \\((\\alpha-\\lambda I)^{k+h}(av)=a.(\\alpha-\\lambda I)^{k+h}(v)=0\\).\n\n\nThis implies that \\(M_{\\lambda}\\) is a subspace of \\(V\\).\n\n\nInvariance: If \\(\\beta\\in End(V)\\) commutes with \\(\\alpha\\) and if \\(v\\) is a generalized eigenvector of \\(\\alpha\\) associated with \\(\\lambda\\) such that \\(v\\in ker(\\alpha-\\lambda I)^k\\) then,\n\n\n\\((\\alpha-\\lambda I)^k\\beta(v)=\\beta(\\alpha-\\lambda I)^k(v)=0_V\\)\n\n\n\\(\\implies \\beta(v)\\) is also a generalized eigenvector of \\(\\alpha\\) associated with \\(\\lambda\\)\n\n\n\n\n\nLinearly Independence: If \\(v\\) is a generalized vector of \\(\\alpha\\) associated with \\(\\lambda\\) then \\((\\alpha-\\lambda I)^k(v)=0\\) and \\((\\alpha-\\lambda I)^{k-1}(v)\\ne 0\\). Now we assume that,\n\n\n\\(c_0v+c_1(\\alpha-\\lambda I)(v)+\\cdots+c_{k-1}(\\alpha-\\lambda I)^{k-1}(v)=0\\).\n\n\nWe need to show that \\(c_i's\\) are zero for \\(0\\le i\\le k-1\\).\n\n\nApplying \\((\\alpha-\\lambda)^{k-1}\\) we get,\n\n\n\\((\\alpha-\\lambda)^{k-1}(c_0v+c_1(\\alpha-\\lambda I)(v)+\\cdots+c_{k-1}(\\alpha-\\lambda I)^{k-1}(v))=0\\)\n\n\n\\(\\implies c_0(\\alpha-\\lambda I)^{k-1}(v)=0\\). Since \\((\\alpha-\\lambda I)^{k-1}(v)\\ne 0\\) so we get \\(c_0=0\\).\n\n\nSimilarly applying \\((\\alpha-\\lambda I)^{k-2},(\\alpha-\\lambda I)^{k-3},\\) and so on, we have\n\n\n\\(c_i=0\\) for \\(1\\le i\\le k-1\\).\n\n\nHence, \\(\\{v,(\\alpha-\\lambda I)v,\\cdots, (\\alpha-\\lambda I)^{k-1}v\\}\\) is linearly independent.\n\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\nYou may also like\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation of Gradient Descent (GD) and Stochastic Gradient Descent (SGD)\n\n14 min\n\n\nRafiq Islam\n\n\nThursday, September 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLU Factorization of a Full rank Matrix using Fortran\n\n26 min\n\n\nRafiq Islam\n\n\nTuesday, November 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Representation: Change of Basis\n\n3 min\n\n\nRafiq Islam\n\n\nThursday, January 21, 2021\n\n\n\n\n\n\nNo matching items\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Generalized Eigenvectors and Eigenspaces},\n  date = {2021-01-25},\n  url = {https://mrislambd.github.io/posts/eigen/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Generalized Eigenvectors and\nEigenspaces.” January 25, 2021. https://mrislambd.github.io/posts/eigen/."
  },
  {
    "objectID": "codepages/titanic/index.html",
    "href": "codepages/titanic/index.html",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\n\ntitanic = pd.read_csv('titanic_train.csv')"
  },
  {
    "objectID": "codepages/titanic/index.html#the-data",
    "href": "codepages/titanic/index.html#the-data",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\n\ntitanic = pd.read_csv('titanic_train.csv')"
  },
  {
    "objectID": "codepages/titanic/index.html#exploratory-data-analysis",
    "href": "codepages/titanic/index.html#exploratory-data-analysis",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nDescriptive Statistics\n\ntitanic.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ntitanic.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nSeems like there are some missing data for the Age, Cabin, and Emberked features. To see with visualization\n\nsns.heatmap(titanic.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n\n\n\n\n\n\n\n\nApproximately \\(20\\%\\) of the Age variable is missing. For the feature Cabin, it’s too many observations missing. For the Emberked, there are only two missing observations. So, we need to take extra care of these features in the data cleaning and preparation stage.\n\n\n\nData Visualization\n\nsns.countplot(x='Survived', hue='Sex', data= titanic, palette='RdBu_r')\n\n\n\n\n\n\n\n\nLooks like maximum of the passenger who didn’t survived are male.\n\nsns.countplot(x='Survived', hue='Pclass', data=titanic, palette='rainbow')\n\n\n\n\n\n\n\n\nFrom this plot we see that people from class 3 has the highest proportion who didn’t survive. In the survival class, passenger class 1 has the highest proportion.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.9, 4))\ntitanic['Age'].hist(bins=35, color='darkred', alpha=0.6, ax=ax1)\nax1.set_xlabel('Age')\nax1.set_title('Age Distribution')\ntitanic['Fare'].hist(bins=30, color='darkred', alpha=0.6, ax=ax2)\nax2.set_xlabel('Fare')\nax2.set_title('Fare Distribution')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSeems like Age is almost normally distributed. However, the the Fare is positively skewed. Other categorical features\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7.9, 4))\n\n\nsns.countplot(\n    x='SibSp',data=titanic, ax=ax1\n    )\nax1.set_title('Number of Siblings/Spouse')\n\n\nsns.countplot(\n    x='Parch', data=titanic, ax=ax2\n    )\nax2.set_title('Number of Parents/Children')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "codepages/titanic/index.html#data-cleaning-and-preparation",
    "href": "codepages/titanic/index.html#data-cleaning-and-preparation",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "Data Cleaning and Preparation",
    "text": "Data Cleaning and Preparation\n\nHandling Missing Data\n\nHere, the Age feature is a continuous feature and almost normally distributed. So we can impute this by the mean of the Age variable. However, this feature can be classified by other categorical features such as Sex, Pclass, SibSp, or Perch. But we can be smarter by taking consideration of greater and homogeneously diversified categorical feature. In this case, Pclass is the perfect one.\n\n\nsns.boxplot(\n    x='Pclass', y='Age', hue='Pclass',\n    data=titanic, palette='winter'\n    )\n\n\n\n\n\n\n\n\nSo, whenever a passenger is in the 1st class, the mean Age is around 37 and for the 2nd class and 3rd class the mean Age are 29 and 24, respectively.\n\ndef age_imputation(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    if pd.isnull(Age):\n        if Pclass==1:\n            return 37\n        elif Pclass==2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age\ntitanic.Age = titanic[['Age','Pclass']].apply(age_imputation, axis=1)\nsns.heatmap(titanic.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n\n\n\n\n\n\n\nSince there are too many missing in Cabin, so we can drop it along with two missing values from the Emberked feature.\n\ntitanic.drop('Cabin', axis=1, inplace=True)\ntitanic.dropna(inplace=True)\nsns.heatmap(titanic.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n\n\n\n\n\n\n\nSo there is no missing value in any column. Next we convert the categorical features\n\n\nConverting the Categorical Features\n\ntitanic['Male'] = pd.get_dummies(titanic.Sex,dtype=int)['male']\nemb = pd.get_dummies(titanic['Embarked'],drop_first=True, dtype=int)\ntitanic = pd.concat([titanic, emb], axis=1)\ntitanic.drop(['Sex','Embarked','Name','Ticket'], axis = 1, inplace=True)\ntitanic.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nMale\nQ\nS\n\n\n\n\n0\n1\n0\n3\n22.0\n1\n0\n7.2500\n1\n0\n1\n\n\n1\n2\n1\n1\n38.0\n1\n0\n71.2833\n0\n0\n0\n\n\n2\n3\n1\n3\n26.0\n0\n0\n7.9250\n0\n0\n1\n\n\n3\n4\n1\n1\n35.0\n1\n0\n53.1000\n0\n0\n1\n\n\n4\n5\n0\n3\n35.0\n0\n0\n8.0500\n1\n0\n1"
  },
  {
    "objectID": "codepages/titanic/index.html#modeling",
    "href": "codepages/titanic/index.html#modeling",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nX_train, X_test, y_train, y_test = train_test_split(\n    titanic.drop('Survived', axis=1),\n    titanic.Survived, test_size=0.30,\n    random_state=123\n    )\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\npred = logreg.predict(X_test)\n\n/opt/hostedtoolcache/Python/3.10.18/x64/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning:\n\nlbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
  },
  {
    "objectID": "codepages/titanic/index.html#evaluation",
    "href": "codepages/titanic/index.html#evaluation",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "Evaluation",
    "text": "Evaluation\n\nprint(metrics.classification_report(y_test,pred))\n\n              precision    recall  f1-score   support\n\n           0       0.79      0.89      0.84       161\n           1       0.79      0.64      0.71       106\n\n    accuracy                           0.79       267\n   macro avg       0.79      0.76      0.77       267\nweighted avg       0.79      0.79      0.79       267"
  },
  {
    "objectID": "codepages/lendingclub/temp.html",
    "href": "codepages/lendingclub/temp.html",
    "title": "Mohammad Rafiqul Islam",
    "section": "",
    "text": "from mywebstyle import plot_style\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport kaggle\nplot_style('#f4f4f4')\nloandata = pd.read_csv('lending_club_loan_two.csv')\nfirst_13_cols = loandata.iloc[:, :13]\nfirst_13_cols.head()\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/macpc/.kaggle/kaggle.json'\n\n\n\n\n\n\n\n\n\nloan_amnt\nterm\nint_rate\ninstallment\ngrade\nsub_grade\nemp_title\nemp_length\nhome_ownership\nannual_inc\nverification_status\nissue_d\nloan_status\n\n\n\n\n0\n10000\n36 months\n11.44\n329.48\nB\nB4\nMarketing\n10+ years\nRENT\n117000.0\nNot Verified\nJan-15\nFully Paid\n\n\n1\n8000\n36 months\n11.99\n265.68\nB\nB5\nCredit analyst\n4 years\nMORTGAGE\n65000.0\nNot Verified\nJan-15\nFully Paid\n\n\n2\n15600\n36 months\n10.49\n506.97\nB\nB3\nStatistician\n&lt; 1 year\nRENT\n43057.0\nSource Verified\nJan-15\nFully Paid\n\n\n3\n7200\n36 months\n6.49\n220.65\nA\nA2\nClient Advocate\n6 years\nRENT\n54000.0\nNot Verified\nNov-14\nFully Paid\n\n\n4\n24375\n60 months\n17.27\n609.33\nC\nC5\nDestiny Management Inc.\n9 years\nMORTGAGE\n55000.0\nVerified\nApr-13\nCharged Off\n\n\n\n\n\n\n\n\nsecond_13_cols = loandata.iloc[:, 13:]\nsecond_13_cols.head()\n\n\n\n\n\n\n\n\npurpose\ntitle\ndti\nearliest_cr_line\nopen_acc\npub_rec\nrevol_bal\nrevol_util\ntotal_acc\ninitial_list_status\napplication_type\nmort_acc\npub_rec_bankruptcies\naddress\n\n\n\n\n0\nvacation\nVacation\n26.24\nJun-90\n16\n0\n36369\n41.8\n25\nw\nINDIVIDUAL\n0.0\n0.0\n0174 Michelle Gateway\\r\\nMendozaberg, OK 22690\n\n\n1\ndebt_consolidation\nDebt consolidation\n22.05\nJul-04\n17\n0\n20131\n53.3\n27\nf\nINDIVIDUAL\n3.0\n0.0\n1076 Carney Fort Apt. 347\\r\\nLoganmouth, SD 05113\n\n\n2\ncredit_card\nCredit card refinancing\n12.79\nAug-07\n13\n0\n11987\n92.2\n26\nf\nINDIVIDUAL\n0.0\n0.0\n87025 Mark Dale Apt. 269\\r\\nNew Sabrina, WV 05113\n\n\n3\ncredit_card\nCredit card refinancing\n2.60\nSep-06\n6\n0\n5472\n21.5\n13\nf\nINDIVIDUAL\n0.0\n0.0\n823 Reid Ford\\r\\nDelacruzside, MA 00813\n\n\n4\ncredit_card\nCredit Card Refinance\n33.95\nMar-99\n13\n0\n24584\n69.8\n43\nf\nINDIVIDUAL\n1.0\n0.0\n679 Luna Roads\\r\\nGreggshire, VA 11650\n\n\n\n\n\n\n\n\nloandata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 395900 entries, 0 to 395899\nData columns (total 27 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   loan_amnt             395900 non-null  int64  \n 1   term                  395900 non-null  object \n 2   int_rate              395900 non-null  float64\n 3   installment           395900 non-null  float64\n 4   grade                 395900 non-null  object \n 5   sub_grade             395900 non-null  object \n 6   emp_title             372982 non-null  object \n 7   emp_length            377608 non-null  object \n 8   home_ownership        395900 non-null  object \n 9   annual_inc            395900 non-null  float64\n 10  verification_status   395900 non-null  object \n 11  issue_d               395900 non-null  object \n 12  loan_status           395900 non-null  object \n 13  purpose               395900 non-null  object \n 14  title                 394145 non-null  object \n 15  dti                   395900 non-null  float64\n 16  earliest_cr_line      395900 non-null  object \n 17  open_acc              395900 non-null  int64  \n 18  pub_rec               395900 non-null  int64  \n 19  revol_bal             395900 non-null  int64  \n 20  revol_util            395624 non-null  float64\n 21  total_acc             395900 non-null  int64  \n 22  initial_list_status   395900 non-null  object \n 23  application_type      395900 non-null  object \n 24  mort_acc              358117 non-null  float64\n 25  pub_rec_bankruptcies  395365 non-null  float64\n 26  address               395900 non-null  object \ndtypes: float64(7), int64(5), object(15)\nmemory usage: 81.6+ MB\n\n\n\nsns.heatmap(loandata.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n\n\n\n\n\n\n\n\n\nloandata.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nloan_amnt\n395900.0\n14114.249305\n8357.637338\n500.00\n8000.00\n12000.00\n20000.00\n40000.00\n\n\nint_rate\n395900.0\n13.639385\n4.472112\n5.32\n10.49\n13.33\n16.49\n30.99\n\n\ninstallment\n395900.0\n431.859947\n250.733444\n16.08\n250.33\n375.43\n567.30\n1533.81\n\n\nannual_inc\n395900.0\n74206.819251\n61645.032777\n0.00\n45000.00\n64000.00\n90000.00\n8706582.00\n\n\ndti\n395900.0\n17.379187\n18.021550\n0.00\n11.28\n16.91\n22.98\n9999.00\n\n\nopen_acc\n395900.0\n11.311081\n5.137591\n0.00\n8.00\n10.00\n14.00\n90.00\n\n\npub_rec\n395900.0\n0.178204\n0.530716\n0.00\n0.00\n0.00\n0.00\n86.00\n\n\nrevol_bal\n395900.0\n15844.331435\n20589.846553\n0.00\n6026.00\n11181.00\n19620.00\n1743266.00\n\n\nrevol_util\n395624.0\n53.793449\n24.452575\n0.00\n35.80\n54.80\n72.90\n892.30\n\n\ntotal_acc\n395900.0\n25.414622\n11.887279\n2.00\n17.00\n24.00\n32.00\n151.00\n\n\nmort_acc\n358117.0\n1.814091\n2.148006\n0.00\n0.00\n1.00\n3.00\n34.00\n\n\npub_rec_bankruptcies\n395365.0\n0.121647\n0.356176\n0.00\n0.00\n0.00\n0.00\n8.00\n\n\n\n\n\n\n\n\nsns.countplot(x='loan_status', data=loandata)\nfp = np.round(\n    len(loandata[loandata['loan_status'] == 'Fully Paid'])/len(loandata)*100, 2\n)\n\nco = np.round(\n    len(loandata[loandata['loan_status'] == 'Charged Off']) /\n    len(loandata)*100, 2\n)\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nloandata[loandata['loan_status'] == 'Charged Off']['loan_amnt'].hist(\n    alpha=0.5, color='red', bins=30, ax=ax1,\n    label='Charged Off'\n)\nloandata[loandata['loan_status'] == 'Fully Paid']['loan_amnt'].hist(\n    alpha=0.5, color='blue', bins=30, ax=ax1,\n    label='Fully Paid'\n)\nax1.set_title('Loan Amount Distribution')\nax1.set_xlabel('Loan Amount')\nax1.legend()\n\nax2 = fig.add_subplot(122)\nloandata[loandata['loan_status'] == 'Charged Off']['installment'].hist(\n    alpha=0.5, color='red', bins=30, ax=ax2,\n    label='Charged Off'\n)\nloandata[loandata['loan_status'] == 'Fully Paid']['installment'].hist(\n    alpha=0.5, color='blue', bins=30, ax=ax2,\n    label='Fully Paid'\n)\nax2.set_title('Installment Distribution')\nax2.set_xlabel('Installment')\nax2.legend()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8.8, 4))\nax1 = fig.add_subplot(121)\nsns.boxplot(\n    x='loan_status', y='loan_amnt', hue='loan_status',\n    data=loandata, ax=ax1, palette='winter'\n)\nax1.set_title('Loan Amount Boxplot')\n\nax2 = fig.add_subplot(122)\nsns.boxplot(\n    x='loan_status', y='installment', hue='loan_status',\n    data=loandata, ax=ax2, palette='winter'\n)\nax2.set_title('Installment Boxplot')\n\nText(0.5, 1.0, 'Installment Boxplot')\n\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8.8, 4))\nax1 = fig.add_subplot(121)\nsns.countplot(\n    x='loan_status',\n    hue='term', data=loandata,\n    palette='RdBu_r', ax=ax1\n)\nax2 = fig.add_subplot(122)\nsns.countplot(\n    x='loan_status',\n    hue='grade', data=loandata,\n    palette='winter', ax=ax2\n)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.histplot(x='sub_grade', hue='loan_status', data=loandata, ax=ax)\n\n\n\n\n\n\n\n\n\nloandata['emp_title'] = loandata['emp_title'].str.lower()\nloandata.emp_title.value_counts()[:25]\n\nemp_title\nmanager                     5635\nteacher                     5426\nregistered nurse            2626\nsupervisor                  2589\nsales                       2381\ndriver                      2306\nowner                       2200\nrn                          2072\nproject manager             1776\noffice manager              1638\ngeneral manager             1460\ntruck driver                1288\ndirector                    1192\nengineer                    1187\npolice officer              1041\nvice president               961\nsales manager                961\noperations manager           960\nstore manager                941\npresident                    877\nadministrative assistant     865\naccountant                   845\naccount manager              845\ntechnician                   839\nmechanic                     753\nName: count, dtype: int64\n\n\n\npd.set_option('future.no_silent_downcasting', True)\nloandata['emp_length'] = loandata['emp_length'].replace({\n    '&lt; 1 year': 0,\n    '1 year': 1,\n    '2 years': 2,\n    '3 years': 3,\n    '4 years': 4,\n    '5 years': 5,\n    '6 years': 6,\n    '7 years': 7,\n    '8 years': 8,\n    '9 years': 9,\n    '10+ years': 10\n}\n).infer_objects(copy=False)\n\nloandata['emp_length_group'] = pd.cut(\n    loandata['emp_length'],\n    bins=[-1, 2, 7, 10],  # Bins: &lt;3 years, 3-7 years, &gt; 7 years\n    labels=['Short-term', 'Mid-term', 'Long-term']\n)\n\nsns.countplot(\n    x='emp_length_group',\n    hue='loan_status',\n    data=loandata,\n    palette='winter',\n    stat='count'\n)\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8, 10))\nax1 = fig.add_subplot(211)\nannual_income_threshod = loandata['annual_inc'].quantile(0.95)\nfiltered_income = loandata[loandata['annual_inc'] &lt;= annual_income_threshod]\nsns.boxplot(\n    x='loan_status', y='annual_inc',\n    hue='loan_status', palette='winter',\n    data=filtered_income, ax=ax1\n)\nax1.set_title('Income Distribution')\nax1.set_xlabel('Loan Status')\nax1.set_ylabel('Annual Income')\n\nax2 = fig.add_subplot(212)\nsns.countplot(\n    x='home_ownership', hue='loan_status',\n    data=loandata, ax=ax2\n)\n\n\n\n\n\n\n\n\n\nloandata['issue_d'] = pd.to_datetime(\n    loandata['issue_d'], format='%b-%y'\n)\nloandata = loandata.sort_values('issue_d')\nloan_status_trend = loandata.groupby(\n    ['issue_d', 'loan_status']).size().unstack()\n\nfig = plt.figure(figsize=(8, 10))\nax1 = fig.add_subplot(211)\nloan_status_trend.plot(\n    kind='line', marker='o', ax=ax1\n)\nax1.set_title('Loan Status Over Time by Issue Date')\nax1.set_xlabel('Issue Date(mm-yyyy)')\nax1.set_ylabel('Number of Loans')\nax1.legend(title='Loan Status')\n\nax2 = fig.add_subplot(212)\nsns.countplot(\n    x='verification_status', hue='loan_status',\n    data=loandata, palette='winter', ax=ax2\n)\n\n\n\n\n\n\n\n\n\nloandata['purpose'].value_counts()\n\npurpose\ndebt_consolidation    234420\ncredit_card            82998\nhome_improvement       24024\nother                  21177\nmajor_purchase          8788\nsmall_business          5701\ncar                     4696\nmedical                 4194\nmoving                  2853\nvacation                2452\nhouse                   2201\nwedding                 1811\nrenewable_energy         328\neducational              257\nName: count, dtype: int64\n\n\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nsns.countplot(\n    y='purpose', hue='loan_status',\n    data=loandata, palette='coolwarm'\n)\nax1.set_title('Loan Purpose')\n\nax2 = fig.add_subplot(122)\ndti_threshold = loandata['dti'].quantile(0.95)\nfiltereddata = loandata[loandata['dti'] &lt;= dti_threshold]\n\nsns.boxplot(\n    x='loan_status', y='dti',\n    hue='loan_status', data=filtereddata,\n    palette='coolwarm', ax=ax2\n)\nax2.set_title('Debt-to-Income Ratio on Loan Status')\n\nText(0.5, 1.0, 'Debt-to-Income Ratio on Loan Status')\n\n\n\n\n\n\n\n\n\n\nInsights: From the purpose column, we see that most of the loans that were charged off were used to make debt consolidation. Therefore, debt consolidation may have been a significant factor when a loan is charged off. Another insight we obtain from the debt-to-income ratio is that the charged off loans have higher dti ratio\n\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nfiltered_open_account_threshold = loandata['open_acc'].quantile(0.98)\nfiltered_open_account = loandata[loandata['open_acc']\n                                 &lt;= filtered_open_account_threshold]\nfiltered_open_account[filtered_open_account['loan_status'] == 'Fully Paid']['open_acc'].hist(\n    alpha=0.5, color='green', bins=30, label='Fully Paid', ax=ax1\n)\nfiltered_open_account[filtered_open_account['loan_status'] == 'Charged Off']['open_acc'].hist(\n    alpha=0.5, color='red', bins=30, label='Charged Off', ax=ax1\n)\nax1.set_xlabel('Number of Credit Acc.')\nax1.legend()\nax1.set_title('Number of credit accounts and Loan Status')\n\nloandata['pub_rec_group'] = pd.cut(\n    loandata['pub_rec'], bins=[-1, 0, 1, 3, loandata['pub_rec'].max()],\n    labels=['0', '1', '2-3', '4+']\n)\nloan_status_by_pub_rec = loandata.groupby(\n    ['pub_rec_group', 'loan_status'], observed=False\n).size().unstack()\n\nax2 = fig.add_subplot(122)\nloan_status_by_pub_rec.plot(\n    kind='bar', stacked=False, edgecolor='black',\n    color=['#1f77b4', '#ff7f0e'], ax=ax2\n)\nax2.set_title('Public Records and Loan Status')\nax2.set_xlabel('Public Records')\n\nText(0.5, 0, 'Public Records')\n\n\n\n\n\n\n\n\n\n\nsns.boxplot(\n    y='open_acc', x='loan_status',\n    hue='loan_status',\n    data=filtered_open_account\n)\n\n\n\n\n\n\n\n\n\n\n\nInsights: Number of credit account seems normally distributed among both groups except for some outliers. However, the mean number of credit accounts are slightly higher for the charged off category than the fully paid category. So, higher credit account has some sort of relation with loan being charged off. Also, people who doesn’t have any public record seems to have higher chance of loan status being charged off\n\n\nrevol_bal_threshold = loandata['revol_bal'].quantile(0.95)\nfiltered_revol_bal = loandata[loandata['revol_bal'] &lt;= revol_bal_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_bal',\n    data=filtered_revol_bal, hue='loan_status',\n    palette='Set2'\n)\n\n\n\n\n\n\n\n\n\nrevol_util_threshold = loandata['revol_util'].quantile(0.95)\nfiltered_revol_util = loandata[loandata['revol_util'] &lt;= revol_util_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_util',\n    hue='loan_status', data=filtered_revol_util,\n    palette='Set1'\n)\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nrevol_bal_threshold = loandata['revol_bal'].quantile(0.95)\nfiltered_revol_bal = loandata[loandata['revol_bal'] &lt;= revol_bal_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_bal',\n    data=filtered_revol_bal, hue='loan_status',\n    palette='Set2', ax=ax1\n)\nax1.set_xlabel('Loan Status')\nax1.set_ylabel('Revolving Balance')\nax1.set_title('Revolving Balance vs Loan Status')\n\nax2 = fig.add_subplot(122)\nrevol_util_threshold = loandata['revol_util'].quantile(0.95)\nfiltered_revol_util = loandata[loandata['revol_util'] &lt;= revol_util_threshold]\nsns.boxplot(\n    x='loan_status', y='revol_util',\n    hue='loan_status', data=filtered_revol_util,\n    palette='Set1', ax=ax2\n)\nax2.set_xlabel('Loan Status')\nax2.set_ylabel('Revolving Utilization')\nax2.set_title('Revolving Utilization vs Loan Status')\n\nText(0.5, 1.0, 'Revolving Utilization vs Loan Status')\n\n\n\n\n\n\n\n\n\n\nloandata['total_acc'].value_counts()\n\ntotal_acc\n21     14274\n22     14255\n20     14220\n23     13915\n24     13874\n       ...  \n151        1\n104        1\n135        1\n108        1\n115        1\nName: count, Length: 118, dtype: int64\n\n\n\nfig = plt.figure(figsize=(9, 4))\nax1 = fig.add_subplot(121)\nfiltered_total_account_threshold = loandata['total_acc'].quantile(0.95)\nfiltered_total_account = loandata[loandata['total_acc']\n                                  &lt;= filtered_total_account_threshold]\nfiltered_total_account[filtered_total_account['loan_status'] == 'Fully Paid']['total_acc'].hist(\n    alpha=0.5, color='green', bins=30, label='Fully Paid', ax=ax1\n)\nfiltered_total_account[filtered_total_account['loan_status'] == 'Charged Off']['total_acc'].hist(\n    alpha=0.5, color='red', bins=30, label='Charged Off', ax=ax1\n)\nax1.set_xlabel('Number of Total Credit Acc.')\nax1.legend()\nax1.set_title('Number of Total credit accounts and Loan Status')\n\n\nax2 = fig.add_subplot(122)\nsns.countplot(\n    x='initial_list_status', hue='loan_status',\n    data=loandata, palette='winter'\n)\nax2.set_title('Initial Status and Loan Status')\nax2.set_xlabel('Initial Status (Funded or Withdrawn)')\n\nText(0.5, 0, 'Initial Status (Funded or Withdrawn)')\n\n\n\n\n\n\n\n\n\n\nsns.boxplot(\n    y='total_acc', x='loan_status',\n    hue='loan_status',\n    data=filtered_total_account\n)\n\n\n\n\n\n\n\n\n\nloandata['application_type'].value_counts()\n\napplication_type\nINDIVIDUAL    395189\nJOINT            425\nDIRECT_PAY       286\nName: count, dtype: int64\n\n\n\nloandata['mort_acc'].value_counts()\n\nmort_acc\n0.0     139727\n1.0      60392\n2.0      49931\n3.0      38040\n4.0      27880\n5.0      18188\n6.0      11067\n7.0       6050\n8.0       3121\n9.0       1655\n10.0       865\n11.0       479\n12.0       264\n13.0       146\n14.0       107\n15.0        61\n16.0        37\n17.0        22\n18.0        18\n19.0        15\n20.0        13\n24.0        10\n22.0         7\n25.0         4\n21.0         4\n27.0         3\n23.0         2\n31.0         2\n32.0         2\n26.0         2\n34.0         1\n30.0         1\n28.0         1\nName: count, dtype: int64\n\n\n\nmort_acc_summary = loandata.groupby('loan_status')['mort_acc'].describe()\nprint(mort_acc_summary)\n\n                count      mean       std  min  25%  50%  75%   max\nloan_status                                                        \nCharged Off   72103.0  1.501214  1.974335  0.0  0.0  1.0  2.0  23.0\nFully Paid   286014.0  1.892967  2.182550  0.0  0.0  1.0  3.0  34.0\n\n\n\n# Bin the mort_acc column into categories\nloandata['mort_acc_group'] = pd.cut(\n    loandata['mort_acc'], bins=[-1, 0, 2, 5, 10, loandata['mort_acc'].max()],\n    labels=['0', '1-2', '3-5', '6-10', '10+']\n)\n\n# Plot the grouped bar chart\nmort_acc_counts = loandata.groupby(\n    ['mort_acc_group', 'loan_status'], observed=False\n).size().unstack()\n\nmort_acc_counts.plot(kind='bar', stacked=False,\n                     figsize=(10, 6), colormap='viridis')\nplt.title('Loan Status by Number of Mortgage Accounts')\nplt.xlabel('Number of Mortgage Accounts (Grouped)')\nplt.ylabel('Number of Loans')\nplt.xticks(rotation=0)\nplt.legend(title='Loan Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nloandata['pub_rec_bankruptcies'].value_counts()\n\npub_rec_bankruptcies\n0.0    350265\n1.0     42776\n2.0      1846\n3.0       351\n4.0        82\n5.0        32\n6.0         7\n7.0         4\n8.0         2\nName: count, dtype: int64\n\n\n\nloandata['pub_rec_bankruptcies_group'] = pd.cut(\n    loandata['pub_rec_bankruptcies'], bins=[-1, 0,\n                                            loandata['pub_rec_bankruptcies'].max()],\n    labels=['0', '1 or More']\n)\n\n# Plot the grouped bar chart\npub_rec_bankruptcies_counts = loandata.groupby(\n    ['pub_rec_bankruptcies_group', 'loan_status'], observed=False\n).size().unstack()\n\npub_rec_bankruptcies_counts.plot(\n    kind='bar', stacked=False, figsize=(10, 6), colormap='viridis')\nplt.title('Loan Status by Number of Public Record of Bankruptcies')\nplt.xlabel('Public Record of Bankruptcies (Grouped)')\nplt.ylabel('Number of Loans')\nplt.xticks(rotation=0)\nplt.legend(title='Loan Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(9, 4))\n\nax1 = fig.add_subplot(121)\n\nloandata['mort_acc_group'] = pd.cut(\n    loandata['mort_acc'], bins=[-1, 0, 2, 5, 10, loandata['mort_acc'].max()],\n    labels=['0', '1-2', '3-5', '6-10', '10+']\n)\n\nmort_acc_counts = loandata.groupby(\n    ['mort_acc_group', 'loan_status'], observed=False\n).size().unstack()\n\nmort_acc_counts.plot(\n    kind='bar', stacked=False,\n    colormap='viridis', ax=ax1\n)\nax1.set_title('Loan Status by the # of Mort. Acc')\nax1.set_xlabel('Number of Mortgage Accounts (Grouped)')\nax1.set_ylabel('Number of Loans')\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)\nax1.legend(title='Loan Status')\n\nax2 = fig.add_subplot(122)\n\nloandata['pub_rec_bankruptcies_group'] = pd.cut(\n    loandata['pub_rec_bankruptcies'], bins=[-1, 0,\n                                            loandata['pub_rec_bankruptcies'].max()],\n    labels=['0', '1 or More']\n)\n\n# Plot the grouped bar chart\npub_rec_bankruptcies_counts = loandata.groupby(\n    ['pub_rec_bankruptcies_group', 'loan_status'], observed=False\n).size().unstack()\n\npub_rec_bankruptcies_counts.plot(\n    kind='bar', stacked=False,\n    colormap='coolwarm', ax=ax2\n)\nax2.set_title('Loan Status by the # of Pub Rec of Bankruptcies')\nax2.set_xlabel('Public Record of Bankruptcies (Grouped)')\nax2.set_ylabel('Number of Loans')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\nax2.legend(title='Loan Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nloandata['application_type'].value_counts()\n\napplication_type\nINDIVIDUAL    395189\nJOINT            425\nDIRECT_PAY       286\nName: count, dtype: int64\n\n\n\nsns.countplot(\n    x='loan_status', hue= 'application_type', \n    data=loandata, palette='winter'\n)\n\n\n\n\n\n\n\n\n\nnumeric_loandata = loandata.select_dtypes(include=['float64','int64'])\nplt.figure(figsize=(10,8))\nsns.heatmap(numeric_loandata.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "portfolio.html#data-science-and-machine-learning-projects",
    "href": "portfolio.html#data-science-and-machine-learning-projects",
    "title": "",
    "section": "Data Science and Machine Learning Projects",
    "text": "Data Science and Machine Learning Projects\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nAuto Loan Decision Model\n\n\n\n\n\n\nRafiq Islam\n\n\nNov 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information\n\n\n\n\n\n\nRafiq Islam\n\n\nOct 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisease diagnosis using classification and NLP\n\n\n\n\n\n\nRebecca Ceppas de Castro, Fulya Tastan, Philip Barron, Mohammad Rafiqul Islam, Nina Adhikari, Viraj Meruliya\n\n\nJun 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Selection: A linear regression approach to find the impact of the features of e-commerce sales data\n\n\n\n\n\n\nRafiq Islam\n\n\nAug 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsurance Cost Forecast by using Linear Regression\n\n\n\n\n\n\nRafiq Islam\n\n\nAug 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nLendingclub’s loan default prediction\n\n\n\n\n\n\nRafiq Islam\n\n\nFeb 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Product Success Using Customer Reviews and Sales Data\n\n\n\n\n\n\nRafiq Islam\n\n\nOct 11, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio.html#software-package-and-development",
    "href": "portfolio.html#software-package-and-development",
    "title": "",
    "section": "Software, Package, and Development",
    "text": "Software, Package, and Development\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nStreamlit Web App\n\n\n\nFriday, August 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Application Library: desgld packaging\n\n\n\nFriday, May 3, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio.html#teaching",
    "href": "portfolio.html#teaching",
    "title": "",
    "section": "Teaching",
    "text": "Teaching\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\nSpring 2025: MAC2311 Calculus With Analytic Geometry I\n\n\n\n\n\n\nSpring 2024: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\nFall 2023: MAP4170 Introduction to Actuarial Mathematics\n\n\n\n\n\n\nSpring 2023: MAC1140 PreCalculus Algebra\n\n\n\n\n\n\nFall 2022: MAC2311 Calculus and Analytic Geometry I\n\n\n\n\n\n\nFall 2021 and Spring 2022: PreCalculus and Algebra\n\n\n\n\n\n\nFall 2018 to Spring 2020: College Algebra, Trigonometry\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/spd/desgld/index.html",
    "href": "portfolio/spd/desgld/index.html",
    "title": "Python Application Library: desgld packaging",
    "section": "",
    "text": "This package is related to my ongoing project “EXTRA decentralized stochastic gradient Langevin dynamics”. The detail of the package can be found here\nShare on\n\n\n\n\n\n\nShare\n\n\n\nTweet\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2024,\n  author = {Islam, Rafiq},\n  title = {Python {Application} {Library:} Desgld Packaging},\n  date = {2024-05-03},\n  url = {https://mrislambd.github.io/portfolio/spd/desgld/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2024. “Python Application Library: Desgld\nPackaging.” May 3, 2024. https://mrislambd.github.io/portfolio/spd/desgld/."
  },
  {
    "objectID": "portfolio/dsp/nlp-sales/index.html",
    "href": "portfolio/dsp/nlp-sales/index.html",
    "title": "Predicting Product Success Using Customer Reviews and Sales Data",
    "section": "",
    "text": "Notebook GitHub (Private until finished)\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2024,\n  author = {Islam, Rafiq},\n  title = {Predicting {Product} {Success} {Using} {Customer} {Reviews}\n    and {Sales} {Data}},\n  date = {2024-10-11},\n  url = {https://mrislambd.github.io/portfolio/dsp/nlp-sales/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2024. “Predicting Product Success Using Customer\nReviews and Sales Data.” October 11, 2024. https://mrislambd.github.io/portfolio/dsp/nlp-sales/."
  },
  {
    "objectID": "portfolio/dsp/titanic/index.html",
    "href": "portfolio/dsp/titanic/index.html",
    "title": "Classification Probelm: Predict the chance of survival of a voager on Titanic based on the voager’s information",
    "section": "",
    "text": "Notebook\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{islam2021,\n  author = {Islam, Rafiq},\n  title = {Classification {Probelm:} {Predict} the Chance of Survival of\n    a Voager on {Titanic} Based on the Voager’s Information},\n  date = {2021-10-15},\n  url = {https://mrislambd.github.io/portfolio/dsp/titanic/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nIslam, Rafiq. 2021. “Classification Probelm: Predict the Chance of\nSurvival of a Voager on Titanic Based on the Voager’s\nInformation.” October 15, 2021. https://mrislambd.github.io/portfolio/dsp/titanic/."
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html",
    "href": "portfolio/dsp/lendingclub/index.html",
    "title": "Lendingclub’s loan default prediction",
    "section": "",
    "text": "Notebook"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#project-overview",
    "href": "portfolio/dsp/lendingclub/index.html#project-overview",
    "title": "Lendingclub’s loan default prediction",
    "section": "Project Overview",
    "text": "Project Overview\n\nLendingClub is a U.S.-based financial services company that initially began as a peer-to-peer (P2P) lending platform, allowing individual investors to lend directly to individual borrowers through a marketplace. Founded in 2006, it grew quickly to become one of the largest and most popular P2P lending platforms, helping connect borrowers in need of personal loans with investors looking for alternative investment opportunities.  LendingClub has been known for its transparent data-sharing practices, making anonymized loan data available to researchers, data scientists, and investors. This data is widely used in financial research, especially for predictive modeling of loan risk and borrower behavior.   The aim of this data science project is to build a machine learning model to predict the likelihood of a loan default.\n\n\nMore information about LendingClub\n\nP2P Lending Model (Early Focus):\n\nBorrowers could apply for loans, typically unsecured personal loans, through LendingClub’s platform.\nLoans were then funded by individual investors who could review borrowers’ profiles, risk grades, and other financial information before committing funds.\nThe model offered borrowers a way to access loans outside traditional banks, often at lower interest rates, and allowed investors to diversify by spreading investments across multiple loans.\n\nRisk and Return:\n\nLendingClub assigned credit grades (A–G) to each loan based on creditworthiness, which affected interest rates. Higher risk meant potentially higher returns for investors but also higher chances of default.\nInvestors bore the risk if a borrower defaulted, which was a notable risk factor compared to FDIC-insured deposits.\n\nShift to a Bank Model:\n\nOver time, LendingClub transitioned away from P2P lending and restructured as a more traditional bank, obtaining a bank charter in 2021.\nIt now offers banking products, such as high-yield savings accounts, and operates more like a digital bank while still focusing on lending products.\n\nBorrower and Loan Profiles:\n\nLendingClub primarily focuses on personal loans for debt consolidation, credit card refinancing, home improvement, and other purposes.\nBorrowers’ profiles typically include information on income, credit score, debt-to-income ratio, and loan purpose, which is used for assessing risk."
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#dataset",
    "href": "portfolio/dsp/lendingclub/index.html#dataset",
    "title": "Lendingclub’s loan default prediction",
    "section": "Dataset",
    "text": "Dataset\n\nThe dataset is a publicly available data from kaggle.com. It originally contains 396030 entries, with 100.4 MB. However, for easy github push, I reduce the dataset slightly in order to have size smaller than 100 MB. So, in the reduced form, it has 395900 entries with the following columns:\n\n\nloan_amnt: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n\nterm: The number of payments on the loan. Values are in months and can be either 36 or 60.\n\nint_rate: Interest Rate on the loan installment The monthly payment owed by the borrower if the loan originates.\n\ngrade: LC assigned loan grade\n\nsub_grade: LC assigned loan subgrade\n\nemp_title: The job title supplied by the Borrower when applying for the loan.\n\nemp_length: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n\nhome_ownership: The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n\nannual_inc: The self-reported annual income provided by the borrower during registration.\n\nverification_status: Indicates if income was verified by LC, not verified, or if the income source was verified\n\nissue_d: The month which the loan was funded\n\nloan_status: Current status of the loan\n\npurpose: A category provided by the borrower for the loan request.\n\ntitle: The loan title provided by the borrower\n\nzip_code: The first 3 numbers of the zip code provided by the borrower in the loan application.\n\naddr_state: The state provided by the borrower in the loan application.\n\ndti: A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n\nearliest_cr_line: The month the borrower’s earliest reported credit line was opened.\n\nopen_acc: The number of open credit lines in the borrower’s credit file.\n\npub_rec: Number of derogatory public records.\n\nrevol_bal: Total credit revolving balance.\n\nrevol_util: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n\ntotal_acc: The total number of credit lines currently in the borrower’s credit file\ninitial_list_status: The initial listing status of the loan. Possible values are – W, F\napplication_type: Indicates whether the loan is an individual application or a joint application with two co-borrowers.\n\nmort_acc: Number of mortgage accounts.\n\npub_rec_bankruptcies: Number of public record bankruptcies\n\n\nHere loan_status is the predictive or dependent variable and the rest are predicting or independent variables aka features. We want to predict if the loan will be Fully Paid or Charged Off given the feature values."
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#stakeholders",
    "href": "portfolio/dsp/lendingclub/index.html#stakeholders",
    "title": "Lendingclub’s loan default prediction",
    "section": "Stakeholders",
    "text": "Stakeholders"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#key-performance-indicators-kpis-of-lendingclub",
    "href": "portfolio/dsp/lendingclub/index.html#key-performance-indicators-kpis-of-lendingclub",
    "title": "Lendingclub’s loan default prediction",
    "section": "Key Performance Indicators (KPIs) of LendingClub",
    "text": "Key Performance Indicators (KPIs) of LendingClub"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#modeling",
    "href": "portfolio/dsp/lendingclub/index.html#modeling",
    "title": "Lendingclub’s loan default prediction",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#results-and-outcome",
    "href": "portfolio/dsp/lendingclub/index.html#results-and-outcome",
    "title": "Lendingclub’s loan default prediction",
    "section": "Results and Outcome",
    "text": "Results and Outcome"
  },
  {
    "objectID": "portfolio/dsp/lendingclub/index.html#future-directions",
    "href": "portfolio/dsp/lendingclub/index.html#future-directions",
    "title": "Lendingclub’s loan default prediction",
    "section": "Future Directions",
    "text": "Future Directions"
  },
  {
    "objectID": "talks/2020-04-26-project.html",
    "href": "talks/2020-04-26-project.html",
    "title": "Sensitivity analysis for Monte Carlo and Quasi Monte Carlo option pricing",
    "section": "",
    "text": "A copy of the presentation can be found  here \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "talks/2023-11-20-candi.html",
    "href": "talks/2023-11-20-candi.html",
    "title": "The Heavy-Tail Phenomenon in Decentralized Stochastic Gradient Descent",
    "section": "",
    "text": "Stochastic Gradient Descent (SGD) method is one of the most popular optimization techniques in machine learning, particularly in Deep Neural Network (DNN). The gradient noise in this method is often modeled by Gaussian or assumed to have finite variance. However, empirical evidence suggests that the gradient noises can be highly non-Gaussian and often exhibit heavy tails in nature. This heaviness has a direct relationship to the generalization performance of the algorithm.\n\n\nIn this candidacy paper we discuss materials from three papers where we first present the tail-index analysis in SGD that shows empirically that gradient noise can have heavy tails, and through metastability analysis, the heavy-tailed SGD validates the wide minima phenomenon. We then present the paper about the heavy-tail phenomenon in SGD and investigates the origins of the heavy tails. We show that the heaviness of the tail is related to the choice of stepsize, batch-size and other hyperparameters of the algorithm. Finally, we discuss the heavy-tail phenomenon in decentralized SGD. We conclude the candidacy paper by proposing a few future research directions.\n\nA copy of the presentation can be found here\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "research.html#current-research",
    "href": "research.html#current-research",
    "title": "",
    "section": "Current Research",
    "text": "Current Research\n\n\n\n\n\n\n\n\n\n\nHigher-order Langevin Algorithms\n\n\n\nThanh L. Dang, Mert Gurbuzbalaban, Mohammad Rafiqul Islam, Nihan Yao, Lingjiong Zhu\n\n\nJul 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflected Underdamped Langevin Monte Carlo\n\n\n\nHengrong Du, Qi Feng, Mert Gurbuzbalaban, Mohammad Rafiqul Islam, Lingjiong Zhu\n\n\nMar 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized EXTRA stochastic gradient Langevin dynamics\n\n\n\nMert Gurbuzbalaban, Mohammad Rafiqul Islam, Xiaoyu Wang, Lingjiong Zhu\n\n\nFeb 12, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "",
    "section": "Publications",
    "text": "Publications\n\nGeneralized EXTRA stochastic gradient Langevin dynamics\nMert, Gurbuzbalaban; Islam, Mohammad Rafiqul; Wang, Xiaoyu; Zhu, Lingjiong (2024) “Generalized EXTRA stochastic gradient Langevin dynamics.” arXiv preprint arXiv.2412.01993\nGJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies \nMostafa, F; Saha, P; Islam, Mohammad R.; Nguyen, N. (2020) “GJR-GARCH Volatility Modeling under NIG and ANN for Predicting Top Cryptocurrencies.” Journal of Risk and Financial Management.\nComparison of Financial Models for Stock Price Prediction \nIslam, Mohammad R.; Nguyen, N. (2020) “Comparison of financial models for stock price prediction.” Journal of Risk and Financial Management."
  },
  {
    "objectID": "research.html#course-projects",
    "href": "research.html#course-projects",
    "title": "",
    "section": "Course Projects",
    "text": "Course Projects\n\nOption pricing techniques: A performance-based comparative study of the randomized quasi-Monte Carlo method and Fourier cosine method\nAdvisor: Prof. Giray Ökten\n\nPricing financial derivatives such as options with desired accuracy can be hard due to the nature of the functions and complicated integrals required by the pricing techniques. In this paper we investigate the pricing methodology of the European style options using two advanced numerical methods, namely, Quasi-Monte Carlo and Fourier Cosine (COS). For the RQMC method, we use the random-start Halton sequence. We use the Black-Scholes-Merton model to measure the pricing quality of both of the methods. For the numerical results we compute the option price of the call option and we found a few reasons to prefer the RQMC method over the COS method to approximate the European style options.\n\nThe Relationship Between Forced Sexual Activities And Suicidal Attempts Of The Victims\nAdvisor: Dr. Andy Chang\n\nIn project, we apply data-analytic methods to further explore the relationship between forced sexual activities and suicidal behavior among adolescents in the United States. Our findings build on existing literature that explores this relationship. The sample of the study was taken from the Youth Risk Behavior Surveillance System survey 2017. We used a chi-squared test to find the association of forced sexual activities and suicidal behavior, and we found a strong association. Then we used bi-variate logistic regression analysis to ascertain the association of race, age, sex, and education with suicidal attempts after experiencing forced sexual activity (sexual assault). The results of the following paper provide greater insight into the relationship between forced sexual activities and suicide attempts by the adolescents.\n\nStudy of Runge-Kutta Method of Higher orders and its Applications\nAdvisor: Dr. Md. Abdus Samad \n\nThis project is concerned with the study on Runge-Kutta method to apply on different order of differential equation and solve different types of problem such as initial value problem and boundary value problem in ordinary differential equation. At first we discuss about the definition and generation of differential equation specially based on partial differential equation and then definition of Runge-kutta method and the derivation of midpoint method and the formula of Runge-Kutta metod of fourth order and sixth order. We also write FORTRAN 90/95 program for different order of Runge-Kutta methods. We have solved some examples of fourth order R-K method and sixth order R-K method to get the application of R-K method. We also compared the solution of R-K method with exact solution for different step sizes. Then we have given simultaneous first order differential equation and second order differential equation and then solved them by fourth order Runge-Kutta method. At last we have discussed the boundary value problem which we have solved by fourth and sixth order R-K method. After that we have written the algorithm of shooting method and showed computer results with the difference between two answer along with percentages of error."
  },
  {
    "objectID": "research.html#talks-and-presentations",
    "href": "research.html#talks-and-presentations",
    "title": "",
    "section": "Talks and Presentations",
    "text": "Talks and Presentations"
  },
  {
    "objectID": "publication/pub1/index.html",
    "href": "publication/pub1/index.html",
    "title": "Comparison of financial models for stock price prediction",
    "section": "",
    "text": "Time series analysis of daily stock data and building predictive models are complicated. This project presents a comparative study for stock price prediction using three different methods, namely autoregressive integrated moving average, artificial neural network, and stochastic process-geometric Brownian motion. Each of the methods is used to build predictive models using historical stock data collected from Yahoo Finance. Finally, output from each of the models is compared to the actual stock price. Empirical results show that the conventional statistical model and the stochastic model provide better approximation for next-day stock price prediction compared to the neural network model.\n\n\n    \n        \n    \n    \n        \n    \n\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to topCitationBibTeX citation:@article{rafiqul_islam2020,\n  author = {Rafiqul Islam, Mohammad and Nguyen, Nguyet},\n  publisher = {MDPI},\n  title = {Comparison of Financial Models for Stock Price Prediction},\n  journal = {Journal of Risk and Financial Management},\n  date = {2020-08-14},\n  url = {https://www.mdpi.com/1911-8074/13/8/181},\n  doi = {10.3390/jrfm13080181},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRafiqul Islam, Mohammad, and Nguyet Nguyen. 2020. “Comparison of\nFinancial Models for Stock Price Prediction.” Journal of Risk\nand Financial Management, August. https://doi.org/10.3390/jrfm13080181."
  }
]